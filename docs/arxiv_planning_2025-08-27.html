<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-08-27</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-08-27</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19220v1' target='_blank'>The 2025 Roadmaps for the US Magnet Development Program</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lance Cooley, Paolo Ferracin, Steve Gourlay, David Larbalestier, Mark Palmer, Soren Prestemon, George Velev, Giorgio Ambrosio, Diego Arbelaez, Karie Badgley, Lucas Brouwer, Daniel Davis, Jose Luis Fernandez, Vadim Kashikhin, Steven Krave, Maxim Marchevsky, Igor Novitski, Ian Pong, Tengming Shen, Stoyan Stoynev, Reed Teyber, Giorgio Vallone, Xiaorong Wang, Xingchen Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 17:36:52</h6>
<p class='card-text'>The US Physics community completed the Snowmass planning process in 2022,
culminating in the HEPAP Particle Physics Project Prioritization Panel (P5)
publishing its summary report at the end of 2023. Building on this, the US
Magnet Development Program, a national accelerator magnet R&D program
established by DOE-OHEP in 2016, has updated its strategic plan to align with
the 2023 P5 report, resulting in this roadmap document.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19199v1' target='_blank'>Planning-Query-Guided Model Generation for Model-Based Deformable Object
  Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alex LaGrassa, Zixuan Huang, Dmitry Berenson, Oliver Kroemer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 17:03:39</h6>
<p class='card-text'>Efficient planning in high-dimensional spaces, such as those involving
deformable objects, requires computationally tractable yet sufficiently
expressive dynamics models. This paper introduces a method that automatically
generates task-specific, spatially adaptive dynamics models by learning which
regions of the object require high-resolution modeling to achieve good task
performance for a given planning query. Task performance depends on the complex
interplay between the dynamics model, world dynamics, control, and task
requirements. Our proposed diffusion-based model generator predicts per-region
model resolutions based on start and goal pointclouds that define the planning
query. To efficiently collect the data for learning this mapping, a two-stage
process optimizes resolution using predictive dynamics as a prior before
directly optimizing using closed-loop performance. On a tree-manipulation task,
our method doubles planning speed with only a small decrease in task
performance over using a full-resolution model. This approach informs a path
towards using previous planning and control data to generate computationally
efficient yet sufficiently expressive dynamics models for new tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19186v1' target='_blank'>Real-Time Model Checking for Closed-Loop Robot Reactive Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Christopher Chandler, Bernd Porr, Giulia Lafratta, Alice Miller</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 16:49:30</h6>
<p class='card-text'>We present a new application of model checking which achieves real-time
multi-step planning and obstacle avoidance on a real autonomous robot. We have
developed a small, purpose-built model checking algorithm which generates plans
in situ based on "core" knowledge and attention as found in biological agents.
This is achieved in real-time using no pre-computed data on a low-powered
device. Our approach is based on chaining temporary control systems which are
spawned to counteract disturbances in the local environment that disrupt an
autonomous agent from its preferred action (or resting state). A novel
discretization of 2D LiDAR data sensitive to bounded variations in the local
environment is used. Multi-step planning using model checking by forward
depth-first search is applied to cul-de-sac and playground scenarios. Both
empirical results and informal proofs of two fundamental properties of our
approach demonstrate that model checking can be used to create efficient
multi-step plans for local obstacle avoidance, improving on the performance of
a reactive agent which can only plan one step. Our approach is an instructional
case study for the development of safe, reliable and explainable planning in
the context of autonomous vehicles.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19175v1' target='_blank'>Development and Potential of new Micro-Mirror Devices Optimized for
  Astronomy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Massimo Robberto, Cuiling Gong, Jim Huffman, Zoran Ninkov, Ivan Puchades, Mario Gennaro, Susan A. Kassin, Steven A. Smee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 16:27:16</h6>
<p class='card-text'>We introduce our new program to develop two-dimensional MEMS arrays of
individually addressable micro-mirrors (''Micro-Mirror Devices'', MMDs)
specifically optimized for astronomy, multi-slit spectroscopy in particular.
After reviewing the main characteristics and performance of the currently
available options, Micro Shutter Arrays by NASA/Goddard and Digital Micromirror
Devices by Texas Instruments, we present our planned first generation/baseline
devices with 30 micron x 30 miron pixel size arranged in a 1K x 1K format with
tilt angle 15 degrees. Our goal is to bring to maturity a technology capable of
delivering arrays of 2K x 2K element of 100 micron x 100 micron, buttable on
two sides to achieve even larger formats. In additions to MEMS design, we will
develop the associated device packaging and electronic control circuitry
leveraging on the extensive expertise gained in the last 30+ years by leading
experts from digital imaging industry.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19168v1' target='_blank'>Direction Informed Trees (DIT*): Optimal Path Planning via Direction
  Filter and Direction Cost Heuristic</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Liding Zhang, Kejia Chen, Kuanqi Cai, Yu Zhang, Yixuan Dang, Yansong Wu, Zhenshan Bing, Fan Wu, Sami Haddadin, Alois Knoll</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 16:16:18</h6>
<p class='card-text'>Optimal path planning requires finding a series of feasible states from the
starting point to the goal to optimize objectives. Popular path planning
algorithms, such as Effort Informed Trees (EIT*), employ effort heuristics to
guide the search. Effective heuristics are accurate and computationally
efficient, but achieving both can be challenging due to their conflicting
nature. This paper proposes Direction Informed Trees (DIT*), a sampling-based
planner that focuses on optimizing the search direction for each edge,
resulting in goal bias during exploration. We define edges as generalized
vectors and integrate similarity indexes to establish a directional filter that
selects the nearest neighbors and estimates direction costs. The estimated
direction cost heuristics are utilized in edge evaluation. This strategy allows
the exploration to share directional information efficiently. DIT* convergence
faster than existing single-query, sampling-based planners on tested problems
in R^4 to R^16 and has been demonstrated in real-world environments with
various planning tasks. A video showcasing our experimental results is
available at: https://youtu.be/2SX6QT2NOek</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19150v1' target='_blank'>Uncertainty-Resilient Active Intention Recognition for Robotic
  Assistants</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Juan Carlos Saborío, Marc Vinci, Oscar Lima, Sebastian Stock, Lennart Niecksch, Martin Günther, Alexander Sung, Joachim Hertzberg, Martin Atzmüller</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 16:00:38</h6>
<p class='card-text'>Purposeful behavior in robotic assistants requires the integration of
multiple components and technological advances. Often, the problem is reduced
to recognizing explicit prompts, which limits autonomy, or is oversimplified
through assumptions such as near-perfect information. We argue that a critical
gap remains unaddressed -- specifically, the challenge of reasoning about the
uncertain outcomes and perception errors inherent to human intention
recognition. In response, we present a framework designed to be resilient to
uncertainty and sensor noise, integrating real-time sensor data with a
combination of planners. Centered around an intention-recognition POMDP, our
approach addresses cooperative planning and acting under uncertainty. Our
integrated framework has been successfully tested on a physical robot with
promising results.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19126v1' target='_blank'>Probing the HI distribution at small scales using 21-cm Intensity
  Mapping at large scales</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Minal Chhabra, Somnath Bharadwaj</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 15:24:57</h6>
<p class='card-text'>Neutral hydrogen (HI) 21-cm Intensity Mapping (IM) holds the potential to map
the large-scale structures in the Universe over a wide redshift range $(z
\lesssim 5.5)$, measure cosmological parameters, and shed light on the nature
of dark energy. In addition, the signal is also sensitive to how the HI is
distributed among the dark matter haloes, this being quantified through the
HIHM relation, which relates the HI mass to the halo mass. In this work, we
investigate whether measurements of the 21-cm power spectrum (PS) and
bispectrum (BS) at large scales can be used to estimate the HIHM relation,
which quantifies the HI distribution at small scales. As a proof of concept, we
consider the simulated 21-cm IM signal at $z=1$. We find that the measured
21-cm PS and BS at large scales $(k \le k_{ul} = 0.32 \, {\rm Mpc}^{-1})$ are
well modeled using perturbation theory, with only two free parameters namely
$[\Omega_{\rm HI} b_1]$ and $\gamma = b_2/b_1$. Combining the measured 21-cm PS
and BS with an independent measurement of $\Omega_{\rm HI} $, we show that it
is possible to estimate the three parameters that quantify the HIHM relation.
We expect observational estimates of the HIHM relation to shed light on galaxy
formation and the evolution of the ISM. Our preliminary analysis ignores
redshift space distortion and the system noise in IM observations, which we
plan to address in future work.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19119v1' target='_blank'>Enhancing Kinematics Understanding through a Video Game Based on
  Real-Time Motion Graphs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mateo Dutra, Marcos Abreu, Martín Monteiro, Silvia Sguilla, Cecilia Stari, Álvaro Suárez, Arturo C. Marti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 15:20:38</h6>
<p class='card-text'>Interpreting kinematic graphs remains a significant challenge in physics
education. The MissionMotion Project addresses this issue by providing a
gamified physical-computational environment combining low-cost sensors,
physical activity, computational thinking, and real-time visualization of
motion graphs. This paper presents the design, development, and implementation
of the project, with a particular focus on the pilot phase conducted with high
school students in Uruguay. During this phase, we primarily used the MEEGA+
questionnaire to evaluate the gaming experience, usability, and motivation of
the participants. Our analysis of the results shows high levels of
satisfaction, perceived learning, and engagement, supporting the proposal's
viability. Finally, we plan to conduct a large-scale conceptual evaluation to
analyze how the proposal impacts understanding of kinematic graphs using
standardized assessment tools.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19112v1' target='_blank'>Random forest-based out-of-distribution detection for robust lung cancer
  segmentation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aneesh Rangnekar, Harini Veeraraghavan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 15:14:29</h6>
<p class='card-text'>Accurate detection and segmentation of cancerous lesions from computed
tomography (CT) scans is essential for automated treatment planning and cancer
treatment response assessment. Transformer-based models with self-supervised
pretraining can produce reliably accurate segmentation from in-distribution
(ID) data but degrade when applied to out-of-distribution (OOD) datasets. We
address this challenge with RF-Deep, a random forest classifier that utilizes
deep features from a pretrained transformer encoder of the segmentation model
to detect OOD scans and enhance segmentation reliability. The segmentation
model comprises a Swin Transformer encoder, pretrained with masked image
modeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous and
non-cancerous conditions, with a convolution decoder, trained to segment lung
cancers in 317 3D scans. Independent testing was performed on 603 3D CT public
datasets that included one ID dataset and four OOD datasets comprising chest
CTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidney
cancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of
18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs,
consistently outperforming established OOD approaches. The RF-Deep classifier
provides a simple and effective approach to enhance reliability of cancer
segmentation in ID and OOD scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19077v1' target='_blank'>"Where does it hurt?" -- Dataset and Study on Physician Intent
  Trajectories in Doctor Patient Dialogues</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tom Röhr, Soumyadeep Roy, Fares Al Mohamad, Jens-Michalis Papaioannou, Wolfgang Nejdl, Felix Gers, Alexander Löser</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 14:38:17</h6>
<p class='card-text'>In a doctor-patient dialogue, the primary objective of physicians is to
diagnose patients and propose a treatment plan. Medical doctors guide these
conversations through targeted questioning to efficiently gather the
information required to provide the best possible outcomes for patients. To the
best of our knowledge, this is the first work that studies physician intent
trajectories in doctor-patient dialogues. We use the `Ambient Clinical
Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with
medical professionals to develop a fine-grained taxonomy of physician intents
based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We
then conduct a large-scale annotation effort to label over 5000 doctor-patient
turns with the help of a large number of medical experts recruited using
Prolific, a popular crowd-sourcing platform. This large labeled dataset is an
important resource contribution that we use for benchmarking the
state-of-the-art generative and encoder models for medical intent
classification tasks. Our findings show that our models understand the general
structure of medical dialogues with high accuracy, but often fail to identify
transitions between SOAP categories. We also report for the first time common
trajectories in medical dialogue structures that provide valuable insights for
designing `differential diagnosis' systems. Finally, we extensively study the
impact of intent filtering for medical dialogue summarization and observe a
significant boost in performance. We make the codes and data, including
annotation guidelines, publicly available at
https://github.com/DATEXIS/medical-intent-classification.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19054v1' target='_blank'>An optimistic planning algorithm for switched discrete-time LQR</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mathieu Granzotto, Romain Postoyan, Dragan Nešić, Jamal Daafouz, Lucian Buşoniu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 14:10:51</h6>
<p class='card-text'>We introduce TROOP, a tree-based Riccati optimistic online planner, that is
designed to generate near-optimal control laws for discrete-time switched
linear systems with switched quadratic costs. The key challenge that we address
is balancing computational resources against control performance, which is
important as constructing near-optimal inputs often requires substantial amount
of computations. TROOP addresses this trade-off by adopting an online
best-first search strategy inspired by A*, allowing for efficient estimates of
the optimal value function. The control laws obtained guarantee both
near-optimality and stability properties for the closed-loop system. These
properties depend on the planning depth, which determines how far into the
future the algorithm explores and is closely related to the amount of
computations. TROOP thus strikes a balance between computational efficiency and
control performance, which is illustrated by numerical simulations on an
example.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19045v1' target='_blank'>A Quick Estimation of Fréchet Quantizers for a Dynamic Solution to
  Flood Risk Management Problems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anna Timonina-Farkas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 13:59:27</h6>
<p class='card-text'>Multi-stage stochastic optimization is a well-known quantitative tool for
decision-making under uncertainty. It is broadly used in financial and
investment planning, inventory control, and also natural disaster risk
management. Theoretical solutions of multi-stage stochastic programs can be
found explicitly only in very exceptional cases due to their variational form
and interdependency of uncertainty in time. Nevertheless, numerical solutions
are often inaccurate, as they rely on Monte-Carlo sampling, which requires the
Law of Large Numbers to hold for the approximation quality. In this article, we
introduce a new approximation scheme, which computes and groups together
stage-wise optimal quantizers of conditional Fr\'echet distributions for
optimal weighting of value functions in the dynamic programming. We consider
optimality of scenario quantization methods in the sense of minimal
Kantorovich-Wasserstein distance at each stage of the scenario tree. By this,
we bound the approximation error with convergence guarantees. We also provide
global solution guarantees under convexity and monotonicity conditions on the
value function. We apply the developed methods to the governmental budget
allocation problem for risk management of flood events in Austria. For this, we
propose an extremely efficient way to approximate optimal quantizers for
conditional Fr\'echet distributions. Our approach allows to enhance the overall
efficiency of dynamic programming via the use of different parameter estimation
methods for different groups of quantizers. The groups are distinguished by a
particular risk threshold and are able to differentiate between higher- and
lower-impact flood events.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19031v1' target='_blank'>Breaking the Black Box: Inherently Interpretable Physics-Informed
  Machine Learning for Imbalanced Seismic Data</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vemula Sreenath, Filippo Gatti, Pierre Jehel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 13:48:09</h6>
<p class='card-text'>Ground motion models (GMMs) predict how strongly the ground will shake during
an earthquake. They are essential for structural analysis, seismic design, and
seismic risk assessment studies. Traditional machine learning (ML) approaches
are popular to develop GMMs, due to large earthquake databases worldwide.
However, they operate as "black boxes," which are hard to interpret and trust,
limiting their use in high-stake decisions. Additionally, these databases
suffer from significant data imbalances: fewer large, critically damaging
records near the fault compared to abundant, less severely damaging distant
records. These two limitations are addressed in this work by developing a
transparent ML architecture using the HazBinLoss function. Each input (e.g.,
magnitude, distance, their interaction term, etc.) is processed separately and
added linearly to obtain the output, resulting in exact contribution of each
term. The HazBinLoss function assigns higher weights to critical near-field
large magnitude records and lower weights to less-critical far-field smaller
magnitude records, during training to prevent underprediction of the most
damaging scenarios. Our model captures known seismological principles and
achieves comparable performance with established GMMs while maintaining
transparency. This framework enables broader adoption of ML-based approaches
for risk assessment studies and disaster planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.19016v1' target='_blank'>Working My Way Back to You: Resource-Centric Next-Activity Prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kelly Kurowski, Xixi Lu, Hajo A Reijers</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 13:27:09</h6>
<p class='card-text'>Predictive Process Monitoring (PPM) aims to train models that forecast
upcoming events in process executions. These predictions support early
bottleneck detection, improved scheduling, proactive interventions, and timely
communication with stakeholders. While existing research adopts a control-flow
perspective, we investigate next-activity prediction from a resource-centric
viewpoint, which offers additional benefits such as improved work organization,
workload balancing, and capacity forecasting. Although resource information has
been shown to enhance tasks such as process performance analysis, its role in
next-activity prediction remains unexplored. In this study, we evaluate four
prediction models and three encoding strategies across four real-life datasets.
Compared to the baseline, our results show that LightGBM and Transformer models
perform best with an encoding based on 2-gram activity transitions, while
Random Forest benefits most from an encoding that combines 2-gram transitions
and activity repetition features. This combined encoding also achieves the
highest average accuracy. This resource-centric approach could enable smarter
resource allocation, strategic workforce planning, and personalized employee
support by analyzing individual behavior rather than case-level progression.
The findings underscore the potential of resource-centric next-activity
prediction, opening up new venues for research on PPM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18967v1' target='_blank'>Enhanced UAV Path Planning Using the Tangent Intersection Guidance (TIG)
  Algorithm</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hichem Cheriet, Khellat Kihel Badra, Chouraqui Samira</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 12:11:59</h6>
<p class='card-text'>Efficient and safe navigation of Unmanned Aerial Vehicles (UAVs) is critical
for various applications, including combat support, package delivery and Search
and Rescue Operations. This paper introduces the Tangent Intersection Guidance
(TIG) algorithm, an advanced approach for UAV path planning in both static and
dynamic environments. The algorithm uses the elliptic tangent intersection
method to generate feasible paths. It generates two sub-paths for each threat,
selects the optimal route based on a heuristic rule, and iteratively refines
the path until the target is reached. Considering the UAV kinematic and dynamic
constraints, a modified smoothing technique based on quadratic B\'ezier curves
is adopted to generate a smooth and efficient route. Experimental results show
that the TIG algorithm can generate the shortest path in less time, starting
from 0.01 seconds, with fewer turning angles compared to A*, PRM, RRT*, Tangent
Graph, and Static APPATT algorithms in static environments. Furthermore, in
completely unknown and partially known environments, TIG demonstrates efficient
real-time path planning capabilities for collision avoidance, outperforming APF
and Dynamic APPATT algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18781v1' target='_blank'>AniME: Adaptive Multi-Agent Planning for Long Animation Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lisai Zhang, Baohan Xu, Siqian Yang, Mingyu Yin, Jing Liu, Chao Xu, Siqi Wang, Yidi Wu, Yuxin Hong, Zihao Zhang, Yanzhang Liang, Yudong Jiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 08:06:10</h6>
<p class='card-text'>We present AniME, a director-oriented multi-agent system for automated
long-form anime production, covering the full workflow from a story to the
final video. The director agent keeps a global memory for the whole workflow,
and coordinates several downstream specialized agents. By integrating
customized Model Context Protocol (MCP) with downstream model instruction, the
specialized agent adaptively selects control conditions for diverse sub-tasks.
AniME produces cinematic animation with consistent characters and synchronized
audio visual elements, offering a scalable solution for AI-driven anime
creation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18627v1' target='_blank'>Integration of Robot and Scene Kinematics for Sequential Mobile
  Manipulation Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyuan Jiao, Yida Niu, Zeyu Zhang, Yangyang Wu, Yao Su, Yixin Zhu, Hangxin Liu, Song-Chun Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 03:08:54</h6>
<p class='card-text'>We present a Sequential Mobile Manipulation Planning (SMMP) framework that
can solve long-horizon multi-step mobile manipulation tasks with coordinated
whole-body motion, even when interacting with articulated objects. By
abstracting environmental structures as kinematic models and integrating them
with the robot's kinematics, we construct an Augmented Configuration Apace
(A-Space) that unifies the previously separate task constraints for navigation
and manipulation, while accounting for the joint reachability of the robot
base, arm, and manipulated objects. This integration facilitates efficient
planning within a tri-level framework: a task planner generates symbolic action
sequences to model the evolution of A-Space, an optimization-based motion
planner computes continuous trajectories within A-Space to achieve desired
configurations for both the robot and scene elements, and an intermediate plan
refinement stage selects action goals that ensure long-horizon feasibility. Our
simulation studies first confirm that planning in A-Space achieves an 84.6\%
higher task success rate compared to baseline methods. Validation on real
robotic systems demonstrates fluid mobile manipulation involving (i) seven
types of rigid and articulated objects across 17 distinct contexts, and (ii)
long-horizon tasks of up to 14 sequential steps. Our results highlight the
significance of modeling scene kinematics into planning entities, rather than
encoding task-specific constraints, offering a scalable and generalizable
approach to complex robotic manipulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18606v1' target='_blank'>SignLoc: Robust Localization using Navigation Signs and Public Maps</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nicky Zimmerman, Joel Loo, Ayush Agrawal, David Hsu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 02:24:04</h6>
<p class='card-text'>Navigation signs and maps, such as floor plans and street maps, are widely
available and serve as ubiquitous aids for way-finding in human environments.
Yet, they are rarely used by robot systems. This paper presents SignLoc, a
global localization method that leverages navigation signs to localize the
robot on publicly available maps -- specifically floor plans and OpenStreetMap
(OSM) graphs -- without prior sensor-based mapping. SignLoc first extracts a
navigation graph from the input map. It then employs a probabilistic
observation model to match directional and locational cues from the detected
signs to the graph, enabling robust topo-semantic localization within a Monte
Carlo framework. We evaluated SignLoc in diverse large-scale environments: part
of a university campus, a shopping mall, and a hospital complex. Experimental
results show that SignLoc reliably localizes the robot after observing only one
to two signs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18573v1' target='_blank'>Results of the LEGEND-200 experiment in the search for neutrinoless
  double beta decay</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Carmen Romo-Luque</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-26 00:13:14</h6>
<p class='card-text'>The LEGEND experiment is looking for the extremely rare neutrinoless double
beta ($0\nu\beta\beta$) decay of $^{76}$Ge using isotopically-enriched
high-purity germanium (HPGe) detectors. The detection of this process would
imply that the neutrino is a Majorana particle and the total lepton number
would not be conserved, which could be related to the cosmological asymmetry
between matter and antimatter through leptogenesis. The long-term goal of the
collaboration is LEGEND-1000: a 1-ton detector array planned to run for 10
years, with a projected half-life sensitivity exceeding $10^{28}$ years, fully
covering the inverted neutrino mass hierarchy. A first search for the
$0\nu\beta\beta$ decay has been carried out by LEGEND-200 building on the
experience gained from GERDA and the MAJORANA DEMONSTRATOR. The experiment has
been collecting physics data for a year at the Gran Sasso National Laboratory
in Italy with 140 kg of HPGe detectors. With a total exposure of 61 kg yr,
LEGEND-200 has achieved a background index of $5^{+3}_{-2}\times10^{-4}$
counts/(keV kg yr) in the $0\nu\beta\beta$ decay signal region from the highest
performing detectors. After combining the results from GERDA, the MAJORANA
Demonstrator and LEGEND-200, an exclusion sensitivity $ > 2.8\times10^{26}$ yr
has been obtained at 90\% confidence level for the $0\nu\beta\beta$ decay
half-life, with no evidence for a signal. A new observed lower limit of
$T^{0\nu}_{1/2} > 1.9\times10^{26}$ yr at 90\% confidence level has been
established.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18520v1' target='_blank'>Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman
  Features</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dillon Z. Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-25 21:46:19</h6>
<p class='card-text'>Novelty heuristics aid heuristic search by exploring states that exhibit
novel atoms. However, novelty heuristics are not symmetry invariant and hence
may sometimes lead to redundant exploration. In this preliminary report, we
propose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms
for detecting novelty. WLFs are recently introduced features for learning
domain-dependent heuristics for generalised planning problems. We explore an
unsupervised usage of WLFs for synthesising lifted, domain-independent novelty
heuristics that are invariant to symmetric states. Experiments on the classical
International Planning Competition and Hard To Ground benchmark suites yield
promising results for novelty heuristics synthesised from WLFs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18515v1' target='_blank'>Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size
  Hyperparameter Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dillon Z. Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-25 21:39:03</h6>
<p class='card-text'>Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine
learning tool for learning to plan and search. They have been shown to be both
theoretically and empirically superior to existing deep learning approaches for
learning value functions for search in symbolic planning. In this paper, we
introduce new WLF hyperparameters and study their various tradeoffs and
effects. We utilise the efficiency of WLFs and run planning experiments on
single core CPUs with a sample size of 1,000,000 to understand the effect of
hyperparameters on training and planning. Our experimental analysis show that
there is a robust and best set of hyperparameters for WLFs across the tested
planning domains. We find that the best WLF hyperparameters for learning
heuristic functions minimise execution time rather than maximise model
expressivity. We further statistically analyse and observe no significant
correlation between training and planning metrics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18507v1' target='_blank'>Language Models For Generalised PDDL Planning: Synthesising Sound and
  Programmatic Policies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dillon Z. Chen, Johannes Zenn, Tristan Cinquin, Sheila A. McIlraith</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-25 21:28:14</h6>
<p class='card-text'>We study the usage of language models (LMs) for planning over world models
specified in the Planning Domain Definition Language (PDDL). We prompt LMs to
generate Python programs that serve as generalised policies for solving PDDL
problems from a given domain. Notably, our approach synthesises policies that
are provably sound relative to the PDDL domain without reliance on external
verifiers. We conduct experiments on competition benchmarks which show that our
policies can solve more PDDL problems than PDDL planners and recent LM
approaches within a fixed time and memory constraint. Our approach manifests in
the LMPlan planner which can solve planning problems with several hundreds of
relevant objects. Surprisingly, we observe that LMs used in our framework
sometimes plan more effectively over PDDL problems written in meaningless
symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1
o3). This finding challenges hypotheses that LMs reason over word semantics and
memorise solutions from its training corpus, and is worth further exploration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18497v1' target='_blank'>Can Classical Initialization Help Variational Quantum Circuits Escape
  the Barren Plateau?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yifeng Peng, Xinyi Li, Zhemin Zhang, Samuel Yen-Chi Chen, Zhiding Liang, Ying Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-25 21:12:15</h6>
<p class='card-text'>Variational quantum algorithms (VQAs) have emerged as a leading paradigm in
near-term quantum computing, yet their performance can be hindered by the
so-called barren plateau problem, where gradients vanish exponentially with
system size or circuit depth. While most existing VQA research employs simple
Gaussian or zero-initialization schemes, classical deep learning has long
benefited from sophisticated weight initialization strategies such as Xavier,
He, and orthogonal initialization to improve gradient flow and expedite
convergence. In this work, we systematically investigate whether these
classical methods can mitigate barren plateaus in quantum circuits. We first
review each initialization's theoretical grounding and outline how to adapt the
notions from neural networks to VQAs. We then conduct extensive numerical
experiments on various circuit architectures and optimization tasks. Our
findings indicate that while the initial heuristics, inspired by classical
initialization, yield moderate improvements in certain experiments, their
overall benefits remain marginal. By outlining a preliminary exploration plan
in this paper, we aim to offer the research community a broader perspective and
accessible demonstrations. Furthermore, we propose future research directions
that may be further refined by leveraging the insights gained from this work.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18415v1' target='_blank'>Securing Face and Fingerprint Templates in Humanitarian Biometric
  Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Giuseppe Stragapede, Sam Merrick, Vedrana Krivokuća Hahn, Justin Sukaitis, Vincent Graf Narbel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-25 19:03:33</h6>
<p class='card-text'>In humanitarian and emergency scenarios, the use of biometrics can
dramatically improve the efficiency of operations, but it poses risks for the
data subjects, which are exacerbated in contexts of vulnerability. To address
this, we present a mobile biometric system implementing a biometric template
protection (BTP) scheme suitable for these scenarios. After rigorously
formulating the functional, operational, and security and privacy requirements
of these contexts, we perform a broad comparative analysis of the BTP
landscape. PolyProtect, a method designed to operate on neural network face
embeddings, is identified as the most suitable method due to its effectiveness,
modularity, and lightweight computational burden. We evaluate PolyProtect in
terms of verification and identification accuracy, irreversibility, and
unlinkability, when this BTP method is applied to face embeddings extracted
using EdgeFace, a novel state-of-the-art efficient feature extractor, on a
real-world face dataset from a humanitarian field project in Ethiopia.
Moreover, as PolyProtect promises to be modality-independent, we extend its
evaluation to fingerprints. To the best of our knowledge, this is the first
time that PolyProtect has been evaluated for the identification scenario and
for fingerprint biometrics. Our experimental results are promising, and we plan
to release our code</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18400v1' target='_blank'>Efficient task and path planning for maintenance automation using a
  robot system</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Christian Friedrich, Akos Csiszar, Armin Lechler, Alexander Verl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-25 18:40:27</h6>
<p class='card-text'>The research and development of intelligent automation solutions is a
ground-breaking point for the factory of the future. A promising and
challenging mission is the use of autonomous robot systems to automate tasks in
the field of maintenance. For this purpose, the robot system must be able to
plan autonomously the different manipulation tasks and the corresponding paths.
Basic requirements are the development of algorithms with a low computational
complexity and the possibility to deal with environmental uncertainties. In
this work, an approach is presented, which is especially suited to solve the
problem of maintenance automation. For this purpose, offline data from CAD is
combined with online data from an RGBD vision system via a probabilistic
filter, to compensate uncertainties from offline data. For planning the
different tasks, a method is explained, which use a symbolic description,
founded on a novel sampling-based method to compute the disassembly space. For
path planning we use global state-of-the art algorithms with a method that
allows the adaption of the exploration stepsize in order to reduce the planning
time. Every method is experimentally validated and discussed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18399v1' target='_blank'>Maintenance automation: methods for robotics manipulation planning and
  execution</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Christian Friedrich, Ralf Gulde, Armin Lechler, Alexander Verl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-25 18:38:38</h6>
<p class='card-text'>Automating complex tasks using robotic systems requires skills for planning,
control and execution. This paper proposes a complete robotic system for
maintenance automation, which can automate disassembly and assembly operations
under environmental uncertainties (e.g. deviations between prior plan
information). The cognition of the robotic system is based on a planning
approach (using CAD and RGBD data) and includes a method to interpret a
symbolic plan and transform it to a set of executable robot instructions. The
complete system is experimentally evaluated using real-world applications. This
work shows the first step to transfer these theoretical results into a
practical robotic solution.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18397v1' target='_blank'>Mining the Long Tail: A Comparative Study of Data-Centric Criticality
  Metrics for Robust Offline Reinforcement Learning in Autonomous Motion
  Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Antonio Guillen-Perez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-25 18:37:29</h6>
<p class='card-text'>Offline Reinforcement Learning (RL) presents a promising paradigm for
training autonomous vehicle (AV) planning policies from large-scale, real-world
driving logs. However, the extreme data imbalance in these logs, where mundane
scenarios vastly outnumber rare "long-tail" events, leads to brittle and unsafe
policies when using standard uniform data sampling. In this work, we address
this challenge through a systematic, large-scale comparative study of data
curation strategies designed to focus the learning process on information-rich
samples. We investigate six distinct criticality weighting schemes which are
categorized into three families: heuristic-based, uncertainty-based, and
behavior-based. These are evaluated at two temporal scales, the individual
timestep and the complete scenario. We train seven goal-conditioned
Conservative Q-Learning (CQL) agents with a state-of-the-art, attention-based
architecture and evaluate them in the high-fidelity Waymax simulator. Our
results demonstrate that all data curation methods significantly outperform the
baseline. Notably, data-driven curation using model uncertainty as a signal
achieves the most significant safety improvements, reducing the collision rate
by nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear
trade-off where timestep-level weighting excels at reactive safety while
scenario-level weighting improves long-horizon planning. Our work provides a
comprehensive framework for data curation in Offline RL and underscores that
intelligent, non-uniform sampling is a critical component for building safe and
reliable autonomous agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18269v2' target='_blank'>FlowVLA: Thinking in Motion with a Visual Chain of Thought</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Wenxuan Song, Jiayi Chen, Haoang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-25 17:59:21</h6>
<p class='card-text'>Many Vision-Language-Action (VLA) models are built upon an internal world
model trained via direct next-frame prediction ($v_t \rightarrow v_{t+1}$).
This paradigm, however, presents a fundamental challenge: it \textbf{conflates}
the task of predicting physical motion with that of rendering static
appearance, forcing a single mechanism to handle both. This inherent coupling
often leads to physically implausible forecasts and inefficient policy
learning. To address this limitation, we introduce the \textbf{Visual Chain of
Thought (Visual CoT)}, a framework that disentangles these processes by
compelling the model to first reason about \textbf{motion dynamics} before
generating the future frame's \textbf{visual appearance}. We instantiate this
principle by proposing \textbf{FlowVLA}, an autoregressive Transformer that
explicitly materializes this reasoning process as ``$v_t \rightarrow f_t
\rightarrow v_{t+1}$'', where $f_t$ is an intermediate optical flow prediction.
By forcing the model to first commit to a motion plan ($f_t$), FlowVLA learns
disentangled dynamics, resulting in more coherent visual predictions and
significantly more efficient policy learning. Experiments on challenging
robotics manipulation benchmarks demonstrate that FlowVLA achieves
state-of-the-art performance with substantially improved sample efficiency,
pointing toward a more principled foundation for world modeling in VLAs.
Project page: https://irpn-lab.github.io/FlowVLA/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18202v1' target='_blank'>Uncertain data assimilation for urban wind flow simulations with
  OpenLB-UQ</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingliang Zhong, Dennis Teutscher, Adrian Kummerländer, Mathias J. Krause, Martin Frank, Stephan Simonis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-25 17:01:36</h6>
<p class='card-text'>Accurate prediction of urban wind flow is essential for urban planning,
pedestrian safety, and environmental management. Yet, it remains challenging
due to uncertain boundary conditions and the high cost of conventional CFD
simulations. This paper presents the use of the modular and efficient
uncertainty quantification (UQ) framework OpenLB-UQ for urban wind flow
simulations. We specifically use the lattice Boltzmann method (LBM) coupled
with a stochastic collocation (SC) approach based on generalized polynomial
chaos (gPC). The framework introduces a relative-error noise model for inflow
wind speeds based on real measurements. The model is propagated through a
non-intrusive SC LBM pipeline using sparse-grid quadrature. Key quantities of
interest, including mean flow fields, standard deviations, and vertical
profiles with confidence intervals, are efficiently computed without altering
the underlying deterministic solver. We demonstrate this on a real urban
scenario, highlighting how uncertainty localizes in complex flow regions such
as wakes and shear layers. The results show that the SC LBM approach provides
accurate, uncertainty-aware predictions with significant computational
efficiency, making OpenLB-UQ a practical tool for real-time urban wind
analysis.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.18153v1' target='_blank'>DANCeRS: A Distributed Algorithm for Negotiating Consensus in Robot
  Swarms with Gaussian Belief Propagation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aalok Patwardhan, Andrew J. Davison</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-25 15:58:19</h6>
<p class='card-text'>Robot swarms require cohesive collective behaviour to address diverse
challenges, including shape formation and decision-making. Existing approaches
often treat consensus in discrete and continuous decision spaces as distinct
problems. We present DANCeRS, a unified, distributed algorithm leveraging
Gaussian Belief Propagation (GBP) to achieve consensus in both domains. By
representing a swarm as a factor graph our method ensures scalability and
robustness in dynamic environments, relying on purely peer-to-peer message
passing. We demonstrate the effectiveness of our general framework through two
applications where agents in a swarm must achieve consensus on global behaviour
whilst relying on local communication. In the first, robots must perform path
planning and collision avoidance to create shape formations. In the second, we
show how the same framework can be used by a group of robots to form a
consensus over a set of discrete decisions. Experimental results highlight our
method's scalability and efficiency compared to recent approaches to these
problems making it a promising solution for multi-robot systems requiring
distributed consensus. We encourage the reader to see the supplementary video
demo.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>