<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>multi-agent - 2025-06-24</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>multi-agent - 2025-06-24</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.18679v1' target='_blank'>MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning
  for Active Contour Optimization in Medical Image Segmentation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruicheng Zhang, Yu Sun, Zeyu Zhang, Jinai Li, Xiaofan Liu, Au Hoi Fan, Haowei Guo, Puxin Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-23 14:22:49</h6>
<p class='card-text'>We introduce MARL-MambaContour, the first contour-based medical image
segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our
approach reframes segmentation as a multi-agent cooperation task focused on
generate topologically consistent object-level contours, addressing the
limitations of traditional pixel-based methods which could lack topological
constraints and holistic structural awareness of anatomical regions. Each
contour point is modeled as an autonomous agent that iteratively adjusts its
position to align precisely with the target boundary, enabling adaptation to
blurred edges and intricate morphologies common in medical images. This
iterative adjustment process is optimized by a contour-specific Soft
Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization
Adjustment Mechanism (ERAM) which dynamically balance agent exploration with
contour smoothness. Furthermore, the framework incorporates a Mamba-based
policy network featuring a novel Bidirectional Cross-attention Hidden-state
Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion
limitations associated with long-range modeling in state space models, thereby
facilitating more accurate inter-agent information exchange and informed
decision-making. Extensive experiments on five diverse medical imaging datasets
demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting
its potential as an accurate and robust clinical application.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.18651v1' target='_blank'>Dual-level Behavioral Consistency for Inter-group and Intra-group
  Coordination in Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuocun Yang, Huawen Hu, Enze Shi, Shu Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-23 13:54:34</h6>
<p class='card-text'>Behavioral diversity in Multi-agent reinforcement learning(MARL) represents
an emerging and promising research area. Prior work has largely centered on
intra-group behavioral consistency in multi-agent systems, with limited
attention given to behavioral consistency in multi-agent grouping scenarios. In
this paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL
control method designed to explicitly regulate agent behaviors at both
intra-group and inter-group levels. DLBC partitions agents into distinct groups
and dynamically modulates behavioral diversity both within and between these
groups. By dynamically modulating behavioral diversity within and between these
groups, DLBC achieves enhanced division of labor through inter-group
consistency, which constrains behavioral strategies across different groups.
Simultaneously, intra-group consistency, achieved by aligning behavioral
strategies within each group, fosters stronger intra-group cooperation.
Crucially, DLBC's direct constraint of agent policy functions ensures its broad
applicability across various algorithmic frameworks. Experimental results in
various grouping cooperation scenarios demonstrate that DLBC significantly
enhances both intra-group cooperative performance and inter-group task
specialization, yielding substantial performance improvements. DLBC provides
new ideas for behavioral consistency control of multi-intelligent body systems,
and its potential for application in more complex tasks and dynamic
environments can be further explored in the future.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.17029v1' target='_blank'>Scalable and Reliable Multi-agent Reinforcement Learning for Traffic
  Assignment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Leizhen Wang, Peibo Duan, Cheng Lyu, Zewen Wang, Zhiqiang He, Nan Zheng, Zhenliang Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-20 14:25:23</h6>
<p class='card-text'>The evolution of metropolitan cities and the increase in travel demands
impose stringent requirements on traffic assignment methods. Multi-agent
reinforcement learning (MARL) approaches outperform traditional methods in
modeling adaptive routing behavior without requiring explicit system dynamics,
which is beneficial for real-world deployment. However, MARL frameworks face
challenges in scalability and reliability when managing extensive networks with
substantial travel demand, which limiting their practical applicability in
solving large-scale traffic assignment problems. To address these challenges,
this study introduces MARL-OD-DA, a new MARL framework for the traffic
assignment problem, which redefines agents as origin-destination (OD) pair
routers rather than individual travelers, significantly enhancing scalability.
Additionally, a Dirichlet-based action space with action pruning and a reward
function based on the local relative gap are designed to enhance solution
reliability and improve convergence efficiency. Experiments demonstrate that
the proposed MARL framework effectively handles medium-sized networks with
extensive and varied city-level OD demand, surpassing existing MARL methods.
When implemented in the SiouxFalls network, MARL-OD-DA achieves better
assignment solutions in 10 steps, with a relative gap that is 94.99% lower than
that of conventional methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.15292v1' target='_blank'>Multivariate and Multiple Contrast Testing in General Covariate-adjusted
  Factorial Designs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marléne Baumeister, Konstantin Emil Thiel, Lynn Matits, Georg Zimmermann, Markus Pauly, Paavo Sattler</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-18 09:22:08</h6>
<p class='card-text'>Evaluating intervention effects on multiple outcomes is a central research
goal in a wide range of quantitative sciences. It is thereby common to compare
interventions among each other and with a control across several, potentially
highly correlated, outcome variables. In this context, researchers are
interested in identifying effects at both, the global level (across all outcome
variables) and the local level (for specific variables). At the same time,
potential confounding must be accounted for. This leads to the need for
powerful multiple contrast testing procedures (MCTPs) capable of handling
multivariate outcomes and covariates. Given this background, we propose an
extension of MCTPs within a semiparametric MANCOVA framework that allows
applicability beyond multivariate normality, homoscedasticity, or non-singular
covariance structures. We illustrate our approach by analysing multivariate
psychological intervention data, evaluating joint physiological and
psychological constructs such as heart rate variability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.15207v1' target='_blank'>Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth
  Observation: A Realistic Case Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-18 07:42:11</h6>
<p class='card-text'>The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised
Earth Observation (EO) missions, addressing challenges in climate monitoring,
disaster management, and more. However, autonomous coordination in
multi-satellite systems remains a fundamental challenge. Traditional
optimisation approaches struggle to handle the real-time decision-making
demands of dynamic EO missions, necessitating the use of Reinforcement Learning
(RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we
investigate RL-based autonomous EO mission planning by modelling
single-satellite operations and extending to multi-satellite constellations
using MARL frameworks. We address key challenges, including energy and data
storage limitations, uncertainties in satellite observations, and the
complexities of decentralised coordination under partial observability. By
leveraging a near-realistic satellite simulation environment, we evaluate the
training stability and performance of state-of-the-art MARL algorithms,
including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can
effectively balance imaging and resource management while addressing
non-stationarity and reward interdependency in multi-satellite coordination.
The insights gained from this study provide a foundation for autonomous
satellite operations, offering practical guidelines for improving policy
learning in decentralised EO missions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.14990v1' target='_blank'>MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tristan Tomilin, Luka van den Boogaard, Samuel Garcin, Bram Grooten, Meng Fang, Mykola Pechenizkiy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-17 21:50:04</h6>
<p class='card-text'>Benchmarks play a crucial role in the development and analysis of
reinforcement learning (RL) algorithms, with environment availability strongly
impacting research. One particularly underexplored intersection is continual
learning (CL) in cooperative multi-agent settings. To remedy this, we introduce
MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark
tailored for continual multi-agent reinforcement learning (CMARL). Existing CL
benchmarks run environments on the CPU, leading to computational bottlenecks
and limiting the length of task sequences. MEAL leverages JAX for GPU
acceleration, enabling continual learning across sequences of 100 tasks on a
standard desktop PC in a few hours. We show that naively combining popular CL
and MARL methods yields strong performance on simple environments, but fails to
scale to more complex settings requiring sustained coordination and adaptation.
Our ablation study identifies architectural and algorithmic features critical
for CMARL on MEAL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.14164v1' target='_blank'>Light Aircraft Game : Basic Implementation and training results analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanzhong Cao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-17 03:57:28</h6>
<p class='card-text'>This paper investigates multi-agent reinforcement learning (MARL) in a
partially observable, cooperative-competitive combat environment known as LAG.
We describe the environment's setup, including agent actions, hierarchical
controls, and reward design across different combat modes such as No Weapon and
ShootMissile. Two representative algorithms are evaluated: HAPPO, an on-policy
hierarchical variant of PPO, and HASAC, an off-policy method based on soft
actor-critic. We analyze their training stability, reward progression, and
inter-agent coordination capabilities. Experimental results show that HASAC
performs well in simpler coordination tasks without weapons, while HAPPO
demonstrates stronger adaptability in more dynamic and expressive scenarios
involving missile combat. These findings provide insights into the trade-offs
between on-policy and off-policy methods in multi-agent settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.13755v1' target='_blank'>MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with
  Multi-Agent Reinforcement Learning and Conformal Prediction Filtering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arya Fayyazi, Mehdi Kamal, Massoud Pedram</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-16 17:58:09</h6>
<p class='card-text'>This paper introduces MARCO (Multi-Agent Reinforcement learning with
Conformal Optimization), a novel hardware-aware framework for efficient neural
architecture search (NAS) targeting resource-constrained edge devices. By
significantly reducing search time and maintaining accuracy under strict
hardware constraints, MARCO bridges the gap between automated DNN design and
CAD for edge AI deployment. MARCO's core technical contribution lies in its
unique combination of multi-agent reinforcement learning (MARL) with Conformal
Prediction (CP) to accelerate the hardware/software co-design process for
deploying deep neural networks. Unlike conventional once-for-all (OFA) supernet
approaches that require extensive pretraining, MARCO decomposes the NAS task
into a hardware configuration agent (HCA) and a Quantization Agent (QA). The
HCA optimizes high-level design parameters, while the QA determines per-layer
bit-widths under strict memory and latency budgets using a shared reward signal
within a centralized-critic, decentralized-execution (CTDE) paradigm. A key
innovation is the integration of a calibrated CP surrogate model that provides
statistical guarantees (with a user-defined miscoverage rate) to prune
unpromising candidate architectures before incurring the high costs of partial
training or hardware simulation. This early filtering drastically reduces the
search space while ensuring that high-quality designs are retained with a high
probability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100
demonstrate that MARCO achieves a 3-4x reduction in total search time compared
to an OFA baseline while maintaining near-baseline accuracy (within 0.3%).
Furthermore, MARCO also reduces inference latency. Validation on a MAX78000
evaluation board confirms that simulator trends hold in practice, with
simulator estimates deviating from measured values by less than 5%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.13113v1' target='_blank'>Dynamic Reinsurance Treaty Bidding via Multi-Agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Stella C. Dong, James R. Finlay</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-16 05:43:22</h6>
<p class='card-text'>This paper develops a novel multi-agent reinforcement learning (MARL)
framework for reinsurance treaty bidding, addressing long-standing
inefficiencies in traditional broker-mediated placement processes. We pose the
core research question: Can autonomous, learning-based bidding systems improve
risk transfer efficiency and outperform conventional pricing approaches in
reinsurance markets?
  In our model, each reinsurer is represented by an adaptive agent that
iteratively refines its bidding strategy within a competitive, partially
observable environment. The simulation explicitly incorporates institutional
frictions including broker intermediation, incumbent advantages, last-look
privileges, and asymmetric access to underwriting information.
  Empirical analysis demonstrates that MARL agents achieve up to 15% higher
underwriting profit, 20% lower tail risk (CVaR), and over 25% improvement in
Sharpe ratios relative to actuarial and heuristic baselines. Sensitivity tests
confirm robustness across hyperparameter settings, and stress testing reveals
strong resilience under simulated catastrophe shocks and capital constraints.
  These findings suggest that MARL offers a viable path toward more
transparent, adaptive, and risk-sensitive reinsurance markets. The proposed
framework contributes to emerging literature at the intersection of algorithmic
market design, strategic bidding, and AI-enabled financial decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.12600v1' target='_blank'>Trust-MARL: Trust-Based Multi-Agent Reinforcement Learning Framework for
  Cooperative On-Ramp Merging Control in Heterogeneous Traffic Flow</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jie Pan, Tianyi Wang, Christian Claudel, Jing Shi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-14 18:35:10</h6>
<p class='card-text'>Intelligent transportation systems require connected and automated vehicles
(CAVs) to conduct safe and efficient cooperation with human-driven vehicles
(HVs) in complex real-world traffic environments. However, the inherent
unpredictability of human behaviour, especially at bottlenecks such as highway
on-ramp merging areas, often disrupts traffic flow and compromises system
performance. To address the challenge of cooperative on-ramp merging in
heterogeneous traffic environments, this study proposes a trust-based
multi-agent reinforcement learning (Trust-MARL) framework. At the macro level,
Trust-MARL enhances global traffic efficiency by leveraging inter-agent trust
to improve bottleneck throughput and mitigate traffic shockwave through
emergent group-level coordination. At the micro level, a dynamic trust
mechanism is designed to enable CAVs to adjust their cooperative strategies in
response to real-time behaviors and historical interactions with both HVs and
other CAVs. Furthermore, a trust-triggered game-theoretic decision-making
module is integrated to guide each CAV in adapting its cooperation factor and
executing context-aware lane-changing decisions under safety, comfort, and
efficiency constraints. An extensive set of ablation studies and comparative
experiments validates the effectiveness of the proposed Trust-MARL approach,
demonstrating significant improvements in safety, efficiency, comfort, and
adaptability across varying CAV penetration rates and traffic densities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.12497v2' target='_blank'>Wasserstein-Barycenter Consensus for Cooperative Multi-Agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ali Baheri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-14 13:17:47</h6>
<p class='card-text'>Cooperative multi-agent reinforcement learning (MARL) demands principled
mechanisms to align heterogeneous policies while preserving the capacity for
specialized behavior. We introduce a novel consensus framework that defines the
team strategy as the entropic-regularized $p$-Wasserstein barycenter of agents'
joint state--action visitation measures. By augmenting each agent's policy
objective with a soft penalty proportional to its Sinkhorn divergence from this
barycenter, the proposed approach encourages coherent group behavior without
enforcing rigid parameter sharing. We derive an algorithm that alternates
between Sinkhorn-barycenter computation and policy-gradient updates, and we
prove that, under standard Lipschitz and compactness assumptions, the maximal
pairwise policy discrepancy contracts at a geometric rate. Empirical evaluation
on a cooperative navigation case study demonstrates that our OT-barycenter
consensus outperforms an independent learners baseline in convergence speed and
final coordination success.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.12453v1' target='_blank'>Topology-Assisted Spatio-Temporal Pattern Disentangling for Scalable
  MARL in Large-scale Autonomous Traffic Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rongpeng Li, Jianhang Zhu, Jiahao Huang, Zhifeng Zhao, Honggang Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-14 11:18:12</h6>
<p class='card-text'>Intelligent Transportation Systems (ITSs) have emerged as a promising
solution towards ameliorating urban traffic congestion, with Traffic Signal
Control (TSC) identified as a critical component. Although Multi-Agent
Reinforcement Learning (MARL) algorithms have shown potential in optimizing TSC
through real-time decision-making, their scalability and effectiveness often
suffer from large-scale and complex environments. Typically, these limitations
primarily stem from a fundamental mismatch between the exponential growth of
the state space driven by the environmental heterogeneities and the limited
modeling capacity of current solutions. To address these issues, this paper
introduces a novel MARL framework that integrates Dynamic Graph Neural Networks
(DGNNs) and Topological Data Analysis (TDA), aiming to enhance the
expressiveness of environmental representations and improve agent coordination.
Furthermore, inspired by the Mixture of Experts (MoE) architecture in Large
Language Models (LLMs), a topology-assisted spatial pattern disentangling
(TSD)-enhanced MoE is proposed, which leverages topological signatures to
decouple graph features for specialized processing, thus improving the model's
ability to characterize dynamic and heterogeneous local observations. The TSD
module is also integrated into the policy and value networks of the Multi-agent
Proximal Policy Optimization (MAPPO) algorithm, further improving
decision-making efficiency and robustness. Extensive experiments conducted on
real-world traffic scenarios, together with comprehensive theoretical analysis,
validate the superior performance of the proposed framework, highlighting the
model's scalability and effectiveness in addressing the complexities of
large-scale TSC tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.11445v1' target='_blank'>Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local
  State Attention</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuan Duy Ta, Bang Giang Le, Thanh Ha Le, Viet Cuong Ta</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-13 03:48:54</h6>
<p class='card-text'>In mixed-traffic environments, autonomous vehicles must adapt to
human-controlled vehicles and other unusual driving situations. This setting
can be framed as a multi-agent reinforcement learning (MARL) environment with
full cooperative reward among the autonomous vehicles. While methods such as
Multi-agent Proximal Policy Optimization can be effective in training MARL
tasks, they often fail to resolve local conflict between agents and are unable
to generalize to stochastic events. In this paper, we propose a Local State
Attention module to assist the input state representation. By relying on the
self-attention operator, the module is expected to compress the essential
information of nearby agents to resolve the conflict in traffic situations.
Utilizing a simulated highway merging scenario with the priority vehicle as the
unexpected event, our approach is able to prioritize other vehicles'
information to manage the merging process. The results demonstrate significant
improvements in merging efficiency compared to popular baselines, especially in
high-density traffic settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.09434v1' target='_blank'>When Is Diversity Rewarded in Cooperative Multi-Agent Learning?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michael Amir, Matteo Bettini, Amanda Prorok</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-11 06:33:55</h6>
<p class='card-text'>The success of teams in robotics, nature, and society often depends on the
division of labor among diverse specialists; however, a principled explanation
for when such diversity surpasses a homogeneous team is still missing. Focusing
on multi-agent task allocation problems, our goal is to study this question
from the perspective of reward design: what kinds of objectives are best suited
for heterogeneous teams? We first consider an instantaneous, non-spatial
setting where the global reward is built by two generalized aggregation
operators: an inner operator that maps the $N$ agents' effort allocations on
individual tasks to a task score, and an outer operator that merges the $M$
task scores into the global team reward. We prove that the curvature of these
operators determines whether heterogeneity can increase reward, and that for
broad reward families this collapses to a simple convexity test. Next, we ask
what incentivizes heterogeneity to emerge when embodied, time-extended agents
must learn an effort allocation policy. To study heterogeneity in such
settings, we use multi-agent reinforcement learning (MARL) as our computational
paradigm, and introduce Heterogeneous Environment Design (HED), a
gradient-based algorithm that optimizes the parameter space of underspecified
MARL environments to find scenarios where heterogeneity is advantageous.
Experiments in matrix games and an embodied Multi-Goal-Capture environment show
that, despite the difference in settings, HED rediscovers the reward regimes
predicted by our theory to maximize the advantage of heterogeneity, both
validating HED and connecting our theoretical insights to reward design in
MARL. Together, these results help us understand when behavioral diversity
delivers a measurable benefit.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.09331v2' target='_blank'>Multi-Agent Language Models: Advancing Cooperation, Coordination, and
  Adaptation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arjun Vaithilingam Sudhakar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-11 02:12:34</h6>
<p class='card-text'>Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.08149v1' target='_blank'>Ego-centric Learning of Communicative World Models for Autonomous
  Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hang Wang, Dechen Gao, Junshan Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 18:56:40</h6>
<p class='card-text'>We study multi-agent reinforcement learning (MARL) for tasks in complex
high-dimensional environments, such as autonomous driving. MARL is known to
suffer from the \textit{partial observability} and \textit{non-stationarity}
issues. To tackle these challenges, information sharing is often employed,
which however faces major hurdles in practice, including overwhelming
communication overhead and scalability concerns. By making use of generative AI
embodied in world model together with its latent representation, we develop
{\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d
Mode\underline{l}, for MARL, where 1) each agent first learns its world model
that encodes its state and intention into low-dimensional latent representation
with smaller memory footprint, which can be shared with other agents of
interest via lightweight communication; and 2) each agent carries out
ego-centric learning while exploiting lightweight information sharing to enrich
her world model, and then exploits its generalization capacity to improve
prediction for better planning. We characterize the gain on the prediction
accuracy from the information sharing and its impact on performance gap.
Extensive experiments are carried out on the challenging local trajectory
planning tasks in the CARLA platform to demonstrate the performance gains of
using \textit{CALL}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.07548v1' target='_blank'>Curriculum Learning With Counterfactual Group Relative Policy Advantage
  For Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weiqiang Jin, Hongyang Du, Guizhong Liu, Dong In Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 08:38:18</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) has achieved strong performance in
cooperative adversarial tasks. However, most existing methods typically train
agents against fixed opponent strategies and rely on such meta-static
difficulty conditions, which limits their adaptability to changing environments
and often leads to suboptimal policies. Inspired by the success of curriculum
learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL
that employs an self-adaptive difficulty adjustment mechanism. This mechanism
continuously modulates opponent strength based on real-time agent training
performance, allowing agents to progressively learn from easier to more
challenging scenarios. However, the dynamic nature of CL introduces instability
due to nonstationary environments and sparse global rewards. To address this
challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA),
which is tightly coupled with the curriculum by providing intrinsic credit
signals that reflect each agent's impact under evolving task demands. CGRPA
constructs a counterfactual advantage function that isolates individual
contributions within group behavior, facilitating more reliable policy updates
throughout the curriculum. CGRPA evaluates each agent's contribution through
constructing counterfactual action advantage function, providing intrinsic
rewards that enhance credit assignment and stabilize learning under
non-stationary conditions. Extensive experiments demonstrate that our method
improves both training stability and final performance, achieving competitive
results against state-of-the-art methods. The code is available at
https://github.com/NICE-HKU/CL2MARL-SMAC.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.07468v1' target='_blank'>Chasing Moving Targets with Online Self-Play Reinforcement Learning for
  Safer Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 06:35:12</h6>
<p class='card-text'>Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05437v1' target='_blank'>A MARL-based Approach for Easing MAS Organization Engineering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Julien Soulé, Jean-Paul Jamont, Michel Occello, Louis-Marie Traonouez, Paul Théron</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 09:59:36</h6>
<p class='card-text'>Multi-Agent Systems (MAS) have been successfully applied in industry for
their ability to address complex, distributed problems, especially in IoT-based
systems. Their efficiency in achieving given objectives and meeting design
requirements is strongly dependent on the MAS organization during the
engineering process of an application-specific MAS. To design a MAS that can
achieve given goals, available methods rely on the designer's knowledge of the
deployment environment. However, high complexity and low readability in some
deployment environments make the application of these methods to be costly or
raise safety concerns. In order to ease the MAS organization design regarding
those concerns, we introduce an original Assisted MAS Organization Engineering
Approach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement
Learning (MARL) process with an organizational model to suggest relevant
organizational specifications to help in MAS engineering.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.04276v1' target='_blank'>Autonomous Collaborative Scheduling of Time-dependent UAVs, Workers and
  Vehicles for Crowdsensing in Disaster Response</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lei Han, Yitong Guo, Pengfei Yang, Zhiyong Yu, Liang Wang, Quan Wang, Zhiwen Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-04 01:58:05</h6>
<p class='card-text'>Natural disasters have caused significant losses to human society, and the
timely and efficient acquisition of post-disaster environmental information is
crucial for the effective implementation of rescue operations. Due to the
complexity of post-disaster environments, existing sensing technologies face
challenges such as weak environmental adaptability, insufficient specialized
sensing capabilities, and limited practicality of sensing solutions. This paper
explores the heterogeneous multi-agent online autonomous collaborative
scheduling algorithm HoAs-PALN, aimed at achieving efficient collection of
post-disaster environmental information. HoAs-PALN is realized through adaptive
dimensionality reduction in the matching process and local Nash equilibrium
game, facilitating autonomous collaboration among time-dependent UAVs, workers
and vehicles to enhance sensing scheduling. (1) In terms of adaptive
dimensionality reduction during the matching process, HoAs-PALN significantly
reduces scheduling decision time by transforming a five-dimensional matching
process into two categories of three-dimensional matching processes; (2)
Regarding the local Nash equilibrium game, HoAs-PALN combines the softmax
function to optimize behavior selection probabilities and introduces a local
Nash equilibrium determination mechanism to ensure scheduling decision
performance. Finally, we conducted detailed experiments based on extensive
real-world and simulated data. Compared with the baselines (GREEDY, K-WTA, MADL
and MARL), HoAs-PALN improves task completion rates by 64.12%, 46.48%, 16.55%,
and 14.03% on average, respectively, while each online scheduling decision
takes less than 10 seconds, demonstrating its effectiveness in dynamic
post-disaster environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.02841v2' target='_blank'>Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using
  Ensemble Methods</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tom Danino, Nahum Shimkin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-03 13:13:15</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) methods have achieved
state-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms
typically require significantly more environment interactions than their
single-agent counterparts to converge, a problem exacerbated by the difficulty
in exploring over a large joint action space and the high variance intrinsic to
MARL environments. To tackle these issues, we propose a novel algorithm that
combines a decomposed centralized critic with decentralized ensemble learning,
incorporating several key contributions. The main component in our scheme is a
selective exploration method that leverages ensemble kurtosis. We extend the
global decomposed critic with a diversity-regularized ensemble of individual
critics and utilize its excess kurtosis to guide exploration toward
high-uncertainty states and actions. To improve sample efficiency, we train the
centralized critic with a novel truncated variation of the TD($\lambda$)
algorithm, enabling efficient off-policy learning with reduced variance. On the
actor side, our suggested algorithm adapts the mixed samples approach to MARL,
mixing on-policy and off-policy loss functions for training the actors. This
approach balances between stability and efficiency and outperforms purely
off-policy learning. The evaluation shows our method outperforms
state-of-the-art baselines on standard MARL benchmarks, including a variety of
SMAC II maps.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.02718v1' target='_blank'>Heterogeneous Group-Based Reinforcement Learning for LLM-based
  Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guanzhong Chen, Shaoxiong Yang, Chao Li, Wei Liu, Jian Luan, Zenglin Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-03 10:17:19</h6>
<p class='card-text'>Large Language Models (LLMs) have achieved remarkable success across diverse
natural language processing tasks, yet their deployment in real-world
applications is hindered by fixed knowledge cutoffs and difficulties in
generating controllable, accurate outputs in a single inference. Multi-agent
systems (MAS) built from specialized LLM agents offer a promising solution,
enabling dynamic collaboration and iterative reasoning. However, optimizing
these systems remains a challenge, as conventional methods such as prompt
engineering and supervised fine-tuning entail high engineering overhead and
limited adaptability. Reinforcement learning (RL), particularly multi-agent
reinforcement learning (MARL), provides a scalable framework by refining agent
policies based on system-level feedback. Nevertheless, existing MARL
algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on
Critic networks, which can cause training instability and increase
computational burden. To address these limitations and target the prototypical
Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group
Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy
updates by estimating relative reward advantages across heterogeneous groups of
rollouts. MHGPO eliminates the need for Critic networks, enhancing stability
and reducing computational overhead. Additionally, we introduce three group
rollout sampling strategies that trade off between efficiency and
effectiveness. Experiments on a multi-agent LLM-based search system demonstrate
that MHGPO consistently outperforms MAPPO in both task performance and
computational efficiency, without requiring warm-up, underscoring its potential
for stable and scalable optimization of complex LLM-based MAS.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.04265v2' target='_blank'>CORA: Coalitional Rational Advantage Decomposition for Multi-Agent
  Policy Gradients</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mengda Ji, Genjiu Xu, Liying Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-03 08:04:43</h6>
<p class='card-text'>This work focuses on the credit assignment problem in cooperative multi-agent
reinforcement learning (MARL). Sharing the global advantage among agents often
leads to suboptimal policy updates as it fails to account for the distinct
contributions of agents. Although numerous methods consider global or
individual contributions for credit assignment, a detailed analysis at the
coalition level remains lacking in many approaches. This work analyzes the
over-updating problem during multi-agent policy updates from a coalition-level
perspective. To address this issue, we propose a credit assignment method
called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates
coalitional advantages via marginal contributions from all possible coalitions
and decomposes advantages using the core solution from cooperative game theory,
ensuring coalitional rationality. To reduce computational overhead, CORA
employs random coalition sampling. Experiments on matrix games, differential
games, and multi-agent collaboration benchmarks demonstrate that CORA
outperforms strong baselines, particularly in tasks with multiple local optima.
These findings highlight the importance of coalition-aware credit assignment
for improving MARL performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.01538v2' target='_blank'>LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative
  Policy Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guobin Zhu, Rui Zhou, Wenkang Ji, Shiyu Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-02 10:59:54</h6>
<p class='card-text'>Although Multi-Agent Reinforcement Learning (MARL) is effective for complex
multi-robot tasks, it suffers from low sample efficiency and requires iterative
manual reward tuning. Large Language Models (LLMs) have shown promise in
single-robot settings, but their application in multi-robot systems remains
largely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL)
approach, which integrates MARL with LLMs, significantly enhancing sample
efficiency without requiring manual design. LAMARL consists of two modules: the
first module leverages LLMs to fully automate the generation of prior policy
and reward functions. The second module is MARL, which uses the generated
functions to guide robot policy training effectively. On a shape assembly
benchmark, both simulation and real-world experiments demonstrate the unique
advantages of LAMARL. Ablation studies show that the prior policy improves
sample efficiency by an average of 185.9% and enhances task completion, while
structured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM
output success rates by 28.5%-67.5%. Videos and code are available at
https://windylab.github.io/LAMARL/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.00982v1' target='_blank'>Robust and Safe Multi-Agent Reinforcement Learning Framework with
  Communication for Autonomous Vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Keshawn Smith, Zhili Zhang, H M Sabbir Ahmad, Ehsan Sabouni, Maniak Mondal, Song Han, Wenchao Li, Fei Miao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-01 12:29:53</h6>
<p class='card-text'>Deep multi-agent reinforcement learning (MARL) has been demonstrated
effectively in simulations for many multi-robot problems. For autonomous
vehicles, the development of vehicle-to-vehicle (V2V) communication
technologies provide opportunities to further enhance safety of the system.
However, zero-shot transfer of simulator-trained MARL policies to hardware
dynamic systems remains challenging, and how to leverage communication and
shared information for MARL has limited demonstrations on hardware. This
problem is challenged by discrepancies between simulated and physical states,
system state and model uncertainties, practical shared information design, and
the need for safety guarantees in both simulation and hardware. This paper
introduces RSR-RSMARL, a novel Robust and Safe MARL framework that supports
Real-Sim-Real (RSR) policy adaptation for multi-agent systems with
communication among agents, with both simulation and hardware demonstrations.
RSR-RSMARL leverages state (includes shared state information among agents) and
action representations considering real system complexities for MARL
formulation. The MARL policy is trained with robust MARL algorithm to enable
zero-shot transfer to hardware considering the sim-to-real gap. A safety shield
module using Control Barrier Functions (CBFs) provides safety guarantee for
each individual agent. Experiment results on F1/10th-scale autonomous vehicles
with V2V communication demonstrate the ability of RSR-RSMARL framework to
enhance driving safety and coordination across multiple configurations. These
findings emphasize the importance of jointly designing robust policy
representations and modular safety architectures to enable scalable,
generalizable RSR transfer in multi-agent autonomy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.04251v1' target='_blank'>Language-Guided Multi-Agent Learning in Simulations: A Unified Framework
  and Evaluation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhengyang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-01 06:46:49</h6>
<p class='card-text'>This paper introduces LLM-MARL, a unified framework that incorporates large
language models (LLMs) into multi-agent reinforcement learning (MARL) to
enhance coordination, communication, and generalization in simulated game
environments. The framework features three modular components of Coordinator,
Communicator, and Memory, which dynamically generate subgoals, facilitate
symbolic inter-agent messaging, and support episodic recall. Training combines
PPO with a language-conditioned loss and LLM query gating. LLM-MARL is
evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results
show consistent improvements over MAPPO and QMIX in win rate, coordination
score, and zero-shot generalization. Ablation studies demonstrate that subgoal
generation and language-based messaging each contribute significantly to
performance gains. Qualitative analysis reveals emergent behaviors such as role
specialization and communication-driven tactics. By bridging language modeling
and policy learning, this work contributes to the design of intelligent,
cooperative agents in interactive simulations. It offers a path forward for
leveraging LLMs in multi-agent systems used for training, games, and human-AI
collaboration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.00797v1' target='_blank'>Action Dependency Graphs for Globally Optimal Coordinated Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianglin Ding, Jingcheng Tang, Gangshan Jing</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-01 02:58:20</h6>
<p class='card-text'>Action-dependent individual policies, which incorporate both environmental
states and the actions of other agents in decision-making, have emerged as a
promising paradigm for achieving global optimality in multi-agent reinforcement
learning (MARL). However, the existing literature often adopts auto-regressive
action-dependent policies, where each agent's policy depends on the actions of
all preceding agents. This formulation incurs substantial computational
complexity as the number of agents increases, thereby limiting scalability. In
this work, we consider a more generalized class of action-dependent policies,
which do not necessarily follow the auto-regressive form. We propose to use the
`action dependency graph (ADG)' to model the inter-agent action dependencies.
Within the context of MARL problems structured by coordination graphs, we prove
that an action-dependent policy with a sparse ADG can achieve global
optimality, provided the ADG satisfies specific conditions specified by the
coordination graph. Building on this theoretical foundation, we develop a
tabular policy iteration algorithm with guaranteed global optimality.
Furthermore, we integrate our framework into several SOTA algorithms and
conduct experiments in complex environments. The empirical results affirm the
robustness and applicability of our approach in more general scenarios,
underscoring its potential for broader MARL challenges.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.00576v1' target='_blank'>ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement
  Learning in O-RAN Network Slicing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fatemeh Lotfi, Hossein Rajoli, Fatemeh Afghah</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-31 14:21:19</h6>
<p class='card-text'>Advanced wireless networks must support highly dynamic and heterogeneous
service demands. Open Radio Access Network (O-RAN) architecture enables this
flexibility by adopting modular, disaggregated components, such as the RAN
Intelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU),
that can support intelligent control via machine learning (ML). While deep
reinforcement learning (DRL) is a powerful tool for managing dynamic resource
allocation and slicing, it often struggles to process raw, unstructured input
like RF features, QoS metrics, and traffic trends. These limitations hinder
policy generalization and decision efficiency in partially observable and
evolving environments. To address this, we propose \textit{ORAN-GUIDE}, a
dual-LLM framework that enhances multi-agent RL (MARL) with task-relevant,
semantically enriched state representations. The architecture employs a
domain-specific language model, ORANSight, pretrained on O-RAN control and
configuration data, to generate structured, context-aware prompts. These
prompts are fused with learnable tokens and passed to a frozen GPT-based
encoder that outputs high-level semantic representations for DRL agents. This
design adopts a retrieval-augmented generation (RAG) style pipeline tailored
for technical decision-making in wireless systems. Experimental results show
that ORAN-GUIDE improves sample efficiency, policy convergence, and performance
generalization over standard MARL and single-LLM baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.24618v1' target='_blank'>Distributed Intelligence in the Computing Continuum with Active
  Inference</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Victor Casamayor Pujol, Boris Sedlak, Tommaso Salvatori, Karl Friston, Schahram Dustdar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-30 14:10:33</h6>
<p class='card-text'>The Computing Continuum (CC) is an emerging Internet-based computing paradigm
that spans from local Internet of Things sensors and constrained edge devices
to large-scale cloud data centers. Its goal is to orchestrate a vast array of
diverse and distributed computing resources to support the next generation of
Internet-based applications. However, the distributed, heterogeneous, and
dynamic nature of CC platforms demands distributed intelligence for adaptive
and resilient service management. This article introduces a distributed stream
processing pipeline as a CC use case, where each service is managed by an
Active Inference (AIF) agent. These agents collaborate to fulfill service needs
specified by SLOiDs, a term we introduce to denote Service Level Objectives
that are aware of its deployed devices, meaning that non-functional
requirements must consider the characteristics of the hosting device. We
demonstrate how AIF agents can be modeled and deployed alongside distributed
services to manage them autonomously. Our experiments show that AIF agents
achieve over 90% SLOiD fulfillment when using tested transition models, and
around 80% when learning the models during deployment. We compare their
performance to a multi-agent reinforcement learning algorithm, finding that
while both approaches yield similar results, MARL requires extensive training,
whereas AIF agents can operate effectively from the start. Additionally, we
evaluate the behavior of AIF agents in offloading scenarios, observing a strong
capacity for adaptation. Finally, we outline key research directions to advance
AIF integration in CC platforms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.24265v1' target='_blank'>R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in
  Multi-agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Harsh Goel, Mohammad Omama, Behdad Chalaki, Vaishnav Tadiparthi, Ehsan Moradi Pari, Sandeep Chinchali</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-30 06:40:19</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) has achieved significant progress
in large-scale traffic control, autonomous vehicles, and robotics. Drawing
inspiration from biological systems where roles naturally emerge to enable
coordination, role-based MARL methods have been proposed to enhance cooperation
learning for complex tasks. However, existing methods exclusively derive roles
from an agent's past experience during training, neglecting their influence on
its future trajectories. This paper introduces a key insight: an agent's role
should shape its future behavior to enable effective coordination. Hence, we
propose Role Discovery and Diversity through Dynamics Models (R3DM), a novel
role-based MARL framework that learns emergent roles by maximizing the mutual
information between agents' roles, observed trajectories, and expected future
behaviors. R3DM optimizes the proposed objective through contrastive learning
on past trajectories to first derive intermediate roles that shape intrinsic
rewards to promote diversity in future behaviors across different roles through
a learned dynamics model. Benchmarking on SMAC and SMACv2 environments
demonstrates that R3DM outperforms state-of-the-art MARL approaches, improving
multi-agent coordination to increase win rates by up to 20%.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>