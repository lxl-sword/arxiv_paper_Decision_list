<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>multi-agent - 2025-03-03</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>multi-agent - 2025-03-03</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20217v1' target='_blank'>MARVEL: Multi-Agent Reinforcement Learning for constrained field-of-View
  multi-robot Exploration in Large-scale environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jimmy Chiun, Shizhe Zhang, Yizhuo Wang, Yuhong Cao, Guillaume Sartoretti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 15:58:42</h6>
<p class='card-text'>In multi-robot exploration, a team of mobile robot is tasked with efficiently
mapping an unknown environments. While most exploration planners assume
omnidirectional sensors like LiDAR, this is impractical for small robots such
as drones, where lightweight, directional sensors like cameras may be the only
option due to payload constraints. These sensors have a constrained
field-of-view (FoV), which adds complexity to the exploration problem,
requiring not only optimal robot positioning but also sensor orientation during
movement. In this work, we propose MARVEL, a neural framework that leverages
graph attention networks, together with novel frontiers and orientation
features fusion technique, to develop a collaborative, decentralized policy
using multi-agent reinforcement learning (MARL) for robots with constrained
FoV. To handle the large action space of viewpoints planning, we further
introduce a novel information-driven action pruning strategy. MARVEL improves
multi-robot coordination and decision-making in challenging large-scale indoor
environments, while adapting to various team sizes and sensor configurations
(i.e., FoV and sensor range) without additional training. Our extensive
evaluation shows that MARVEL's learned policies exhibit effective coordinated
behaviors, outperforming state-of-the-art exploration planners across multiple
metrics. We experimentally demonstrate MARVEL's generalizability in large-scale
environments, of up to 90m by 90m, and validate its practical applicability
through successful deployment on a team of real drone hardware.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20065v1' target='_blank'>RouteRL: Multi-agent reinforcement learning framework for urban route
  choice with autonomous vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ahmet Onur Akman, Anastasia Psarou, Łukasz Gorczyca, Zoltán György Varga, Grzegorz Jamróz, Rafał Kucharski</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 13:13:09</h6>
<p class='card-text'>RouteRL is a novel framework that integrates multi-agent reinforcement
learning (MARL) with a microscopic traffic simulation, facilitating the testing
and development of efficient route choice strategies for autonomous vehicles
(AVs). The proposed framework simulates the daily route choices of driver
agents in a city, including two types: human drivers, emulated using behavioral
route choice models, and AVs, modeled as MARL agents optimizing their policies
for a predefined objective. RouteRL aims to advance research in MARL, transport
modeling, and human-AI interaction for transportation applications. This study
presents a technical report on RouteRL, outlines its potential research
contributions, and showcases its impact via illustrative examples.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19717v1' target='_blank'>Exponential Topology-enabled Scalable Communication in Multi-agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinran Li, Xiaolu Wang, Chenjia Bai, Jun Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 03:15:31</h6>
<p class='card-text'>In cooperative multi-agent reinforcement learning (MARL), well-designed
communication protocols can effectively facilitate consensus among agents,
thereby enhancing task performance. Moreover, in large-scale multi-agent
systems commonly found in real-world applications, effective communication
plays an even more critical role due to the escalated challenge of partial
observability compared to smaller-scale setups. In this work, we endeavor to
develop a scalable communication protocol for MARL. Unlike previous methods
that focus on selecting optimal pairwise communication links-a task that
becomes increasingly complex as the number of agents grows-we adopt a global
perspective on communication topology design. Specifically, we propose
utilizing the exponential topology to enable rapid information dissemination
among agents by leveraging its small-diameter and small-size properties. This
approach leads to a scalable communication protocol, named ExpoComm. To fully
unlock the potential of exponential graphs as communication topologies, we
employ memory-based message processors and auxiliary tasks to ground messages,
ensuring that they reflect global information and benefit decision-making.
Extensive experiments on large-scale cooperative benchmarks, including MAgent
and Infrastructure Management Planning, demonstrate the superior performance
and robust zero-shot transferability of ExpoComm compared to existing
communication strategies. The code is publicly available at
https://github.com/LXXXXR/ExpoComm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19675v1' target='_blank'>Joint Power Allocation and Phase Shift Design for Stacked Intelligent
  Metasurfaces-aided Cell-Free Massive MIMO Systems with MARL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiyang Zhu, Jiayi Zhang, Enyu Shi, Ziheng Liu, Chau Yuen, Bo Ai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 01:34:34</h6>
<p class='card-text'>Cell-free (CF) massive multiple-input multiple-output (mMIMO) systems offer
high spectral efficiency (SE) through multiple distributed access points (APs).
However, the large number of antennas increases power consumption. We propose
incorporating stacked intelligent metasurfaces (SIM) into CF mMIMO systems as a
cost-effective, energy-efficient solution. This paper focuses on optimizing the
joint power allocation of APs and the phase shift of SIMs to maximize the sum
SE. To address this complex problem, we introduce a fully distributed
multi-agent reinforcement learning (MARL) algorithm. Our novel algorithm, the
noisy value method with a recurrent policy in multi-agent policy optimization
(NVR-MAPPO), enhances performance by encouraging diverse exploration under
centralized training and decentralized execution. Simulations demonstrate that
NVR-MAPPO significantly improves sum SE and robustness across various
scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19297v1' target='_blank'>Combining Planning and Reinforcement Learning for Solving Relational
  Multiagent Domains</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nikhilesh Prabhakar, Ranveer Singh, Harsha Kokel, Sriraam Natarajan, Prasad Tadepalli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 16:55:23</h6>
<p class='card-text'>Multiagent Reinforcement Learning (MARL) poses significant challenges due to
the exponential growth of state and action spaces and the non-stationary nature
of multiagent environments. This results in notable sample inefficiency and
hinders generalization across diverse tasks. The complexity is further
pronounced in relational settings, where domain knowledge is crucial but often
underutilized by existing MARL algorithms. To overcome these hurdles, we
propose integrating relational planners as centralized controllers with
efficient state abstractions and reinforcement learning. This approach proves
to be sample-efficient and facilitates effective task transfer and
generalization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16608v1' target='_blank'>Toward Dependency Dynamics in Multi-Agent Reinforcement Learning for
  Traffic Signal Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuli Zhang, Shangbo Wang, Dongyao Jia, Pengfei Fan, Ruiyuan Jiang, Hankang Gu, Andy H. F. Chow</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-23 15:29:12</h6>
<p class='card-text'>Reinforcement learning (RL) emerges as a promising data-driven approach for
adaptive traffic signal control (ATSC) in complex urban traffic networks, with
deep neural networks substantially augmenting its learning capabilities.
However, centralized RL becomes impractical for ATSC involving multiple agents
due to the exceedingly high dimensionality of the joint action space.
Multi-agent RL (MARL) mitigates this scalability issue by decentralizing
control to local RL agents. Nevertheless, this decentralized method introduces
new challenges: the environment becomes partially observable from the
perspective of each local agent due to constrained inter-agent communication.
Both centralized RL and MARL exhibit distinct strengths and weaknesses,
particularly under heavy intersectional traffic conditions. In this paper, we
justify that MARL can achieve the optimal global Q-value by separating into
multiple IRL (Independent Reinforcement Learning) processes when no spill-back
congestion occurs (no agent dependency) among agents (intersections). In the
presence of spill-back congestion (with agent dependency), the maximum global
Q-value can be achieved by using centralized RL. Building upon the conclusions,
we propose a novel Dynamic Parameter Update Strategy for Deep Q-Network
(DQN-DPUS), which updates the weights and bias based on the dependency dynamics
among agents, i.e. updating only the diagonal sub-matrices for the scenario
without spill-back congestion. We validate the DQN-DPUS in a simple network
with two intersections under varying traffic, and show that the proposed
strategy can speed up the convergence rate without sacrificing optimal
exploration. The results corroborate our theoretical findings, demonstrating
the efficacy of DQN-DPUS in optimizing traffic signal control.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16496v1' target='_blank'>PMAT: Optimizing Action Generation Order in Multi-Agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kun Hu, Muning Wen, Xihuai Wang, Shao Zhang, Yiwei Shi, Minne Li, Minglong Li, Ying Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-23 08:30:14</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) faces challenges in coordinating
agents due to complex interdependencies within multi-agent systems. Most MARL
algorithms use the simultaneous decision-making paradigm but ignore the
action-level dependencies among agents, which reduces coordination efficiency.
In contrast, the sequential decision-making paradigm provides finer-grained
supervision for agent decision order, presenting the potential for handling
dependencies via better decision order management. However, determining the
optimal decision order remains a challenge. In this paper, we introduce Action
Generation with Plackett-Luce Sampling (AGPS), a novel mechanism for agent
decision order optimization. We model the order determination task as a
Plackett-Luce sampling process to address issues such as ranking instability
and vanishing gradient during the network training process. AGPS realizes
credit-based decision order determination by establishing a bridge between the
significance of agents' local observations and their decision credits, thus
facilitating order optimization and dependency management. Integrating AGPS
with the Multi-Agent Transformer, we propose the Prioritized Multi-Agent
Transformer (PMAT), a sequential decision-making MARL algorithm with decision
order optimization. Experiments on benchmarks including StarCraft II
Multi-Agent Challenge, Google Research Football, and Multi-Agent MuJoCo show
that PMAT outperforms state-of-the-art algorithms, greatly enhancing
coordination efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.14606v1' target='_blank'>Curiosity Driven Multi-agent Reinforcement Learning for 3D Game Testing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Raihana Ferdous, Fitsum Kifetew, Davide Prandi, Angelo Susi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-20 14:43:46</h6>
<p class='card-text'>Recently testing of games via autonomous agents has shown great promise in
tackling challenges faced by the game industry, which mainly relied on either
manual testing or record/replay. In particular Reinforcement Learning (RL)
solutions have shown potential by learning directly from playing the game
without the need for human intervention. In this paper, we present cMarlTest,
an approach for testing 3D games through curiosity driven Multi-Agent
Reinforcement Learning (MARL). cMarlTest deploys multiple agents that work
collaboratively to achieve the testing objective. The use of multiple agents
helps resolve issues faced by a single agent approach. We carried out
experiments on different levels of a 3D game comparing the performance of
cMarlTest with a single agent RL variant. Results are promising where,
considering three different types of coverage criteria, cMarlTest achieved
higher coverage. cMarlTest was also more efficient in terms of the time taken,
with respect to the single agent based variant.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13188v1' target='_blank'>Autonomous Vehicles Using Multi-Agent Reinforcement Learning for Routing
  Decisions Can Harm Urban Traffic</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anastasia Psarou, Ahmet Onur Akman, Łukasz Gorczyca, Michał Hoffmann, Zoltán György Varga, Grzegorz Jamróz, Rafał Kucharski</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-18 13:37:02</h6>
<p class='card-text'>Autonomous vehicles (AVs) using Multi-Agent Reinforcement Learning (MARL) for
simultaneous route optimization may destabilize traffic environments, with
human drivers possibly experiencing longer travel times. We study this
interaction by simulating human drivers and AVs. Our experiments with standard
MARL algorithms reveal that, even in trivial cases, policies often fail to
converge to an optimal solution or require long training periods. The problem
is amplified by the fact that we cannot rely entirely on simulated training, as
there are no accurate models of human routing behavior. At the same time,
real-world training in cities risks destabilizing urban traffic systems,
increasing externalities, such as $CO_2$ emissions, and introducing
non-stationarity as human drivers adapt unpredictably to AV behaviors.
Centralization can improve convergence in some cases, however, it raises
privacy concerns for the travelers' destination data. In this position paper,
we argue that future research must prioritize realistic benchmarks, cautious
deployment strategies, and tools for monitoring and regulating AV routing
behaviors to ensure sustainable and equitable urban mobility systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11882v2' target='_blank'>Leveraging Dual Process Theory in Language Agent Framework for Real-time
  Simultaneous Human-AI Collaboration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-17 15:09:45</h6>
<p class='card-text'>Agents built on large language models (LLMs) have excelled in turn-by-turn
human-AI collaboration but struggle with simultaneous tasks requiring real-time
interaction. Latency issues and the challenge of inferring variable human
strategies hinder their ability to make autonomous decisions without explicit
instructions. Through experiments with current independent System 1 and System
2 methods, we validate the necessity of using Dual Process Theory (DPT) in
real-time tasks. We propose DPT-Agent, a novel language agent framework that
integrates System 1 and System 2 for efficient real-time simultaneous human-AI
collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and
code-as-policy for fast, intuitive, and controllable decision-making.
DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous
reflection to infer human intentions and perform reasoning-based autonomous
decisions. We demonstrate the effectiveness of DPT-Agent through further
experiments with rule-based agents and human collaborators, showing significant
improvements over mainstream LLM-based frameworks. To the best of our
knowledge, DPT-Agent is the first language agent framework that achieves
successful real-time simultaneous human-AI collaboration autonomously. Code of
DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11260v1' target='_blank'>Scalable Multi-Agent Offline Reinforcement Learning and the Role of
  Information</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Riccardo Zamboni, Enrico Brunetti, Marcello Restelli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-16 20:28:42</h6>
<p class='card-text'>Offline Reinforcement Learning (RL) focuses on learning policies solely from
a batch of previously collected data. offering the potential to leverage such
datasets effectively without the need for costly or risky active exploration.
While recent advances in Offline Multi-Agent RL (MARL) have shown promise, most
existing methods either rely on large datasets jointly collected by all agents
or agent-specific datasets collected independently. The former approach ensures
strong performance but raises scalability concerns, while the latter emphasizes
scalability at the expense of performance guarantees. In this work, we propose
a novel scalable routine for both dataset collection and offline learning.
Agents first collect diverse datasets coherently with a pre-specified
information-sharing network and subsequently learn coherent localized policies
without requiring either full observability or falling back to complete
decentralization. We theoretically demonstrate that this structured approach
allows a multi-agent extension of the seminal Fitted Q-Iteration (FQI)
algorithm to globally converge, in high probability, to near-optimal policies.
The convergence is subject to error terms that depend on the informativeness of
the shared information. Furthermore, we show how this approach allows to bound
the inherent error of the supervised-learning phase of FQI with the mutual
information between shared and unshared information. Our algorithm, SCAlable
Multi-agent FQI (SCAM-FQI), is then evaluated on a distributed decision-making
problem. The empirical results align with our theoretical findings, supporting
the effectiveness of SCAM-FQI in achieving a balance between scalability and
policy performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.10148v1' target='_blank'>Cooperative Multi-Agent Planning with Adaptive Skill Synthesis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiyuan Li, Wenshuai Zhao, Joni Pajarinen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-14 13:23:18</h6>
<p class='card-text'>Despite much progress in training distributed artificial intelligence (AI),
building cooperative multi-agent systems with multi-agent reinforcement
learning (MARL) faces challenges in sample efficiency, interpretability, and
transferability. Unlike traditional learning-based methods that require
extensive interaction with the environment, large language models (LLMs)
demonstrate remarkable capabilities in zero-shot planning and complex
reasoning. However, existing LLM-based approaches heavily rely on text-based
observations and struggle with the non-Markovian nature of multi-agent
interactions under partial observability. We present COMPASS, a novel
multi-agent architecture that integrates vision-language models (VLMs) with a
dynamic skill library and structured communication for decentralized
closed-loop decision-making. The skill library, bootstrapped from
demonstrations, evolves via planner-guided tasks to enable adaptive strategies.
COMPASS propagates entity information through multi-hop communication under
partial observability. Evaluations on the improved StarCraft Multi-Agent
Challenge (SMACv2) demonstrate COMPASS achieves up to 30\% higher win rates
than state-of-the-art MARL algorithms in symmetric scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.09780v1' target='_blank'>Incentivize without Bonus: Provably Efficient Model-based Online
  Multi-agent RL for Markov Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tong Yang, Bo Dai, Lin Xiao, Yuejie Chi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-13 21:28:51</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) lies at the heart of a plethora of
applications involving the interaction of a group of agents in a shared unknown
environment. A prominent framework for studying MARL is Markov games, with the
goal of finding various notions of equilibria in a sample-efficient manner,
such as the Nash equilibrium (NE) and the coarse correlated equilibrium (CCE).
However, existing sample-efficient approaches either require tailored
uncertainty estimation under function approximation, or careful coordination of
the players. In this paper, we propose a novel model-based algorithm, called
VMG, that incentivizes exploration via biasing the empirical estimate of the
model parameters towards those with a higher collective best-response values of
all the players when fixing the other players' policies, thus encouraging the
policy to deviate from its current equilibrium for more exploration. VMG is
oblivious to different forms of function approximation, and permits
simultaneous and uncoupled policy updates of all players. Theoretically, we
also establish that VMG achieves a near-optimal regret for finding both the NEs
of two-player zero-sum Markov games and CCEs of multi-player general-sum Markov
games under linear function approximation in an online environment, which
nearly match their counterparts with sophisticated uncertainty quantification.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.08985v1' target='_blank'>Few is More: Task-Efficient Skill-Discovery for Multi-Task Offline
  Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xun Wang, Zhuoran Li, Hai Zhong, Longbo Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-13 05:47:57</h6>
<p class='card-text'>As a data-driven approach, offline MARL learns superior policies solely from
offline datasets, ideal for domains rich in historical data but with high
interaction costs and risks. However, most existing methods are task-specific,
requiring retraining for new tasks, leading to redundancy and inefficiency. To
address this issue, in this paper, we propose a task-efficient multi-task
offline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL).
Unlike existing offline skill-discovery methods, SD-CQL discovers skills by
reconstructing the next observation. It then evaluates fixed and variable
actions separately and employs behavior-regularized conservative Q-learning to
execute the optimal action for each skill. This approach eliminates the need
for local-global alignment and enables strong multi-task generalization from
limited small-scale source tasks. Substantial experiments on StarCraftII
demonstrates the superior generalization performance and task-efficiency of
SD-CQL. It achieves the best performance on $\textbf{10}$ out of $14$ task
sets, with up to $\textbf{65%}$ improvement on individual task sets, and is
within $4\%$ of the best baseline on the remaining four.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.07635v1' target='_blank'>Distributed Value Decomposition Networks with Networked Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guilherme S. Varela, Alberto Sardinha, Francisco S. Melo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-11 15:23:05</h6>
<p class='card-text'>We investigate the problem of distributed training under partial
observability, whereby cooperative multi-agent reinforcement learning agents
(MARL) maximize the expected cumulative joint reward. We propose distributed
value decomposition networks (DVDN) that generate a joint Q-function that
factorizes into agent-wise Q-functions. Whereas the original value
decomposition networks rely on centralized training, our approach is suitable
for domains where centralized training is not possible and agents must learn by
interacting with the physical environment in a decentralized manner while
communicating with their peers. DVDN overcomes the need for centralized
training by locally estimating the shared objective. We contribute with two
innovative algorithms, DVDN and DVDN (GT), for the heterogeneous and
homogeneous agents settings respectively. Empirically, both algorithms
approximate the performance of value decomposition networks, in spite of the
information loss during communication, as demonstrated in ten MARL tasks in
three standard environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06976v1' target='_blank'>Who is Helping Whom? Analyzing Inter-dependencies to Evaluate
  Cooperation in Human-AI Teaming</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Upasana Biswas, Siddhant Bhambri, Subbarao Kambhampati</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 19:16:20</h6>
<p class='card-text'>The long-standing research challenges of Human-AI Teaming(HAT) and Zero-shot
Cooperation(ZSC) have been tackled by applying multi-agent reinforcement
learning(MARL) to train an agent by optimizing the environment reward function
and evaluating their performance through task performance metrics such as task
reward. However, such evaluation focuses only on task completion, while being
agnostic to `how' the two agents work with each other. Specifically, we are
interested in understanding the cooperation arising within the team when
trained agents are paired with humans. To formally address this problem, we
propose the concept of interdependence to measure how much agents rely on each
other's actions to achieve the shared goal, as a key metric for evaluating
cooperation in human-agent teams. Towards this, we ground this concept through
a symbolic formalism and define evaluation metrics that allow us to assess the
degree of reliance between the agents' actions. We pair state-of-the-art agents
trained through MARL for HAT, with learned human models for the the popular
Overcooked domain, and evaluate the team performance for these human-agent
teams. Our results demonstrate that trained agents are not able to induce
cooperative behavior, reporting very low levels of interdependence across all
the teams. We also report that teaming performance of a team is not necessarily
correlated with the task reward.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06113v1' target='_blank'>Towards Bio-inspired Heuristically Accelerated Reinforcement Learning
  for Adaptive Underwater Multi-Agents Behaviour</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Antoine Vivien, Thomas Chaffre, Matthew Stephenson, Eva Artusi, Paulo Santos, Benoit Clement, Karl Sammut</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 02:47:33</h6>
<p class='card-text'>This paper describes the problem of coordination of an autonomous Multi-Agent
System which aims to solve the coverage planning problem in a complex
environment. The considered applications are the detection and identification
of objects of interest while covering an area. These tasks, which are highly
relevant for space applications, are also of interest among various domains
including the underwater context, which is the focus of this study. In this
context, coverage planning is traditionally modelled as a Markov Decision
Process where a coordinated MAS, a swarm of heterogeneous autonomous underwater
vehicles, is required to survey an area and search for objects. This MDP is
associated with several challenges: environment uncertainties, communication
constraints, and an ensemble of hazards, including time-varying and
unpredictable changes in the underwater environment. MARL algorithms can solve
highly non-linear problems using deep neural networks and display great
scalability against an increased number of agents. Nevertheless, most of the
current results in the underwater domain are limited to simulation due to the
high learning time of MARL algorithms. For this reason, a novel strategy is
introduced to accelerate this convergence rate by incorporating biologically
inspired heuristics to guide the policy during training. The PSO method, which
is inspired by the behaviour of a group of animals, is selected as a heuristic.
It allows the policy to explore the highest quality regions of the action and
state spaces, from the beginning of the training, optimizing the
exploration/exploitation trade-off. The resulting agent requires fewer
interactions to reach optimal performance. The method is applied to the MSAC
algorithm and evaluated for a 2D covering area mission in a continuous control
environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05812v1' target='_blank'>Multi-Agent Reinforcement Learning in Wireless Distributed Networks for
  6G</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiayi Zhang, Ziheng Liu, Yiyang Zhu, Enyu Shi, Bokai Xu, Chau Yuen, Dusit Niyato, Mérouane Debbah, Shi Jin, Bo Ai, Xuemin, Shen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-09 08:35:09</h6>
<p class='card-text'>The introduction of intelligent interconnectivity between the physical and
human worlds has attracted great attention for future sixth-generation (6G)
networks, emphasizing massive capacity, ultra-low latency, and unparalleled
reliability. Wireless distributed networks and multi-agent reinforcement
learning (MARL), both of which have evolved from centralized paradigms, are two
promising solutions for the great attention. Given their distinct capabilities,
such as decentralization and collaborative mechanisms, integrating these two
paradigms holds great promise for unleashing the full power of 6G, attracting
significant research and development attention. This paper provides a
comprehensive study on MARL-assisted wireless distributed networks for 6G. In
particular, we introduce the basic mathematical background and evolution of
wireless distributed networks and MARL, as well as demonstrate their
interrelationships. Subsequently, we analyze different structures of wireless
distributed networks from the perspectives of homogeneous and heterogeneous.
Furthermore, we introduce the basic concepts of MARL and discuss two typical
categories, including model-based and model-free. We then present critical
challenges faced by MARL-assisted wireless distributed networks, providing
important guidance and insights for actual implementation. We also explore an
interplay between MARL-assisted wireless distributed networks and emerging
techniques, such as information bottleneck and mirror learning, delivering
in-depth analyses and application scenarios. Finally, we outline several
compelling research directions for future MARL-assisted wireless distributed
networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05573v1' target='_blank'>Low-Rank Agent-Specific Adaptation (LoRASA) for Multi-Agent Policy
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Beining Zhang, Aditya Kapoor, Mingfei Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 13:57:53</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) often relies on \emph{parameter
sharing (PS)} to scale efficiently. However, purely shared policies can stifle
each agent's unique specialization, reducing overall performance in
heterogeneous environments. We propose \textbf{Low-Rank Agent-Specific
Adaptation (LoRASA)}, a novel approach that treats each agent's policy as a
specialized ``task'' fine-tuned from a shared backbone. Drawing inspiration
from parameter-efficient transfer methods, LoRASA appends small, low-rank
adaptation matrices to each layer of the shared policy, naturally inducing
\emph{parameter-space sparsity} that promotes both specialization and
scalability. We evaluate LoRASA on challenging benchmarks including the
StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent MuJoCo (MAMuJoCo),
implementing it atop widely used algorithms such as MAPPO and A2PO. Across
diverse tasks, LoRASA matches or outperforms existing baselines \emph{while
reducing memory and computational overhead}. Ablation studies on adapter rank,
placement, and timing validate the method's flexibility and efficiency. Our
results suggest LoRASA's potential to establish a new norm for MARL policy
parameterization: combining a shared foundation for coordination with low-rank
agent-specific refinements for individual specialization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05453v1' target='_blank'>LLM-Powered Decentralized Generative Agents with Adaptive Hierarchical
  Knowledge Graph for Cooperative Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanqing Yang, Jingdi Chen, Marie Siew, Tania Lorido-Botran, Carlee Joe-Wong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 05:26:02</h6>
<p class='card-text'>Developing intelligent agents for long-term cooperation in dynamic open-world
scenarios is a major challenge in multi-agent systems. Traditional Multi-agent
Reinforcement Learning (MARL) frameworks like centralized training
decentralized execution (CTDE) struggle with scalability and flexibility. They
require centralized long-term planning, which is difficult without custom
reward functions, and face challenges in processing multi-modal data. CTDE
approaches also assume fixed cooperation strategies, making them impractical in
dynamic environments where agents need to adapt and plan independently. To
address decentralized multi-agent cooperation, we propose Decentralized
Adaptive Knowledge Graph Memory and Structured Communication System (DAMCS) in
a novel Multi-agent Crafter environment. Our generative agents, powered by
Large Language Models (LLMs), are more scalable than traditional MARL agents by
leveraging external knowledge and language for long-term planning and
reasoning. Instead of fully sharing information from all past experiences,
DAMCS introduces a multi-modal memory system organized as a hierarchical
knowledge graph and a structured communication protocol to optimize agent
cooperation. This allows agents to reason from past interactions and share
relevant information efficiently. Experiments on novel multi-agent open-world
tasks show that DAMCS outperforms both MARL and LLM baselines in task
efficiency and collaboration. Compared to single-agent scenarios, the two-agent
scenario achieves the same goal with 63% fewer steps, and the six-agent
scenario with 74% fewer steps, highlighting the importance of adaptive memory
and structured communication in achieving long-term goals. We publicly release
our project at: https://happyeureka.github.io/damcs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04864v1' target='_blank'>$TAR^2$: Temporal-Agent Reward Redistribution for Optimal Policy
  Preservation in Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aditya Kapoor, Kale-ab Tessera, Mayank Baranwal, Harshad Khadilkar, Stefano Albrecht, Mingfei Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-07 12:07:57</h6>
<p class='card-text'>In cooperative multi-agent reinforcement learning (MARL), learning effective
policies is challenging when global rewards are sparse and delayed. This
difficulty arises from the need to assign credit across both agents and time
steps, a problem that existing methods often fail to address in episodic,
long-horizon tasks. We propose Temporal-Agent Reward Redistribution $TAR^2$, a
novel approach that decomposes sparse global rewards into agent-specific,
time-step-specific components, thereby providing more frequent and accurate
feedback for policy learning. Theoretically, we show that $TAR^2$ (i) aligns
with potential-based reward shaping, preserving the same optimal policies as
the original environment, and (ii) maintains policy gradient update directions
identical to those under the original sparse reward, ensuring unbiased credit
signals. Empirical results on two challenging benchmarks, SMACLite and Google
Research Football, demonstrate that $TAR^2$ significantly stabilizes and
accelerates convergence, outperforming strong baselines like AREL and STAS in
both learning speed and final performance. These findings establish $TAR^2$ as
a principled and practical solution for agent-temporal credit assignment in
sparse-reward multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04773v1' target='_blank'>An Extended Benchmarking of Multi-Agent Reinforcement Learning
  Algorithms in Complex Fully Cooperative Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:George Papadopoulos, Andreas Kontogiannis, Foteini Papadopoulou, Chaido Poulianou, Ioannis Koumentis, George Vouros</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-07 09:17:02</h6>
<p class='card-text'>Multi-Agent Reinforcement Learning (MARL) has recently emerged as a
significant area of research. However, MARL evaluation often lacks systematic
diversity, hindering a comprehensive understanding of algorithms' capabilities.
In particular, cooperative MARL algorithms are predominantly evaluated on
benchmarks such as SMAC and GRF, which primarily feature team game scenarios
without assessing adequately various aspects of agents' capabilities required
in fully cooperative real-world tasks such as multi-robot cooperation and
warehouse, resource management, search and rescue, and human-AI cooperation.
Moreover, MARL algorithms are mainly evaluated on low dimensional state spaces,
and thus their performance on high-dimensional (e.g., image) observations is
not well-studied. To fill this gap, this paper highlights the crucial need for
expanding systematic evaluation across a wider array of existing benchmarks. To
this end, we conduct extensive evaluation and comparisons of well-known MARL
algorithms on complex fully cooperative benchmarks, including tasks with images
as agents' observations. Interestingly, our analysis shows that many
algorithms, hailed as state-of-the-art on SMAC and GRF, may underperform
standard MARL baselines on fully cooperative benchmarks. Finally, towards more
systematic and better evaluation of cooperative MARL algorithms, we have
open-sourced PyMARLzoo+, an extension of the widely used (E)PyMARL libraries,
which addresses an open challenge from [TBG++21], facilitating seamless
integration and support with all benchmarks of PettingZoo, as well as
Overcooked, PressurePlate, Capture Target and Box Pushing.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04492v1' target='_blank'>Multi-Agent Reinforcement Learning with Focal Diversity Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Selim Furkan Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Zachary Yahn, Ling Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 20:44:26</h6>
<p class='card-text'>The advancement of Large Language Models (LLMs) and their finetuning
strategies has triggered the renewed interests in multi-agent reinforcement
learning. In this paper, we introduce a focal diversity-optimized multi-agent
reinforcement learning approach, coined as MARL-Focal, with three unique
characteristics. First, we develop an agent-fusion framework for encouraging
multiple LLM based agents to collaborate in producing the final inference
output for each LLM query. Second, we develop a focal-diversity optimized agent
selection algorithm that can choose a small subset of the available agents
based on how well they can complement one another to generate the query output.
Finally, we design a conflict-resolution method to detect output inconsistency
among multiple agents and produce our MARL-Focal output through reward-aware
and policy-adaptive inference fusion. Extensive evaluations on five benchmarks
show that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent
fusion model achieves performance improvement of 5.51\% compared to the best
individual LLM-agent and offers stronger robustness over the TruthfulQA
benchmark. Code is available at https://github.com/sftekin/rl-focal</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04281v1' target='_blank'>DECAF: Learning to be Fair in Multi-agent Resource Allocation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ashwin Kumar, William Yeoh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 18:29:11</h6>
<p class='card-text'>A wide variety of resource allocation problems operate under resource
constraints that are managed by a central arbitrator, with agents who evaluate
and communicate preferences over these resources. We formulate this broad class
of problems as Distributed Evaluation, Centralized Allocation (DECA) problems
and propose methods to learn fair and efficient policies in centralized
resource allocation. Our methods are applied to learning long-term fairness in
a novel and general framework for fairness in multi-agent systems. We show
three different methods based on Double Deep Q-Learning: (1) A joint weighted
optimization of fairness and utility, (2) a split optimization, learning two
separate Q-estimators for utility and fairness, and (3) an online policy
perturbation to guide existing black-box utility functions toward fair
solutions. Our methods outperform existing fair MARL approaches on multiple
resource allocation domains, even when evaluated using diverse fairness
functions, and allow for flexible online trade-offs between utility and
fairness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04028v1' target='_blank'>Deep Meta Coordination Graphs for Multi-agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nikunj Gupta, James Zachary Hare, Rajgopal Kannan, Viktor Prasanna</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 12:35:52</h6>
<p class='card-text'>This paper presents deep meta coordination graphs (DMCG) for learning
cooperative policies in multi-agent reinforcement learning (MARL). Coordination
graph formulations encode local interactions and accordingly factorize the
joint value function of all agents to improve efficiency in MARL. However,
existing approaches rely solely on pairwise relations between agents, which
potentially oversimplifies complex multi-agent interactions. DMCG goes beyond
these simple direct interactions by also capturing useful higher-order and
indirect relationships among agents. It generates novel graph structures
accommodating multiple types of interactions and arbitrary lengths of multi-hop
connections in coordination graphs to model such interactions. It then employs
a graph convolutional network module to learn powerful representations in an
end-to-end manner. We demonstrate its effectiveness in multiple coordination
problems in MARL where other state-of-the-art methods can suffer from sample
inefficiency or fail entirely. All codes can be found here:
https://github.com/Nikunj-Gupta/dmcg-marl.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03845v1' target='_blank'>PAGNet: Pluggable Adaptive Generative Networks for Information
  Completion in Multi-Agent Communication</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuohui Zhang, Bin Cheng, Zhipeng Wang, Yanmin Zhou, Gang Li, Ping Lu, Bin He, Jie Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 07:55:24</h6>
<p class='card-text'>For partially observable cooperative tasks, multi-agent systems must develop
effective communication and understand the interplay among agents in order to
achieve cooperative goals. However, existing multi-agent reinforcement learning
(MARL) with communication methods lack evaluation metrics for information
weights and information-level communication modeling. This causes agents to
neglect the aggregation of multiple messages, thereby significantly reducing
policy learning efficiency. In this paper, we propose pluggable adaptive
generative networks (PAGNet), a novel framework that integrates generative
models into MARL to enhance communication and decision-making. PAGNet enables
agents to synthesize global states representations from weighted local
observations and use these representations alongside learned communication
weights for coordinated decision-making. This pluggable approach reduces the
computational demands typically associated with the joint training of
communication and policy networks. Extensive experimental evaluations across
diverse benchmarks and communication scenarios demonstrate the significant
performance improvements achieved by PAGNet. Furthermore, we analyze the
emergent communication patterns and the quality of generated global states,
providing insights into operational mechanisms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.04399v1' target='_blank'>Online Location Planning for AI-Defined Vehicles: Optimizing Joint Tasks
  of Order Serving and Spatio-Temporal Heterogeneous Model Fine-Tuning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bokeng Zheng, Bo Rao, Tianxiang Zhu, Chee Wei Tan, Jingpu Duan, Zhi Zhou, Xu Chen, Xiaoxi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 07:23:40</h6>
<p class='card-text'>Advances in artificial intelligence (AI) including foundation models (FMs),
are increasingly transforming human society, with smart city driving the
evolution of urban living.Meanwhile, vehicle crowdsensing (VCS) has emerged as
a key enabler, leveraging vehicles' mobility and sensor-equipped capabilities.
In particular, ride-hailing vehicles can effectively facilitate flexible data
collection and contribute towards urban intelligence, despite resource
limitations. Therefore, this work explores a promising scenario, where
edge-assisted vehicles perform joint tasks of order serving and the emerging
foundation model fine-tuning using various urban data. However, integrating the
VCS AI task with the conventional order serving task is challenging, due to
their inconsistent spatio-temporal characteristics: (i) The distributions of
ride orders and data point-of-interests (PoIs) may not coincide in geography,
both following a priori unknown patterns; (ii) they have distinct forms of
temporal effects, i.e., prolonged waiting makes orders become instantly invalid
while data with increased staleness gradually reduces its utility for model
fine-tuning.To overcome these obstacles, we propose an online framework based
on multi-agent reinforcement learning (MARL) with careful augmentation. A new
quality-of-service (QoS) metric is designed to characterize and balance the
utility of the two joint tasks, under the effects of varying data volumes and
staleness. We also integrate graph neural networks (GNNs) with MARL to enhance
state representations, capturing graph-structured, time-varying dependencies
among vehicles and across locations. Extensive experiments on our testbed
simulator, utilizing various real-world foundation model fine-tuning tasks and
the New York City Taxi ride order dataset, demonstrate the advantage of our
proposed method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06835v1' target='_blank'>Reinforcement Learning on AYA Dyads to Enhance Medication Adherence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziping Xu, Hinal Jajal, Sung Won Choi, Inbal Nahum-Shani, Guy Shani, Alexandra M. Psihogios, Pei-Yao Hung, Susan Murphy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 02:27:35</h6>
<p class='card-text'>Medication adherence is critical for the recovery of adolescents and young
adults (AYAs) who have undergone hematopoietic cell transplantation (HCT).
However, maintaining adherence is challenging for AYAs after hospital
discharge, who experience both individual (e.g. physical and emotional
symptoms) and interpersonal barriers (e.g., relational difficulties with their
care partner, who is often involved in medication management). To optimize the
effectiveness of a three-component digital intervention targeting both members
of the dyad as well as their relationship, we propose a novel Multi-Agent
Reinforcement Learning (MARL) approach to personalize the delivery of
interventions. By incorporating the domain knowledge, the MARL framework, where
each agent is responsible for the delivery of one intervention component,
allows for faster learning compared with a flattened agent. Evaluation using a
dyadic simulator environment, based on real clinical data, shows a significant
improvement in medication adherence (approximately 3%) compared to purely
random intervention delivery. The effectiveness of this approach will be
further evaluated in an upcoming trial.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03723v1' target='_blank'>Speaking the Language of Teamwork: LLM-Guided Credit Assignment in
  Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhan Lin, Shuyang Shi, Yue Guo, Vaishnav Tadiparthi, Behdad Chalaki, Ehsan Moradi Pari, Simon Stepputtis, Woojun Kim, Joseph Campbell, Katia Sycara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 02:26:47</h6>
<p class='card-text'>Credit assignment, the process of attributing credit or blame to individual
agents for their contributions to a team's success or failure, remains a
fundamental challenge in multi-agent reinforcement learning (MARL),
particularly in environments with sparse rewards. Commonly-used approaches such
as value decomposition often lead to suboptimal policies in these settings, and
designing dense reward functions that align with human intuition can be complex
and labor-intensive. In this work, we propose a novel framework where a large
language model (LLM) generates dense, agent-specific rewards based on a natural
language description of the task and the overall team goal. By learning a
potential-based reward function over multiple queries, our method reduces the
impact of ranking errors while allowing the LLM to evaluate each agent's
contribution to the overall task. Through extensive experiments, we demonstrate
that our approach achieves faster convergence and higher policy returns
compared to state-of-the-art MARL baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03377v1' target='_blank'>Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement
  Learning Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abdullahi Isa Ahmed, El Mehdi Amhoud</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-05 17:16:40</h6>
<p class='card-text'>With the rapid development of next-generation Internet of Things (NG-IoT)
networks, the increasing number of connected devices has led to a surge in
power consumption. This rise in energy demand poses significant challenges to
resource availability and raises sustainability concerns for large-scale IoT
deployments. Efficient energy utilization in communication networks,
particularly for power-constrained IoT devices, has thus become a critical area
of research. In this paper, we deployed flying LoRa gateways (GWs) mounted on
unmanned aerial vehicles (UAVs) to collect data from LoRa end devices (EDs) and
transmit it to a central server. Our primary objective is to maximize the
global system energy efficiency (EE) of wireless LoRa networks by joint
optimization of transmission power (TP), spreading factor (SF), bandwidth (W),
and ED association. To solve this challenging problem, we model the problem as
a partially observable Markov decision process (POMDP), where each flying LoRa
GW acts as a learning agent using a cooperative Multi-Agent Reinforcement
Learning (MARL) approach under centralized training and decentralized execution
(CTDE). Simulation results demonstrate that our proposed method, based on the
multi-agent proximal policy optimization (MAPPO) algorithm, significantly
improves the global system EE and surpasses the conventional MARL schemes.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>