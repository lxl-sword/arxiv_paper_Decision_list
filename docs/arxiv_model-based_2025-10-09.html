<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>model-based - 2025-10-09</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>model-based - 2025-10-09</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04280v1' target='_blank'>A KL-regularization framework for learning to plan with adaptive priors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:√Ålvaro Serra-Gomez, Daniel Jarne Ornia, Dhruva Tirumala, Thomas Moerland</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 16:45:38</h6>
<p class='card-text'>Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.24804v1' target='_blank'>DyMoDreamer: World Modeling with Dynamic Modulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Boxuan Zhang, Runqing Wang, Wei Xiao, Weipu Zhang, Jian Sun, Gao Huang, Jie Chen, Gang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-29 13:54:42</h6>
<p class='card-text'>A critical bottleneck in deep reinforcement learning (DRL) is sample
inefficiency, as training high-performance agents often demands extensive
environmental interactions. Model-based reinforcement learning (MBRL) mitigates
this by building world models that simulate environmental dynamics and generate
synthetic experience, improving sample efficiency. However, conventional world
models process observations holistically, failing to decouple dynamic objects
and temporal features from static backgrounds. This approach is computationally
inefficient, especially for visual tasks where dynamic objects significantly
influence rewards and decision-making performance. To address this, we
introduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic
modulation mechanism to improve the extraction of dynamic features and enrich
the temporal information. DyMoDreamer employs differential observations derived
from a novel inter-frame differencing mask, explicitly encoding object-level
motion cues and temporal dynamics. Dynamic modulation is modeled as stochastic
categorical distributions and integrated into a recurrent state-space model
(RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments
demonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k
benchmark with a $156.6$\% mean human-normalized score, establishes a new
record of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\%
performance improvement after $1$M steps on the Crafter benchmark. Our code is
released at https://github.com/Ultraman-Tiga1/DyMoDreamer.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13095v2' target='_blank'>Empowering Multi-Robot Cooperation via Sequential World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijie Zhao, Honglei Guo, Shengqian Chen, Kaixuan Xu, Bo Jiang, Yuanheng Zhu, Dongbin Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 13:52:30</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) has shown significant potential in
robotics due to its high sample efficiency and planning capability. However,
extending MBRL to multi-robot cooperation remains challenging due to the
complexity of joint dynamics and the reliance on synchronous communication.
SeqWM employs independent, autoregressive agent-wise world models to represent
joint dynamics, where each agent generates its future trajectory and plans its
actions based on the predictions of its predecessors. This design lowers
modeling complexity, alleviates the reliance on communication synchronization,
and enables the emergence of advanced cooperative behaviors through explicit
intention sharing. Experiments in challenging simulated environments
(Bi-DexHands and Multi-Quad) demonstrate that SeqWM outperforms existing
state-of-the-art model-based and model-free baselines in both overall
performance and sample efficiency, while exhibiting advanced cooperative
behaviors such as predictive adaptation, temporal alignment, and role division.
Furthermore, SeqWM has been success fully deployed on physical quadruped
robots, demonstrating its effectiveness in real-world multi-robot systems.
Demos and code are available at: https://sites.google.com/view/seqwm-marl</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12531v1' target='_blank'>Pre-trained Visual Representations Generalize Where it Matters in
  Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Scott Jones, Liyou Zhou, Sebastian W. Pattinson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 00:13:14</h6>
<p class='card-text'>In visuomotor policy learning, the control policy for the robotic agent is
derived directly from visual inputs. The typical approach, where a policy and
vision encoder are trained jointly from scratch, generalizes poorly to novel
visual scene changes. Using pre-trained vision models (PVMs) to inform a policy
network improves robustness in model-free reinforcement learning (MFRL). Recent
developments in Model-based reinforcement learning (MBRL) suggest that MBRL is
more sample-efficient than MFRL. However, counterintuitively, existing work has
found PVMs to be ineffective in MBRL. Here, we investigate PVM's effectiveness
in MBRL, specifically on generalization under visual domain shifts. We show
that, in scenarios with severe shifts, PVMs perform much better than a baseline
model trained from scratch. We further investigate the effects of varying
levels of fine-tuning of PVMs. Our results show that partial fine-tuning can
maintain the highest average task performance under the most extreme
distribution shifts. Our results demonstrate that PVMs are highly successful in
promoting robustness in visual policy learning, providing compelling evidence
for their wider adoption in model-based robotic learning applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.06296v1' target='_blank'>Learning to Walk with Less: a Dyna-Style Approach to Quadrupedal
  Locomotion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Francisco Affonso, Felipe Andrade G. Tommaselli, Juliano Negri, Vivian S. Medeiros, Mateus V. Gasparino, Girish Chowdhary, Marcelo Becker</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-08 02:48:23</h6>
<p class='card-text'>Traditional RL-based locomotion controllers often suffer from low data
efficiency, requiring extensive interaction to achieve robust performance. We
present a model-based reinforcement learning (MBRL) framework that improves
sample efficiency for quadrupedal locomotion by appending synthetic data to the
end of standard rollouts in PPO-based controllers, following the Dyna-Style
paradigm. A predictive model, trained alongside the policy, generates
short-horizon synthetic transitions that are gradually integrated using a
scheduling strategy based on the policy update iterations. Through an ablation
study, we identified a strong correlation between sample efficiency and rollout
length, which guided the design of our experiments. We validated our approach
in simulation on the Unitree Go1 robot and showed that replacing part of the
simulated steps with synthetic ones not only mimics extended rollouts but also
improves policy return and reduces variance. Finally, we demonstrate that this
improvement transfers to the ability to track a wide range of locomotion
commands using fewer simulated steps.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.00215v2' target='_blank'>First Order Model-Based RL through Decoupled Backpropagation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joseph Amigo, Rooholla Khorrambakht, Elliot Chane-Sane, Nicolas Mansard, Ludovic Righetti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-29 19:55:25</h6>
<p class='card-text'>There is growing interest in reinforcement learning (RL) methods that
leverage the simulator's derivatives to improve learning efficiency. While
early gradient-based approaches have demonstrated superior performance compared
to derivative-free methods, accessing simulator gradients is often impractical
due to their implementation cost or unavailability. Model-based RL (MBRL) can
approximate these gradients via learned dynamics models, but the solver
efficiency suffers from compounding prediction errors during training rollouts,
which can degrade policy performance. We propose an approach that decouples
trajectory generation from gradient computation: trajectories are unrolled
using a simulator, while gradients are computed via backpropagation through a
learned differentiable model of the simulator. This hybrid design enables
efficient and consistent first-order policy optimization, even when simulator
gradients are unavailable, as well as learning a critic from simulation
rollouts, which is more accurate. Our method achieves the sample efficiency and
speed of specialized optimizers such as SHAC, while maintaining the generality
of standard approaches like PPO and avoiding ill behaviors observed in other
first-order MBRL methods. We empirically validate our algorithm on benchmark
control tasks and demonstrate its effectiveness on a real Go2 quadruped robot,
across both quadrupedal and bipedal locomotion tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.06994v1' target='_blank'>Learning-Enabled Adaptive Power Capping Scheme for Cloud Data Centers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yimeng Sun, Zhaohao Ding, Payman Dehghanian, Fei Teng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-09 14:11:37</h6>
<p class='card-text'>The rapid growth of the digital economy and artificial intelligence has
transformed cloud data centers into essential infrastructure with substantial
energy consumption and carbon emission, necessitating effective energy
management. However, existing methods face challenges such as incomplete
information, uncertain parameters, and dynamic environments, which hinder their
real-world implementation. This paper proposes an adaptive power capping
framework tailored to cloud data centers. By dynamically setting the energy
consumption upper bound, the power load of data centers can be reshaped to
align with the electricity price or other market signals. To this end, we
formulate the power capping problem as a partially observable Markov decision
process. Subsequently, we develop an uncertainty-aware model-based
reinforcement learning (MBRL) method to perceive the cloud data center
operational environment and optimize power-capping decisions. By incorporating
a two-stage uncertainty-aware optimization algorithm into the MBRL, we improve
its adaptability to the ever-changing environment. Additionally, we derive the
optimality gap of the proposed scheme under finite iterations, ensuring
effective decisions under complex and uncertain scenarios. The numerical
experiments validate the effectiveness of the proposed method using a cloud
data center operational environment simulator built on real-world production
traces from Alibaba, which demonstrates its potential as an efficient energy
management solution for cloud data centers.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.21782v1' target='_blank'>M3PO: Massively Multi-Task Model-Based Policy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aditya Narendra, Dmitry Makarov, Aleksandr Panov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-26 21:39:01</h6>
<p class='card-text'>We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a
scalable model-based reinforcement learning (MBRL) framework designed to
address sample inefficiency in single-task settings and poor generalization in
multi-task domains. Existing model-based approaches like DreamerV3 rely on
pixel-level generative models that neglect control-centric representations,
while model-free methods such as PPO suffer from high sample complexity and
weak exploration. M3PO integrates an implicit world model, trained to predict
task outcomes without observation reconstruction, with a hybrid exploration
strategy that combines model-based planning and model-free uncertainty-driven
bonuses. This eliminates the bias-variance trade-off in prior methods by using
discrepancies between model-based and model-free value estimates to guide
exploration, while maintaining stable policy updates through a trust-region
optimizer. M3PO provides an efficient and robust alternative to existing
model-based policy optimization approaches and achieves state-of-the-art
performance across multiple benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.12095v1' target='_blank'>DoublyAware: Dual Planning and Policy Awareness for Temporal Difference
  Learning in Humanoid Locomotion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Khang Nguyen, An T. Le, Jan Peters, Minh Nhat Vu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-12 11:39:21</h6>
<p class='card-text'>Achieving robust robot learning for humanoid locomotion is a fundamental
challenge in model-based reinforcement learning (MBRL), where environmental
stochasticity and randomness can hinder efficient exploration and learning
stability. The environmental, so-called aleatoric, uncertainty can be amplified
in high-dimensional action spaces with complex contact dynamics, and further
entangled with epistemic uncertainty in the models during learning phases. In
this work, we propose DoublyAware, an uncertainty-aware extension of Temporal
Difference Model Predictive Control (TD-MPC) that explicitly decomposes
uncertainty into two disjoint interpretable components, i.e., planning and
policy uncertainties. To handle the planning uncertainty, DoublyAware employs
conformal prediction to filter candidate trajectories using quantile-calibrated
risk bounds, ensuring statistical consistency and robustness against stochastic
dynamics. Meanwhile, policy rollouts are leveraged as structured informative
priors to support the learning phase with Group-Relative Policy Constraint
(GRPC) optimizers that impose a group-based adaptive trust-region in the latent
action space. This principled combination enables the robot agent to prioritize
high-confidence, high-reward behavior while maintaining effective, targeted
exploration under uncertainty. Evaluated on the HumanoidBench locomotion suite
with the Unitree 26-DoF H1-2 humanoid, DoublyAware demonstrates improved sample
efficiency, accelerated convergence, and enhanced motion feasibility compared
to RL baselines. Our simulation results emphasize the significance of
structured uncertainty modeling for data-efficient and reliable decision-making
in TD-MPC-based humanoid locomotion learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05419v1' target='_blank'>Dream to Generalize: Zero-Shot Model-Based Reinforcement Learning for
  Unseen Visual Distractions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jeongsoo Ha, Kyungsoo Kim, Yusung Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 00:39:03</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) has been used to efficiently solve
vision-based control tasks in highdimensional image observations. Although
recent MBRL algorithms perform well in trained observations, they fail when
faced with visual distractions in observations. These task-irrelevant
distractions (e.g., clouds, shadows, and light) may be constantly present in
real-world scenarios. In this study, we propose a novel self-supervised method,
Dream to Generalize (Dr. G), for zero-shot MBRL. Dr. G trains its encoder and
world model with dual contrastive learning which efficiently captures
task-relevant features among multi-view data augmentations. We also introduce a
recurrent state inverse dynamics model that helps the world model to better
understand the temporal structure. The proposed methods can enhance the
robustness of the world model against visual distractions. To evaluate the
generalization performance, we first train Dr. G on simple backgrounds and then
test it on complex natural video backgrounds in the DeepMind Control suite, and
the randomizing environments in Robosuite. Dr. G yields a performance
improvement of 117% and 14% over prior works, respectively. Our code is
open-sourced and available at https://github.com/JeongsooHa/DrG.git</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.02767v1' target='_blank'>Accelerating Model-Based Reinforcement Learning using Non-Linear
  Trajectory Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marco Cal√¨, Giulio Giacomuzzo, Ruggero Carli, Alberto Dalla Libera</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-03 11:30:59</h6>
<p class='card-text'>This paper addresses the slow policy optimization convergence of Monte Carlo
Probabilistic Inference for Learning Control (MC-PILCO), a state-of-the-art
model-based reinforcement learning (MBRL) algorithm, by integrating it with
iterative Linear Quadratic Regulator (iLQR), a fast trajectory optimization
method suitable for nonlinear systems. The proposed method, Exploration-Boosted
MC-PILCO (EB-MC-PILCO), leverages iLQR to generate informative, exploratory
trajectories and initialize the policy, significantly reducing the number of
required optimization steps. Experiments on the cart-pole task demonstrate that
EB-MC-PILCO accelerates convergence compared to standard MC-PILCO, achieving up
to $\bm{45.9\%}$ reduction in execution time when both methods solve the task
in four trials. EB-MC-PILCO also maintains a $\bm{100\%}$ success rate across
trials while solving the task faster, even in cases where MC-PILCO converges in
fewer iterations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.02205v2' target='_blank'>Bregman Centroid Guided Cross-Entropy Method</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuliang Gu, Hongpeng Cao, Marco Caccamo, Naira Hovakimyan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-02 19:44:40</h6>
<p class='card-text'>The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in
model-based reinforcement learning (MBRL), but its unimodal sampling strategy
often leads to premature convergence in multimodal landscapes. In this work, we
propose Bregman Centroid Guided CEM ($\mathcal{BC}$-EvoCEM), a lightweight
enhancement to ensemble CEM that leverages $\textit{Bregman centroids}$ for
principled information aggregation and diversity control.
$\textbf{$\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman
centroid across CEM workers and updates the least contributing ones by sampling
within a trust region around the centroid. Leveraging the duality between
Bregman divergences and exponential family distributions, we show that
$\textbf{$\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM
pipelines with negligible overhead. Empirical results on synthetic benchmarks,
a cluttered navigation task, and full MBRL pipelines demonstrate that
$\textbf{$\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution
quality, providing a simple yet effective upgrade for CEM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19698v2' target='_blank'>JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance
  Asymmetry in Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jing Yu Lim, Zarif Ikram, Samson Yu, Haozhe Ma, Tze-Yun Leong, Dianbo Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 08:52:45</h6>
<p class='card-text'>Recent advances in model-based reinforcement learning (MBRL) have achieved
super-human level performance on the Atari100k benchmark, driven by
reinforcement learning agents trained on powerful diffusion world models.
However, we identify that the current aggregates mask a major performance
asymmetry: MBRL agents dramatically outperform humans in some tasks despite
drastically underperforming in others, with the former inflating the aggregate
metrics. This is especially pronounced in pixel-based agents trained with
diffusion world models. In this work, we address the pronounced asymmetry
observed in pixel-based agents as an initial attempt to reverse the worrying
upward trend observed in them. We address the problematic aggregates by
delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal
importance on metrics from both sets. Next, we hypothesize this pronounced
asymmetry is due to the lack of temporally-structured latent space trained with
the World Model objective in pixel-based methods. Lastly, to address this
issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion
world model trained end-to-end with the self-consistency objective. JEDI
outperforms SOTA models in human-optimal tasks while staying competitive across
the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the
latest pixel-based diffusion baseline. Overall, our work rethinks what it truly
means to cross human-level performance in Atari100k.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16787v2' target='_blank'>Enter the Void - Planning to Seek Entropy When Reward is Scarce</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ashish Sundar, Chunbo Luo, Xiaoyang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 15:28:50</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) offers an intuitive way to increase
the sample efficiency of model-free RL methods by simultaneously training a
world model that learns to predict the future. MBRL methods have progressed by
largely prioritising the actor; optimising the world model learning has been
neglected meanwhile. Improving the fidelity of the world model and reducing its
time to convergence can yield significant downstream benefits, one of which is
improving the ensuing performance of any actor it may train. We propose a novel
approach that anticipates and actively seeks out high-entropy states using
short-horizon latent predictions generated by the world model, offering a
principled alternative to traditional curiosity-driven methods that chase
once-novel states well after they were stumbled into. While many model
predictive control (MPC) based methods offer similar alternatives, they
typically lack commitment, synthesising multi step plans after every step. To
mitigate this, we present a hierarchical planner that dynamically decides when
to replan, planning horizon length, and the weighting between reward and
entropy. While our method can theoretically be applied to any model that trains
its own actors with solely model generated data, we have applied it to just
Dreamer as a proof of concept. Our method finishes the Miniworld procedurally
generated mazes 50% faster than base Dreamer at convergence and the policy
trained in imagination converges in only 60% of the environment steps that base
Dreamer needs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16394v1' target='_blank'>Raw2Drive: Reinforcement Learning with Aligned World Models for
  End-to-End Autonomous Driving (in CARLA v2)</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenjie Yang, Xiaosong Jia, Qifeng Li, Xue Yang, Maoqing Yao, Junchi Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:46:53</h6>
<p class='card-text'>Reinforcement Learning (RL) can mitigate the causal confusion and
distribution shift inherent to imitation learning (IL). However, applying RL to
end-to-end autonomous driving (E2E-AD) remains an open problem for its training
difficulty, and IL is still the mainstream paradigm in both academia and
industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated
promising results in neural planning; however, these methods typically require
privileged information as input rather than raw sensor data. We fill this gap
by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently
train an auxiliary privileged world model paired with a neural planner that
uses privileged information as input. Subsequently, we introduce a raw sensor
world model trained via our proposed Guidance Mechanism, which ensures
consistency between the raw sensor world model and the privileged world model
during rollouts. Finally, the raw sensor world model combines the prior
knowledge embedded in the heads of the privileged world model to effectively
guide the training of the raw sensor policy. Raw2Drive is so far the only RL
based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it
achieves state-of-the-art performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15754v1' target='_blank'>Improving planning and MBRL with temporally-extended actions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Palash Chatterjee, Roni Khardon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 16:59:32</h6>
<p class='card-text'>Continuous time systems are often modeled using discrete time dynamics but
this requires a small simulation step to maintain accuracy. In turn, this
requires a large planning horizon which leads to computationally demanding
planning problems and reduced performance. Previous work in model free
reinforcement learning has partially addressed this issue using action repeats
where a policy is learned to determine a discrete action duration. Instead we
propose to control the continuous decision timescale directly by using
temporally-extended actions and letting the planner treat the duration of the
action as an additional optimization variable along with the standard action
variables. This additional structure has multiple advantages. It speeds up
simulation time of trajectories and, importantly, it allows for deep horizon
search in terms of primitive actions while using a shallow search depth in the
planner. In addition, in the model based reinforcement learning (MBRL) setting,
it reduces compounding errors from model learning and improves training time
for models. We show that this idea is effective and that the range for action
durations can be automatically selected using a multi-armed bandit formulation
and integrated into the MBRL framework. An extensive experimental evaluation
both in planning and in MBRL, shows that our approach yields faster planning,
better solutions, and that it enables solutions to problems that are not solved
in the standard formulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13709v1' target='_blank'>Policy-Driven World Model Adaptation for Robust Offline Model-based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiayu Chen, Aravind Venugopal, Jeff Schneider</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-19 20:14:33</h6>
<p class='card-text'>Offline reinforcement learning (RL) offers a powerful paradigm for
data-driven control. Compared to model-free approaches, offline model-based RL
(MBRL) explicitly learns a world model from a static dataset and uses it as a
surrogate simulator, improving data efficiency and enabling potential
generalization beyond the dataset support. However, most existing offline MBRL
methods follow a two-stage training procedure: first learning a world model by
maximizing the likelihood of the observed transitions, then optimizing a policy
to maximize its expected return under the learned model. This objective
mismatch results in a world model that is not necessarily optimized for
effective policy learning. Moreover, we observe that policies learned via
offline MBRL often lack robustness during deployment, and small adversarial
noise in the environment can lead to significant performance degradation. To
address these, we propose a framework that dynamically adapts the world model
alongside the policy under a unified learning objective aimed at improving
robustness. At the core of our method is a maximin optimization problem, which
we solve by innovatively utilizing Stackelberg learning dynamics. We provide
theoretical analysis to support our design and introduce computationally
efficient implementations. We benchmark our algorithm on twelve noisy D4RL
MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its
state-of-the-art performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13144v1' target='_blank'>Temporal Distance-aware Transition Augmentation for Offline Model-based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongsu Lee, Minhae Kwon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-19 14:11:14</h6>
<p class='card-text'>The goal of offline reinforcement learning (RL) is to extract a
high-performance policy from the fixed datasets, minimizing performance
degradation due to out-of-distribution (OOD) samples. Offline model-based RL
(MBRL) is a promising approach that ameliorates OOD issues by enriching
state-action transitions with augmentations synthesized via a learned dynamics
model. Unfortunately, seminal offline MBRL methods often struggle in
sparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL
framework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA),
that generates augmented transitions in a temporally structured latent space
rather than in raw state space. To model long-horizon behavior, TempDATA learns
a latent abstraction that captures a temporal distance from both trajectory and
transition levels of state space. Our experiments confirm that TempDATA
outperforms previous offline MBRL methods and achieves matching or surpassing
the performance of diffusion-based trajectory augmentation and goal-conditioned
RL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.01712v2' target='_blank'>World Model-Based Learning for Long-Term Age of Information Minimization
  in Vehicular Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lingyi Wang, Rashed Shelim, Walid Saad, Naren Ramakrishnan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-03 06:23:18</h6>
<p class='card-text'>Traditional reinforcement learning (RL)-based learning approaches for
wireless networks rely on expensive trial-and-error mechanisms and real-time
feedback based on extensive environment interactions, which leads to low data
efficiency and short-sighted policies. These limitations become particularly
problematic in complex, dynamic networks with high uncertainty and long-term
planning requirements. To address these limitations, in this paper, a novel
world model-based learning framework is proposed to minimize
packet-completeness-aware age of information (CAoI) in a vehicular network.
Particularly, a challenging representative scenario is considered pertaining to
a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network,
which is characterized by high mobility, frequent signal blockages, and
extremely short coherence time. Then, a world model framework is proposed to
jointly learn a dynamic model of the mmWave V2X environment and use it to
imagine trajectories for learning how to perform link scheduling. In
particular, the long-term policy is learned in differentiable imagined
trajectories instead of environment interactions. Moreover, owing to its
imagination abilities, the world model can jointly predict time-varying
wireless data and optimize link scheduling in real-world wireless and V2X
networks. Thus, during intervals without actual observations, the world model
remains capable of making efficient decisions. Extensive experiments are
performed on a realistic simulator based on Sionna that integrates
physics-based end-to-end channel modeling, ray-tracing, and scene geometries
with material properties. Simulation results show that the proposed world model
achieves a significant improvement in data efficiency, and achieves 26%
improvement and 16% improvement in CAoI, respectively, compared to the
model-based RL (MBRL) method and the model-free RL (MFRL) method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.16680v1' target='_blank'>Offline Robotic World Model: Learning Robotic Policies without a Physics
  Simulator</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenhao Li, Andreas Krause, Marco Hutter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-23 12:58:15</h6>
<p class='card-text'>Reinforcement Learning (RL) has demonstrated impressive capabilities in
robotic control but remains challenging due to high sample complexity, safety
concerns, and the sim-to-real gap. While offline RL eliminates the need for
risky real-world exploration by learning from pre-collected data, it suffers
from distributional shift, limiting policy generalization. Model-Based RL
(MBRL) addresses this by leveraging predictive models for synthetic rollouts,
yet existing approaches often lack robust uncertainty estimation, leading to
compounding errors in offline settings. We introduce Offline Robotic World
Model (RWM-O), a model-based approach that explicitly estimates epistemic
uncertainty to improve policy learning without reliance on a physics simulator.
By integrating these uncertainty estimates into policy optimization, our
approach penalizes unreliable transitions, reducing overfitting to model errors
and enhancing stability. Experimental results show that RWM-O improves
generalization and safety, enabling policy learning purely from real-world data
and advancing scalable, data-efficient RL for robotics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.16588v1' target='_blank'>Data-Assimilated Model-Based Reinforcement Learning for Partially
  Observed Chaotic Flows</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Defne E. Ozan, Andrea N√≥voa, Luca Magri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-23 10:12:53</h6>
<p class='card-text'>The goal of many applications in energy and transport sectors is to control
turbulent flows. However, because of chaotic dynamics and high dimensionality,
the control of turbulent flows is exceedingly difficult. Model-free
reinforcement learning (RL) methods can discover optimal control policies by
interacting with the environment, but they require full state information,
which is often unavailable in experimental settings. We propose a
data-assimilated model-based RL (DA-MBRL) framework for systems with partial
observability and noisy measurements. Our framework employs a control-aware
Echo State Network for data-driven prediction of the dynamics, and integrates
data assimilation with an Ensemble Kalman Filter for real-time state
estimation. An off-policy actor-critic algorithm is employed to learn optimal
control strategies from state estimates. The framework is tested on the
Kuramoto-Sivashinsky equation, demonstrating its effectiveness in stabilizing a
spatiotemporally chaotic flow from noisy and partial measurements.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.06721v1' target='_blank'>Learning global control of underactuated systems with Model-Based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Niccol√≤ Turcato, Marco Cal√¨, Alberto Dalla Libera, Giulio Giacomuzzo, Ruggero Carli, Diego Romeres</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-09 09:20:37</h6>
<p class='card-text'>This short paper describes our proposed solution for the third edition of the
"AI Olympics with RealAIGym" competition, held at ICRA 2025. We employed
Monte-Carlo Probabilistic Inference for Learning Control (MC-PILCO), an MBRL
algorithm recognized for its exceptional data efficiency across various
low-dimensional robotic tasks, including cart-pole, ball \& plate, and Furuta
pendulum systems. MC-PILCO optimizes a system dynamics model using interaction
data, enabling policy refinement through simulation rather than direct system
data optimization. This approach has proven highly effective in physical
systems, offering greater data efficiency than Model-Free (MF) alternatives.
Notably, MC-PILCO has previously won the first two editions of this
competition, demonstrating its robustness in both simulated and real-world
environments. Besides briefly reviewing the algorithm, we discuss the most
critical aspects of the MC-PILCO implementation in the tasks at hand: learning
a global policy for the pendubot and acrobot systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.04164v3' target='_blank'>MInCo: Mitigating Information Conflicts in Distracted Visual Model-based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shiguang Sun, Hanbo Zhang, Zeyang Liu, Xinrui Yang, Lipeng Wan, Xingyu Chen, Xuguang Lan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-05 12:57:31</h6>
<p class='card-text'>Existing visual model-based reinforcement learning (MBRL) algorithms with
observation reconstruction often suffer from information conflicts, making it
difficult to learn compact representations and hence result in less robust
policies, especially in the presence of task-irrelevant visual distractions. In
this paper, we first reveal that the information conflicts in current visual
MBRL algorithms stem from visual representation learning and latent dynamics
modeling with an information-theoretic perspective. Based on this finding, we
present a new algorithm to resolve information conflicts for visual MBRL, named
MInCo, which mitigates information conflicts by leveraging negative-free
contrastive learning, aiding in learning invariant representation and robust
policies despite noisy observations. To prevent the dominance of visual
representation learning, we introduce time-varying reweighting to bias the
learning towards dynamics modeling as training proceeds. We evaluate our method
on several robotic control tasks with dynamic background distractions. Our
experiments demonstrate that MInCo learns invariant representations against
background noise and consistently outperforms current state-of-the-art visual
MBRL methods. Code is available at https://github.com/ShiguangSun/minco.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.21983v2' target='_blank'>Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams
  and Teams of LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abed Kareem Musaffar, Anand Gokhale, Sirui Zeng, Rasta Tadayon, Xifeng Yan, Ambuj Singh, Francesco Bullo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-27 21:01:02</h6>
<p class='card-text'>As artificial intelligence (AI) assistants become more widely adopted in
safety-critical domains, it becomes important to develop safeguards against
potential failures or adversarial attacks. A key prerequisite to developing
these safeguards is understanding the ability of these AI assistants to mislead
human teammates. We investigate this attack problem within the context of an
intellective strategy game where a team of three humans and one AI assistant
collaborate to answer a series of trivia questions. Unbeknownst to the humans,
the AI assistant is adversarial. Leveraging techniques from Model-Based
Reinforcement Learning (MBRL), the AI assistant learns a model of the humans'
trust evolution and uses that model to manipulate the group decision-making
process to harm the team. We evaluate two models -- one inspired by literature
and the other data-driven -- and find that both can effectively harm the human
team. Moreover, we find that in this setting our data-driven model is capable
of accurately predicting how human agents appraise their teammates given
limited information on prior interactions. Finally, we compare the performance
of state-of-the-art LLM models to human agents on our influence allocation task
to evaluate whether the LLMs allocate influence similarly to humans or if they
are more robust to our attack. These results enhance our understanding of
decision-making dynamics in small human-AI teams and lay the foundation for
defense strategies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.20462v1' target='_blank'>Multi-agent Uncertainty-Aware Pessimistic Model-Based Reinforcement
  Learning for Connected Autonomous Vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruoqi Wen, Rongpeng Li, Xing Xu, Zhifeng Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-26 11:49:02</h6>
<p class='card-text'>Deep Reinforcement Learning (DRL) holds significant promise for achieving
human-like Autonomous Vehicle (AV) capabilities, but suffers from low sample
efficiency and challenges in reward design. Model-Based Reinforcement Learning
(MBRL) offers improved sample efficiency and generalizability compared to
Model-Free Reinforcement Learning (MFRL) in various multi-agent decision-making
scenarios. Nevertheless, MBRL faces critical difficulties in estimating
uncertainty during the model learning phase, thereby limiting its scalability
and applicability in real-world scenarios. Additionally, most Connected
Autonomous Vehicle (CAV) studies focus on single-agent decision-making, while
existing multi-agent MBRL solutions lack computationally tractable algorithms
with Probably Approximately Correct (PAC) guarantees, an essential factor for
ensuring policy reliability with limited training data. To address these
challenges, we propose MA-PMBRL, a novel Multi-Agent Pessimistic Model-Based
Reinforcement Learning framework for CAVs, incorporating a max-min optimization
approach to enhance robustness and decision-making. To mitigate the inherent
subjectivity of uncertainty estimation in MBRL and avoid incurring catastrophic
failures in AV, MA-PMBRL employs a pessimistic optimization framework combined
with Projected Gradient Descent (PGD) for both model and policy learning.
MA-PMBRL also employs general function approximations under partial dataset
coverage to enhance learning efficiency and system-level performance. By
bounding the suboptimality of the resulting policy under mild theoretical
assumptions, we successfully establish PAC guarantees for MA-PMBRL,
demonstrating that the proposed framework represents a significant step toward
scalable, efficient, and reliable multi-agent decision-making for CAVs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.20139v1' target='_blank'>Look Before Leap: Look-Ahead Planning with Uncertainty in Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongshuai Liu, Xin Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-26 01:07:35</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) has demonstrated superior sample
efficiency compared to model-free reinforcement learning (MFRL). However, the
presence of inaccurate models can introduce biases during policy learning,
resulting in misleading trajectories. The challenge lies in obtaining accurate
models due to limited diverse training data, particularly in regions with
limited visits (uncertain regions). Existing approaches passively quantify
uncertainty after sample generation, failing to actively collect uncertain
samples that could enhance state coverage and improve model accuracy. Moreover,
MBRL often faces difficulties in making accurate multi-step predictions,
thereby impacting overall performance. To address these limitations, we propose
a novel framework for uncertainty-aware policy optimization with model-based
exploratory planning. In the model-based planning phase, we introduce an
uncertainty-aware k-step lookahead planning approach to guide action selection
at each step. This process involves a trade-off analysis between model
uncertainty and value function approximation error, effectively enhancing
policy performance. In the policy optimization phase, we leverage an
uncertainty-driven exploratory policy to actively collect diverse training
samples, resulting in improved model accuracy and overall performance of the RL
agent. Our approach offers flexibility and applicability to tasks with varying
state/action spaces and reward structures. We validate its effectiveness
through experiments on challenging robotic manipulation tasks and Atari games,
surpassing state-of-the-art methods with fewer interactions, thereby leading to
significant performance improvements.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.17693v1' target='_blank'>Conditional Diffusion Model with OOD Mitigation as High-Dimensional
  Offline Resource Allocation Planner in Clustered Ad Hoc Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kechen Meng, Sinuo Zhang, Rongpeng Li, Chan Wang, Ming Lei, Zhifeng Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-22 08:27:09</h6>
<p class='card-text'>Due to network delays and scalability limitations, clustered ad hoc networks
widely adopt Reinforcement Learning (RL) for on-demand resource allocation.
Albeit its demonstrated agility, traditional Model-Free RL (MFRL) solutions
struggle to tackle the huge action space, which generally explodes
exponentially along with the number of resource allocation units, enduring low
sampling efficiency and high interaction cost. In contrast to MFRL, Model-Based
RL (MBRL) offers an alternative solution to boost sample efficiency and
stabilize the training by explicitly leveraging a learned environment model.
However, establishing an accurate dynamic model for complex and noisy
environments necessitates a careful balance between model accuracy and
computational complexity $\&$ stability. To address these issues, we propose a
Conditional Diffusion Model Planner (CDMP) for high-dimensional offline
resource allocation in clustered ad hoc networks. By leveraging the astonishing
generative capability of Diffusion Models (DMs), our approach enables the
accurate modeling of high-quality environmental dynamics while leveraging an
inverse dynamics model to plan a superior policy. Beyond simply adopting DMs in
offline RL, we further incorporate the CDMP algorithm with a theoretically
guaranteed, uncertainty-aware penalty metric, which theoretically and
empirically manifests itself in mitigating the Out-of-Distribution
(OOD)-induced distribution shift issue underlying scarce training data.
Extensive experiments also show that our model outperforms MFRL in average
reward and Quality of Service (QoS) while demonstrating comparable performance
to other MBRL algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09719v1' target='_blank'>Towards Causal Model-Based Policy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alberto Caron, Vasilios Mavroudis, Chris Hicks</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 18:09:02</h6>
<p class='card-text'>Real-world decision-making problems are often marked by complex, uncertain
dynamics that can shift or break under changing conditions. Traditional
Model-Based Reinforcement Learning (MBRL) approaches learn predictive models of
environment dynamics from queried trajectories and then use these models to
simulate rollouts for policy optimization. However, such methods do not account
for the underlying causal mechanisms that govern the environment, and thus
inadvertently capture spurious correlations, making them sensitive to
distributional shifts and limiting their ability to generalize. The same
naturally holds for model-free approaches. In this work, we introduce Causal
Model-Based Policy Optimization (C-MBPO), a novel framework that integrates
causal learning into the MBRL pipeline to achieve more robust, explainable, and
generalizable policy learning algorithms.
  Our approach centers on first inferring a Causal Markov Decision Process
(C-MDP) by learning a local Structural Causal Model (SCM) of both the state and
reward transition dynamics from trajectories gathered online. C-MDPs differ
from classic MDPs in that we can decompose causal dependencies in the
environment dynamics via specifying an associated Causal Bayesian Network.
C-MDPs allow for targeted interventions and counterfactual reasoning, enabling
the agent to distinguish between mere statistical correlations and causal
relationships. The learned SCM is then used to simulate counterfactual
on-policy transitions and rewards under hypothetical actions (or
``interventions"), thereby guiding policy optimization more effectively. The
resulting policy learned by C-MBPO can be shown to be robust to a class of
distributional shifts that affect spurious, non-causal relationships in the
dynamics. We demonstrate this through some simple experiments involving near
and far OOD dynamics drifts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05573v1' target='_blank'>InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle
  Exploration through Curiosity Driven Generalized World Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Feeza Khan Khanzada, Jaerock Kwon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 16:56:00</h6>
<p class='card-text'>Model-based Reinforcement Learning (MBRL) has emerged as a promising paradigm
for autonomous driving, where data efficiency and robustness are critical. Yet,
existing solutions often rely on carefully crafted, task specific extrinsic
rewards, limiting generalization to new tasks or environments. In this paper,
we propose InDRiVE (Intrinsic Disagreement based Reinforcement for Vehicle
Exploration), a method that leverages purely intrinsic, disagreement based
rewards within a Dreamer based MBRL framework. By training an ensemble of world
models, the agent actively explores high uncertainty regions of environments
without any task specific feedback. This approach yields a task agnostic latent
representation, allowing for rapid zero shot or few shot fine tuning on
downstream driving tasks such as lane following and collision avoidance.
Experimental results in both seen and unseen environments demonstrate that
InDRiVE achieves higher success rates and fewer infractions compared to
DreamerV2 and DreamerV3 baselines despite using significantly fewer training
steps. Our findings highlight the effectiveness of purely intrinsic exploration
for learning robust vehicle control behaviors, paving the way for more scalable
and adaptable autonomous driving systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01178v1' target='_blank'>Differentiable Information Enhanced Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoyuan Zhang, Xinyan Cai, Bo Liu, Weidong Huang, Song-Chun Zhu, Siyuan Qi, Yaodong Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 04:51:40</h6>
<p class='card-text'>Differentiable environments have heralded new possibilities for learning
control policies by offering rich differentiable information that facilitates
gradient-based methods. In comparison to prevailing model-free reinforcement
learning approaches, model-based reinforcement learning (MBRL) methods exhibit
the potential to effectively harness the power of differentiable information
for recovering the underlying physical dynamics. However, this presents two
primary challenges: effectively utilizing differentiable information to 1)
construct models with more accurate dynamic prediction and 2) enhance the
stability of policy training. In this paper, we propose a Differentiable
Information Enhanced MBRL method, MB-MIX, to address both challenges. Firstly,
we adopt a Sobolev model training approach that penalizes incorrect model
gradient outputs, enhancing prediction accuracy and yielding more precise
models that faithfully capture system dynamics. Secondly, we introduce mixing
lengths of truncated learning windows to reduce the variance in policy gradient
estimation, resulting in improved stability during policy learning. To validate
the effectiveness of our approach in differentiable environments, we provide
theoretical analysis and empirical results. Notably, our approach outperforms
previous model-based and model-free methods, in multiple challenging tasks
involving controllable rigid robots such as humanoid robots' motion control and
deformable object manipulation.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>