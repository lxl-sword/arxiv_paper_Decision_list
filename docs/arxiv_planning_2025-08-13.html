<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-08-13</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-08-13</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.09054v1' target='_blank'>CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive
  Maintenance Using Deep Neural Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Debdeep Mukherjee, Eduardo Di Santi, Cl√©ment Lefebvre, Nenad Mijatovic, Victor Martin, Thierry Josse, Jonathan Brown, Kenza Saiah</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 16:13:51</h6>
<p class='card-text'>Track circuits are critical for railway operations, acting as the main
signalling sub-system to locate trains. Continuous Variable Current Modulation
(CVCM) is one such technology. Like any field-deployed, safety-critical asset,
it can fail, triggering cascading disruptions. Many failures originate as
subtle anomalies that evolve over time, often not visually apparent in
monitored signals. Conventional approaches, which rely on clear signal changes,
struggle to detect them early. Early identification of failure types is
essential to improve maintenance planning, minimising downtime and revenue
loss. Leveraging deep neural networks, we propose a predictive maintenance
framework that classifies anomalies well before they escalate into failures.
Validated on 10 CVCM failure cases across different installations, the method
is ISO-17359 compliant and outperforms conventional techniques, achieving
99.31% overall accuracy with detection within 1% of anomaly onset. Through
conformal prediction, we provide uncertainty estimates, reaching 99% confidence
with consistent coverage across classes. Given CVCMs global deployment, the
approach is scalable and adaptable to other track circuits and railway systems,
enhancing operational reliability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.09027v1' target='_blank'>A First Look at Predictability and Explainability of Pre-request
  Passenger Waiting Time in Ridesharing Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jie Wang, Guang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 15:42:14</h6>
<p class='card-text'>Passenger waiting time prediction plays a critical role in enhancing both
ridesharing user experience and platform efficiency. While most existing
research focuses on post-request waiting time prediction with knowing the
matched driver information, pre-request waiting time prediction (i.e., before
submitting a ride request and without matching a driver) is also important, as
it enables passengers to plan their trips more effectively and enhance the
experience of both passengers and drivers. However, it has not been fully
studied by existing works. In this paper, we take the first step toward
understanding the predictability and explainability of pre-request passenger
waiting time in ridesharing systems. Particularly, we conduct an in-depth
data-driven study to investigate the impact of demand&supply dynamics on
passenger waiting time. Based on this analysis and feature engineering, we
propose FiXGBoost, a novel feature interaction-based XGBoost model designed to
predict waiting time without knowing the assigned driver information. We
further perform an importance analysis to quantify the contribution of each
factor. Experiments on a large-scale real-world ridesharing dataset including
over 30 million trip records show that our FiXGBoost can achieve a good
performance for pre-request passenger waiting time prediction with high
explainability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.09003v1' target='_blank'>Large Scale Robotic Material Handling: Learning, Planning, and Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Filippo A. Spinelli, Yifan Zhai, Fang Nan, Pascal Egli, Julian Nubert, Thilo Bleumer, Lukas Miller, Ferdinand Hofmann, Marco Hutter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 15:13:52</h6>
<p class='card-text'>Bulk material handling involves the efficient and precise moving of large
quantities of materials, a core operation in many industries, including cargo
ship unloading, waste sorting, construction, and demolition. These repetitive,
labor-intensive, and safety-critical operations are typically performed using
large hydraulic material handlers equipped with underactuated grippers. In this
work, we present a comprehensive framework for the autonomous execution of
large-scale material handling tasks. The system integrates specialized modules
for environment perception, pile attack point selection, path planning, and
motion control. The main contributions of this work are two reinforcement
learning-based modules: an attack point planner that selects optimal grasping
locations on the material pile to maximize removal efficiency and minimize the
number of scoops, and a robust trajectory following controller that addresses
the precision and safety challenges associated with underactuated grippers in
movement, while utilizing their free-swinging nature to release material
through dynamic throwing. We validate our framework through real-world
experiments on a 40 t material handler in a representative worksite, focusing
on two key tasks: high-throughput bulk pile management and high-precision truck
loading. Comparative evaluations against human operators demonstrate the
system's effectiveness in terms of precision, repeatability, and operational
safety. To the best of our knowledge, this is the first complete automation of
material handling tasks on a full scale.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08978v1' target='_blank'>TaoCache: Structure-Maintained Video Generation Acceleration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhentao Fan, Zongzuo Wang, Weiwei Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 14:40:36</h6>
<p class='card-text'>Existing cache-based acceleration methods for video diffusion models
primarily skip early or mid denoising steps, which often leads to structural
discrepancies relative to full-timestep generation and can hinder instruction
following and character consistency. We present TaoCache, a training-free,
plug-and-play caching strategy that, instead of residual-based caching, adopts
a fixed-point perspective to predict the model's noise output and is
specifically effective in late denoising stages. By calibrating cosine
similarities and norm ratios of consecutive noise deltas, TaoCache preserves
high-resolution structure while enabling aggressive skipping. The approach is
orthogonal to complementary accelerations such as Pyramid Attention Broadcast
(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.
Across Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially
higher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the
same speedups.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08976v1' target='_blank'>Urban-STA4CLC: Urban Theory-Informed Spatio-Temporal Attention Model for
  Predicting Post-Disaster Commercial Land Use Change</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyi Guo, Yan Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 14:39:42</h6>
<p class='card-text'>Natural disasters such as hurricanes and wildfires increasingly introduce
unusual disturbance on economic activities, which are especially likely to
reshape commercial land use pattern given their sensitive to customer
visitation. However, current modeling approaches are limited in capturing such
complex interplay between human activities and commercial land use change under
and following disturbances. Such interactions have been more effectively
captured in current resilient urban planning theories. This study designs and
calibrates a Urban Theory-Informed Spatio-Temporal Attention Model for
Predicting Post-Disaster Commercial Land Use Change (Urban-STA4CLC) to predict
both the yearly decline and expansion of commercial land use at census block
level under cumulative impact of disasters on human activities over two years.
Guided by urban theories, Urban-STA4CLC integrates both spatial and temporal
attention mechanisms with three theory-informed modules. Resilience theory
guides a disaster-aware temporal attention module that captures visitation
dynamics. Spatial economic theory informs a multi-relational spatial attention
module for inter-block representation. Diffusion theory contributes a
regularization term that constrains land use transitions. The model performs
significantly better than non-theoretical baselines in predicting commercial
land use change under the scenario of recurrent hurricanes, with around 19%
improvement in F1 score (0.8763). The effectiveness of the theory-guided
modules was further validated through ablation studies. The research
demonstrates that embedding urban theory into commercial land use modeling
models may substantially enhance the capacity to capture its gains and losses.
These advances in commercial land use modeling contribute to land use research
that accounts for cumulative impacts of recurrent disasters and shifts in
economic activity patterns.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08916v1' target='_blank'>Automatic and standardized surgical reporting for central nervous system
  tumors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:David Bouget, Mathilde Gajda Faanes, Asgeir Store Jakola, Frederik Barkhof, Hilko Ardon, Lorenzo Bello, Mitchel S. Berger, Shawn L. Hervey-Jumper, Julia Furtner, Albert J. S. Idema, Barbara Kiesel, Georg Widhalm, Rishi Nandoe Tewarie, Emmanuel Mandonnet, Pierre A. Robe, Michiel Wagemakers, Timothy R. Smith, Philip C. De Witt Hamer, Ole solheim, Ingerid Reinertsen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 13:08:49</h6>
<p class='card-text'>Magnetic resonance (MR) imaging is essential for evaluating central nervous
system (CNS) tumors, guiding surgical planning, treatment decisions, and
assessing postoperative outcomes and complication risks. While recent work has
advanced automated tumor segmentation and report generation, most efforts have
focused on preoperative data, with limited attention to postoperative imaging
analysis. This study introduces a comprehensive pipeline for standardized
postsurtical reporting in CNS tumors. Using the Attention U-Net architecture,
segmentation models were trained for the preoperative (non-enhancing) tumor
core, postoperative contrast-enhancing residual tumor, and resection cavity.
Additionally, MR sequence classification and tumor type identification for
contrast-enhancing lesions were explored using the DenseNet architecture. The
models were integrated into a reporting pipeline, following the RANO 2.0
guidelines. Training was conducted on multicentric datasets comprising 2000 to
7000 patients, using a 5-fold cross-validation. Evaluation included patient-,
voxel-, and object-wise metrics, with benchmarking against the latest BraTS
challenge results. The segmentation models achieved average voxel-wise Dice
scores of 87%, 66%, 70%, and 77% for the tumor core, non-enhancing tumor core,
contrast-enhancing residual tumor, and resection cavity, respectively.
Classification models reached 99.5% balanced accuracy in MR sequence
classification and 80% in tumor type classification. The pipeline presented in
this study enables robust, automated segmentation, MR sequence classification,
and standardized report generation aligned with RANO 2.0 guidelines, enhancing
postoperative evaluation and clinical decision-making. The proposed models and
methods were integrated into Raidionics, open-source software platform for CNS
tumor analysis, now including a dedicated module for postsurgical analysis.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08816v1' target='_blank'>Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval
  Augmented Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuechen Wang, Yuming Qiao, Dan Meng, Jun Yang, Haonan Lu, Zhenyu Yang, Xudong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 10:17:12</h6>
<p class='card-text'>Multimodal Retrieval-Augmented Generation (mRAG) has emerged as a promising
solution to address the temporal limitations of Multimodal Large Language
Models (MLLMs) in real-world scenarios like news analysis and trending topics.
However, existing approaches often suffer from rigid retrieval strategies and
under-utilization of visual information. To bridge this gap, we propose
E-Agent, an agent framework featuring two key innovations: a mRAG planner
trained to dynamically orchestrate multimodal tools based on contextual
reasoning, and a task executor employing tool-aware execution sequencing to
implement optimized mRAG workflows. E-Agent adopts a one-time mRAG planning
strategy that enables efficient information retrieval while minimizing
redundant tool invocations. To rigorously assess the planning capabilities of
mRAG systems, we introduce the Real-World mRAG Planning (RemPlan) benchmark.
This novel benchmark contains both retrieval-dependent and
retrieval-independent question types, systematically annotated with essential
retrieval tools required for each instance. The benchmark's explicit mRAG
planning annotations and diverse question design enhance its practical
relevance by simulating real-world scenarios requiring dynamic mRAG decisions.
Experiments across RemPlan and three established benchmarks demonstrate
E-Agent's superiority: 13% accuracy gain over state-of-the-art mRAG methods
while reducing redundant searches by 37%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08748v1' target='_blank'>Visual Prompting for Robotic Manipulation with Annotation-Guided
  Pick-and-Place Using ACT</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhammad A. Muttaqien, Tomohiro Motoda, Ryo Hanai, Yukiyasu Domae</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 08:45:09</h6>
<p class='card-text'>Robotic pick-and-place tasks in convenience stores pose challenges due to
dense object arrangements, occlusions, and variations in object properties such
as color, shape, size, and texture. These factors complicate trajectory
planning and grasping. This paper introduces a perception-action pipeline
leveraging annotation-guided visual prompting, where bounding box annotations
identify both pickable objects and placement locations, providing structured
spatial guidance. Instead of traditional step-by-step planning, we employ
Action Chunking with Transformers (ACT) as an imitation learning algorithm,
enabling the robotic arm to predict chunked action sequences from human
demonstrations. This facilitates smooth, adaptive, and data-driven
pick-and-place operations. We evaluate our system based on success rate and
visual analysis of grasping behavior, demonstrating improved grasp accuracy and
adaptability in retail environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08745v1' target='_blank'>Comprehensive Comparison Network: a framework for locality-aware,
  routes-comparable and interpretable route recommendation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chao Chen, Longfei Xu, Hanyu Guo, Chengzhang Wang, Ying Wang, Kaikui Liu, Xiangxiang Chu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 08:40:52</h6>
<p class='card-text'>Route recommendation (RR) is a core task of route planning in the Amap app,
with the goal of recommending the optimal route among candidate routes to
users. Unlike traditional recommendation methods, insights into the local
quality of routes and comparisons between candidate routes are crucial for
enhancing recommendation performance but often overlooked in previous studies.
To achieve these, we propose a novel model called Comprehensive Comparison
Network (CCN). CCN not only uses query-level features (e.g. user features) and
item-level features (e.g. route features, item embedding) that are common in
traditional recommendations, but also introduces comparison-level features
which describe the non-overlapping segments between different routes to capture
the local quality of routes. The key component Comprehensive Comparison Block
(CCB) in CCN is designed to enable comparisons between routes. CCB includes a
Comprehensive Comparison Operator (CCO) and a multi-scenario MLP, which can
update the representations of candidate routes based on a comprehensive
comparison. By stacking multiple CCBs, CCN can determine the final scores of
candidate routes and recommend the optimal one to the user. Additionally, since
routes directly affect the costs and risks experienced by users, the RR model
must be interpretable for online deployment. Therefore, we designed an
interpretable pair scoring network to achieve interpretability. Both offline
and online experiments demonstrate that CCN significantly improves RR
performance and exhibits strong interpretability. CCN has been fully deployed
in the Amap app for over a year, providing stable and optimal benefits for
route recommendations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08687v1' target='_blank'>Expert-Guided Diffusion Planner for Auto-bidding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yunshan Peng, Wenzheng Shu, Jiahao Sun, Yanxiang Zeng, Jinan Pang, Wentao Bai, Yunke Bai, Xialong Liu, Peng Jiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 07:23:51</h6>
<p class='card-text'>Auto-bidding is extensively applied in advertising systems, serving a
multitude of advertisers. Generative bidding is gradually gaining traction due
to its robust planning capabilities and generalizability. In contrast to
traditional reinforcement learning-based bidding, generative bidding does not
rely on the Markov Decision Process (MDP) exhibiting superior planning
capabilities in long-horizon scenarios. Conditional diffusion modeling
approaches have demonstrated significant potential in the realm of
auto-bidding. However, relying solely on return as the optimality condition is
weak to guarantee the generation of genuinely optimal decision sequences,
lacking personalized structural information. Moreover, diffusion models' t-step
autoregressive generation mechanism inherently carries timeliness risks. To
address these issues, we propose a novel conditional diffusion modeling method
based on expert trajectory guidance combined with a skip-step sampling strategy
to enhance generation efficiency. We have validated the effectiveness of this
approach through extensive offline experiments and achieved statistically
significant results in online A/B testing, achieving an increase of 11.29% in
conversion and a 12.35% in revenue compared with the baseline.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08576v1' target='_blank'>Developing a Calibrated Physics-Based Digital Twin for Construction
  Vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Deniz Karanfil, Daniel Lindmark, Martin Servin, David Torick, Bahram Ravani</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 02:23:50</h6>
<p class='card-text'>This paper presents the development of a calibrated digital twin of a wheel
loader. A calibrated digital twin integrates a construction vehicle with a
high-fidelity digital model allowing for automated diagnostics and optimization
of operations as well as pre-planning simulations enhancing automation
capabilities. The high-fidelity digital model is a virtual twin of the physical
wheel loader. It uses a physics-based multibody dynamic model of the wheel
loader in the software AGX Dynamics. Interactions of the wheel loader's bucket
while in use in construction can be simulated in the virtual model. Calibration
makes this simulation of high-fidelity which can enhance realistic planning for
automation of construction operations. In this work, a wheel loader was
instrumented with several sensors used to calibrate the digital model. The
calibrated digital twin was able to estimate the magnitude of the forces on the
bucket base with high accuracy, providing a high-fidelity simulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08574v1' target='_blank'>DeepFleet: Multi-Agent Foundation Models for Mobile Robots</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ameya Agaskar, Sriram Siva, William Pickering, Kyle O'Brien, Charles Kekeh, Ang Li, Brianna Gallo Sarker, Alicia Chua, Mayur Nemade, Charun Thattai, Jiaming Di, Isaac Iyengar, Ramya Dharoor, Dino Kirouani, Jimmy Erskine, Tamir Hegazy, Scott Niekum, Usman A. Khan, Federico Pecora, Joseph W. Durham</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 02:19:15</h6>
<p class='card-text'>We introduce DeepFleet, a suite of foundation models designed to support
coordination and planning for large-scale mobile robot fleets. These models are
trained on fleet movement data, including robot positions, goals, and
interactions, from hundreds of thousands of robots in Amazon warehouses
worldwide. DeepFleet consists of four architectures that each embody a distinct
inductive bias and collectively explore key points in the design space for
multi-agent foundation models: the robot-centric (RC) model is an
autoregressive decision transformer operating on neighborhoods of individual
robots; the robot-floor (RF) model uses a transformer with cross-attention
between robots and the warehouse floor; the image-floor (IF) model applies
convolutional encoding to a multi-channel image representation of the full
fleet; and the graph-floor (GF) model combines temporal attention with graph
neural networks for spatial relationships. In this paper, we describe these
models and present our evaluation of the impact of these design choices on
prediction task performance. We find that the robot-centric and graph-floor
models, which both use asynchronous robot state updates and incorporate the
localized structure of robot interactions, show the most promise. We also
present experiments that show that these two models can make effective use of
larger warehouses operation datasets as the models are scaled up.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08551v1' target='_blank'>UQGNN: Uncertainty Quantification of Graph Neural Networks for
  Multivariate Spatiotemporal Prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dahai Yu, Dingyi Zhuang, Lin Jiang, Rongchao Xu, Xinyue Ye, Yuheng Bu, Shenhao Wang, Guang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 01:40:05</h6>
<p class='card-text'>Spatiotemporal prediction plays a critical role in numerous real-world
applications such as urban planning, transportation optimization, disaster
response, and pandemic control. In recent years, researchers have made
significant progress by developing advanced deep learning models for
spatiotemporal prediction. However, most existing models are deterministic,
i.e., predicting only the expected mean values without quantifying uncertainty,
leading to potentially unreliable and inaccurate outcomes. While recent studies
have introduced probabilistic models to quantify uncertainty, they typically
focus on a single phenomenon (e.g., taxi, bike, crime, or traffic crashes),
thereby neglecting the inherent correlations among heterogeneous urban
phenomena. To address the research gap, we propose a novel Graph Neural Network
with Uncertainty Quantification, termed UQGNN for multivariate spatiotemporal
prediction. UQGNN introduces two key innovations: (i) an Interaction-aware
Spatiotemporal Embedding Module that integrates a multivariate diffusion graph
convolutional network and an interaction-aware temporal convolutional network
to effectively capture complex spatial and temporal interaction patterns, and
(ii) a multivariate probabilistic prediction module designed to estimate both
expected mean values and associated uncertainties. Extensive experiments on
four real-world multivariate spatiotemporal datasets from Shenzhen, New York
City, and Chicago demonstrate that UQGNN consistently outperforms
state-of-the-art baselines in both prediction accuracy and uncertainty
quantification. For example, on the Shenzhen dataset, UQGNN achieves a 5%
improvement in both prediction accuracy and uncertainty quantification.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08524v1' target='_blank'>StreetViewAI: Making Street View Accessible Using Context-Aware
  Multimodal AI</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jon E. Froehlich, Alexander Fiannaca, Nimer Jaber, Victor Tsara, Shaun Kane</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 23:30:39</h6>
<p class='card-text'>Interactive streetscape mapping tools such as Google Street View (GSV) and
Meta Mapillary enable users to virtually navigate and experience real-world
environments via immersive 360{\deg} imagery but remain fundamentally
inaccessible to blind users. We introduce StreetViewAI, the first-ever
accessible street view tool, which combines context-aware, multimodal AI,
accessible navigation controls, and conversational speech. With StreetViewAI,
blind users can virtually examine destinations, engage in open-world
exploration, or virtually tour any of the over 220 billion images and 100+
countries where GSV is deployed. We iteratively designed StreetViewAI with a
mixed-visual ability team and performed an evaluation with eleven blind users.
Our findings demonstrate the value of an accessible street view in supporting
POI investigations and remote route planning. We close by enumerating key
guidelines for future work.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08513v1' target='_blank'>Identification of pressure points in modern power systems using transfer
  entropy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Katerina Tang, M. Vivienne Liu, C. Lindsay Anderson, Vivek Srikrishnan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 22:48:46</h6>
<p class='card-text'>Integration of variable energy resources -- e.g., solar, wind, and hydro --
and end-use electrification increase modern energy systems' weather-dependence.
Identifying critical infrastructure constraining the power grid's ability to
meet electricity demand under weather-induced shocks and stressors is essential
for understanding risks and guiding adaptation. We use transfer entropy to
identify predictive pressure points: grid components whose utilization patterns
provide early signals of downstream power shortages. We apply this method to
simulations of New York State's proposed future grid under various
meteorological and technological scenarios, showing that pressure points often
arise from complex, system-wide interactions between generation, transmission,
and demand. While transfer entropy does not support conclusions about
causality, the identified pressure points align with known bottlenecks and
offer insight into failure pathways. Furthermore, these pressure points are not
easily predicted by high-level scenario features alone, underscoring the need
for holistic and adaptive approaches to reliability planning in power systems
with intermittent resources.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08476v1' target='_blank'>Sparse Partial Optimal Transport via Quadratic Regularization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Khang Tran, Khoa Nguyen, Anh Nguyen, Thong Huynh, Son Pham, Sy-Hoang Nguyen-Dang, Manh Pham, Bang Vo, Mai Ngoc Tran, Mai Ngoc Tran, Dung Luong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 21:22:35</h6>
<p class='card-text'>Partial Optimal Transport (POT) has recently emerged as a central tool in
various Machine Learning (ML) applications. It lifts the stringent assumption
of the conventional Optimal Transport (OT) that input measures are of equal
masses, which is often not guaranteed in real-world datasets, and thus offers
greater flexibility by permitting transport between unbalanced input measures.
Nevertheless, existing major solvers for POT commonly rely on entropic
regularization for acceleration and thus return dense transport plans,
hindering the adoption of POT in various applications that favor sparsity. In
this paper, as an alternative approach to the entropic POT formulation in the
literature, we propose a novel formulation of POT with quadratic
regularization, hence termed quadratic regularized POT (QPOT), which induces
sparsity to the transport plan and consequently facilitates the adoption of POT
in many applications with sparsity requirements. Extensive experiments on
synthetic and CIFAR-10 datasets, as well as real-world applications such as
color transfer and domain adaptations, consistently demonstrate the improved
sparsity and favorable performance of our proposed QPOT formulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08453v1' target='_blank'>A Bayesian Benchmarking of GBEES Applied to Outer Planet Orbiter
  Estimation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Benjamin L. Hanson, Todd A. Ely, Thomas R. Bewley, Aaron J. Rosengren</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 20:21:43</h6>
<p class='card-text'>Moment-based estimation filters have successfully aided spacecraft navigation
for decades. However, future missions plan to venture into deep-space regimes
with significant round-trip light-time telecommunication delays, operate in
unstable, quasi-periodic orbits, and perform highly precise, low-altitude
flybys of outer planet moons. These complex trajectories may necessitate
ensemble-based filters for accurate estimation over realistic measurement
cadences. To mitigate the inherent risk associated with testing novel
navigation software, ensemble filters must be accurate, efficient, and robust.
Grid-based, Bayesian Estimation Exploiting Sparsity, a high-dimensional
Godunov-type finite volume method that propagates the full probability
distribution function, demonstrates strong overall performance across all these
criteria when compared with the contemporary landscape of filters. These
qualities are exhibited via a Bayesian investigation in which the state
uncertainty of a Saturn-Enceladus Distant Prograde Orbit is propagated,
incorporating infrequent, nonlinear measurement updates. Along with root mean
square error, we use the Bhattacharyya coefficient, a non-normal metric for
measuring the dissimilarity between distributions, and the Effective Sampling
Size, a measure of particle degeneracy, to quantitatively ascertain that in
this application, Grid-based, Bayesian Estimation Exploiting Sparsity
outperforms the other ensemble filters assessed, though it comes at a
nontrivial computational cost.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08385v1' target='_blank'>Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Masataro Asai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 18:12:40</h6>
<p class='card-text'>We study an efficient implementation of Multi-Armed Bandit (MAB)-based
Monte-Carlo Tree Search (MCTS) for classical planning. One weakness of MCTS is
that it spends a significant time deciding which node to expand next. While
selecting a node from an OPEN list with $N$ nodes has $O(1)$ runtime complexity
with traditional array-based priority-queues for dense integer keys, the
tree-based OPEN list used by MCTS requires $O(\log N)$, which roughly
corresponds to the search depth $d$. In classical planning, $d$ is arbitrarily
large (e.g., $2^k-1$ in $k$-disk Tower-of-Hanoi) and the runtime for node
selection is significant, unlike in game tree search, where the cost is
negligible compared to the node evaluation (rollouts) because $d$ is inherently
limited by the game (e.g., $d\leq 361$ in Go). To improve this bottleneck, we
propose a bilevel modification to MCTS that runs a best-first search from each
selected leaf node with an expansion budget proportional to $d$, which achieves
amortized $O(1)$ runtime for node selection, equivalent to the traditional
queue-based OPEN list. In addition, we introduce Tree Collapsing, an
enhancement that reduces action selection steps and further improves the
performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08240v1' target='_blank'>ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for
  Long-Horizon Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaijun Wang, Liqin Lu, Mingyu Liu, Jianuo Jiang, Zeju Li, Bolin Zhang, Wancai Zheng, Xinyi Yu, Hao Chen, Chunhua Shen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 17:54:31</h6>
<p class='card-text'>Language-guided long-horizon mobile manipulation has long been a grand
challenge in embodied semantic reasoning, generalizable manipulation, and
adaptive locomotion. Three fundamental limitations hinder progress: First,
although large language models have improved spatial reasoning and task
planning through semantic priors, existing implementations remain confined to
tabletop scenarios, failing to address the constrained perception and limited
actuation ranges of mobile platforms. Second, current manipulation strategies
exhibit insufficient generalization when confronted with the diverse object
configurations encountered in open-world environments. Third, while crucial for
practical deployment, the dual requirement of maintaining high platform
maneuverability alongside precise end-effector control in unstructured settings
remains understudied.
  In this work, we present ODYSSEY, a unified mobile manipulation framework for
agile quadruped robots equipped with manipulators, which seamlessly integrates
high-level task planning with low-level whole-body control. To address the
challenge of egocentric perception in language-conditioned tasks, we introduce
a hierarchical planner powered by a vision-language model, enabling
long-horizon instruction decomposition and precise action execution. At the
control level, our novel whole-body policy achieves robust coordination across
challenging terrains. We further present the first benchmark for long-horizon
mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through
successful sim-to-real transfer, we demonstrate the system's generalization and
robustness in real-world deployments, underscoring the practicality of legged
manipulators in unstructured environments. Our work advances the feasibility of
generalized robotic assistants capable of complex, dynamic tasks. Our project
page: https://kaijwang.github.io/odyssey.github.io/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08216v1' target='_blank'>Cross-Subject and Cross-Montage EEG Transfer Learning via Individual
  Tangent Space Alignment and Spatial-Riemannian Feature Fusion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nicole Lai-Tan, Xiao Gu, Marios G. Philiastides, Fani Deligianni</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 17:37:17</h6>
<p class='card-text'>Personalised music-based interventions offer a powerful means of supporting
motor rehabilitation by dynamically tailoring auditory stimuli to provide
external timekeeping cues, modulate affective states, and stabilise gait
patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for
adapting these interventions across individuals. However, inter-subject
variability in EEG signals, further compounded by movement-induced artefacts
and motor planning differences, hinders the generalisability of BCIs and
results in lengthy calibration processes. We propose Individual Tangent Space
Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific
recentering, distribution matching, and supervised rotational alignment to
enhance cross-subject generalisation. Our hybrid architecture fuses Regularised
Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and
sequential configurations, improving class separability while maintaining the
geometric structure of covariance matrices for robust statistical computation.
Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant
performance improvements across subjects and conditions. The parallel fusion
approach shows the greatest enhancement over its sequential counterpart, with
robust performance maintained across varying data conditions and electrode
configurations. The code will be made publicly available at the time of
publication.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08177v1' target='_blank'>MedReasoner: Reinforcement Learning Drives Reasoning Grounding from
  Clinical Thought to Pixel-Level Precision</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhonghao Yan, Muxi Diao, Yuxuan Yang, Jiayuan Xu, Kaizhou Zhang, Ruoyan Jing, Lele Yang, Yanxi Liu, Kongming Liang, Zhanyu Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 16:59:06</h6>
<p class='card-text'>Accurately grounding regions of interest (ROIs) is critical for diagnosis and
treatment planning in medical imaging. While multimodal large language models
(MLLMs) combine visual perception with natural language, current
medical-grounding pipelines still rely on supervised fine-tuning with explicit
spatial hints, making them ill-equipped to handle the implicit queries common
in clinical practice. This work makes three core contributions. We first define
Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that
demands clinical reasoning and pixel-level grounding. Second, we release
U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside
implicit clinical queries and reasoning traces, spanning 10 modalities, 15
super-categories, and 108 specific categories. Finally, we introduce
MedReasoner, a modular framework that distinctly separates reasoning from
segmentation: an MLLM reasoner is optimized with reinforcement learning, while
a frozen segmentation expert converts spatial prompts into masks, with
alignment achieved through format and accuracy rewards. MedReasoner achieves
state-of-the-art performance on U-MRG-14K and demonstrates strong
generalization to unseen clinical queries, underscoring the significant promise
of reinforcement learning for interpretable medical grounding.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08108v1' target='_blank'>Capsizing-Guided Trajectory Optimization for Autonomous Navigation with
  Rough Terrain</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Zhang, Yinchuan Wang, Wangtao Lu, Pengyu Zhang, Xiang Zhang, Yue Wang, Chaoqun Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 15:47:24</h6>
<p class='card-text'>It is a challenging task for ground robots to autonomously navigate in harsh
environments due to the presence of non-trivial obstacles and uneven terrain.
This requires trajectory planning that balances safety and efficiency. The
primary challenge is to generate a feasible trajectory that prevents robot from
tip-over while ensuring effective navigation. In this paper, we propose a
capsizing-aware trajectory planner (CAP) to achieve trajectory planning on the
uneven terrain. The tip-over stability of the robot on rough terrain is
analyzed. Based on the tip-over stability, we define the traversable
orientation, which indicates the safe range of robot orientations. This
orientation is then incorporated into a capsizing-safety constraint for
trajectory optimization. We employ a graph-based solver to compute a robust and
feasible trajectory while adhering to the capsizing-safety constraint.
Extensive simulation and real-world experiments validate the effectiveness and
robustness of the proposed method. The results demonstrate that CAP outperforms
existing state-of-the-art approaches, providing enhanced navigation performance
on uneven terrains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08063v1' target='_blank'>Flight masks of the Roman Space Telescope Coronagraph Instrument</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:A. J. Eldorado Riggs, Vanessa P. Bailey, Dwight Moody, Kunjithapatham Balasubramanian, Scott A. Basinger, Ruslan Belikov, Eduardo Bendek, John Debes, Brandon D. Dube, Jessica Gersh-Range, Tyler D. Groff, N. Jeremy Kasdin, Bertrand Mennesson, Brian Monacelli, Douglas M. Moore, Garreth Ruane, Jagmit Sandhu, Fang Shi, Erkin Sidick, Nicholas Siegler, Dan Sirbu, John Trauger, Carey L. Weisberg, Victor E. White, Daniel W. Wilson, Robert C. Wilson, Karl Y. Yee, Neil T. Zimmerman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 15:08:23</h6>
<p class='card-text'>Over the past two decades, thousands of confirmed exoplanets have been
detected. The next major challenge is to characterize these other worlds and
their stellar systems. Much information on the composition and formation of
exoplanets and circumstellar debris disks can only be achieved via direct
imaging. Direct imaging is challenging because of the small angular separations
(< 1 arcsec) and high star-to-planet flux ratios such as ~1e9 for a Jupiter
analog or ~1e10 for an Earth analog in the visible. Atmospheric turbulence
prohibits reaching such high flux ratios on the ground, so observations must be
made above the Earth's atmosphere. The Nancy Grace Roman Space Telescope
(Roman), planned to launch in late 2026, will be the first space-based
observatory to demonstrate high-contrast imaging with active wavefront control
using its Coronagraph Instrument. The instrument's main purpose is to mature
the various technologies needed for a future flagship mission to image and
characterize Earth-like exoplanets. These technologies include two
high-actuator-count deformable mirrors, photon-counting detectors, two
complementary wavefront sensing and control loops, and two different
coronagraph types. In this paper, we describe the complete set of flight masks
in the Roman Coronagraph Instrument, their intended combinations, and how they
were laid out, fabricated, and measured.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.07917v1' target='_blank'>MolmoAct: Action Reasoning Models that can Reason in Space</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 12:32:45</h6>
<p class='card-text'>Reasoning is central to purposeful action, yet most robotic foundation models
map perception and instructions directly to control, which limits adaptability,
generalization, and semantic grounding. We introduce Action Reasoning Models
(ARMs), a class of vision-language-action models that integrate perception,
planning, and control through a structured three-stage pipeline. Our model,
MolmoAct, encodes observations and instructions into depth-aware perception
tokens, generates mid-level spatial plans as editable trajectory traces, and
predicts precise low-level actions, enabling explainable and steerable
behavior. MolmoAct-7B-D achieves strong performance across simulation and
real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching
tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on
LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;
and in real-world fine-tuning, an additional 10% (single-arm) and an additional
22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines
by an additional 23.3% on out-of-distribution generalization and achieves top
human-preference scores for open-ended instruction following and trajectory
steering. Furthermore, we release, for the first time, the MolmoAct Dataset --
a mid-training robot dataset comprising over 10,000 high quality robot
trajectories across diverse scenarios and tasks. Training with this dataset
yields an average 5.5% improvement in general performance over the base model.
We release all model weights, training code, our collected dataset, and our
action reasoning dataset, establishing MolmoAct as both a state-of-the-art
robotics foundation model and an open blueprint for building ARMs that
transform perception into purposeful action through structured reasoning.
Blogpost: https://allenai.org/blog/molmoact</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.07842v1' target='_blank'>DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of
  Disentangled Experts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yutong Shen, Hangxu Liu, Penghui Liu, Ruizhe Xia, Tianyi Yao, Yitong Sun, Tongtong Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 10:54:28</h6>
<p class='card-text'>Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex
multi-step tasks that require continuous planning, sequential decision-making,
and extended execution across domains to achieve the final goal. However,
existing methods heavily rely on skill chaining by concatenating pre-trained
subtasks, with environment observations and self-state tightly coupled, lacking
the ability to generalize to new combinations of environments and skills,
failing to complete various LH tasks across domains. To solve this problem,
this paper presents DETACH, a cross-domain learning framework for LH tasks via
biologically inspired dual-stream disentanglement. Inspired by the brain's
"where-what" dual pathway mechanism, DETACH comprises two core modules: i) an
environment learning module for spatial understanding, which captures object
functions, spatial relationships, and scene semantics, achieving cross-domain
transfer through complete environment-self disentanglement; ii) a skill
learning module for task execution, which processes self-state information
including joint degrees of freedom and motor patterns, enabling cross-skill
transfer through independent motor pattern encoding. We conducted extensive
experiments on various LH tasks in HSI scenes. Compared with existing methods,
DETACH can achieve an average subtasks success rate improvement of 23% and
average execution efficiency improvement of 29%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.07814v1' target='_blank'>SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of
  Heterogeneous Robots in Dynamic Warehousing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Malaika Zafar, Roohan Ahmed Khan, Faryal Batool, Yasheerah Yaqoot, Ziang Guo, Mikhail Litvinov, Aleksey Fedoseev, Dzmitry Tsetserukou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 09:56:33</h6>
<p class='card-text'>With the growing demand for efficient logistics, unmanned aerial vehicles
(UAVs) are increasingly being paired with automated guided vehicles (AGVs).
While UAVs offer the ability to navigate through dense environments and varying
altitudes, they are limited by battery life, payload capacity, and flight
duration, necessitating coordinated ground support.
  Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by
enabling semantic collaboration between UAVs and ground robots through
impedance control. The system leverages the Vision Language Model (VLM) and the
Retrieval-Augmented Generation (RAG) to adjust impedance control parameters in
response to environmental changes. In this framework, the UAV acts as a leader
using Artificial Potential Field (APF) planning for real-time navigation, while
the ground robot follows via virtual impedance links with adaptive link
topology to avoid collisions with short obstacles.
  The system demonstrated a 92% success rate across 12 real-world trials. Under
optimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in
object detection and selection of impedance parameters. The mobile robot
prioritized short obstacle avoidance, occasionally resulting in a lateral
deviation of up to 50 cm from the UAV path, which showcases safe navigation in
a cluttered setting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.07743v1' target='_blank'>Symmetry-Aware Transformer Training for Automated Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Markus Fritzsche, Elliot Gestrin, Jendrik Seipp</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 08:23:34</h6>
<p class='card-text'>While transformers excel in many settings, their application in the field of
automated planning is limited. Prior work like PlanGPT, a state-of-the-art
decoder-only transformer, struggles with extrapolation from easy to hard
planning problems. This in turn stems from problem symmetries: planning tasks
can be represented with arbitrary variable names that carry no meaning beyond
being identifiers. This causes a combinatorial explosion of equivalent
representations that pure transformers cannot efficiently learn from. We
propose a novel contrastive learning objective to make transformers
symmetry-aware and thereby compensate for their lack of inductive bias.
Combining this with architectural improvements, we show that transformers can
be efficiently trained for either plan-generation or heuristic-prediction. Our
results across multiple planning domains demonstrate that our symmetry-aware
training effectively and efficiently addresses the limitations of PlanGPT.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.07686v1' target='_blank'>Risk Map As Middleware: Towards Interpretable Cooperative End-to-end
  Autonomous Driving for Risk-Aware Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingyue Lei, Zewei Zhou, Hongchen Li, Jiaqi Ma, Jia Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 07:00:52</h6>
<p class='card-text'>End-to-end paradigm has emerged as a promising approach to autonomous
driving. However, existing single-agent end-to-end pipelines are often
constrained by occlusion and limited perception range, resulting in hazardous
driving. Furthermore, their black-box nature prevents the interpretability of
the driving behavior, leading to an untrustworthiness system. To address these
limitations, we introduce Risk Map as Middleware (RiskMM) and propose an
interpretable cooperative end-to-end driving framework. The risk map learns
directly from the driving data and provides an interpretable spatiotemporal
representation of the scenario from the upstream perception and the
interactions between the ego vehicle and the surrounding environment for
downstream planning. RiskMM first constructs a multi-agent spatiotemporal
representation with unified Transformer-based architecture, then derives
risk-aware representations by modeling interactions among surrounding
environments with attention. These representations are subsequently fed into a
learning-based Model Predictive Control (MPC) module. The MPC planner
inherently accommodates physical constraints and different vehicle types and
can provide interpretation by aligning learned parameters with explicit MPC
elements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm
that RiskMM achieves superior and robust performance in risk-aware trajectory
planning, significantly enhancing the interpretability of the cooperative
end-to-end driving framework. The codebase will be released to facilitate
future research in this field.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.07657v1' target='_blank'>MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration
  under Restricted Communication</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuoli Tian, Yuyang Zhang, Jinsheng Wei, Meng Guo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 06:10:38</h6>
<p class='card-text'>Fleets of autonomous robots are increasingly deployed alongside multiple
human operators to explore unknown environments, identify salient features, and
perform complex tasks in scenarios such as subterranean exploration,
reconnaissance, and search-and-rescue missions. In these contexts,
communication is often severely limited to short-range exchanges via ad-hoc
networks, posing challenges to coordination. While recent studies have
addressed multi-robot exploration under communication constraints, they largely
overlook the essential role of human operators and their real-time interaction
with robotic teams. Operators may demand timely updates on the exploration
progress and robot status, reprioritize or cancel tasks dynamically, or request
live video feeds and control access. Conversely, robots may seek human
confirmation for anomalous events or require help recovering from motion or
planning failures. To enable such bilateral, context-aware interactions under
restricted communication, this work proposes MoRoCo, a unified framework for
online coordination and exploration in multi-operator, multi-robot systems.
MoRoCo enables the team to adaptively switch among three coordination modes:
spread mode for parallelized exploration with intermittent data sharing,
migrate mode for coordinated relocation, and chain mode for maintaining
high-bandwidth connectivity through multi-hop links. These transitions are
managed through distributed algorithms via only local communication. Extensive
large-scale human-in-the-loop simulations and hardware experiments validate the
necessity of incorporating human robot interactions and demonstrate that MoRoCo
enables efficient, reliable coordination under limited communication, marking a
significant step toward robust human-in-the-loop multi-robot autonomy in
challenging environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.07654v1' target='_blank'>MLego: Interactive and Scalable Topic Exploration Through Model Reuse</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fei Ye, Jiapan Liu, Yinan Jing, Zhenying He, Weirao Wang, X. Sean Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 06:06:26</h6>
<p class='card-text'>With massive texts on social media, users and analysts often rely on topic
modeling techniques to quickly extract key themes and gain insights.
Traditional topic modeling techniques, such as Latent Dirichlet Allocation
(LDA), provide valuable insights but are computationally expensive, making them
impractical for real-time data analysis. Although recent advances in
distributed training and fast sampling methods have improved efficiency,
real-time topic exploration remains a significant challenge. In this paper, we
present MLego, an interactive query framework designed to support real-time
topic modeling analysis by leveraging model materialization and reuse. Instead
of retraining models from scratch, MLego efficiently merges materialized topic
models to construct approximate results at interactive speeds. To further
enhance efficiency, we introduce a hierarchical plan search strategy for single
queries and an optimized query reordering technique for batch queries. We
integrate MLego into a visual analytics prototype system, enabling users to
explore large-scale textual datasets through interactive queries. Extensive
experiments demonstrate that MLego significantly reduces computation costs
while maintaining high-quality topic modeling results. MLego enhances existing
visual analytics approaches, which primarily focus on user-driven topic
modeling, by enabling real-time, query-driven exploration. This complements
traditional methods and bridges the gap between scalable topic modeling and
interactive data analysis.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>