<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-07-19</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-07-19</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.13285v1' target='_blank'>Multi-Agent Synergy-Driven Iterative Visual Narrative Synthesis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wang Xi, Quan Shi, Tian Yu, Yujie Peng, Jiayi Sun, Mengxing Ren, Zenghui Ding, Ningguang Yao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 16:50:07</h6>
<p class='card-text'>Automated generation of high-quality media presentations is challenging,
requiring robust content extraction, narrative planning, visual design, and
overall quality optimization. Existing methods often produce presentations with
logical inconsistencies and suboptimal layouts, thereby struggling to meet
professional standards. To address these challenges, we introduce RCPS
(Reflective Coherent Presentation Synthesis), a novel framework integrating
three key components: (1) Deep Structured Narrative Planning; (2) Adaptive
Layout Generation; (3) an Iterative Optimization Loop. Additionally, we propose
PREVAL, a preference-based evaluation framework employing rationale-enhanced
multi-dimensional models to assess presentation quality across Content,
Coherence, and Design. Experimental results demonstrate that RCPS significantly
outperforms baseline methods across all quality dimensions, producing
presentations that closely approximate human expert standards. PREVAL shows
strong correlation with human judgments, validating it as a reliable automated
tool for assessing presentation quality.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.13275v1' target='_blank'>Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for
  Human Capital Management</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Luis Gasco, Hermenegildo Fabregat, Laura García-Sardiña, Paula Estrella, Daniel Deniz, Alvaro Rodrigo, Rabih Zbib</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 16:33:57</h6>
<p class='card-text'>Advances in natural language processing and large language models are driving
a major transformation in Human Capital Management, with a growing interest in
building smart systems based on language technologies for talent acquisition,
upskilling strategies, and workforce planning. However, the adoption and
progress of these technologies critically depend on the development of reliable
and fair models, properly evaluated on public data and open benchmarks, which
have so far been unavailable in this domain.
  To address this gap, we present TalentCLEF 2025, the first evaluation
campaign focused on skill and job title intelligence. The lab consists of two
tasks: Task A - Multilingual Job Title Matching, covering English, Spanish,
German, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.
Both corpora were built from real job applications, carefully anonymized, and
manually annotated to reflect the complexity and diversity of real-world labor
market data, including linguistic variability and gender-marked expressions.
  The evaluations included monolingual and cross-lingual scenarios and covered
the evaluation of gender bias.
  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most
systems relied on information retrieval techniques built with multilingual
encoder-based models fine-tuned with contrastive learning, and several of them
incorporated large language models for data augmentation or re-ranking. The
results show that the training strategies have a larger effect than the size of
the model alone. TalentCLEF provides the first public benchmark in this field
and encourages the development of robust, fair, and transferable language
technologies for the labor market.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.13225v1' target='_blank'>Signal Temporal Logic Compliant Co-design of Planning and Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Manas Sashank Juvvi, Tushar Dilip Kurne, Vaishnavi J, Shishir Kolathaya, Pushpak Jagtap</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 15:37:24</h6>
<p class='card-text'>This work presents a novel co-design strategy that integrates trajectory
planning and control to handle STL-based tasks in autonomous robots. The method
consists of two phases: $(i)$ learning spatio-temporal motion primitives to
encapsulate the inherent robot-specific constraints and $(ii)$ constructing an
STL-compliant motion plan from these primitives. Initially, we employ
reinforcement learning to construct a library of control policies that perform
trajectories described by the motion primitives. Then, we map motion primitives
to spatio-temporal characteristics. Subsequently, we present a sampling-based
STL-compliant motion planning strategy tailored to meet the STL specification.
The proposed model-free approach, which generates feasible STL-compliant motion
plans across various environments, is validated on differential-drive and
quadruped robots across various STL specifications. Demonstration videos are
available at https://tinyurl.com/m6zp7rsm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.13224v1' target='_blank'>Leveraging Pre-Trained Visual Models for AI-Generated Video Detection</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Keerthi Veeramachaneni, Praveen Tirupattur, Amrit Singh Bedi, Mubarak Shah</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 15:36:39</h6>
<p class='card-text'>Recent advances in Generative AI (GenAI) have led to significant improvements
in the quality of generated visual content. As AI-generated visual content
becomes increasingly indistinguishable from real content, the challenge of
detecting the generated content becomes critical in combating misinformation,
ensuring privacy, and preventing security threats. Although there has been
substantial progress in detecting AI-generated images, current methods for
video detection are largely focused on deepfakes, which primarily involve human
faces. However, the field of video generation has advanced beyond DeepFakes,
creating an urgent need for methods capable of detecting AI-generated videos
with generic content. To address this gap, we propose a novel approach that
leverages pre-trained visual models to distinguish between real and generated
videos. The features extracted from these pre-trained models, which have been
trained on extensive real visual content, contain inherent signals that can
help distinguish real from generated videos. Using these extracted features, we
achieve high detection performance without requiring additional model training,
and we further improve performance by training a simple linear classification
layer on top of the extracted features. We validated our method on a dataset we
compiled (VID-AID), which includes around 10,000 AI-generated videos produced
by 9 different text-to-video models, along with 4,000 real videos, totaling
over 7 hours of video content. Our evaluation shows that our approach achieves
high detection accuracy, above 90% on average, underscoring its effectiveness.
Upon acceptance, we plan to publicly release the code, the pre-trained models,
and our dataset to support ongoing research in this critical area.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.13115v1' target='_blank'>A Computational Framework to Identify Self-Aspects in Text</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jaya Caporusso, Matthew Purver, Senja Pollak</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 13:31:04</h6>
<p class='card-text'>This Ph.D. proposal introduces a plan to develop a computational framework to
identify Self-aspects in text. The Self is a multifaceted construct and it is
reflected in language. While it is described across disciplines like cognitive
science and phenomenology, it remains underexplored in natural language
processing (NLP). Many of the aspects of the Self align with psychological and
other well-researched phenomena (e.g., those related to mental health),
highlighting the need for systematic NLP-based analysis. In line with this, we
plan to introduce an ontology of Self-aspects and a gold-standard annotated
dataset. Using this foundation, we will develop and evaluate conventional
discriminative models, generative large language models, and embedding-based
retrieval approaches against four main criteria: interpretability, ground-truth
adherence, accuracy, and computational efficiency. Top-performing models will
be applied in case studies in mental health and empirical phenomenology.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.13075v1' target='_blank'>Undulating patterns of Hysteresis loops in diurnal seasonality of air
  temperature in Urban Heat Island effect: Insights from Paris and Madrid</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Suman Dharmasthala, Vittal Hari, Rohini Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 12:43:39</h6>
<p class='card-text'>This study examines the dynamics of the urban heat island (UHI) effect by
conducting a comparative analysis of air temperature hysteresis patterns in
Paris and Madrid, two major European cities with distinct climatic and urban
characteristics. Utilizing high-resolution modelled air temperature data
aggregated at a fine temporal resolution of three-hour intervals from 2008 to
2017, we investigate how diurnal and seasonal hysteresis loops reveal both
unique and universal aspects of UHI variability. Paris, located in a temperate
oceanic climate, and Madrid, situated in a cold semi-arid zone, display
pronounced differences in UHI intensity, seasonal distribution, and diurnal
patterns. Despite these contrasts, both cities exhibit remarkably similar
hysteresis loop directions and slopes, suggesting that time-dependent
mechanisms such as solar radiation and heat storage fundamentally govern air
temperature UHI across diverse urban contexts. Our findings underscore the
importance of considering both local climate and universal physical processes
in developing targeted, climate-resilient urban strategies. The results pave
the way for group-based interventions and classification of cities by
hysteresis patterns to inform urban planning and heat mitigation efforts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.13053v1' target='_blank'>Efficient Online Learning and Adaptive Planning for Robotic Information
  Gathering Based on Streaming Data</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sanjeev Ramkumar Sudha, Joel Jose, Erlend M. Coates</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 12:26:03</h6>
<p class='card-text'>Robotic information gathering (RIG) techniques refer to methods where mobile
robots are used to acquire data about the physical environment with a suite of
sensors. Informative planning is an important part of RIG where the goal is to
find sequences of actions or paths that maximize efficiency or the quality of
information collected. Many existing solutions solve this problem by assuming
that the environment is known in advance. However, real environments could be
unknown or time-varying, and adaptive informative planning remains an active
area of research. Adaptive planning and incremental online mapping are required
for mapping initially unknown or varying spatial fields. Gaussian process (GP)
regression is a widely used technique in RIG for mapping continuous spatial
fields. However, it falls short in many applications as its real-time
performance does not scale well to large datasets. To address these challenges,
this paper proposes an efficient adaptive informative planning approach for
mapping continuous scalar fields with GPs with streaming sparse GPs. Simulation
experiments are performed with a synthetic dataset and compared against
existing benchmarks. Finally, it is also verified with a real-world dataset to
further validate the efficacy of the proposed method. Results show that our
method achieves similar mapping accuracy to the baselines while reducing
computational complexity for longer missions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12989v1' target='_blank'>A Translation of Probabilistic Event Calculus into Markov Decision
  Processes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lyris Xu, Fabio Aurelio D'Asaro, Luke Dickens</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 10:56:22</h6>
<p class='card-text'>Probabilistic Event Calculus (PEC) is a logical framework for reasoning about
actions and their effects in uncertain environments, which enables the
representation of probabilistic narratives and computation of temporal
projections. The PEC formalism offers significant advantages in
interpretability and expressiveness for narrative reasoning. However, it lacks
mechanisms for goal-directed reasoning. This paper bridges this gap by
developing a formal translation of PEC domains into Markov Decision Processes
(MDPs), introducing the concept of "action-taking situations" to preserve PEC's
flexible action semantics. The resulting PEC-MDP formalism enables the
extensive collection of algorithms and theoretical tools developed for MDPs to
be applied to PEC's interpretable narrative domains. We demonstrate how the
translation supports both temporal reasoning tasks and objective-driven
planning, with methods for mapping learned policies back into human-readable
PEC representations, maintaining interpretability while extending PEC's
capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12985v1' target='_blank'>From Variability To Accuracy: Conditional Bernoulli Diffusion Models
  with Consensus-Driven Correction for Thin Structure Segmentation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinseo An, Min Jin Lee, Kyu Won Shim, Helen Hong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 10:44:06</h6>
<p class='card-text'>Accurate segmentation of orbital bones in facial computed tomography (CT)
images is essential for the creation of customized implants for reconstruction
of defected orbital bones, particularly challenging due to the ambiguous
boundaries and thin structures such as the orbital medial wall and orbital
floor. In these ambiguous regions, existing segmentation approaches often
output disconnected or under-segmented results. We propose a novel framework
that corrects segmentation results by leveraging consensus from multiple
diffusion model outputs. Our approach employs a conditional Bernoulli diffusion
model trained on diverse annotation patterns per image to generate multiple
plausible segmentations, followed by a consensus-driven correction that
incorporates position proximity, consensus level, and gradient direction
similarity to correct challenging regions. Experimental results demonstrate
that our method outperforms existing methods, significantly improving recall in
ambiguous regions while preserving the continuity of thin structures.
Furthermore, our method automates the manual process of segmentation result
correction and can be applied to image-guided surgical planning and surgery.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12977v1' target='_blank'>Non-differentiable Reward Optimization for Diffusion-based Autonomous
  Motion Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Giwon Lee, Daehee Park, Jaewoo Jeong, Kuk-Jin Yoon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 10:26:06</h6>
<p class='card-text'>Safe and effective motion planning is crucial for autonomous robots.
Diffusion models excel at capturing complex agent interactions, a fundamental
aspect of decision-making in dynamic environments. Recent studies have
successfully applied diffusion models to motion planning, demonstrating their
competence in handling complex scenarios and accurately predicting multi-modal
future trajectories. Despite their effectiveness, diffusion models have
limitations in training objectives, as they approximate data distributions
rather than explicitly capturing the underlying decision-making dynamics.
However, the crux of motion planning lies in non-differentiable downstream
objectives, such as safety (collision avoidance) and effectiveness
(goal-reaching), which conventional learning algorithms cannot directly
optimize. In this paper, we propose a reinforcement learning-based training
scheme for diffusion motion planning models, enabling them to effectively learn
non-differentiable objectives that explicitly measure safety and effectiveness.
Specifically, we introduce a reward-weighted dynamic thresholding algorithm to
shape a dense reward signal, facilitating more effective training and
outperforming models trained with differentiable objectives. State-of-the-art
performance on pedestrian datasets (CrowdNav, ETH-UCY) compared to various
baselines demonstrates the versatility of our approach for safe and effective
motion planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12935v1' target='_blank'>MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov
  Chain Monte Carlo Acceleration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shirui Zhao, Jun Yin, Lingyun Yao, Martin Andraud, Wannes Meert, Marian Verhelst</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 09:20:51</h6>
<p class='card-text'>An increasing number of applications are exploiting sampling-based algorithms
for planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)
algorithms form the computational backbone of this emerging branch of machine
learning. Unfortunately, the high computational cost limits their feasibility
for large-scale problems and real-world applications, and the existing MCMC
acceleration solutions are either limited in hardware flexibility or fail to
maintain efficiency at the system level across a variety of end-to-end
applications. This paper introduces \textbf{MC$^2$A}, an algorithm-hardware
co-design framework, enabling efficient and flexible optimization for MCMC
acceleration. Firstly, \textbf{MC$^2$A} analyzes the MCMC workload diversity
through an extension of the processor performance roofline model with a 3rd
dimension to derive the optimal balance between the compute, sampling and
memory parameters. Secondly, \textbf{MC$^2$A} proposes a parametrized hardware
accelerator architecture with flexible and efficient support of MCMC kernels
with a pipeline of ISA-programmable tree-structured processing units,
reconfigurable samplers and a crossbar interconnect to support irregular
access. Thirdly, the core of \textbf{MC$^2$A} is powered by a novel Gumbel
sampler that eliminates exponential and normalization operations. In the
end-to-end case study, \textbf{MC$^2$A} achieves an overall {$307.6\times$,
$1.4\times$, $2.0\times$, $84.2\times$} speedup compared to the CPU, GPU, TPU
and state-of-the-art MCMC accelerator. Evaluated on various representative MCMC
workloads, this work demonstrates and exploits the feasibility of general
hardware acceleration to popularize MCMC-based solutions in diverse application
domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12911v1' target='_blank'>LaViPlan : Language-Guided Visual Path Planning with RLVR</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hayeon Oh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 08:58:24</h6>
<p class='card-text'>Out-of-distribution (OOD) scenarios in autonomous driving refer to situations
that deviate from the training domain, often leading to unexpected and
potentially hazardous behavior from planners that lack prior exposure to such
cases. Recently, Vision-Language Models (VLMs) have been introduced into
autonomous driving research for their promising generalization capabilities in
OOD settings. Early studies demonstrated that VLMs could recognize OOD
scenarios and generate user-level decisions such as "go straight" or "turn
right." However, a new challenge has emerged due to the misalignment between
the VLM's high-level decisions or visual reasoning expressed in language, and
the low-level predicted trajectories interpreted as actions. In this paper, we
propose LaViPlan, a framework that leverages Reinforcement Learning with
Verifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics.
This approach addresses the vision-language-action misalignment observed in
existing VLMs fine-tuned via supervised learning, which can recognize driving
scenarios but often produce context-unaware decisions. Experimental results
demonstrate that our method improves situational awareness and decision-making
under OOD conditions, highlighting its potential to mitigate the misalignment
issue. This work introduces a promising post-training paradigm for VLM agents
in the context of autonomous driving.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12877v1' target='_blank'>Impact Analysis of Optimal EV Bi-directional Charging with
  Spatial-temporal Constraints</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xian-Long Lee, Adel N. Toosi, Peter Pudney, Ian McLeod, Muhammad Aamir Cheema, Hao Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 07:53:41</h6>
<p class='card-text'>The growth in Electric Vehicle (EV) market share is expected to increase
power demand on distribution networks. Uncoordinated residential EV charging,
based on driving routines, creates peak demand at various zone substations
depending on location and time. Leveraging smart charge scheduling and
Vehicle-to-Grid (V2G) technologies offers opportunities to adjust charge
schedules, allowing for load shifting and grid support, which can reduce both
charging costs and grid stress. In this work, we develop a charge scheduling
optimization method that can be used to assess the impact of spatial power
capacity constraints and real-time price profiles. We formulate a mixed-integer
linear programming problem to minimize overall charging costs, taking into
account factors such as time-varying EV locations, EV charging requirements,
and local power demands across different zones. Our analysis uses real data for
pricing signals and local power demands, combined with simulated data for EV
driving plans. Four metrics are introduced to assess impacts from the
perspectives of both EV users and zones. Results indicate that overall EV
charging costs are only minimally affected under extreme power capacity
constraints.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12846v1' target='_blank'>Enter the Mind Palace: Reasoning and Planning for Long-term Active
  Embodied Question Answering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhammad Fadhil Ginting, Dong-Ki Kim, Xiangyun Meng, Andrzej Reinke, Bandi Jai Krishna, Navid Kayhani, Oriana Peltzer, David D. Fan, Amirreza Shaban, Sung-Kyun Kim, Mykel J. Kochenderfer, Ali-akbar Agha-mohammadi, Shayegan Omidshafiei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 07:11:32</h6>
<p class='card-text'>As robots become increasingly capable of operating over extended periods --
spanning days, weeks, and even months -- they are expected to accumulate
knowledge of their environments and leverage this experience to assist humans
more effectively. This paper studies the problem of Long-term Active Embodied
Question Answering (LA-EQA), a new task in which a robot must both recall past
experiences and actively explore its environment to answer complex,
temporally-grounded questions. Unlike traditional EQA settings, which typically
focus either on understanding the present environment alone or on recalling a
single past observation, LA-EQA challenges an agent to reason over past,
present, and possible future states, deciding when to explore, when to consult
its memory, and when to stop gathering observations and provide a final answer.
Standard EQA approaches based on large models struggle in this setting due to
limited context windows, absence of persistent memory, and an inability to
combine memory recall with active exploration. To address this, we propose a
structured memory system for robots, inspired by the mind palace method from
cognitive science. Our method encodes episodic experiences as scene-graph-based
world instances, forming a reasoning and planning algorithm that enables
targeted memory retrieval and guided navigation. To balance the
exploration-recall trade-off, we introduce value-of-information-based stopping
criteria that determines when the agent has gathered sufficient information. We
evaluate our method on real-world experiments and introduce a new benchmark
that spans popular simulation environments and actual industrial sites. Our
approach significantly outperforms state-of-the-art baselines, yielding
substantial gains in both answer accuracy and exploration efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12845v1' target='_blank'>SEMT: Static-Expansion-Mesh Transformer Network Architecture for Remote
  Sensing Image Captioning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Khang Truong, Lam Pham, Hieu Tang, Jasmin Lampert, Martin Boyer, Son Phan, Truong Nguyen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 07:11:01</h6>
<p class='card-text'>Image captioning has emerged as a crucial task in the intersection of
computer vision and natural language processing, enabling automated generation
of descriptive text from visual content. In the context of remote sensing,
image captioning plays a significant role in interpreting vast and complex
satellite imagery, aiding applications such as environmental monitoring,
disaster assessment, and urban planning. This motivates us, in this paper, to
present a transformer based network architecture for remote sensing image
captioning (RSIC) in which multiple techniques of Static Expansion,
Memory-Augmented Self-Attention, Mesh Transformer are evaluated and integrated.
We evaluate our proposed models using two benchmark remote sensing image
datasets of UCM-Caption and NWPU-Caption. Our best model outperforms the
state-of-the-art systems on most of evaluation metrics, which demonstrates
potential to apply for real-life remote sensing image systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12800v1' target='_blank'>FFI-VTR: Lightweight and Robust Visual Teach and Repeat Navigation based
  on Feature Flow Indicator and Probabilistic Motion Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jikai Wang, Yunqi Cheng, Zonghai Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 05:36:14</h6>
<p class='card-text'>Though visual and repeat navigation is a convenient solution for mobile robot
self-navigation, achieving balance between efficiency and robustness in task
environment still remains challenges. In this paper, we propose a novel visual
and repeat robotic autonomous navigation method that requires no accurate
localization and dense reconstruction modules, which makes our system featured
by lightweight and robustness. Firstly, feature flow is introduced and we
develop a qualitative mapping between feature flow and robot's motion, in which
feature flow is defined as pixel location bias between matched features. Based
on the mapping model, the map outputted by the teaching phase is represented as
a keyframe graph, in which the feature flow on the edge encodes the relative
motion between adjacent keyframes. Secondly, the visual repeating navigation is
essentially modeled as a feature flow minimization problem between current
observation and the map keyframe. To drive the robot to consistently reduce the
feature flow between current frame and map keyframes without accurate
localization, a probabilistic motion planning is developed based on our
qualitative feature flow-motion mapping indicator. Extensive experiments using
our mobile platform demonstrates that our proposed method is lightweight,
robust, and superior to baselines. The source code has been made public at
https://github.com/wangjks/FFI-VTR to benefit the community.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12731v1' target='_blank'>Learning to Predict Mobile Robot Stability in Off-Road Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nathaniel Rose, Arif Ahmed, Emanuel Gutierrez-Cornejo, Parikshit Maini</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-17 02:24:35</h6>
<p class='card-text'>Navigating in off-road environments for wheeled mobile robots is challenging
due to dynamic and rugged terrain. Traditional physics-based stability metrics,
such as Static Stability Margin (SSM) or Zero Moment Point (ZMP) require
knowledge of contact forces, terrain geometry, and the robot's precise
center-of-mass that are difficult to measure accurately in real-world field
conditions. In this work, we propose a learning-based approach to estimate
robot platform stability directly from proprioceptive data using a lightweight
neural network, IMUnet. Our method enables data-driven inference of robot
stability without requiring an explicit terrain model or force sensing.
  We also develop a novel vision-based ArUco tracking method to compute a
scalar score to quantify robot platform stability called C3 score. The score
captures image-space perturbations over time as a proxy for physical
instability and is used as a training signal for the neural network based
model. As a pilot study, we evaluate our approach on data collected across
multiple terrain types and speeds and demonstrate generalization to previously
unseen conditions. These initial results highlight the potential of using IMU
and robot velocity as inputs to estimate platform stability. The proposed
method finds application in gating robot tasks such as precision actuation and
sensing, especially for mobile manipulation tasks in agricultural and space
applications. Our learning method also provides a supervision mechanism for
perception based traversability estimation and planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12644v1' target='_blank'>VLMgineer: Vision Language Models as Robotic Toolsmiths</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:George Jiayuan Gao, Tianyu Li, Junyao Shi, Yihan Li, Zizhe Zhang, Nadia Figueroa, Dinesh Jayaraman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 21:30:05</h6>
<p class='card-text'>Tool design and use reflect the ability to understand and manipulate the
physical world through creativity, planning, and foresight. As such, these
capabilities are often regarded as measurable indicators of intelligence across
biological species. While much of today's research on robotic intelligence
focuses on generating better controllers, inventing smarter tools offers a
complementary form of physical intelligence: shifting the onus of
problem-solving onto the tool's design. Given the vast and impressive
common-sense, reasoning, and creative capabilities of today's foundation
models, we investigate whether these models can provide useful priors to
automatically design and effectively wield such tools? We present VLMgineer, a
framework that harnesses the code generation abilities of vision language
models (VLMs) together with evolutionary search to iteratively co-design
physical tools and the action plans that operate them to perform a task. We
evaluate VLMgineer on a diverse new benchmark of everyday manipulation
scenarios that demand creative tool design and use. Across this suite,
VLMgineer consistently discovers tools and policies that solve tasks more
effectively and innovatively, transforming challenging robotics problems into
straightforward executions. It also outperforms VLM-generated designs from
human specifications and existing human-crafted tools for everyday tasks. To
facilitate future research on automated tool invention, we will release our
benchmark and code.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12555v1' target='_blank'>Can Mental Imagery Improve the Thinking Capabilities of AI Systems?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Slimane Larabi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 18:06:13</h6>
<p class='card-text'>Although existing models can interact with humans and provide satisfactory
responses, they lack the ability to act autonomously or engage in independent
reasoning. Furthermore, input data in these models is typically provided as
explicit queries, even when some sensory data is already acquired.
  In addition, AI agents, which are computational entities designed to perform
tasks and make decisions autonomously based on their programming, data inputs,
and learned knowledge, have shown significant progress. However, they struggle
with integrating knowledge across multiple domains, unlike humans.
  Mental imagery plays a fundamental role in the brain's thinking process,
which involves performing tasks based on internal multisensory data, planned
actions, needs, and reasoning capabilities. In this paper, we investigate how
to integrate mental imagery into a machine thinking framework and how this
could be beneficial in initiating the thinking process. Our proposed machine
thinking framework integrates a Cognitive thinking unit supported by three
auxiliary units: the Input Data Unit, the Needs Unit, and the Mental Imagery
Unit. Within this framework, data is represented as natural language sentences
or drawn sketches, serving both informative and decision-making purposes. We
conducted validation tests for this framework, and the results are presented
and discussed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12449v1' target='_blank'>Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance
  Scenarios</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Van-Hoang-Anh Phan, Chi-Tam Nguyen, Doan-Trung Au, Thanh-Danh Phan, Minh-Thien Duong, My-Ha Le</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 17:41:14</h6>
<p class='card-text'>Obstacle avoidance is essential for ensuring the safety of autonomous
vehicles. Accurate perception and motion planning are crucial to enabling
vehicles to navigate complex environments while avoiding collisions. In this
paper, we propose an efficient obstacle avoidance pipeline that leverages a
camera-only perception module and a Frenet-Pure Pursuit-based planning
strategy. By integrating advancements in computer vision, the system utilizes
YOLOv11 for object detection and state-of-the-art monocular depth estimation
models, such as Depth Anything V2, to estimate object distances. A comparative
analysis of these models provides valuable insights into their accuracy,
efficiency, and robustness in real-world conditions. The system is evaluated in
diverse scenarios on a university campus, demonstrating its effectiveness in
handling various obstacles and enhancing autonomous navigation. The video
presenting the results of the obstacle avoidance experiments is available at:
https://www.youtube.com/watch?v=FoXiO5S_tA8</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12407v1' target='_blank'>Regrasp Maps for Sequential Manipulation Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Svetlana Levit, Marc Toussaint</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 16:53:07</h6>
<p class='card-text'>We consider manipulation problems in constrained and cluttered settings,
which require several regrasps at unknown locations. We propose to inform an
optimization-based task and motion planning (TAMP) solver with possible regrasp
areas and grasp sequences to speed up the search. Our main idea is to use a
state space abstraction, a regrasp map, capturing the combinations of available
grasps in different parts of the configuration space, and allowing us to
provide the solver with guesses for the mode switches and additional
constraints for the object placements. By interleaving the creation of regrasp
maps, their adaptation based on failed refinements, and solving TAMP
(sub)problems, we are able to provide a robust search method for challenging
regrasp manipulation problems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12276v1' target='_blank'>Forecasting Climate Policy Uncertainty: Evidence from the United States</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Donia Besher, Anirban Sengupta, Tanujit Chakraborty</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 14:24:31</h6>
<p class='card-text'>Forecasting Climate Policy Uncertainty (CPU) is essential as policymakers
strive to balance economic growth with environmental goals. High levels of CPU
can slow down investments in green technologies, make regulatory planning more
difficult, and increase public resistance to climate reforms, especially during
times of economic stress. This study addresses the challenge of forecasting the
US CPU index by building the Bayesian Structural Time Series (BSTS) model with
a large set of covariates, including economic indicators, financial cycle data,
and public sentiments captured through Google Trends. The key strength of the
BSTS model lies in its ability to efficiently manage a large number of
covariates through its dynamic feature selection mechanism based on the
spike-and-slab prior. To validate the effectiveness of the selected features of
the BSTS model, an impulse response analysis is performed. The results show
that macro-financial shocks impact CPU in different ways over time. Numerical
experiments are performed to evaluate the performance of the BSTS model with
exogenous variables on the US CPU dataset over different forecasting horizons.
The empirical results confirm that BSTS consistently outperforms classical and
deep learning frameworks, particularly for semi-long-term and long-term
forecasts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12186v1' target='_blank'>Partially Observable Reference Policy Programming: Solving POMDPs Sans
  Numerical Optimisation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Edward Kim, Hanna Kurniawati</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 12:33:32</h6>
<p class='card-text'>This paper proposes Partially Observable Reference Policy Programming, a
novel anytime online approximate POMDP solver which samples meaningful future
histories very deeply while simultaneously forcing a gradual policy update. We
provide theoretical guarantees for the algorithm's underlying scheme which say
that the performance loss is bounded by the average of the sampling
approximation errors rather than the usual maximum, a crucial requirement given
the sampling sparsity of online planning. Empirical evaluations on two
large-scale problems with dynamically evolving environments -- including a
helicopter emergency scenario in the Corsica region requiring approximately 150
planning steps -- corroborate the theoretical results and indicate that our
solver considerably outperforms current online benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12174v1' target='_blank'>Fast and Scalable Game-Theoretic Trajectory Planning with Intentional
  Uncertainties</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenmin Huang, Yusen Xie, Benshan Ma, Shaojie Shen, Jun Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 12:12:25</h6>
<p class='card-text'>Trajectory planning involving multi-agent interactions has been a
long-standing challenge in the field of robotics, primarily burdened by the
inherent yet intricate interactions among agents. While game-theoretic methods
are widely acknowledged for their effectiveness in managing multi-agent
interactions, significant impediments persist when it comes to accommodating
the intentional uncertainties of agents. In the context of intentional
uncertainties, the heavy computational burdens associated with existing
game-theoretic methods are induced, leading to inefficiencies and poor
scalability. In this paper, we propose a novel game-theoretic interactive
trajectory planning method to effectively address the intentional uncertainties
of agents, and it demonstrates both high efficiency and enhanced scalability.
As the underpinning basis, we model the interactions between agents under
intentional uncertainties as a general Bayesian game, and we show that its
agent-form equivalence can be represented as a potential game under certain
minor assumptions. The existence and attainability of the optimal interactive
trajectories are illustrated, as the corresponding Bayesian Nash equilibrium
can be attained by optimizing a unified optimization problem. Additionally, we
present a distributed algorithm based on the dual consensus alternating
direction method of multipliers (ADMM) tailored to the parallel solving of the
problem, thereby significantly improving the scalability. The attendant
outcomes from simulations and experiments demonstrate that the proposed method
is effective across a range of scenarios characterized by general forms of
intentional uncertainties. Its scalability surpasses that of existing
centralized and decentralized baselines, allowing for real-time interactive
trajectory planning in uncertain game settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12103v1' target='_blank'>DeepShade: Enable Shade Simulation by Text-conditioned Image Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Longchao Da, Xiangrui Liu, Mithun Shivakoti, Thirulogasankar Pranav Kutralingam, Yezhou Yang, Hua Wei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 10:19:12</h6>
<p class='card-text'>Heatwaves pose a significant threat to public health, especially as global
warming intensifies. However, current routing systems (e.g., online maps) fail
to incorporate shade information due to the difficulty of estimating shades
directly from noisy satellite imagery and the limited availability of training
data for generative models. In this paper, we address these challenges through
two main contributions. First, we build an extensive dataset covering diverse
longitude-latitude regions, varying levels of building density, and different
urban layouts. Leveraging Blender-based 3D simulations alongside building
outlines, we capture building shadows under various solar zenith angles
throughout the year and at different times of day. These simulated shadows are
aligned with satellite images, providing a rich resource for learning shade
patterns. Second, we propose the DeepShade, a diffusion-based model designed to
learn and synthesize shade variations over time. It emphasizes the nuance of
edge features by jointly considering RGB with the Canny edge layer, and
incorporates contrastive learning to capture the temporal change rules of
shade. Then, by conditioning on textual descriptions of known conditions (e.g.,
time of day, solar angles), our framework provides improved performance in
generating shade images. We demonstrate the utility of our approach by using
our shade predictions to calculate shade ratios for real-world route planning
in Tempe, Arizona. We believe this work will benefit society by providing a
reference for urban planning in extreme heat weather and its potential
practical applications in the environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12095v1' target='_blank'>BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Davide Di Nucci, Matteo Tomei, Guido Borghi, Luca Ciuffreda, Roberto Vezzani, Rita Cucchiara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 10:04:35</h6>
<p class='card-text'>Accurate 3D reconstruction of vehicles is vital for applications such as
vehicle inspection, predictive maintenance, and urban planning. Existing
methods like Neural Radiance Fields and Gaussian Splatting have shown
impressive results but remain limited by their reliance on dense input views,
which hinders real-world applicability. This paper addresses the challenge of
reconstructing vehicles from sparse-view inputs, leveraging depth maps and a
robust pose estimation architecture to synthesize novel views and augment
training data. Specifically, we enhance Gaussian Splatting by integrating a
selective photometric loss, applied only to high-confidence pixels, and
replacing standard Structure-from-Motion pipelines with the DUSt3R architecture
to improve camera pose estimation. Furthermore, we present a novel dataset
featuring both synthetic and real-world public transportation vehicles,
enabling extensive evaluation of our approach. Experimental results demonstrate
state-of-the-art performance across multiple benchmarks, showcasing the
method's ability to achieve high-quality reconstructions even under constrained
input conditions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12083v1' target='_blank'>Foresight in Motion: Reinforcing Trajectory Prediction with Reward
  Heuristics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muleilan Pei, Shaoshuai Shi, Xuesong Chen, Xu Liu, Shaojie Shen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 09:46:17</h6>
<p class='card-text'>Motion forecasting for on-road traffic agents presents both a significant
challenge and a critical necessity for ensuring safety in autonomous driving
systems. In contrast to most existing data-driven approaches that directly
predict future trajectories, we rethink this task from a planning perspective,
advocating a "First Reasoning, Then Forecasting" strategy that explicitly
incorporates behavior intentions as spatial guidance for trajectory prediction.
To achieve this, we introduce an interpretable, reward-driven intention
reasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)
scheme. Our method first encodes traffic agents and scene elements into a
unified vectorized representation, then aggregates contextual features through
a query-centric paradigm. This enables the derivation of a reward distribution,
a compact yet informative representation of the target agent's behavior within
the given scene context via IRL. Guided by this reward heuristic, we perform
policy rollouts to reason about multiple plausible intentions, providing
valuable priors for subsequent trajectory generation. Finally, we develop a
hierarchical DETR-like decoder integrated with bidirectional selective state
space models to produce accurate future trajectories along with their
associated probabilities. Extensive experiments on the large-scale Argoverse
and nuScenes motion forecasting datasets demonstrate that our approach
significantly enhances trajectory prediction confidence, achieving highly
competitive performance relative to state-of-the-art methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12067v1' target='_blank'>Robust Route Planning for Sidewalk Delivery Robots</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xing Tong, Michele D. Simoni</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 09:24:19</h6>
<p class='card-text'>Sidewalk delivery robots are a promising solution for urban freight
distribution, reducing congestion compared to trucks and providing a safer,
higher-capacity alternative to drones. However, unreliable travel times on
sidewalks due to pedestrian density, obstacles, and varying infrastructure
conditions can significantly affect their efficiency. This study addresses the
robust route planning problem for sidewalk robots, explicitly accounting for
travel time uncertainty due to varying sidewalk conditions. Optimization is
integrated with simulation to reproduce the effect of obstacles and pedestrian
flows and generate realistic travel times. The study investigates three
different approaches to derive uncertainty sets, including budgeted,
ellipsoidal, and support vector clustering (SVC)-based methods, along with a
distributionally robust method to solve the shortest path (SP) problem. A
realistic case study reproducing pedestrian patterns in Stockholm's city center
is used to evaluate the efficiency of robust routing across various robot
designs and environmental conditions. The results show that, when compared to a
conventional SP, robust routing significantly enhances operational reliability
under variable sidewalk conditions. The Ellipsoidal and DRSP approaches
outperform the other methods, yielding the most efficient paths in terms of
average and worst-case delay. Sensitivity analyses reveal that robust
approaches consistently outperform the conventional SP, particularly for
sidewalk delivery robots that are wider, slower, and have more conservative
navigation behaviors. These benefits are even more pronounced in adverse
weather conditions and high pedestrian congestion scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.11991v1' target='_blank'>Robust Planning for Autonomous Vehicles with Diffusion-Based Failure
  Samplers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Juanran Wang, Marc R. Schlichting, Mykel J. Kochenderfer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 07:43:55</h6>
<p class='card-text'>High-risk traffic zones such as intersections are a major cause of
collisions. This study leverages deep generative models to enhance the safety
of autonomous vehicles in an intersection context. We train a 1000-step
denoising diffusion probabilistic model to generate collision-causing sensor
noise sequences for an autonomous vehicle navigating a four-way intersection
based on the current relative position and velocity of an intruder. Using the
generative adversarial architecture, the 1000-step model is distilled into a
single-step denoising diffusion model which demonstrates fast inference speed
while maintaining similar sampling quality. We demonstrate one possible
application of the single-step model in building a robust planner for the
autonomous vehicle. The planner uses the single-step model to efficiently
sample potential failure cases based on the currently measured traffic state to
inform its decision-making. Through simulation experiments, the robust planner
demonstrates significantly lower failure rate and delay rate compared with the
baseline Intelligent Driver Model controller.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.11980v1' target='_blank'>EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for
  Diffusion Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiajian Xie, Shengyu Zhang, Zhou Zhao, Fan Wu, Fei Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 07:23:14</h6>
<p class='card-text'>Diffusion Models have shown remarkable proficiency in image and video
synthesis. As model size and latency increase limit user experience, hybrid
edge-cloud collaborative framework was recently proposed to realize fast
inference and high-quality generation, where the cloud model initiates
high-quality semantic planning and the edge model expedites later-stage
refinement. However, excessive cloud denoising prolongs inference time, while
insufficient steps cause semantic ambiguity, leading to inconsistency in edge
model output. To address these challenges, we propose EC-Diff that accelerates
cloud inference through gradient-based noise estimation while identifying the
optimal point for cloud-edge handoff to maintain generation quality.
Specifically, we design a K-step noise approximation strategy to reduce cloud
inference frequency by using noise gradients between steps and applying cloud
inference periodically to adjust errors. Then we design a two-stage greedy
search algorithm to efficiently find the optimal parameters for noise
approximation and edge model switching. Extensive experiments demonstrate that
our method significantly enhances generation quality compared to edge
inference, while achieving up to an average $2\times$ speedup in inference
compared to cloud inference. Video samples and source code are available at
https://ec-diff.github.io/.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>