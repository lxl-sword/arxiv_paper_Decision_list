<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>multi-agent - 2025-11-06</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>multi-agent - 2025-11-06</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.02762v1' target='_blank'>From Solo to Symphony: Orchestrating Multi-Agent Collaboration with
  Single-Agent Demos</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xun Wang, Zhuoran Li, Yanshan Lin, Hai Zhong, Longbo Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-04 17:44:11</h6>
<p class='card-text'>Training a team of agents from scratch in multi-agent reinforcement learning
(MARL) is highly inefficient, much like asking beginners to play a symphony
together without first practicing solo. Existing methods, such as offline or
transferable MARL, can ease this burden, but they still rely on costly
multi-agent data, which often becomes the bottleneck. In contrast, solo
experiences are far easier to obtain in many important scenarios, e.g.,
collaborative coding, household cooperation, and search-and-rescue. To unlock
their potential, we propose Solo-to-Collaborative RL (SoCo), a framework that
transfers solo knowledge into cooperative learning. SoCo first pretrains a
shared solo policy from solo demonstrations, then adapts it for cooperation
during multi-agent training through a policy fusion mechanism that combines an
MoE-like gating selector and an action editor. Experiments across diverse
cooperative tasks show that SoCo significantly boosts the training efficiency
and performance of backbone algorithms. These results demonstrate that solo
demonstrations provide a scalable and effective complement to multi-agent data,
making cooperative learning more practical and broadly applicable.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.02314v1' target='_blank'>Large-scale automatic carbon ion treatment planning for head and neck
  cancers via parallel multi-agent reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jueye Zhang, Chao Yang, Youfang Lai, Kai-Wen Li, Wenting Yan, Yunzhou Xia, Haimei Zhang, Jingjing Zhou, Gen Yang, Chen Lin, Tian Li, Yibao Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-04 06:57:31</h6>
<p class='card-text'>Head-and-neck cancer (HNC) planning is difficult because multiple critical
organs-at-risk (OARs) are close to complex targets. Intensity-modulated
carbon-ion therapy (IMCT) offers superior dose conformity and OAR sparing but
remains slow due to relative biological effectiveness (RBE) modeling, leading
to laborious, experience-based, and often suboptimal tuning of many
treatment-planning parameters (TPPs). Recent deep learning (DL) methods are
limited by data bias and plan feasibility, while reinforcement learning (RL)
struggles to efficiently explore the exponentially large TPP search space. We
propose a scalable multi-agent RL (MARL) framework for parallel tuning of 45
TPPs in IMCT. It uses a centralized-training decentralized-execution (CTDE)
QMIX backbone with Double DQN, Dueling DQN, and recurrent encoding (DRQN) for
stable learning in a high-dimensional, non-stationary environment. To enhance
efficiency, we (1) use compact historical DVH vectors as state inputs, (2)
apply a linear action-to-value transform mapping small discrete actions to
uniform parameter adjustments, and (3) design an absolute, clinically informed
piecewise reward aligned with plan scores. A synchronous multi-process worker
system interfaces with the PHOENIX TPS for parallel optimization and
accelerated data collection. On a head-and-neck dataset (10 training, 10
testing), the method tuned 45 parameters simultaneously and produced plans
comparable to or better than expert manual ones (relative plan score: RL
$85.93\pm7.85%$ vs Manual $85.02\pm6.92%$), with significant (p-value $<$ 0.05)
improvements for five OARs. The framework efficiently explores high-dimensional
TPP spaces and generates clinically competitive IMCT plans through direct TPS
interaction, notably improving OAR sparing.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.02304v1' target='_blank'>Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Beyazit Yalcinkaya, Marcell Vazquez-Chanlatte, Ameesh Shah, Hanna Krasowski, Sanjit A. Seshia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-04 06:37:36</h6>
<p class='card-text'>We study the problem of learning multi-task, multi-agent policies for
cooperative, temporal objectives, under centralized training, decentralized
execution. In this setting, using automata to represent tasks enables the
decomposition of complex tasks into simpler sub-tasks that can be assigned to
agents. However, existing approaches remain sample-inefficient and are limited
to the single-task case. In this work, we present Automata-Conditioned
Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for
learning task-conditioned, decentralized team policies. We identify the main
challenges to ACC-MARL's feasibility in practice, propose solutions, and prove
the correctness of our approach. We further show that the value functions of
learned policies can be used to assign tasks optimally at test time.
Experiments show emergent task-aware, multi-step coordination among agents,
e.g., pressing a button to unlock a door, holding the door, and
short-circuiting tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.02192v1' target='_blank'>A Quantitative Comparison of Centralised and Distributed Reinforcement
  Learning-Based Control for Soft Robotic Arms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Linxin Hou, Qirui Wu, Zhihang Qin, Neil Banerjee, Yongxin Guo, Cecilia Laschi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-04 02:17:30</h6>
<p class='card-text'>This paper presents a quantitative comparison between centralised and
distributed multi-agent reinforcement learning (MARL) architectures for
controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using
PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy
Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical
budgets. Both approaches are based on the arm having $n$ number of controlled
sections. The study systematically varies $n$ and evaluates the performance of
the arm to reach a fixed target in three scenarios: default baseline condition,
recovery from external disturbance, and adaptation to actuator failure.
Quantitative metrics used for the evaluation are mean action magnitude, mean
final distance, mean episode length, and success rate. The results show that
there are no significant benefits of the distributed policy when the number of
controlled sections $n\le4$. In very simple systems, when $n\le2$, the
centralised policy outperforms the distributed one. When $n$ increases to $4<
n\le 12$, the distributed policy shows a high sample efficiency. In these
systems, distributed policy promotes a stronger success rate, resilience, and
robustness under local observability and yields faster convergence given the
same sample size. However, centralised policies achieve much higher time
efficiency during training as it takes much less time to train the same size of
samples. These findings highlight the trade-offs between centralised and
distributed policy in reinforcement learning-based control for soft robotic
systems and provide actionable design guidance for future sim-to-real transfer
in soft rod-like manipulators.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.02136v1' target='_blank'>JaxMARL-HFT: GPU-Accelerated Large-Scale Multi-Agent Reinforcement
  Learning for High-Frequency Trading</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Valentin Mohl, Sascha Frey, Reuben Leyland, Kang Li, George Nigmatulin, Mihai Cucuringu, Stefan Zohren, Jakob Foerster, Anisoara Calinescu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-03 23:56:15</h6>
<p class='card-text'>Agent-based modelling (ABM) approaches for high-frequency financial markets
are difficult to calibrate and validate, partly due to the large parameter
space created by defining fixed agent policies. Multi-agent reinforcement
learning (MARL) enables more realistic agent behaviour and reduces the number
of free parameters, but the heavy computational cost has so far limited
research efforts. To address this, we introduce JaxMARL-HFT (JAX-based
Multi-Agent Reinforcement Learning for High-Frequency Trading), the first
GPU-accelerated open-source multi-agent reinforcement learning environment for
high-frequency trading (HFT) on market-by-order (MBO) data. Extending the
JaxMARL framework and building on the JAX-LOB implementation, JaxMARL-HFT is
designed to handle a heterogeneous set of agents, enabling diverse
observation/action spaces and reward functions. It is designed flexibly, so it
can also be used for single-agent RL, or extended to act as an ABM with
fixed-policy agents. Leveraging JAX enables up to a 240x reduction in
end-to-end training time, compared with state-of-the-art reference
implementations on the same hardware. This significant speed-up makes it
feasible to exploit the large, granular datasets available in high-frequency
trading, and to perform the extensive hyperparameter sweeps required for robust
and efficient MARL research in trading. We demonstrate the use of JaxMARL-HFT
with independent Proximal Policy Optimization (IPPO) for a two-player
environment, with an order execution and a market making agent, using one year
of LOB data (400 million orders), and show that these agents learn to
outperform standard benchmarks. The code for the JaxMARL-HFT framework is
available on GitHub.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.02016v1' target='_blank'>ABIDES-MARL: A Multi-Agent Reinforcement Learning Environment for
  Endogenous Price Formation and Execution in a Limit Order Book</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Patrick Cheridito, Jean-Loup Dupret, Zhexin Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-03 19:42:17</h6>
<p class='card-text'>We present ABIDES-MARL, a framework that combines a new multi-agent
reinforcement learning (MARL) methodology with a new realistic limit-order-book
(LOB) simulation system to study equilibrium behavior in complex financial
market games. The system extends ABIDES-Gym by decoupling state collection from
kernel interruption, enabling synchronized learning and decision-making for
multiple adaptive agents while maintaining compatibility with standard RL
libraries. It preserves key market features such as price-time priority and
discrete tick sizes. Methodologically, we use MARL to approximate
equilibrium-like behavior in multi-period trading games with a finite number of
heterogeneous agents-an informed trader, a liquidity trader, noise traders, and
competing market makers-all with individual price impacts. This setting bridges
optimal execution and market microstructure by embedding the liquidity trader's
optimization problem within a strategic trading environment. We validate the
approach by solving an extended Kyle model within the simulation system,
recovering the gradual price discovery phenomenon. We then extend the analysis
to a liquidity trader's problem where market liquidity arises endogenously and
show that, at equilibrium, execution strategies shape market-maker behavior and
price dynamics. ABIDES-MARL provides a reproducible foundation for analyzing
equilibrium and strategic adaptation in realistic markets and contributes
toward building economically interpretable agentic AI systems for finance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.01554v1' target='_blank'>Learning what to say and how precisely: Efficient Communication via
  Differentiable Discrete Communication Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aditya Kapoor, Yash Bhisikar, Benjamin Freed, Jan Peters, Mingfei Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-03 13:16:57</h6>
<p class='card-text'>Effective communication in multi-agent reinforcement learning (MARL) is
critical for success but constrained by bandwidth, yet past approaches have
been limited to complex gating mechanisms that only decide \textit{whether} to
communicate, not \textit{how precisely}. Learning to optimize message precision
at the bit-level is fundamentally harder, as the required discretization step
breaks gradient flow. We address this by generalizing Differentiable Discrete
Communication Learning (DDCL), a framework for end-to-end optimization of
discrete messages. Our primary contribution is an extension of DDCL to support
unbounded signals, transforming it into a universal, plug-and-play layer for
any MARL architecture. We verify our approach with three key results. First,
through a qualitative analysis in a controlled environment, we demonstrate
\textit{how} agents learn to dynamically modulate message precision according
to the informational needs of the task. Second, we integrate our variant of
DDCL into four state-of-the-art MARL algorithms, showing it reduces bandwidth
by over an order of magnitude while matching or exceeding task performance.
Finally, we provide direct evidence for the \enquote{Bitter Lesson} in MARL
communication: a simple Transformer-based policy leveraging DDCL matches the
performance of complex, specialized architectures, questioning the necessity of
bespoke communication designs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.01310v1' target='_blank'>From Pixels to Cooperation Multi Agent Reinforcement Learning based on
  Multimodal World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sureyya Akin, Kavita Srivastava, Prateek B. Kapoor, Pradeep G. Sethi, Sunita Q. Patel, Rahu Srivastava</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-03 07:44:56</h6>
<p class='card-text'>Learning cooperative multi-agent policies directly from high-dimensional,
multimodal sensory inputs like pixels and audio (from pixels) is notoriously
sample-inefficient. Model-free Multi-Agent Reinforcement Learning (MARL)
algorithms struggle with the joint challenge of representation learning,
partial observability, and credit assignment. To address this, we propose a
novel framework based on a shared, generative Multimodal World Model (MWM). Our
MWM is trained to learn a compressed latent representation of the environment's
dynamics by fusing distributed, multimodal observations from all agents using a
scalable attention-based mechanism. Subsequently, we leverage this learned MWM
as a fast, "imagined" simulator to train cooperative MARL policies (e.g.,
MAPPO) entirely within its latent space, decoupling representation learning
from policy learning. We introduce a new set of challenging multimodal,
multi-agent benchmarks built on a 3D physics simulator. Our experiments
demonstrate that our MWM-MARL framework achieves orders-of-magnitude greater
sample efficiency compared to state-of-the-art model-free MARL baselines. We
further show that our proposed multimodal fusion is essential for task success
in environments with sensory asymmetry and that our architecture provides
superior robustness to sensor-dropout, a critical feature for real-world
deployment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.01078v1' target='_blank'>Predictive Auxiliary Learning for Belief-based Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qinwei Huang, Stefan Wang, Simon Khan, Garrett Katz, Qinru Qiu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-02 21:05:03</h6>
<p class='card-text'>The performance of multi-agent reinforcement learning (MARL) in partially
observable environments depends on effectively aggregating information from
observations, communications, and reward signals. While most existing
multi-agent systems primarily rely on rewards as the only feedback for policy
training, our research shows that introducing auxiliary predictive tasks can
significantly enhance learning efficiency and stability. We propose
Belief-based Predictive Auxiliary Learning (BEPAL), a framework that
incorporates auxiliary training objectives to support policy optimization.
BEPAL follows the centralized training with decentralized execution paradigm.
Each agent learns a belief model that predicts unobservable state information,
such as other agents' rewards or motion directions, alongside its policy model.
By enriching hidden state representations with information that does not
directly contribute to immediate reward maximization, this auxiliary learning
process stabilizes MARL training and improves overall performance. We evaluate
BEPAL in the predator-prey environment and Google Research Football, where it
achieves an average improvement of about 16 percent in performance metrics and
demonstrates more stable convergence compared to baseline methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.27659v1' target='_blank'>Challenges in Credit Assignment for Multi-Agent Reinforcement Learning
  in Open Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alireza Saleh Abadi, Leen-Kiat Soh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-31 17:30:32</h6>
<p class='card-text'>In the rapidly evolving field of multi-agent reinforcement learning (MARL),
understanding the dynamics of open systems is crucial. Openness in MARL refers
to the dynam-ic nature of agent populations, tasks, and agent types with-in a
system. Specifically, there are three types of openness as reported in (Eck et
al. 2023) [2]: agent openness, where agents can enter or leave the system at
any time; task openness, where new tasks emerge, and existing ones evolve or
disappear; and type openness, where the capabil-ities and behaviors of agents
change over time. This report provides a conceptual and empirical review,
focusing on the interplay between openness and the credit assignment problem
(CAP). CAP involves determining the contribution of individual agents to the
overall system performance, a task that becomes increasingly complex in open
environ-ments. Traditional credit assignment (CA) methods often assume static
agent populations, fixed and pre-defined tasks, and stationary types, making
them inadequate for open systems. We first conduct a conceptual analysis,
in-troducing new sub-categories of openness to detail how events like agent
turnover or task cancellation break the assumptions of environmental
stationarity and fixed team composition that underpin existing CAP methods. We
then present an empirical study using representative temporal and structural
algorithms in an open environment. The results demonstrate that openness
directly causes credit misattribution, evidenced by unstable loss functions and
significant performance degradation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.26389v1' target='_blank'>Adaptive Context Length Optimization with Low-Frequency Truncation for
  Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenchang Duan, Yaoliang Yu, Jiwan He, Yi Shi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-30 11:32:45</h6>
<p class='card-text'>Recently, deep multi-agent reinforcement learning (MARL) has demonstrated
promising performance for solving challenging tasks, such as long-term
dependencies and non-Markovian environments. Its success is partly attributed
to conditioning policies on large fixed context length. However, such large
fixed context lengths may lead to limited exploration efficiency and redundant
information. In this paper, we propose a novel MARL framework to obtain
adaptive and effective contextual information. Specifically, we design a
central agent that dynamically optimizes context length via temporal gradient
analysis, enhancing exploration to facilitate convergence to global optima in
MARL. Furthermore, to enhance the adaptive optimization capability of the
context length, we present an efficient input representation for the central
agent, which effectively filters redundant information. By leveraging a
Fourier-based low-frequency truncation method, we extract global temporal
trends across decentralized agents, providing an effective and efficient
representation of the MARL environment. Extensive experiments demonstrate that
the proposed method achieves state-of-the-art (SOTA) performance on long-term
dependency tasks, including PettingZoo, MiniGrid, Google Research Football
(GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.26089v1' target='_blank'>Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle
  Routing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fazel Arasteh, Arian Haghparast, Manos Papagelis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-30 02:49:46</h6>
<p class='card-text'>Traffic congestion in urban road networks leads to longer trip times and
higher emissions, especially during peak periods. While the Shortest Path First
(SPF) algorithm is optimal for a single vehicle in a static network, it
performs poorly in dynamic, multi-vehicle settings, often worsening congestion
by routing all vehicles along identical paths. We address dynamic vehicle
routing through a multi-agent reinforcement learning (MARL) framework for
coordinated, network-aware fleet navigation. We first propose Adaptive
Navigation (AN), a decentralized MARL model where each intersection agent
provides routing guidance based on (i) local traffic and (ii) neighborhood
state modeled using Graph Attention Networks (GAT). To improve scalability in
large networks, we further propose Hierarchical Hub-based Adaptive Navigation
(HHAN), an extension of AN that assigns agents only to key intersections
(hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles
micro-routing within each hub region. For hub coordination, HHAN adopts
centralized training with decentralized execution (CTDE) under the Attentive
Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions
via attention. Hub agents use flow-aware state features that combine local
congestion and predictive dynamics for proactive routing. Experiments on
synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces
average travel time versus SPF and learning baselines, maintaining 100% routing
success. HHAN scales to networks with hundreds of intersections, achieving up
to 15.9% improvement under heavy traffic. These findings highlight the
potential of network-constrained MARL for scalable, coordinated, and
congestion-aware routing in intelligent transportation systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.25340v1' target='_blank'>Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Beiwen Zhang, Yongheng Liang, Hejun Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-29 09:53:07</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARl) has achieved strong results in
cooperative tasks but typically assumes fixed, fully controlled teams. Ad hoc
teamwork (AHT) relaxes this by allowing collaboration with unknown partners,
yet existing variants still presume shared conventions. We introduce
Multil-party Ad Hoc Teamwork (MAHT), where controlled agents must coordinate
with multiple mutually unfamiliar groups of uncontrolled teammates. To address
this, we propose MARs, which builds a sparse skeleton graph and applies
relational modeling to capture cross-group dvnamics. Experiments on MPE and
starCralt ll show that MARs outperforms MARL and AHT baselines while converging
faster.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.25212v1' target='_blank'>Collaborative Scheduling of Time-dependent UAVs,Vehicles and Workers for
  Crowdsensing in Disaster Response</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lei Han, Jinhao Zhang, Jinhui Liu, Zhiyong Yu, Liang Wang, Quan Wang, Zhiwen Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-29 06:26:36</h6>
<p class='card-text'>Frequent natural disasters cause significant losses to human society, and
timely, efficient collection of post-disaster environmental information is the
foundation for effective rescue operations. Due to the extreme complexity of
post-disaster environments, existing sensing technologies such as mobile
crowdsensing suffer from weak environmental adaptability, insufficient
professional sensing capabilities, and poor practicality of sensing solutions.
Therefore, this paper explores a heterogeneous multi-agent online collaborative
scheduling algorithm, HoCs-MPQ, to achieve efficient collection of
post-disaster environmental information. HoCs-MPQ models collaboration and
conflict relationships among multiple elements through weighted undirected
graph construction, and iteratively solves the maximum weight independent set
based on multi-priority queues, ultimately achieving collaborative sensing
scheduling of time-dependent UA Vs, vehicles, and workers. Specifically, (1)
HoCs-MPQ constructs weighted undirected graph nodes based on collaborative
relationships among multiple elements and quantifies their weights, then models
the weighted undirected graph based on conflict relationships between nodes;
(2) HoCs-MPQ solves the maximum weight independent set based on iterated local
search, and accelerates the solution process using multi-priority queues.
Finally, we conducted detailed experiments based on extensive real-world and
simulated data. The experiments show that, compared to baseline methods (e.g.,
HoCs-GREEDY, HoCs-K-WTA, HoCs-MADL, and HoCs-MARL), HoCs-MPQ improves task
completion rates by an average of 54.13%, 23.82%, 14.12%, and 12.89%
respectively, with computation time for single online autonomous scheduling
decisions not exceeding 3 seconds.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.00039v1' target='_blank'>Graph-Attentive MAPPO for Dynamic Retail Pricing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Krishna Kumar Neelakanta Pillai Santha Kumari Amma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-28 00:15:59</h6>
<p class='card-text'>Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23535v1' target='_blank'>Sequential Multi-Agent Dynamic Algorithm Configuration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chen Lu, Ke Xue, Lei Yuan, Yao Wang, Yaoyuan Wang, Sheng Fu, Chao Qian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 17:11:03</h6>
<p class='card-text'>Dynamic algorithm configuration (DAC) is a recent trend in automated machine
learning, which can dynamically adjust the algorithm's configuration during the
execution process and relieve users from tedious trial-and-error tuning tasks.
Recently, multi-agent reinforcement learning (MARL) approaches have improved
the configuration of multiple heterogeneous hyperparameters, making various
parameter configurations for complex algorithms possible. However, many complex
algorithms have inherent inter-dependencies among multiple parameters (e.g.,
determining the operator type first and then the operator's parameter), which
are, however, not considered in previous approaches, thus leading to
sub-optimal results. In this paper, we propose the sequential multi-agent DAC
(Seq-MADAC) framework to address this issue by considering the inherent
inter-dependencies of multiple parameters. Specifically, we propose a
sequential advantage decomposition network, which can leverage action-order
information through sequential advantage decomposition. Experiments from
synthetic functions to the configuration of multi-objective optimization
algorithms demonstrate Seq-MADAC's superior performance over state-of-the-art
MARL methods and show strong generalization across problem classes. Seq-MADAC
establishes a new paradigm for the widespread dependency-aware automated
algorithm configuration. Our code is available at
https://github.com/lamda-bbo/seq-madac.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22969v1' target='_blank'>Multi-Agent Conditional Diffusion Model with Mean Field Communication as
  Wireless Resource Allocation Planner</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kechen Meng, Sinuo Zhang, Rongpeng Li, Xiangming Meng, Chan Wang, Ming Lei, Zhifeng Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 03:42:18</h6>
<p class='card-text'>In wireless communication systems, efficient and adaptive resource allocation
plays a crucial role in enhancing overall Quality of Service (QoS). While
centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a
central coordinator for policy training and resource scheduling, they suffer
from scalability issues and privacy risks. In contrast, the Distributed
Training with Decentralized Execution (DTDE) paradigm enables distributed
learning and decision-making, but it struggles with non-stationarity and
limited inter-agent cooperation, which can severely degrade system performance.
To overcome these challenges, we propose the Multi-Agent Conditional Diffusion
Model Planner (MA-CDMP) for decentralized communication resource management.
Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP
employs Diffusion Models (DMs) to capture environment dynamics and plan future
trajectories, while an inverse dynamics model guides action generation, thereby
alleviating the sample inefficiency and slow convergence of conventional DTDE
methods. Moreover, to approximate large-scale agent interactions, a Mean-Field
(MF) mechanism is introduced as an assistance to the classifier in DMs. This
design mitigates inter-agent non-stationarity and enhances cooperation with
minimal communication overhead in distributed settings. We further
theoretically establish an upper bound on the distributional approximation
error introduced by the MF-based diffusion generation, guaranteeing convergence
stability and reliable modeling of multi-agent stochastic dynamics. Extensive
experiments demonstrate that MA-CDMP consistently outperforms existing MARL
baselines in terms of average reward and QoS metrics, showcasing its
scalability and practicality for real-world wireless network optimization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22740v1' target='_blank'>Policies over Poses: Reinforcement Learning based Distributed Pose-Graph
  Optimization for Multi-Robot SLAM</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sai Krishna Ghanta, Ramviyas Parasuraman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-26 16:21:24</h6>
<p class='card-text'>We consider the distributed pose-graph optimization (PGO) problem, which is
fundamental in accurate trajectory estimation in multi-robot simultaneous
localization and mapping (SLAM). Conventional iterative approaches linearize a
highly non-convex optimization objective, requiring repeated solving of normal
equations, which often converge to local minima and thus produce suboptimal
estimates. We propose a scalable, outlier-robust distributed planar PGO
framework using Multi-Agent Reinforcement Learning (MARL). We cast distributed
PGO as a partially observable Markov game defined on local pose-graphs, where
each action refines a single edge's pose estimate. A graph partitioner
decomposes the global pose graph, and each robot runs a recurrent
edge-conditioned Graph Neural Network (GNN) encoder with adaptive edge-gating
to denoise noisy edges. Robots sequentially refine poses through a hybrid
policy that utilizes prior action memory and graph embeddings. After local
graph correction, a consensus scheme reconciles inter-robot disagreements to
produce a globally consistent estimate. Our extensive evaluations on a
comprehensive suite of synthetic and real-world datasets demonstrate that our
learned MARL-based actors reduce the global objective by an average of 37.5%
more than the state-of-the-art distributed PGO framework, while enhancing
inference efficiency by at least 6X. We also demonstrate that actor replication
allows a single learned policy to scale effortlessly to substantially larger
robot teams without any retraining. Code is publicly available at
https://github.com/herolab-uga/policies-over-poses.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.21103v1' target='_blank'>Sensing and Storing Less: A MARL-based Solution for Energy Saving in
  Edge Internet of Things</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zongyang Yuan, Lailong Luo, Qianzhen Zhang, Bangbang Ren, Deke Guo, Richard T. B. Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-24 02:38:23</h6>
<p class='card-text'>As the number of Internet of Things (IoT) devices continuously grows and
application scenarios constantly enrich, the volume of sensor data experiences
an explosive increase. However, substantial data demands considerable energy
during computation and transmission. Redundant deployment or mobile assistance
is essential to cover the target area reliably with fault-prone sensors.
Consequently, the ``butterfly effect" may appear during the IoT operation,
since unreasonable data overlap could result in many duplicate data. To this
end, we propose Senses, a novel online energy saving solution for edge IoT
networks, with the insight of sensing and storing less at the network edge by
adopting Muti-Agent Reinforcement Learning (MARL). Senses achieves data
de-duplication by dynamically adjusting sensor coverage at the sensor level.
For exceptional cases where sensor coverage cannot be altered, Senses conducts
data partitioning and eliminates redundant data at the controller level.
Furthermore, at the global level, considering the heterogeneity of IoT devices,
Senses balances the operational duration among the devices to prolong the
overall operational duration of edge IoT networks. We evaluate the performance
of Senses through testbed experiments and simulations. The results show that
Senses saves 11.37% of energy consumption on control devices and prolongs 20%
overall operational duration of the IoT device network.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20436v1' target='_blank'>Learning Decentralized Routing Policies via Graph Attention-based
  Multi-Agent Reinforcement Learning in Lunar Delay-Tolerant Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Federico Lozano-Cuadra, Beatriz Soret, Marc Sanchez Net, Abhishek Cauligi, Federico Rossi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 11:13:11</h6>
<p class='card-text'>We present a fully decentralized routing framework for multi-robot
exploration missions operating under the constraints of a Lunar Delay-Tolerant
Network (LDTN). In this setting, autonomous rovers must relay collected data to
a lander under intermittent connectivity and unknown mobility patterns. We
formulate the problem as a Partially Observable Markov Decision Problem (POMDP)
and propose a Graph Attention-based Multi-Agent Reinforcement Learning
(GAT-MARL) policy that performs Centralized Training, Decentralized Execution
(CTDE). Our method relies only on local observations and does not require
global topology updates or packet replication, unlike classical approaches such
as shortest path and controlled flooding-based algorithms. Through Monte Carlo
simulations in randomized exploration environments, GAT-MARL provides higher
delivery rates, no duplications, and fewer packet losses, and is able to
leverage short-term mobility forecasts; offering a scalable solution for future
space robotic systems for planetary exploration, as demonstrated by successful
generalization to larger rover teams.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20218v1' target='_blank'>High-order Interactions Modeling for Interpretable Multi-Agent
  Q-Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qinyu Xu, Yuanyang Zhu, Xuefei Wu, Chunlin Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 05:08:32</h6>
<p class='card-text'>The ability to model interactions among agents is crucial for effective
coordination and understanding their cooperation mechanisms in multi-agent
reinforcement learning (MARL). However, previous efforts to model high-order
interactions have been primarily hindered by the combinatorial explosion or the
opaque nature of their black-box network structures. In this paper, we propose
a novel value decomposition framework, called Continued Fraction Q-Learning
(QCoFr), which can flexibly capture arbitrary-order agent interactions with
only linear complexity $\mathcal{O}\left({n}\right)$ in the number of agents,
thus avoiding the combinatorial explosion when modeling rich cooperation.
Furthermore, we introduce the variational information bottleneck to extract
latent information for estimating credits. This latent information helps agents
filter out noisy interactions, thereby significantly enhancing both cooperation
and interpretability. Extensive experiments demonstrate that QCoFr not only
consistently achieves better performance but also provides interpretability
that aligns with our theoretical analysis.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.17697v3' target='_blank'>A Principle of Targeted Intervention for Multi-Agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anjie Liu, Jianhong Wang, Samuel Kaski, Jun Wang, Mengyue Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-20 16:10:56</h6>
<p class='card-text'>Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing external mechanisms (e.g., intrinsic rewards and
human feedback) to coordinate agents mostly relies on empirical studies,
lacking a easy-to-use research tool. In this work, we employ multi-agent
influence diagrams (MAIDs) as a graphical framework to address the above
issues. First, we introduce the concept of MARL interaction paradigms
(orthogonal to MARL learning paradigms), using MAIDs to analyze and visualize
both unguided self-organization and global guidance mechanisms in MARL. Then,
we design a new MARL interaction paradigm, referred to as the targeted
intervention paradigm that is applied to only a single targeted agent, so the
problem of global guidance can be mitigated. In implementation, we introduce a
causal inference technique, referred to as Pre-Strategy Intervention (PSI), to
realize the targeted intervention paradigm. Since MAIDs can be regarded as a
special class of causal diagrams, a composite desired outcome that integrates
the primary task goal and an additional desired outcome can be achieved by
maximizing the corresponding causal effect through the PSI. Moreover, the
bundled relevance graph analysis of MAIDs provides a tool to identify whether
an MARL learning paradigm is workable under the design of an MARL interaction
paradigm. In experiments, we demonstrate the effectiveness of our proposed
targeted intervention, and verify the result of relevance graph analysis.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.16035v1' target='_blank'>RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced
  Manipulation of Bots Control Interaction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yingguang Yang, Xianghua Zeng, Qi Wu, Hao Peng, Yutong Xia, Hao Liu, Bin Chong, Philip S. Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-16 02:41:49</h6>
<p class='card-text'>Social networks have become a crucial source of real-time information for
individuals. The influence of social bots within these platforms has garnered
considerable attention from researchers, leading to the development of numerous
detection technologies. However, the vulnerability and robustness of these
detection methods is still underexplored. Existing Graph Neural Network
(GNN)-based methods cannot be directly applied due to the issues of limited
control over social agents, the black-box nature of bot detectors, and the
heterogeneity of bots. To address these challenges, this paper proposes the
first adversarial multi-agent Reinforcement learning framework for social Bot
control attacks (RoBCtrl) targeting GNN-based social bot detectors.
Specifically, we use a diffusion model to generate high-fidelity bot accounts
by reconstructing existing account data with minor modifications, thereby
evading detection on social platforms. To the best of our knowledge, this is
the first application of diffusion models to mimic the behavior of evolving
social bots effectively. We then employ a Multi-Agent Reinforcement Learning
(MARL) method to simulate bots adversarial behavior. We categorize social
accounts based on their influence and budget. Different agents are then
employed to control bot accounts across various categories, optimizing the
attachment strategy through reinforcement learning. Additionally, a
hierarchical state abstraction based on structural entropy is designed to
accelerate the reinforcement learning. Extensive experiments on social bot
detection datasets demonstrate that our framework can effectively undermine the
performance of GNN-based detectors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.13343v1' target='_blank'>AOAD-MAT: Transformer-based multi-agent deep reinforcement learning
  model considering agents' order of action decisions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shota Takayama, Katsuhide Fujita</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-15 09:29:36</h6>
<p class='card-text'>Multi-agent reinforcement learning focuses on training the behaviors of
multiple learning agents that coexist in a shared environment. Recently, MARL
models, such as the Multi-Agent Transformer (MAT) and ACtion dEpendent deep
Q-learning (ACE), have significantly improved performance by leveraging
sequential decision-making processes. Although these models can enhance
performance, they do not explicitly consider the importance of the order in
which agents make decisions. In this paper, we propose an Agent Order of Action
Decisions-MAT (AOAD-MAT), a novel MAT model that considers the order in which
agents make decisions. The proposed model explicitly incorporates the sequence
of action decisions into the learning process, allowing the model to learn and
predict the optimal order of agent actions. The AOAD-MAT model leverages a
Transformer-based actor-critic architecture that dynamically adjusts the
sequence of agent actions. To achieve this, we introduce a novel MARL
architecture that cooperates with a subtask focused on predicting the next
agent to act, integrated into a Proximal Policy Optimization based loss
function to synergistically maximize the advantage of the sequential
decision-making. The proposed method was validated through extensive
experiments on the StarCraft Multi-Agent Challenge and Multi-Agent MuJoCo
benchmarks. The experimental results show that the proposed AOAD-MAT model
outperforms existing MAT and other baseline models, demonstrating the
effectiveness of adjusting the AOAD order in MARL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12272v1' target='_blank'>Heterogeneous RBCs via deep multi-agent reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Federico Gabriele, Aldo Glielmo, Marco Taboga</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 08:26:18</h6>
<p class='card-text'>Current macroeconomic models with agent heterogeneity can be broadly divided
into two main groups. Heterogeneous-agent general equilibrium (GE) models, such
as those based on Heterogeneous Agents New Keynesian (HANK) or Krusell-Smith
(KS) approaches, rely on GE and 'rational expectations', somewhat unrealistic
assumptions that make the models very computationally cumbersome, which in turn
limits the amount of heterogeneity that can be modelled. In contrast,
agent-based models (ABMs) can flexibly encompass a large number of arbitrarily
heterogeneous agents, but typically require the specification of explicit
behavioural rules, which can lead to a lengthy trial-and-error
model-development process. To address these limitations, we introduce MARL-BC,
a framework that integrates deep multi-agent reinforcement learning (MARL) with
Real Business Cycle (RBC) models. We demonstrate that MARL-BC can: (1) recover
textbook RBC results when using a single agent; (2) recover the results of the
mean-field KS model using a large number of identical agents; and (3)
effectively simulate rich heterogeneity among agents, a hard task for
traditional GE approaches. Our framework can be thought of as an ABM if used
with a variety of heterogeneous interacting agents, and can reproduce GE
results in limit cases. As such, it is a step towards a synthesis of these
often opposed modelling paradigms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.11824v2' target='_blank'>Empirical Study on Robustness and Resilience in Cooperative Multi-Agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Simin Li, Zihao Mao, Hanxiao Li, Zonglei Jing, Zhuohang bian, Jun Guo, Li Wang, Zhuoran Han, Ruixiao Xu, Xin Yu, Chengdong Ma, Yuqing Ma, Bo An, Yaodong Yang, Weifeng Lv, Xianglong Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-13 18:24:01</h6>
<p class='card-text'>In cooperative Multi-Agent Reinforcement Learning (MARL), it is a common
practice to tune hyperparameters in ideal simulated environments to maximize
cooperative performance. However, policies tuned for cooperation often fail to
maintain robustness and resilience under real-world uncertainties. Building
trustworthy MARL systems requires a deep understanding of robustness, which
ensures stability under uncertainties, and resilience, the ability to recover
from disruptions--a concept extensively studied in control systems but largely
overlooked in MARL. In this paper, we present a large-scale empirical study
comprising over 82,620 experiments to evaluate cooperation, robustness, and
resilience in MARL across 4 real-world environments, 13 uncertainty types, and
15 hyperparameters. Our key findings are: (1) Under mild uncertainty,
optimizing cooperation improves robustness and resilience, but this link
weakens as perturbations intensify. Robustness and resilience also varies by
algorithm and uncertainty type. (2) Robustness and resilience do not generalize
across uncertainty modalities or agent scopes: policies robust to action noise
for all agents may fail under observation noise on a single agent. (3)
Hyperparameter tuning is critical for trustworthy MARL: surprisingly, standard
practices like parameter sharing, GAE, and PopArt can hurt robustness, while
early stopping, high critic learning rates, and Leaky ReLU consistently help.
By optimizing hyperparameters only, we observe substantial improvement in
cooperation, robustness and resilience across all MARL backbones, with the
phenomenon also generalizing to robust MARL methods across these backbones.
Code and results available at
https://github.com/BUAA-TrustworthyMARL/adv_marl_benchmark .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.11410v2' target='_blank'>Autonomous vehicles need social awareness to find optima in multi-agent
  reinforcement learning routing games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anastasia Psarou, ukasz Gorczyca, Dominik Gawe, Rafa Kucharski</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-13 13:48:38</h6>
<p class='card-text'>Previous work has shown that when multiple selfish Autonomous Vehicles (AVs)
are introduced to future cities and start learning optimal routing strategies
using Multi-Agent Reinforcement Learning (MARL), they may destabilize traffic
systems, as they would require a significant amount of time to converge to the
optimal solution, equivalent to years of real-world commuting.
  We demonstrate that moving beyond the selfish component in the reward
significantly relieves this issue. If each AV, apart from minimizing its own
travel time, aims to reduce its impact on the system, this will be beneficial
not only for the system-wide performance but also for each individual player in
this routing game.
  By introducing an intrinsic reward signal based on the marginal cost matrix,
we significantly reduce training time and achieve convergence more reliably.
Marginal cost quantifies the impact of each individual action (route-choice) on
the system (total travel time). Including it as one of the components of the
reward can reduce the degree of non-stationarity by aligning agents'
objectives. Notably, the proposed counterfactual formulation preserves the
system's equilibria and avoids oscillations.
  Our experiments show that training MARL algorithms with our novel reward
formulation enables the agents to converge to the optimal solution, whereas the
baseline algorithms fail to do so. We show these effects in both a toy network
and the real-world network of Saint-Arnoult. Our results optimistically
indicate that social awareness (i.e., including marginal costs in routing
decisions) improves both the system-wide and individual performance of future
urban systems with AVs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.10895v1' target='_blank'>LLM-Empowered Agentic MAC Protocols: A Dynamic Stackelberg Game Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Renxuan Tan, Rongpeng Li, Fei Wang, Chenghui Peng, Shaoyun Wu, Zhifeng Zhao, Honggang Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-13 01:47:24</h6>
<p class='card-text'>Medium Access Control (MAC) protocols, essential for wireless networks, are
typically manually configured. While deep reinforcement learning (DRL)-based
protocols enhance task-specified network performance, they suffer from poor
generalizability and resilience, demanding costly retraining to adapt to
dynamic environments. To overcome this limitation, we introduce a
game-theoretic LLM-empowered multi-agent DRL (MARL) framework, in which the
uplink transmission between a base station and a varying number of user
equipments is modeled as a dynamic multi-follower Stackelberg game (MFSG),
capturing the network's natural hierarchical structure. Within this game,
LLM-driven agents, coordinated through proximal policy optimization (PPO),
synthesize adaptive, semantic MAC protocols in response to network dynamics.
Protocol action grammar (PAG) is employed to ensure the reliability and
efficiency of this process. Under this system, we further analyze the existence
and convergence behavior in terms of a Stackelberg equilibrium by studying the
learning dynamics of LLM-empowered unified policies in response to changing
followers. Simulations corroborate that our framework achieves a 77.6% greater
throughput and a 65.2% fairness improvement over conventional baselines.
Besides, our framework generalizes excellently to a fluctuating number of users
without requiring retraining or architectural changes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.09937v1' target='_blank'>Structured Cooperative Multi-Agent Reinforcement Learning: a Bayesian
  Network Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shahbaz P Qadri Syed, He Bai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-11 00:29:55</h6>
<p class='card-text'>The empirical success of multi-agent reinforcement learning (MARL) has
motivated the search for more efficient and scalable algorithms for large scale
multi-agent systems. However, existing state-of-the-art algorithms do not fully
exploit inter-agent coupling information to develop MARL algorithms. In this
paper, we propose a systematic approach to leverage structures in the
inter-agent couplings for efficient model-free reinforcement learning. We model
the cooperative MARL problem via a Bayesian network and characterize the subset
of agents, termed as the value dependency set, whose information is required by
each agent to estimate its local action value function exactly. Moreover, we
propose a partially decentralized training decentralized execution (P-DTDE)
paradigm based on the value dependency set. We theoretically establish that the
total variance of our P-DTDE policy gradient estimator is less than the
centralized training decentralized execution (CTDE) policy gradient estimator.
We derive a multi-agent policy gradient theorem based on the P-DTDE scheme and
develop a scalable actor-critic algorithm. We demonstrate the efficiency and
scalability of the proposed algorithm on multi-warehouse resource allocation
and multi-zone temperature control examples. For dense value dependency sets,
we propose an approximation scheme based on truncation of the Bayesian network
and empirically show that it achieves a faster convergence than the exact value
dependence set for applications with a large number of agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.07971v1' target='_blank'>Climate Surrogates for Scalable Multi-Agent Reinforcement Learning: A
  Case Study with CICERO-SCM</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Oskar Bohn Lassen, Serio Angelo Maria Agriesti, Filipe Rodrigues, Francisco Camara Pereira</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-09 09:02:49</h6>
<p class='card-text'>Climate policy studies require models that capture the combined effects of
multiple greenhouse gases on global temperature, but these models are
computationally expensive and difficult to embed in reinforcement learning. We
present a multi-agent reinforcement learning (MARL) framework that integrates a
high-fidelity, highly efficient climate surrogate directly in the environment
loop, enabling regional agents to learn climate policies under multi-gas
dynamics. As a proof of concept, we introduce a recurrent neural network
architecture pretrained on ($20{,}000$) multi-gas emission pathways to
surrogate the climate model CICERO-SCM. The surrogate model attains
near-simulator accuracy with global-mean temperature RMSE $\approx 0.0004
\mathrm{K}$ and approximately $1000\times$ faster one-step inference. When
substituted for the original simulator in a climate-policy MARL setting, it
accelerates end-to-end training by $>\!100\times$. We show that the surrogate
and simulator converge to the same optimal policies and propose a methodology
to assess this property in cases where using the simulator is intractable. Our
work allows to bypass the core computational bottleneck without sacrificing
policy fidelity, enabling large-scale multi-agent experiments across
alternative climate-policy regimes with multi-gas dynamics and high-fidelity
climate response.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>