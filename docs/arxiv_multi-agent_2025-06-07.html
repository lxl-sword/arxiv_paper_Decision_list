<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>multi-agent - 2025-06-07</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>multi-agent - 2025-06-07</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.04276v1' target='_blank'>Autonomous Collaborative Scheduling of Time-dependent UAVs, Workers and
  Vehicles for Crowdsensing in Disaster Response</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lei Han, Yitong Guo, Pengfei Yang, Zhiyong Yu, Liang Wang, Quan Wang, Zhiwen Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-04 01:58:05</h6>
<p class='card-text'>Natural disasters have caused significant losses to human society, and the
timely and efficient acquisition of post-disaster environmental information is
crucial for the effective implementation of rescue operations. Due to the
complexity of post-disaster environments, existing sensing technologies face
challenges such as weak environmental adaptability, insufficient specialized
sensing capabilities, and limited practicality of sensing solutions. This paper
explores the heterogeneous multi-agent online autonomous collaborative
scheduling algorithm HoAs-PALN, aimed at achieving efficient collection of
post-disaster environmental information. HoAs-PALN is realized through adaptive
dimensionality reduction in the matching process and local Nash equilibrium
game, facilitating autonomous collaboration among time-dependent UAVs, workers
and vehicles to enhance sensing scheduling. (1) In terms of adaptive
dimensionality reduction during the matching process, HoAs-PALN significantly
reduces scheduling decision time by transforming a five-dimensional matching
process into two categories of three-dimensional matching processes; (2)
Regarding the local Nash equilibrium game, HoAs-PALN combines the softmax
function to optimize behavior selection probabilities and introduces a local
Nash equilibrium determination mechanism to ensure scheduling decision
performance. Finally, we conducted detailed experiments based on extensive
real-world and simulated data. Compared with the baselines (GREEDY, K-WTA, MADL
and MARL), HoAs-PALN improves task completion rates by 64.12%, 46.48%, 16.55%,
and 14.03% on average, respectively, while each online scheduling decision
takes less than 10 seconds, demonstrating its effectiveness in dynamic
post-disaster environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.02841v1' target='_blank'>Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using
  Ensemble Methods</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tom Danino, Nahum Shimkin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-03 13:13:15</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) methods have achieved
state-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms
typically require significantly more environment interactions than their
single-agent counterparts to converge, a problem exacerbated by the difficulty
in exploring over a large joint action space and the high variance intrinsic to
MARL environments. To tackle these issues, we propose a novel algorithm that
combines a decomposed centralized critic with decentralized ensemble learning,
incorporating several key contributions. The main component in our scheme is a
selective exploration method that leverages ensemble kurtosis. We extend the
global decomposed critic with a diversity-regularized ensemble of individual
critics and utilize its excess kurtosis to guide exploration toward
high-uncertainty states and actions. To improve sample efficiency, we train the
centralized critic with a novel truncated variation of the TD($\lambda$)
algorithm, enabling efficient off-policy learning with reduced variance. On the
actor side, our suggested algorithm adapts the mixed samples approach to MARL,
mixing on-policy and off-policy loss functions for training the actors. This
approach balances between stability and efficiency and outperforms purely
off-policy learning. The evaluation shows our method outperforms
state-of-the-art baselines on standard MARL benchmarks, including a variety of
SMAC II maps.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.02718v1' target='_blank'>Heterogeneous Group-Based Reinforcement Learning for LLM-based
  Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guanzhong Chen, Shaoxiong Yang, Chao Li, Wei Liu, Jian Luan, Zenglin Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-03 10:17:19</h6>
<p class='card-text'>Large Language Models (LLMs) have achieved remarkable success across diverse
natural language processing tasks, yet their deployment in real-world
applications is hindered by fixed knowledge cutoffs and difficulties in
generating controllable, accurate outputs in a single inference. Multi-agent
systems (MAS) built from specialized LLM agents offer a promising solution,
enabling dynamic collaboration and iterative reasoning. However, optimizing
these systems remains a challenge, as conventional methods such as prompt
engineering and supervised fine-tuning entail high engineering overhead and
limited adaptability. Reinforcement learning (RL), particularly multi-agent
reinforcement learning (MARL), provides a scalable framework by refining agent
policies based on system-level feedback. Nevertheless, existing MARL
algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on
Critic networks, which can cause training instability and increase
computational burden. To address these limitations and target the prototypical
Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group
Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy
updates by estimating relative reward advantages across heterogeneous groups of
rollouts. MHGPO eliminates the need for Critic networks, enhancing stability
and reducing computational overhead. Additionally, we introduce three group
rollout sampling strategies that trade off between efficiency and
effectiveness. Experiments on a multi-agent LLM-based search system demonstrate
that MHGPO consistently outperforms MAPPO in both task performance and
computational efficiency, without requiring warm-up, underscoring its potential
for stable and scalable optimization of complex LLM-based MAS.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.04265v1' target='_blank'>CORA: Coalitional Rational Advantage Decomposition for Multi-Agent
  Policy Gradients</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mengda Ji, Genjiu Xu, Liying Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-03 08:04:43</h6>
<p class='card-text'>This work focuses on the credit assignment problem in cooperative multi-agent
reinforcement learning (MARL). Sharing the global advantage among agents often
leads to suboptimal policy updates as it fails to account for the distinct
contributions of agents. Although numerous methods consider global or
individual contributions for credit assignment, a detailed analysis at the
coalition level remains lacking in many approaches. This work analyzes the
over-updating problem during multi-agent policy updates from a coalition-level
perspective. To address this issue, we propose a credit assignment method
called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates
coalitional advantages via marginal contributions from all possible coalitions
and decomposes advantages using the core solution from cooperative game theory,
ensuring coalitional rationality. To reduce computational overhead, CORA
employs random coalition sampling. Experiments on matrix games, differential
games, and multi-agent collaboration benchmarks demonstrate that CORA
outperforms strong baselines, particularly in tasks with multiple local optima.
These findings highlight the importance of coalition-aware credit assignment
for improving MARL performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.01538v2' target='_blank'>LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative
  Policy Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guobin Zhu, Rui Zhou, Wenkang Ji, Shiyu Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-02 10:59:54</h6>
<p class='card-text'>Although Multi-Agent Reinforcement Learning (MARL) is effective for complex
multi-robot tasks, it suffers from low sample efficiency and requires iterative
manual reward tuning. Large Language Models (LLMs) have shown promise in
single-robot settings, but their application in multi-robot systems remains
largely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL)
approach, which integrates MARL with LLMs, significantly enhancing sample
efficiency without requiring manual design. LAMARL consists of two modules: the
first module leverages LLMs to fully automate the generation of prior policy
and reward functions. The second module is MARL, which uses the generated
functions to guide robot policy training effectively. On a shape assembly
benchmark, both simulation and real-world experiments demonstrate the unique
advantages of LAMARL. Ablation studies show that the prior policy improves
sample efficiency by an average of 185.9% and enhances task completion, while
structured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM
output success rates by 28.5%-67.5%. Videos and code are available at
https://windylab.github.io/LAMARL/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.00982v1' target='_blank'>Robust and Safe Multi-Agent Reinforcement Learning Framework with
  Communication for Autonomous Vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Keshawn Smith, Zhili Zhang, H M Sabbir Ahmad, Ehsan Sabouni, Maniak Mondal, Song Han, Wenchao Li, Fei Miao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-01 12:29:53</h6>
<p class='card-text'>Deep multi-agent reinforcement learning (MARL) has been demonstrated
effectively in simulations for many multi-robot problems. For autonomous
vehicles, the development of vehicle-to-vehicle (V2V) communication
technologies provide opportunities to further enhance safety of the system.
However, zero-shot transfer of simulator-trained MARL policies to hardware
dynamic systems remains challenging, and how to leverage communication and
shared information for MARL has limited demonstrations on hardware. This
problem is challenged by discrepancies between simulated and physical states,
system state and model uncertainties, practical shared information design, and
the need for safety guarantees in both simulation and hardware. This paper
introduces RSR-RSMARL, a novel Robust and Safe MARL framework that supports
Real-Sim-Real (RSR) policy adaptation for multi-agent systems with
communication among agents, with both simulation and hardware demonstrations.
RSR-RSMARL leverages state (includes shared state information among agents) and
action representations considering real system complexities for MARL
formulation. The MARL policy is trained with robust MARL algorithm to enable
zero-shot transfer to hardware considering the sim-to-real gap. A safety shield
module using Control Barrier Functions (CBFs) provides safety guarantee for
each individual agent. Experiment results on F1/10th-scale autonomous vehicles
with V2V communication demonstrate the ability of RSR-RSMARL framework to
enhance driving safety and coordination across multiple configurations. These
findings emphasize the importance of jointly designing robust policy
representations and modular safety architectures to enable scalable,
generalizable RSR transfer in multi-agent autonomy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.04251v1' target='_blank'>Language-Guided Multi-Agent Learning in Simulations: A Unified Framework
  and Evaluation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhengyang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-01 06:46:49</h6>
<p class='card-text'>This paper introduces LLM-MARL, a unified framework that incorporates large
language models (LLMs) into multi-agent reinforcement learning (MARL) to
enhance coordination, communication, and generalization in simulated game
environments. The framework features three modular components of Coordinator,
Communicator, and Memory, which dynamically generate subgoals, facilitate
symbolic inter-agent messaging, and support episodic recall. Training combines
PPO with a language-conditioned loss and LLM query gating. LLM-MARL is
evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results
show consistent improvements over MAPPO and QMIX in win rate, coordination
score, and zero-shot generalization. Ablation studies demonstrate that subgoal
generation and language-based messaging each contribute significantly to
performance gains. Qualitative analysis reveals emergent behaviors such as role
specialization and communication-driven tactics. By bridging language modeling
and policy learning, this work contributes to the design of intelligent,
cooperative agents in interactive simulations. It offers a path forward for
leveraging LLMs in multi-agent systems used for training, games, and human-AI
collaboration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.00797v1' target='_blank'>Action Dependency Graphs for Globally Optimal Coordinated Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianglin Ding, Jingcheng Tang, Gangshan Jing</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-01 02:58:20</h6>
<p class='card-text'>Action-dependent individual policies, which incorporate both environmental
states and the actions of other agents in decision-making, have emerged as a
promising paradigm for achieving global optimality in multi-agent reinforcement
learning (MARL). However, the existing literature often adopts auto-regressive
action-dependent policies, where each agent's policy depends on the actions of
all preceding agents. This formulation incurs substantial computational
complexity as the number of agents increases, thereby limiting scalability. In
this work, we consider a more generalized class of action-dependent policies,
which do not necessarily follow the auto-regressive form. We propose to use the
`action dependency graph (ADG)' to model the inter-agent action dependencies.
Within the context of MARL problems structured by coordination graphs, we prove
that an action-dependent policy with a sparse ADG can achieve global
optimality, provided the ADG satisfies specific conditions specified by the
coordination graph. Building on this theoretical foundation, we develop a
tabular policy iteration algorithm with guaranteed global optimality.
Furthermore, we integrate our framework into several SOTA algorithms and
conduct experiments in complex environments. The empirical results affirm the
robustness and applicability of our approach in more general scenarios,
underscoring its potential for broader MARL challenges.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.00576v1' target='_blank'>ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement
  Learning in O-RAN Network Slicing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fatemeh Lotfi, Hossein Rajoli, Fatemeh Afghah</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-31 14:21:19</h6>
<p class='card-text'>Advanced wireless networks must support highly dynamic and heterogeneous
service demands. Open Radio Access Network (O-RAN) architecture enables this
flexibility by adopting modular, disaggregated components, such as the RAN
Intelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU),
that can support intelligent control via machine learning (ML). While deep
reinforcement learning (DRL) is a powerful tool for managing dynamic resource
allocation and slicing, it often struggles to process raw, unstructured input
like RF features, QoS metrics, and traffic trends. These limitations hinder
policy generalization and decision efficiency in partially observable and
evolving environments. To address this, we propose \textit{ORAN-GUIDE}, a
dual-LLM framework that enhances multi-agent RL (MARL) with task-relevant,
semantically enriched state representations. The architecture employs a
domain-specific language model, ORANSight, pretrained on O-RAN control and
configuration data, to generate structured, context-aware prompts. These
prompts are fused with learnable tokens and passed to a frozen GPT-based
encoder that outputs high-level semantic representations for DRL agents. This
design adopts a retrieval-augmented generation (RAG) style pipeline tailored
for technical decision-making in wireless systems. Experimental results show
that ORAN-GUIDE improves sample efficiency, policy convergence, and performance
generalization over standard MARL and single-LLM baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.24618v1' target='_blank'>Distributed Intelligence in the Computing Continuum with Active
  Inference</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Victor Casamayor Pujol, Boris Sedlak, Tommaso Salvatori, Karl Friston, Schahram Dustdar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-30 14:10:33</h6>
<p class='card-text'>The Computing Continuum (CC) is an emerging Internet-based computing paradigm
that spans from local Internet of Things sensors and constrained edge devices
to large-scale cloud data centers. Its goal is to orchestrate a vast array of
diverse and distributed computing resources to support the next generation of
Internet-based applications. However, the distributed, heterogeneous, and
dynamic nature of CC platforms demands distributed intelligence for adaptive
and resilient service management. This article introduces a distributed stream
processing pipeline as a CC use case, where each service is managed by an
Active Inference (AIF) agent. These agents collaborate to fulfill service needs
specified by SLOiDs, a term we introduce to denote Service Level Objectives
that are aware of its deployed devices, meaning that non-functional
requirements must consider the characteristics of the hosting device. We
demonstrate how AIF agents can be modeled and deployed alongside distributed
services to manage them autonomously. Our experiments show that AIF agents
achieve over 90% SLOiD fulfillment when using tested transition models, and
around 80% when learning the models during deployment. We compare their
performance to a multi-agent reinforcement learning algorithm, finding that
while both approaches yield similar results, MARL requires extensive training,
whereas AIF agents can operate effectively from the start. Additionally, we
evaluate the behavior of AIF agents in offloading scenarios, observing a strong
capacity for adaptation. Finally, we outline key research directions to advance
AIF integration in CC platforms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.24265v1' target='_blank'>R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in
  Multi-agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Harsh Goel, Mohammad Omama, Behdad Chalaki, Vaishnav Tadiparthi, Ehsan Moradi Pari, Sandeep Chinchali</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-30 06:40:19</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) has achieved significant progress
in large-scale traffic control, autonomous vehicles, and robotics. Drawing
inspiration from biological systems where roles naturally emerge to enable
coordination, role-based MARL methods have been proposed to enhance cooperation
learning for complex tasks. However, existing methods exclusively derive roles
from an agent's past experience during training, neglecting their influence on
its future trajectories. This paper introduces a key insight: an agent's role
should shape its future behavior to enable effective coordination. Hence, we
propose Role Discovery and Diversity through Dynamics Models (R3DM), a novel
role-based MARL framework that learns emergent roles by maximizing the mutual
information between agents' roles, observed trajectories, and expected future
behaviors. R3DM optimizes the proposed objective through contrastive learning
on past trajectories to first derive intermediate roles that shape intrinsic
rewards to promote diversity in future behaviors across different roles through
a learned dynamics model. Benchmarking on SMAC and SMACv2 environments
demonstrates that R3DM outperforms state-of-the-art MARL approaches, improving
multi-agent coordination to increase win rates by up to 20%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.24155v1' target='_blank'>Biological Pathway Guided Gene Selection Through Collaborative
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ehtesamul Azim, Dongjie Wang, Tae Hyun Hwang, Yanjie Fu, Wei Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-30 03:01:07</h6>
<p class='card-text'>Gene selection in high-dimensional genomic data is essential for
understanding disease mechanisms and improving therapeutic outcomes.
Traditional feature selection methods effectively identify predictive genes but
often ignore complex biological pathways and regulatory networks, leading to
unstable and biologically irrelevant signatures. Prior approaches, such as
Lasso-based methods and statistical filtering, either focus solely on
individual gene-outcome associations or fail to capture pathway-level
interactions, presenting a key challenge: how to integrate biological pathway
knowledge while maintaining statistical rigor in gene selection? To address
this gap, we propose a novel two-stage framework that integrates statistical
selection with biological pathway knowledge using multi-agent reinforcement
learning (MARL). First, we introduce a pathway-guided pre-filtering strategy
that leverages multiple statistical methods alongside KEGG pathway information
for initial dimensionality reduction. Next, for refined selection, we model
genes as collaborative agents in a MARL framework, where each agent optimizes
both predictive power and biological relevance. Our framework incorporates
pathway knowledge through Graph Neural Network-based state representations, a
reward mechanism combining prediction performance with gene centrality and
pathway coverage, and collaborative learning strategies using shared memory and
a centralized critic component. Extensive experiments on multiple gene
expression datasets demonstrate that our approach significantly improves both
prediction accuracy and biological interpretability compared to traditional
methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.22151v1' target='_blank'>Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in
  Offline MARL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Claude Formanek, Omayma Mahjoub, Louay Ben Nessir, Sasha Abramowitz, Ruan de Kock, Wiem Khlifi, Simon Du Toit, Felix Chalumeau, Daniel Rajaonarivonivelomanantsoa, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob, Arnu Pretorius</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-28 09:17:44</h6>
<p class='card-text'>A key challenge in offline multi-agent reinforcement learning (MARL) is
achieving effective many-agent multi-step coordination in complex environments.
In this work, we propose Oryx, a novel algorithm for offline cooperative MARL
to directly address this challenge. Oryx adapts the recently proposed
retention-based architecture Sable and combines it with a sequential form of
implicit constraint Q-learning (ICQ), to develop a novel offline
auto-regressive policy update scheme. This allows Oryx to solve complex
coordination challenges while maintaining temporal coherence over lengthy
trajectories. We evaluate Oryx across a diverse set of benchmarks from prior
works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and
continuous control, varying in scale and difficulty. Oryx achieves
state-of-the-art performance on more than 80% of the 65 tested datasets,
outperforming prior offline MARL methods and demonstrating robust
generalisation across domains with many agents and long horizons. Finally, we
introduce new datasets to push the limits of many-agent coordination in offline
MARL, and demonstrate Oryx's superior ability to scale effectively in such
settings. We will make all of our datasets, experimental data, and code
available upon publication.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.21985v1' target='_blank'>Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Naoto Yoshida, Tadahiro Taniguchi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-28 05:23:47</h6>
<p class='card-text'>In multi-agent reinforcement learning (MARL), effective communication
improves agent performance, particularly under partial observability. We
propose MARL-CPC, a framework that enables communication among fully
decentralized, independent agents without parameter sharing. MARL-CPC
incorporates a message learning model based on collective predictive coding
(CPC) from emergent communication research. Unlike conventional methods that
treat messages as part of the action space and assume cooperation, MARL-CPC
links messages to state inference, supporting communication in non-cooperative,
reward-independent settings. We introduce two algorithms -Bandit-CPC and
IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that
both outperform standard message-as-action approaches, establishing effective
communication even when messages offer no direct benefit to the sender. These
results highlight MARL-CPC's potential for enabling coordination in complex,
decentralized environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.21236v1' target='_blank'>Breaking the Performance Ceiling in Complex Reinforcement Learning
  requires Inference Strategies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Felix Chalumeau, Daniel Rajaonarivonivelomanantsoa, Ruan de Kock, Claude Formanek, Sasha Abramowitz, Oumayma Mahjoub, Wiem Khlifi, Simon Du Toit, Louay Ben Nessir, Refiloe Shabe, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob, Arnu Pretorius</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-27 14:19:06</h6>
<p class='card-text'>Reinforcement learning (RL) systems have countless applications, from
energy-grid management to protein design. However, such real-world scenarios
are often extremely difficult, combinatorial in nature, and require complex
coordination between multiple agents. This level of complexity can cause even
state-of-the-art RL systems, trained until convergence, to hit a performance
ceiling which they are unable to break out of with zero-shot inference.
Meanwhile, many digital or simulation-based applications allow for an inference
phase that utilises a specific time and compute budget to explore multiple
attempts before outputting a final solution. In this work, we show that such an
inference phase employed at execution time, and the choice of a corresponding
inference strategy, are key to breaking the performance ceiling observed in
complex multi-agent RL problems. Our main result is striking: we can obtain up
to a 126% and, on average, a 45% improvement over the previous state-of-the-art
across 17 tasks, using only a couple seconds of extra wall-clock time during
execution. We also demonstrate promising compute scaling properties, supported
by over 60k experiments, making it the largest study on inference strategies
for complex RL to date. Our experimental data and code are available at
https://sites.google.com/view/inf-marl.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.20922v1' target='_blank'>Revisiting Multi-Agent World Modeling from a Diffusion-Inspired
  Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yang Zhang, Xinran Li, Jianing Ye, Delin Qu, Shuang Qiu, Chongjie Zhang, Xiu Li, Chenjia Bai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-27 09:11:38</h6>
<p class='card-text'>World models have recently attracted growing interest in Multi-Agent
Reinforcement Learning (MARL) due to their ability to improve sample efficiency
for policy learning. However, accurately modeling environments in MARL is
challenging due to the exponentially large joint action space and highly
uncertain dynamics inherent in multi-agent systems. To address this, we reduce
modeling complexity by shifting from jointly modeling the entire state-action
transition dynamics to focusing on the state space alone at each timestep
through sequential agent modeling. Specifically, our approach enables the model
to progressively resolve uncertainty while capturing the structured
dependencies among agents, providing a more accurate representation of how
agents influence the state. Interestingly, this sequential revelation of
agents' actions in a multi-agent system aligns with the reverse process in
diffusion models--a class of powerful generative models known for their
expressiveness and training stability compared to autoregressive or latent
variable models. Leveraging this insight, we develop a flexible and robust
world model for MARL using diffusion models. Our method, Diffusion-Inspired
Multi-Agent world model (DIMA), achieves state-of-the-art performance across
multiple multi-agent control benchmarks, significantly outperforming prior
world models in terms of final return and sample efficiency, including MAMuJoCo
and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent
world models, advancing the frontier of MARL research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.20579v2' target='_blank'>The challenge of hidden gifts in multi-agent reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dane Malenfant, Blake A. Richards</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 23:28:52</h6>
<p class='card-text'>Sometimes we benefit from actions that others have taken even when we are
unaware that they took those actions. For example, if your neighbor chooses not
to take a parking spot in front of your house when you are not there, you can
benefit, even without being aware that they took this action. These "hidden
gifts" represent an interesting challenge for multi-agent reinforcement
learning (MARL), since assigning credit when the beneficial actions of others
are hidden is non-trivial. Here, we study the impact of hidden gifts with a
very simple MARL task. In this task, agents in a grid-world environment have
individual doors to unlock in order to obtain individual rewards. As well, if
all the agents unlock their door the group receives a larger collective reward.
However, there is only one key for all of the doors, such that the collective
reward can only be obtained when the agents drop the key for others after they
use it. Notably, there is nothing to indicate to an agent that the other agents
have dropped the key, thus the act of dropping the key for others is a "hidden
gift". We show that several different state-of-the-art RL algorithms, including
MARL algorithms, fail to learn how to obtain the collective reward in this
simple task. Interestingly, we find that independent model-free policy gradient
agents can solve the task when we provide them with information about their own
action history, but MARL agents still cannot solve the task with action
history. Finally, we derive a correction term for these independent agents,
inspired by learning aware approaches, which reduces the variance in learning
and helps them to converge to collective success more reliably. These results
show that credit assignment in multi-agent settings can be particularly
challenging in the presence of "hidden gifts", and demonstrate that learning
awareness in independent agents can benefit these settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19837v1' target='_blank'>Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals
  to Applications</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Christoph R. Landolt, Christoph Würsch, Roland Meier, Alain Mermoud, Julian Jang-Jaccard</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 11:19:43</h6>
<p class='card-text'>Multi-Agent Reinforcement Learning (MARL) has shown great potential as an
adaptive solution for addressing modern cybersecurity challenges. MARL enables
decentralized, adaptive, and collaborative defense strategies and provides an
automated mechanism to combat dynamic, coordinated, and sophisticated threats.
This survey investigates the current state of research in MARL applications for
automated cyber defense (ACD), focusing on intruder detection and lateral
movement containment. Additionally, it examines the role of Autonomous
Intelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and
validating MARL agents. Finally, the paper outlines existing challenges, such
as scalability and adversarial robustness, and proposes future research
directions. This also discusses how MARL integrates in AICA to provide
adaptive, scalable, and dynamic solutions to counter the increasingly
sophisticated landscape of cyber threats. It highlights the transformative
potential of MARL in areas like intrusion detection and lateral movement
containment, and underscores the value of Cyber Gyms for training and
validation of AICA.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19637v1' target='_blank'>Adaptive Episode Length Adjustment for Multi-agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Byunghyun Yoo, Younghwan Shin, Hyunwoo Kim, Euisok Chung, Jeongmin Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 07:54:58</h6>
<p class='card-text'>In standard reinforcement learning, an episode is defined as a sequence of
interactions between agents and the environment, which terminates upon reaching
a terminal state or a pre-defined episode length. Setting a shorter episode
length enables the generation of multiple episodes with the same number of data
samples, thereby facilitating an exploration of diverse states. While shorter
episodes may limit the collection of long-term interactions, they may offer
significant advantages when properly managed. For example, trajectory
truncation in single-agent reinforcement learning has shown how the benefits of
shorter episodes can be leveraged despite the trade-off of reduced long-term
interaction experiences. However, this approach remains underexplored in MARL.
This paper proposes a novel MARL approach, Adaptive Episode Length Adjustment
(AELA), where the episode length is initially limited and gradually increased
based on an entropy-based assessment of learning progress. By starting with
shorter episodes, agents can focus on learning effective strategies for initial
states and minimize time spent in dead-end states. The use of entropy as an
assessment metric prevents premature convergence to suboptimal policies and
ensures balanced training over varying episode lengths. We validate our
approach using the StarCraft Multi-agent Challenge (SMAC) and a modified
predator-prey environment, demonstrating significant improvements in both
convergence speed and overall performance compared to existing methods. To the
best of our knowledge, this is the first study to adaptively adjust episode
length in MARL based on learning progress.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19316v1' target='_blank'>Making Teams and Influencing Agents: Efficiently Coordinating Decision
  Trees for Interpretable Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rex Chen, Stephanie Milani, Zhicheng Zhang, Norman Sadeh, Fei Fang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-25 21:05:48</h6>
<p class='card-text'>Poor interpretability hinders the practical applicability of multi-agent
reinforcement learning (MARL) policies. Deploying interpretable surrogates of
uninterpretable policies enhances the safety and verifiability of MARL for
real-world applications. However, if these surrogates are to interact directly
with the environment within human supervisory frameworks, they must be both
performant and computationally efficient. Prior work on interpretable MARL has
either sacrificed performance for computational efficiency or computational
efficiency for performance. To address this issue, we propose HYDRAVIPER, a
decision tree-based interpretable MARL algorithm. HYDRAVIPER coordinates
training between agents based on expected team performance, and adaptively
allocates budgets for environment interaction to improve computational
efficiency. Experiments on standard benchmark environments for multi-agent
coordination and traffic signal control show that HYDRAVIPER matches the
performance of state-of-the-art methods using a fraction of the runtime, and
that it maintains a Pareto frontier of performance for different interaction
budgets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18750v1' target='_blank'>Agent-Based Decentralized Energy Management of EV Charging Station with
  Solar Photovoltaics via Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiarong Fan, Chenghao Huang, Hao Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-24 15:34:37</h6>
<p class='card-text'>In the pursuit of energy net zero within smart cities, transportation
electrification plays a pivotal role. The adoption of Electric Vehicles (EVs)
keeps increasing, making energy management of EV charging stations critically
important. While previous studies have managed to reduce energy cost of EV
charging while maintaining grid stability, they often overlook the robustness
of EV charging management against uncertainties of various forms, such as
varying charging behaviors and possible faults in faults in some chargers. To
address the gap, a novel Multi-Agent Reinforcement Learning (MARL) approach is
proposed treating each charger to be an agent and coordinate all the agents in
the EV charging station with solar photovoltaics in a more realistic scenario,
where system faults may occur. A Long Short-Term Memory (LSTM) network is
incorporated in the MARL algorithm to extract temporal features from
time-series. Additionally, a dense reward mechanism is designed for training
the agents in the MARL algorithm to improve EV charging experience. Through
validation on a real-world dataset, we show that our approach is robust against
system uncertainties and faults and also effective in minimizing EV charging
costs and maximizing charging service satisfaction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18457v1' target='_blank'>EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military
  Communication Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abir Ray</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-24 01:56:32</h6>
<p class='card-text'>This paper introduces EdgeAgentX, a novel framework integrating federated
learning (FL), multi-agent reinforcement learning (MARL), and adversarial
defense mechanisms, tailored for military communication networks. EdgeAgentX
significantly improves autonomous decision-making, reduces latency, enhances
throughput, and robustly withstands adversarial disruptions, as evidenced by
comprehensive simulations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18433v1' target='_blank'>Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic
  Methods for Decentralized Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiyao Zhang, Myeung Suk Oh, FNU Hairi, Ziyue Luo, Alvaro Velasquez, Jia Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-24 00:00:43</h6>
<p class='card-text'>Actor-critic methods for decentralized multi-agent reinforcement learning
(MARL) facilitate collaborative optimal decision making without centralized
coordination, thus enabling a wide range of applications in practice. To date,
however, most theoretical convergence studies for existing actor-critic
decentralized MARL methods are limited to the guarantee of a stationary
solution under the linear function approximation. This leaves a significant gap
between the highly successful use of deep neural actor-critic for decentralized
MARL in practice and the current theoretical understanding. To bridge this gap,
in this paper, we make the first attempt to develop a deep neural actor-critic
method for decentralized MARL, where both the actor and critic components are
inherently non-linear. We show that our proposed method enjoys a global
optimality guarantee with a finite-time convergence rate of O(1/T), where T is
the total iteration times. This marks the first global convergence result for
deep neural actor-critic methods in the MARL literature. We also conduct
extensive numerical experiments, which verify our theoretical results.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17734v1' target='_blank'>URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous
  Vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ahmet Onur Akman, Anastasia Psarou, Michał Hoffmann, Łukasz Gorczyca, Łukasz Kowalski, Paweł Gora, Grzegorz Jamróz, Rafał Kucharski</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 10:54:53</h6>
<p class='card-text'>Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future
urban networks, potentially by optimizing their routing decisions. Unlike for
human drivers, these decisions can be made with collective, data-driven
policies, developed by machine learning algorithms. Reinforcement learning (RL)
can facilitate the development of such collective routing strategies, yet
standardized and realistic benchmarks are missing. To that end, we present
\our{}: Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles.
\our{} is a comprehensive benchmarking environment that unifies evaluation
across 29 real-world traffic networks paired with realistic demand patterns.
\our{} comes with a catalog of predefined tasks, four state-of-the-art
multi-agent RL (MARL) algorithm implementations, three baseline methods,
domain-specific performance metrics, and a modular configuration scheme. Our
results suggest that, despite the lengthy and costly training, state-of-the-art
MARL algorithms rarely outperformed humans. Experimental results reported in
this paper initiate the first leaderboard for MARL in large-scale urban routing
optimization and reveal that current approaches struggle to scale, emphasizing
the urgent need for advancements in this domain.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.14544v1' target='_blank'>Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic
  Signal Optimization: A Simulation Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Saahil Mahato</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 15:59:44</h6>
<p class='card-text'>Urban traffic congestion, particularly at intersections, significantly
impacts travel time, fuel consumption, and emissions. Traditional fixed-time
signal control systems often lack the adaptability to manage dynamic traffic
patterns effectively. This study explores the application of multi-agent
reinforcement learning (MARL) to optimize traffic signal coordination across
multiple intersections within a simulated environment. Utilizing Pygame, a
simulation was developed to model a network of interconnected intersections
with randomly generated vehicle flows to reflect realistic traffic variability.
A decentralized MARL controller was implemented, in which each traffic signal
operates as an autonomous agent, making decisions based on local observations
and information from neighboring agents. Performance was evaluated against a
baseline fixed-time controller using metrics such as average vehicle wait time
and overall throughput. The MARL approach demonstrated statistically
significant improvements, including reduced average waiting times and improved
throughput. These findings suggest that MARL-based dynamic control strategies
hold substantial promise for improving urban traffic management efficiency.
More research is recommended to address scalability and real-world
implementation challenges.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13834v1' target='_blank'>Toward Real-World Cooperative and Competitive Soccer with Quadrupedal
  Robot Teams</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhi Su, Yuman Gao, Emily Lukas, Yunfei Li, Jiaze Cai, Faris Tulbah, Fei Gao, Chao Yu, Zhongyu Li, Yi Wu, Koushil Sreenath</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 02:20:54</h6>
<p class='card-text'>Achieving coordinated teamwork among legged robots requires both fine-grained
locomotion control and long-horizon strategic decision-making. Robot soccer
offers a compelling testbed for this challenge, combining dynamic, competitive,
and multi-agent interactions. In this work, we present a hierarchical
multi-agent reinforcement learning (MARL) framework that enables fully
autonomous and decentralized quadruped robot soccer. First, a set of highly
dynamic low-level skills is trained for legged locomotion and ball
manipulation, such as walking, dribbling, and kicking. On top of these, a
high-level strategic planning policy is trained with Multi-Agent Proximal
Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning
framework allows agents to adapt to diverse opponent strategies and gives rise
to sophisticated team behaviors, including coordinated passing, interception,
and dynamic role allocation. With an extensive ablation study, the proposed
learning method shows significant advantages in the cooperative and competitive
multi-agent soccer game. We deploy the learned policies to real quadruped
robots relying solely on onboard proprioception and decentralized localization,
with the resulting system supporting autonomous robot-robot and robot-human
soccer matches on indoor and outdoor soccer courts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.12811v1' target='_blank'>Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei-Chen Liao, Ti-Rong Wu, I-Chen Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-19 07:40:42</h6>
<p class='card-text'>Multi-agent reinforcement Learning (MARL) is often challenged by the sight
range dilemma, where agents either receive insufficient or excessive
information from their environment. In this paper, we propose a novel method,
called Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes
an Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight
range during training. Experiment results show several advantages of using DSR.
First, we demonstrate using DSR achieves better performance in three common
MARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse
(RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show
that DSR consistently improves performance across multiple MARL algorithms,
including QMIX and MAPPO. Third, DSR offers suitable sight ranges for different
training steps, thereby accelerating the training process. Finally, DSR
provides additional interpretability by indicating the optimal sight range used
during training. Unlike existing methods that rely on global information or
communication mechanisms, our approach operates solely based on the individual
sight ranges of agents. This approach offers a practical and efficient solution
to the sight range dilemma, making it broadly applicable to real-world complex
environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15841v1' target='_blank'>Optimizing Resource Allocation for QoS and Stability in Dynamic VLC-NOMA
  Networks via MARL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aubida A. Al-Hameed, Safwan Hafeedh Younus, Mohamad A. Ahmed, Abdullah Baz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-17 18:54:56</h6>
<p class='card-text'>Visible Light Communication (VLC) combined with Non-Orthogonal Multiple
Access (NOMA) offers a promising solution for dense indoor wireless networks.
Yet, managing resources effectively is challenged by VLC network dynamic
conditions involving user mobility and light dimming. In addition to satisfying
Quality of Service (QoS) and network stability requirements. Traditional
resource allocation methods and simpler RL approaches struggle to jointly
optimize QoS and stability under the dynamic conditions of mobile VLC-NOMA
networks. This paper presents MARL frameworks tailored to perform complex joint
optimization of resource allocation (NOMA power, user scheduling) and network
stability (interference, handovers), considering heterogeneous QoS, user
mobility, and dimming in VLC-NOMA systems. Our MARL frameworks capture dynamic
channel conditions and diverse user QoS , enabling effective joint
optimization. In these frameworks, VLC access points (APs) act as intelligent
agents, learning to allocate power and schedule users to satisfy diverse
requirements while maintaining network stability by managing interference and
minimizing disruptive handovers. We conduct a comparative analysis of two key
MARL paradigms: 1) Centralized Training with Decentralized Execution (CTDE) and
2) Centralized Training with Centralized Execution (CTCE). Comprehensive
simulations validate the effectiveness of both tailored MARL frameworks and
demonstrate an ability to handle complex optimization. The results show key
trade-offs, as the CTDE approach achieved approximately 16\% higher for High
priority (HP) user QoS satisfaction, while the CTCE approach yielded nearly 7
dB higher average SINR and 12\% lower ping-pong handover ratio, offering
valuable insights into the performance differences between these paradigms in
complex VLC-NOMA network scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.11461v2' target='_blank'>Signal attenuation enables scalable decentralized multi-agent
  reinforcement learning over networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wesley A Suttle, Vipul K Sharma, Brian M Sadler</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-16 17:14:37</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) methods typically require that
agents enjoy global state observability, preventing development of
decentralized algorithms and limiting scalability. Recent work has shown that,
under assumptions on decaying inter-agent influence, global observability can
be replaced by local neighborhood observability at each agent, enabling
decentralization and scalability. Real-world applications enjoying such decay
properties remain underexplored, however, despite the fact that signal power
decay, or signal attenuation, due to path loss is an intrinsic feature of many
problems in wireless communications and radar networks. In this paper, we show
that signal attenuation enables decentralization in MARL by considering the
illustrative special case of performing power allocation for target detection
in a radar network. To achieve this, we propose two new constrained multi-agent
Markov decision process formulations of this power allocation problem, derive
local neighborhood approximations for global value function and policy gradient
estimates and establish corresponding error bounds, and develop decentralized
saddle point policy gradient algorithms for solving the proposed problems. Our
approach, though oriented towards the specific radar network problem we
consider, provides a useful model for extensions to additional problems in
wireless communications and radar networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.11311v1' target='_blank'>Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for
  Aerial Combat Tactics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ardian Selmonaj, Alessandro Antonucci, Adrian Schneider, Michael Rüegsegger, Matthias Sommer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-16 14:36:30</h6>
<p class='card-text'>Artificial intelligence (AI) is reshaping strategic planning, with
Multi-Agent Reinforcement Learning (MARL) enabling coordination among
autonomous agents in complex scenarios. However, its practical deployment in
sensitive military contexts is constrained by the lack of explainability, which
is an essential factor for trust, safety, and alignment with human strategies.
This work reviews and assesses current advances in explainability methods for
MARL with a focus on simulated air combat scenarios. We proceed by adapting
various explainability techniques to different aerial combat scenarios to gain
explanatory insights about the model behavior. By linking AI-generated tactics
with human-understandable reasoning, we emphasize the need for transparency to
ensure reliable deployment and meaningful human-machine interaction. By
illuminating the crucial importance of explainability in advancing MARL for
operational defense, our work supports not only strategic planning but also the
training of military personnel with insightful and comprehensible analyses.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>