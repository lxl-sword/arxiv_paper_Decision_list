<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-10-28</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-10-28</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23586v1' target='_blank'>From Zonal to Nodal Capacity Expansion Planning: Spatial Aggregation
  Impacts on a Realistic Test-Case</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Elizabeth Glista, Bernard Knueven, Jean-Paul Watson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 17:53:48</h6>
<p class='card-text'>Solving power system capacity expansion planning (CEP) problems at realistic
spatial resolutions is computationally challenging. Thus, a common practice is
to solve CEP over zonal models with low spatial resolution rather than over
full-scale nodal power networks. Due to improvements in solving large-scale
stochastic mixed integer programs, these computational limitations are becoming
less relevant, and the assumption that zonal models are realistic and useful
approximations of nodal CEP is worth revisiting. This work is the first to
conduct a systematic computational study on the assumption that spatial
aggregation can reasonably be used for ISO- and interconnect-scale CEP. By
considering a realistic, large-scale test network based on the state of
California with over 8,000 buses and 10,000 transmission lines, we demonstrate
that well-designed small spatial aggregations can yield good approximations but
that coarser zonal models result in large distortions of investment decisions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23576v1' target='_blank'>UrbanVLA: A Vision-Language-Action Model for Urban Micromobility</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anqi Li, Zhiyong Wang, Jiazhao Zhang, Minghan Li, Yunpeng Qi, Zhibo Chen, Zhizheng Zhang, He Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 17:46:43</h6>
<p class='card-text'>Urban micromobility applications, such as delivery robots, demand reliable
navigation across large-scale urban environments while following long-horizon
route instructions. This task is particularly challenging due to the dynamic
and unstructured nature of real-world city areas, yet most existing navigation
methods remain tailored to short-scale and controllable scenarios. Effective
urban micromobility requires two complementary levels of navigation skills:
low-level capabilities such as point-goal reaching and obstacle avoidance, and
high-level capabilities, such as route-visual alignment. To this end, we
propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework
designed for scalable urban navigation. Our method explicitly aligns noisy
route waypoints with visual observations during execution, and subsequently
plans trajectories to drive the robot. To enable UrbanVLA to master both levels
of navigation, we employ a two-stage training pipeline. The process begins with
Supervised Fine-Tuning (SFT) using simulated environments and trajectories
parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on
a mixture of simulation and real-world data, which enhances the model's safety
and adaptability in real-world settings. Experiments demonstrate that UrbanVLA
surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban.
Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both
scalability to large-scale urban environments and robustness against real-world
uncertainties.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23552v1' target='_blank'>Generalized Kantorovich-Rubinstein Duality beyond Hausdorff and
  Kantorovich</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Paul Wild, Lutz Schröder, Karla Messing, Barbara König, Jonas Forster</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 17:27:39</h6>
<p class='card-text'>The classical Kantorovich-Rubinstein duality guarantees coincidence between
metrics on the space of probability distributions defined on the one hand via
transport plans (couplings) and on the other hand via price functions. Both
constructions have been lifted to the level of generality of set functors, with
the coupling-based construction referred to as the Wasserstein lifting, and the
price-function-based construction as the Kantorovich lifting, both based on a
choice of quantitative modalities for the given functor. It is known that every
Wasserstein lifting can be expressed as a Kantorovich lifting; however, the
latter in general needs to use additional modalities. We give an example
showing that this cannot be avoided in general. We refer to cases in which the
same modalities can be used as satisfying the generalized
Kantorovich-Rubinstein duality. We establish the generalized
Kantorovich-Rubinstein duality in this sense for two important cases: The
L\'evy-Prokhorov distance on distributions, which finds wide-spread
applications in machine learning due to its favourable stability properties,
and the standard metric on convex sets of distributions that arises by
combining the Hausdorff and Wasserstein distances.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23416v1' target='_blank'>Quality-controlled registration of urban MLS point clouds reducing drift
  effects by adaptive fragmentation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marco Antonio Ortiz Rincon, Yihui Yang, Christoph Holst</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 15:21:39</h6>
<p class='card-text'>This study presents a novel workflow designed to efficiently and accurately
register large-scale mobile laser scanning (MLS) point clouds to a target model
point cloud in urban street scenarios. This workflow specifically targets the
complexities inherent in urban environments and adeptly addresses the
challenges of integrating point clouds that vary in density, noise
characteristics, and occlusion scenarios, which are common in bustling city
centers. Two methodological advancements are introduced. First, the proposed
Semi-sphere Check (SSC) preprocessing technique optimally fragments MLS
trajectory data by identifying mutually orthogonal planar surfaces. This step
reduces the impact of MLS drift on the accuracy of the entire point cloud
registration, while ensuring sufficient geometric features within each fragment
to avoid local minima. Second, we propose Planar Voxel-based Generalized
Iterative Closest Point (PV-GICP), a fine registration method that selectively
utilizes planar surfaces within voxel partitions. This pre-process strategy not
only improves registration accuracy but also reduces computation time by more
than 50% compared to conventional point-to-plane ICP methods. Experiments on
real-world datasets from Munich's inner city demonstrate that our workflow
achieves sub-0.01 m average registration accuracy while significantly
shortening processing times. The results underscore the potential of the
proposed methods to advance automated 3D urban modeling and updating, with
direct applications in urban planning, infrastructure management, and dynamic
city monitoring.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23402v1' target='_blank'>A Sequential Planning Framework for the Operational Reality of
  Interacting Air Traffic Flow Regulations and Traffic Flow Programs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thinh Hoang, Daniel Delahaye</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 14:59:28</h6>
<p class='card-text'>Air Traffic Flow Management (ATFM) traffic regulations are being increasingly
used as rising demand meets persistent workforce shortages. This operational
strain has amplified a critical phenomenon that we call \emph{regulation
cascading}: the compounding, non-linear interactions that occur when multiple
regulations influence one another in unpredictable ways. As the number and
complexity of regulations grow, cascading effects become more pronounced,
undermining the network operator's ability to protect sectors reliably. To
address this challenge, we introduce RegulationZero, a sequential planning
framework that natively operates in the regulation space, optimizing over
ordered sequences of flow-level regulations that remain fully compatible with
existing slot-allocation systems such as CASA and RBS++. At its core, the
method employs a hierarchical Monte Carlo Tree Search (MCTS) that first samples
congestion hotspots and then selects candidate regulations synthesized by a
local proposal engine. Each proposal is evaluated by a fast
First-Planned-First-Served (FPFS) allocator to estimate its reward, with these
feedbacks guiding the subsequent MCTS exploration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23340v1' target='_blank'>Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by
  Projecting User Awareness across Future Timesteps</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anwesha Das, John Duff, Jörg Hoffmann, Vera Demberg</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 13:54:54</h6>
<p class='card-text'>Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23296v1' target='_blank'>Payload trajectory tracking control for aerial transportation systems
  with cable length online optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hai Yu, Zhichao Yang, Wei He, Jianda Han, Yongchun Fang, Xiao Liang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 13:03:47</h6>
<p class='card-text'>Cable-suspended aerial transportation systems are employed extensively across
various industries. The capability to flexibly adjust the relative position
between the multirotor and the payload has spurred growing interest in the
system equipped with variable-length cable, promising broader application
potential. Compared to systems with fixed-length cables, introducing the
variable-length cable adds a new degree of freedom. However, it also results in
increased nonlinearity and more complex dynamic coupling among the multirotor,
the cable and the payload, posing significant challenges in control design.
This paper introduces a backstepping control strategy tailored for aerial
transportation systems with variable-length cable, designed to precisely track
the payload trajectory while dynamically adjusting cable length. Then, a cable
length generator has been developed that achieves online optimization of the
cable length while satisfying state constraints, thus balancing the
multirotor's motion and cable length changes without the need for manual
trajectory planning. The asymptotic stability of the closed-loop system is
guaranteed through Lyapunov techniques and the growth restriction condition.
Finally, simulation results confirm the efficacy of the proposed method in
managing trajectory tracking and cable length adjustments effectively.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23227v1' target='_blank'>Workspace Registration and Collision Detection for Industrial Robotics
  Applications</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Klaus Zauner, Josef El Dib, Hubert Gattringer, Andreas Mueller</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 11:24:44</h6>
<p class='card-text'>Motion planning for robotic manipulators relies on precise knowledge of the
environment in order to be able to define restricted areas and to take
collision objects into account. To capture the workspace, point clouds of the
environment are acquired using various sensors. The collision objects are
identified by region growing segmentation and VCCS algorithm. Subsequently the
point clusters are approximated. The aim of the present paper is to compare
different sensors, to illustrate the process from detection to the finished
collision environment and to detect collisions between the robot and this
environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23205v1' target='_blank'>VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D
  Gaussian Splatting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hoonhee Cho, Jae-Young Kang, Giwon Lee, Hyemin Yang, Heejun Park, Seokwoo Jung, Kuk-Jin Yoon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 10:49:39</h6>
<p class='card-text'>End-to-end autonomous driving (E2E-AD) has emerged as a promising paradigm
that unifies perception, prediction, and planning into a holistic, data-driven
framework. However, achieving robustness to varying camera viewpoints, a common
real-world challenge due to diverse vehicle configurations, remains an open
problem. In this work, we propose VR-Drive, a novel E2E-AD framework that
addresses viewpoint generalization by jointly learning 3D scene reconstruction
as an auxiliary task to enable planning-aware view synthesis. Unlike prior
scene-specific synthesis approaches, VR-Drive adopts a feed-forward inference
strategy that supports online training-time augmentation from sparse views
without additional annotations. To further improve viewpoint consistency, we
introduce a viewpoint-mixed memory bank that facilitates temporal interaction
across multiple viewpoints and a viewpoint-consistent distillation strategy
that transfers knowledge from original to synthesized views. Trained in a fully
end-to-end manner, VR-Drive effectively mitigates synthesis-induced noise and
improves planning under viewpoint shifts. In addition, we release a new
benchmark dataset to evaluate E2E-AD performance under novel camera viewpoints,
enabling comprehensive analysis. Our results demonstrate that VR-Drive is a
scalable and robust solution for the real-world deployment of end-to-end
autonomous driving systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23198v1' target='_blank'>PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation
  Performance at Unseen Pre-Training Budgets</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Etienne Goffinet, Shane Bergsma, Avraham Sheinin, Natalia Vassilieva, Shaheer Muhammad, Preslav Nakov, Gurpreet Gosal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 10:36:15</h6>
<p class='card-text'>Continual pre-training (CPT) for domain adaptation must balance target-domain
gains with stability on the base domain. Existing CPT scaling laws typically
assume a fixed pre-training budget, which limits their ability to forecast
adaptation outcomes for models trained at different tokens-per-parameter
(PTPP). We present \emph{PTPP-aware} adaptation scaling laws that make the
pre-training budget an explicit variable, enabling accurate \emph{prediction}
of adaptation loss at unseen \ptpp. On a multilingual setup (English/Arabic
$\rightarrow$ French), PTPP-aware formulations trained on early stages
(\ptpp{}=\{15,31\}) predict target loss at \ptpp{}=279 and outperform a
PTPP-agnostic \dcpt{} transfer baseline on metrics (Huber-on-log,
MAE$_\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in
the appendix. Beyond forecasting, we show a practical use case: planning replay
ratios and adaptation token budgets that satisfy target and forgetting
constraints under compute limits.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23184v1' target='_blank'>Finding 3D Scene Analogies with Multimodal Foundation Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junho Kim, Young Min Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 10:23:31</h6>
<p class='card-text'>Connecting current observations with prior experiences helps robots adapt and
plan in new, unseen 3D environments. Recently, 3D scene analogies have been
proposed to connect two 3D scenes, which are smooth maps that align scene
regions with common spatial relationships. These maps enable detailed transfer
of trajectories or waypoints, potentially supporting demonstration transfer for
imitation learning or task plan transfer across scenes. However, existing
methods for the task require additional training and fixed object vocabularies.
In this work, we propose to use multimodal foundation models for finding 3D
scene analogies in a zero-shot, open-vocabulary setting. Central to our
approach is a hybrid neural representation of scenes that consists of a sparse
graph based on vision-language model features and a feature field derived from
3D shape foundation models. 3D scene analogies are then found in a
coarse-to-fine manner, by first aligning the graph and refining the
correspondence with feature fields. Our method can establish accurate
correspondences between complex scenes, and we showcase applications in
trajectory and waypoint transfer.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23140v1' target='_blank'>Fast Voxel-Wise Kinetic Modeling in Dynamic PET using a Physics-Informed
  CycleGAN</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Christian Salomonsen, Samuel Kuttner, Michael Kampffmeyer, Robert Jenssen, Kristoffer Wickstrøm, Jong Chul Ye, Elisabeth Wetzer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 09:17:02</h6>
<p class='card-text'>Tracer kinetic modeling serves a vital role in diagnosis, treatment planning,
tracer development and oncology, but burdens practitioners with complex and
invasive arterial input function estimation (AIF). We adopt a physics-informed
CycleGAN showing promise in DCE-MRI quantification to dynamic PET
quantification. Our experiments demonstrate sound AIF predictions and parameter
maps closely resembling the reference.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23057v1' target='_blank'>Seq-DeepIPC: Sequential Sensing for End-to-End Control in Legged Robot
  Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Oskar Natan, Jun Miura</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 06:39:57</h6>
<p class='card-text'>We present Seq-DeepIPC, a sequential end-to-end perception-to-control model
for legged robot navigation in realworld environments. Seq-DeepIPC advances
intelligent sensing for autonomous legged navigation by tightly integrating
multi-modal perception (RGB-D + GNSS) with temporal fusion and control. The
model jointly predicts semantic segmentation and depth estimation, giving
richer spatial features for planning and control. For efficient deployment on
edge devices, we use EfficientNet-B0 as the encoder, reducing computation while
maintaining accuracy. Heading estimation is simplified by removing the noisy
IMU and instead computing the bearing angle directly from consecutive GNSS
positions. We collected a larger and more diverse dataset that includes both
road and grass terrains, and validated Seq-DeepIPC on a robot dog. Comparative
and ablation studies show that sequential inputs improve perception and control
in our models, while other baselines do not benefit. Seq-DeepIPC achieves
competitive or better results with reasonable model size; although GNSS-only
heading is less reliable near tall buildings, it is robust in open areas.
Overall, Seq-DeepIPC extends end-to-end navigation beyond wheeled robots to
more versatile and temporally-aware systems. To support future research, we
will release the codes to our GitHub repository at
https://github.com/oskarnatan/Seq-DeepIPC.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23053v1' target='_blank'>AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for
  Multi-UAV Cooperative Mobile Edge Computing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiyu Wang, Suman Raj, Rajkumar Buyya</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 06:31:35</h6>
<p class='card-text'>Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing
(MEC) systems face critical challenges in coordinating trajectory planning,
task offloading, and resource allocation while ensuring Quality of Service
(QoS) under dynamic and uncertain environments. Existing approaches suffer from
limited scalability, slow convergence, and inefficient knowledge sharing among
UAVs, particularly when handling large-scale IoT device deployments with
stringent deadline constraints. This paper proposes AirFed, a novel federated
graph-enhanced multi-agent reinforcement learning framework that addresses
these challenges through three key innovations. First, we design dual-layer
dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal
dependencies among UAVs and IoT devices, capturing both service relationships
and collaborative interactions within the network topology. Second, we develop
a dual-Actor single-Critic architecture that jointly optimizes continuous
trajectory control and discrete task offloading decisions. Third, we propose a
reputation-based decentralized federated learning mechanism with
gradient-sensitive adaptive quantization, enabling efficient and robust
knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate
that AirFed achieves 42.9% reduction in weighted cost compared to
state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2%
IoT device coverage rate, and reduces communication overhead by 54.5%.
Scalability analysis confirms robust performance across varying UAV numbers,
IoT device densities, and system scales, validating AirFed's practical
applicability for large-scale UAV-MEC deployments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23026v1' target='_blank'>Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal
  Resolution</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Crimson Stambaugh, Rajesh P. N. Rao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 05:45:59</h6>
<p class='card-text'>Recent studies demonstrate that diffusion planners benefit from sparse-step
planning over single-step planning. Training models to skip steps in their
trajectories helps capture long-term dependencies without additional or memory
computational cost. However, predicting excessively sparse plans degrades
performance. We hypothesize this temporal density threshold is non-uniform
across a temporal horizon and that certain parts of a planned trajectory should
be more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion
planner where the densities throughout the horizon are tunable hyperparameters.
MDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL
task domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.23021v1' target='_blank'>Planning Oriented Integrated Sensing and Communication</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xibin Jin, Guoliang Li, Shuai Wang, Fan Liu, Miaowen Wen, Huseyin Arslan, Derrick Wing Kwan Ng, Chengzhong Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 05:33:00</h6>
<p class='card-text'>Integrated sensing and communication (ISAC) enables simultaneous
localization, environment perception, and data exchange for connected
autonomous vehicles. However, most existing ISAC designs prioritize sensing
accuracy and communication throughput, treating all targets uniformly and
overlooking the impact of critical obstacles on motion efficiency. To overcome
this limitation, we propose a planning-oriented ISAC (PISAC) framework that
reduces the sensing uncertainty of planning-bottleneck obstacles and expands
the safe navigable path for the ego-vehicle, thereby bridging the gap between
physical-layer optimization and motion-level planning. The core of PISAC lies
in deriving a closed-form safety bound that explicitly links ISAC transmit
power to sensing uncertainty, based on the Cram\'er-Rao Bound and occupancy
inflation principles. Using this model, we formulate a bilevel power allocation
and motion planning (PAMP) problem, where the inner layer optimizes the ISAC
beam power distribution and the outer layer computes a collision-free
trajectory under uncertainty-aware safety constraints. Comprehensive
simulations in high-fidelity urban driving environments demonstrate that PISAC
achieves up to 40% higher success rates and over 5% shorter traversal times
than existing ISAC-based and communication-oriented benchmarks, validating its
effectiveness in enhancing both safety and efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22994v1' target='_blank'>SceneDecorator: Towards Scene-Oriented Story Generation with Scene
  Planning and Scene Consistency</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Quanjian Song, Donghao Zhou, Jingyu Lin, Fei Shen, Jiaze Wang, Xiaowei Hu, Cunjian Chen, Pheng-Ann Heng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 04:19:22</h6>
<p class='card-text'>Recent text-to-image models have revolutionized image generation, but they
still struggle with maintaining concept consistency across generated images.
While existing works focus on character consistency, they often overlook the
crucial role of scenes in storytelling, which restricts their creativity in
practice. This paper introduces scene-oriented story generation, addressing two
key challenges: (i) scene planning, where current methods fail to ensure
scene-level narrative coherence by relying solely on text descriptions, and
(ii) scene consistency, which remains largely unexplored in terms of
maintaining scene consistency across multiple stories. We propose
SceneDecorator, a training-free framework that employs VLM-Guided Scene
Planning to ensure narrative coherence across different scenes in a
``global-to-local'' manner, and Long-Term Scene-Sharing Attention to maintain
long-term scene consistency and subject diversity across generated stories.
Extensive experiments demonstrate the superior performance of SceneDecorator,
highlighting its potential to unleash creativity in the fields of arts, films,
and games.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22973v1' target='_blank'>Scaling Up Occupancy-centric Driving Scene Generation: Dataset and
  Method</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bohan Li, Xin Jin, Hu Zhu, Hongsi Liu, Ruikai Li, Jiazhe Guo, Kaiwen Cai, Chao Ma, Yueming Jin, Hao Zhao, Xiaokang Yang, Wenjun Zeng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 03:52:45</h6>
<p class='card-text'>Driving scene generation is a critical domain for autonomous driving,
enabling downstream applications, including perception and planning evaluation.
Occupancy-centric methods have recently achieved state-of-the-art results by
offering consistent conditioning across frames and modalities; however, their
performance heavily depends on annotated occupancy data, which still remains
scarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic
occupancy dataset to date, constructed from the widely used Nuplan benchmark.
Its scale and diversity facilitate not only large-scale generative modeling but
also autonomous driving downstream applications. Based on this dataset, we
develop a unified framework that jointly synthesizes high-quality semantic
occupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates
a spatio-temporal disentangled architecture to support high-fidelity spatial
expansion and temporal forecasting of 4D dynamic occupancy. To bridge modal
gaps, we further propose two novel techniques: a Gaussian splatting-based
sparse point map rendering strategy that enhances multi-view video generation,
and a sensor-aware embedding strategy that explicitly models LiDAR sensor
properties for realistic multi-LiDAR simulation. Extensive experiments
demonstrate that our method achieves superior generation fidelity and
scalability compared to existing approaches, and validates its practical value
in downstream tasks. Repo:
https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22969v1' target='_blank'>Multi-Agent Conditional Diffusion Model with Mean Field Communication as
  Wireless Resource Allocation Planner</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kechen Meng, Sinuo Zhang, Rongpeng Li, Xiangming Meng, Chan Wang, Ming Lei, Zhifeng Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 03:42:18</h6>
<p class='card-text'>In wireless communication systems, efficient and adaptive resource allocation
plays a crucial role in enhancing overall Quality of Service (QoS). While
centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a
central coordinator for policy training and resource scheduling, they suffer
from scalability issues and privacy risks. In contrast, the Distributed
Training with Decentralized Execution (DTDE) paradigm enables distributed
learning and decision-making, but it struggles with non-stationarity and
limited inter-agent cooperation, which can severely degrade system performance.
To overcome these challenges, we propose the Multi-Agent Conditional Diffusion
Model Planner (MA-CDMP) for decentralized communication resource management.
Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP
employs Diffusion Models (DMs) to capture environment dynamics and plan future
trajectories, while an inverse dynamics model guides action generation, thereby
alleviating the sample inefficiency and slow convergence of conventional DTDE
methods. Moreover, to approximate large-scale agent interactions, a Mean-Field
(MF) mechanism is introduced as an assistance to the classifier in DMs. This
design mitigates inter-agent non-stationarity and enhances cooperation with
minimal communication overhead in distributed settings. We further
theoretically establish an upper bound on the distributional approximation
error introduced by the MF-based diffusion generation, guaranteeing convergence
stability and reliable modeling of multi-agent stochastic dynamics. Extensive
experiments demonstrate that MA-CDMP consistently outperforms existing MARL
baselines in terms of average reward and QoS metrics, showcasing its
scalability and practicality for real-world wireless network optimization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22937v1' target='_blank'>Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Matthew So, Judah Goldfeder, Mark Lis, Hod Lipson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 02:41:43</h6>
<p class='card-text'>There has been a historic assumption that the biometrics of an individual are
statistically uncorrelated. We test this assumption by training Bi-Encoder
networks on three verification tasks, including fingerprint-to-fingerprint
matching, iris-to-iris matching, and cross-modal fingerprint-to-iris matching
using 274 subjects with $\sim$100k fingerprints and 7k iris images. We trained
ResNet-50 and Vision Transformer backbones in Bi-Encoder architectures such
that the contrastive loss between images sampled from the same individual is
minimized. The iris ResNet architecture reaches 91 ROC AUC score for
iris-to-iris matching, providing clear evidence that the left and right irises
of an individual are correlated. Fingerprint models reproduce the positive
intra-subject suggested by prior work in this space. This is the first work
attempting to use Vision Transformers for this matching. Cross-modal matching
rises only slightly above chance, which suggests that more data and a more
sophisticated pipeline is needed to obtain compelling results. These findings
continue challenge independence assumptions of biometrics and we plan to extend
this work to other biometrics in the future. Code available:
https://github.com/MatthewSo/bio_fingerprints_iris.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22911v1' target='_blank'>Towards Personalized Treatment Plan: Geometrical Model-Agnostic Approach
  to Counterfactual Explanations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel Sin, Milad Toutounchian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 01:28:57</h6>
<p class='card-text'>In our article, we describe a method for generating counterfactual
explanations in high-dimensional spaces using four steps that involve fitting
our dataset to a model, finding the decision boundary, determining constraints
on the problem, and computing the closest point (counterfactual explanation)
from that boundary. We propose a discretized approach where we find many
discrete points on the boundary and then identify the closest feasible
counterfactual explanation. This method, which we later call $\textit{Segmented
Sampling for Boundary Approximation}$ (SSBA), applies binary search to find
decision boundary points and then searches for the closest boundary point.
Across four datasets of varying dimensionality, we show that our method can
outperform current methods for counterfactual generation with reductions in
distance between $5\%$ to $50\%$ in terms of the $L_2$ norm. Our method can
also handle real-world constraints by restricting changes to immutable and
categorical features, such as age, gender, sex, height, and other related
characteristics such as the case for a health-based dataset. In terms of
runtime, the SSBA algorithm generates decision boundary points on multiple
orders of magnitude in the same given time when we compare to a grid-based
approach. In general, our method provides a simple and effective model-agnostic
method that can compute nearest feasible (i.e. realistic with constraints)
counterfactual explanations. All of our results and our code can be found here
at this link:
$\href{https://github.com/dsin85691/SSBA_For_Counterfactuals}{https://github.com/
dsin85691/SSBA\_For\_Counterfactuals}$</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22908v1' target='_blank'>Bridging Stratification and Regression Adjustment: Batch-Adaptive
  Stratification with Post-Design Adjustment in Randomized Experiments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zikai Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 01:25:34</h6>
<p class='card-text'>To increase statistical efficiency in a randomized experiment, researchers
often use stratification (i.e., blocking) in the design stage. However,
conventional practices of stratification fail to exploit valuable information
about the predictive relationship between covariates and potential outcomes. In
this paper, I introduce an adaptive stratification procedure for increasing
statistical efficiency when some information is available about the
relationship between covariates and potential outcomes. I show that, in a
paired design, researchers can rematch observations across different batches.
For inference, I propose a stratified estimator that allows for nonparametric
covariate adjustment. I then discuss the conditions under which researchers
should expect gains in efficiency from stratification. I show that
stratification complements rather than substitutes for regression adjustment,
insuring against adjustment error even when researchers plan to use covariate
adjustment. To evaluate the performance of the method relative to common
alternatives, I conduct simulations using both synthetic data and more
realistic data derived from a political science experiment. Results demonstrate
that the gains in precision and efficiency can be substantial.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22907v1' target='_blank'>Language Server CLI Empowers Language Agents with Process Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yifan Zhang, Lanser Contributors</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 01:25:20</h6>
<p class='card-text'>Large language models routinely hallucinate APIs and mislocalize edits, while
language servers compute verified, IDE-grade facts about real code. We present
Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language
Server Protocol (LSP) server for coding agents and CI, exposing deterministic,
replayable workflows. Our position is that language servers provide not only
structural information (definitions, references, types, diagnostics) but also
an actionable process reward: machine-checked, step-wise signals that align an
agent's planning loop with program reality. In this work, Lanser-CLI
contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via
a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a
principled relocation algorithm; (ii) deterministic Analysis Bundles that
normalize Language Server responses and capture environment/capability metadata
with stable content hashes; (iii) a safety envelope for mutating operations
(rename, code actions) with preview, workspace jails, and Git-aware,
transactional apply; and (iv) a process-reward functional derived from Language
Server facts (diagnostic deltas, disambiguation confidence, and safe-apply
checks) that is computable online and replayable offline. We formalize
determinism under frozen snapshots and establish a monotonicity property for
the process reward, making it suitable for process supervision and
counterfactual analysis. Project Page:
https://github.com/yifanzhang-pro/lanser-cli</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22901v1' target='_blank'>Motion Planning on One-Dimensional Peano Continua</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jeremy Brazas, Petar Pavesic</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-27 01:09:23</h6>
<p class='card-text'>We study the Lusternik-Schnirelmann category and topological complexity of
1-dimensional spaces. We define both invariants as lengths of suitable closed
filtrations, as opposed to a more common definition based on open covers. Our
main results provide a precise description of $\mathbf{cat}(X)$ and
$\mathbf{TC}(X)$ of a 1-dimensional Peano continuum $X$ in terms of the
wildness rank of $X$. A surprising consequence is that $\mathbf{cat}(X)$ and
$\mathbf{TC}(X)$ of a general 1-dimensional space $X$ can be arbitrarily high,
which is in stark contrast with the analogous results for 1-dimensional
CW-complexes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22833v1' target='_blank'>Toward Agents That Reason About Their Computation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Adrian Orenstein, Jessica Chen, Gwyneth Anne Delos Santos, Bayley Sapara, Michael Bowling</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-26 21:01:30</h6>
<p class='card-text'>While reinforcement learning agents can achieve superhuman performance in
many complex tasks, they typically do not become more computationally efficient
as they improve. In contrast, humans gradually require less cognitive effort as
they become more proficient at a task. If agents could reason about their
compute as they learn, could they similarly reduce their computation footprint?
If they could, we could have more energy efficient agents or free up compute
cycles for other processes like planning. In this paper, we experiment with
showing agents the cost of their computation and giving them the ability to
control when they use compute. We conduct our experiments on the Arcade
Learning Environment, and our results demonstrate that with the same training
compute budget, agents that reason about their compute perform better on 75% of
games. Furthermore, these agents use three times less compute on average. We
analyze individual games and show where agents gain these efficiencies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22789v1' target='_blank'>Learning Neural Observer-Predictor Models for Limb-level Sampling-based
  Locomotion Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abhijeet M. Kulkarni, Ioannis Poulakakis, Guoquan Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-26 18:46:46</h6>
<p class='card-text'>Accurate full-body motion prediction is essential for the safe, autonomous
navigation of legged robots, enabling critical capabilities like limb-level
collision checking in cluttered environments. Simplified kinematic models often
fail to capture the complex, closed-loop dynamics of the robot and its
low-level controller, limiting their predictions to simple planar motion. To
address this, we present a learning-based observer-predictor framework that
accurately predicts this motion. Our method features a neural observer with
provable UUB guarantees that provides a reliable latent state estimate from a
history of proprioceptive measurements. This stable estimate initializes a
computationally efficient predictor, designed for the rapid, parallel
evaluation of thousands of potential trajectories required by modern
sampling-based planners. We validated the system by integrating our neural
predictor into an MPPI-based planner on a Vision 60 quadruped. Hardware
experiments successfully demonstrated effective, limb-aware motion planning in
a challenging, narrow passage and over small objects, highlighting our system's
ability to provide a robust foundation for high-performance, collision-aware
planning on dynamic robotic platforms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22781v1' target='_blank'>Agentic Meta-Orchestrator for Multi-task Copilots</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaofeng Zhu, Yunshen Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-26 18:13:04</h6>
<p class='card-text'>Microsoft Copilot suites serve as the universal entry point for various
agents skilled in handling important tasks, ranging from assisting a customer
with product purchases to detecting vulnerabilities in corporate programming
code. Each agent can be powered by language models, software engineering
operations, such as database retrieval, and internal \& external knowledge. The
repertoire of a copilot can expand dynamically with new agents. This requires a
robust orchestrator that can distribute tasks from user prompts to the right
agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for
handling multiple tasks and scalable agents in copilot services, which can
provide both natural language and action responses. We will also demonstrate
the planning that leverages meta-learning, i.e., a trained decision tree model
for deciding the best inference strategy among various agents/models. We
showcase the effectiveness of our AMO through two production use cases:
Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365
E-Commerce Copilot advertises Microsoft products to external customers to
promote sales success. The M365 E-Commerce Copilot provides up-to-date product
information and connects to multiple agents, such as relational databases and
human customer support. The code compliance copilot scans the internal DevOps
code to detect known and new compliance issues in pull requests (PR).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22739v1' target='_blank'>REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for
  E-commerce Visual Search System Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiwen Tang, Qiuyu Zhao, Zenghui Sun, Jinsong Lan, Xiaoyong Zhu, Bo Zheng, Kaifu Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-26 16:15:50</h6>
<p class='card-text'>In Taobao e-commerce visual search, user behavior analysis reveals a large
proportion of no-click requests, suggesting diverse and implicit user intents.
These intents are expressed in various forms and are difficult to mine and
discover, thereby leading to the limited adaptability and lag in platform
strategies. This greatly restricts users' ability to express diverse intents
and hinders the scalability of the visual search system. This mismatch between
user implicit intent expression and system response defines the User-SearchSys
Intent Discrepancy. To alleviate the issue, we propose a novel framework
REVISION. This framework integrates offline reasoning mining with online
decision-making and execution, enabling adaptive strategies to solve implicit
user demands. In the offline stage, we construct a periodic pipeline to mine
discrepancies from historical no-click requests. Leveraging large models, we
analyze implicit intent factors and infer optimal suggestions by jointly
reasoning over query and product metadata. These inferred suggestions serve as
actionable insights for refining platform strategies. In the online stage,
REVISION-R1-3B, trained on the curated offline data, performs holistic analysis
over query images and associated historical products to generate optimization
plans and adaptively schedule strategies across the search pipeline. Our
framework offers a streamlined paradigm for integrating large models with
traditional search systems, enabling end-to-end intelligent optimization across
information aggregation and user interaction. Experimental results demonstrate
that our approach improves the efficiency of implicit intent mining from
large-scale search logs and significantly reduces the no-click rate.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22736v1' target='_blank'>Cross-view Localization and Synthesis - Datasets, Challenges and
  Opportunities</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ningli Xu, Rongjun Qin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-26 16:09:53</h6>
<p class='card-text'>Cross-view localization and synthesis are two fundamental tasks in cross-view
visual understanding, which deals with cross-view datasets: overhead (satellite
or aerial) and ground-level imagery. These tasks have gained increasing
attention due to their broad applications in autonomous navigation, urban
planning, and augmented reality. Cross-view localization aims to estimate the
geographic position of ground-level images based on information provided by
overhead imagery while cross-view synthesis seeks to generate ground-level
images based on information from the overhead imagery. Both tasks remain
challenging due to significant differences in viewing perspective, resolution,
and occlusion, which are widely embedded in cross-view datasets. Recent years
have witnessed rapid progress driven by the availability of large-scale
datasets and novel approaches. Typically, cross-view localization is formulated
as an image retrieval problem where ground-level features are matched with
tiled overhead images feature, extracted by convolutional neural networks
(CNNs) or vision transformers (ViTs) for cross-view feature embedding.
Cross-view synthesis, on the other hand, seeks to generate ground-level views
based on information from overhead imagery, generally using generative
adversarial networks (GANs) or diffusion models. This paper presents a
comprehensive survey of advances in cross-view localization and synthesis,
reviewing widely used datasets, highlighting key challenges, and providing an
organized overview of state-of-the-art techniques. Furthermore, it discusses
current limitations, offers comparative analyses, and outlines promising
directions for future research. We also include the project page via
https://github.com/GDAOSU/Awesome-Cross-View-Methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.22709v1' target='_blank'>Sample size determination for win statistics in cluster-randomized
  trials</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xi Fang, Zhiqiang Cao, Fan Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-26 15:05:49</h6>
<p class='card-text'>Composite endpoints are increasingly used in clinical trials to capture
treatment effects across multiple or hierarchically ordered outcomes. Although
inference procedures based on win statistics, such as the win ratio, win odds,
and net benefit, have gained traction in individually randomized trials, their
methodological development for cluster-randomized trials remains limited. In
particular, there is no formal framework for power and sample size
determination when using win statistics with composite time-to-event outcomes.
We develop a unified framework for power and sample size calculation for win
statistics under cluster randomization. Analytical variance expressions are
derived for a broad class of win statistics, yielding closed-form variance
expressions and power procedures that avoid computationally intensive
simulations. The variance expressions explicitly characterize the roles of the
rank intracluster correlation coefficient, cluster size, tie probability, and
outcome prioritization for study planning purposes. Importantly, our variances
nest existing formulas for univariate outcomes as special cases while extending
them to complex, hierarchically ordered composite endpoints. Simulation studies
confirm accurate finite-sample performance, and we supply a case study to
illustrate the use of our method to re-design a real-world cluster-randomized
trial.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>