<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-03-26</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-03-26</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19815v1' target='_blank'>Thinking agents for zero-shot generalization to qualitatively novel
  tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas Miconi, Kevin McKee, Yicong Zheng, Jed McCaleb</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 16:26:31</h6>
<p class='card-text'>Intelligent organisms can solve truly novel problems which they have never
encountered before, either in their lifetime or their evolution. An important
component of this capacity is the ability to ``think'', that is, to mentally
manipulate objects, concepts and behaviors in order to plan and evaluate
possible solutions to novel problems, even without environment interaction. To
generate problems that are truly qualitatively novel, while still solvable
zero-shot (by mental simulation), we use the combinatorial nature of
environments: we train the agent while withholding a specific combination of
the environment's elements. The novel test task, based on this combination, is
thus guaranteed to be truly novel, while still mentally simulable since the
agent has been exposed to each individual element (and their pairwise
interactions) during training. We propose a method to train agents endowed with
world models to make use their mental simulation abilities, by selecting tasks
based on the difference between the agent's pre-thinking and post-thinking
performance. When tested on the novel, withheld problem, the resulting agent
successfully simulated alternative scenarios and used the resulting information
to guide its behavior in the actual environment, solving the novel task in a
single real-environment trial (zero-shot).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19703v1' target='_blank'>High-Quality Spatial Reconstruction and Orthoimage Generation Using
  Efficient 2D Gaussian Splatting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qian Wang, Zhihao Zhan, Jialei He, Zhituo Tu, Xiang Zhu, Jie Yuan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 14:30:37</h6>
<p class='card-text'>Highly accurate geometric precision and dense image features characterize
True Digital Orthophoto Maps (TDOMs), which are in great demand for
applications such as urban planning, infrastructure management, and
environmental monitoring. Traditional TDOM generation methods need
sophisticated processes, such as Digital Surface Models (DSM) and occlusion
detection, which are computationally expensive and prone to errors. This work
presents an alternative technique rooted in 2D Gaussian Splatting (2DGS), free
of explicit DSM and occlusion detection. With depth map generation, spatial
information for every pixel within the TDOM is retrieved and can reconstruct
the scene with high precision. Divide-and-conquer strategy achieves excellent
GS training and rendering with high-resolution TDOMs at a lower resource cost,
which preserves higher quality of rendering on complex terrain and thin
structure without a decrease in efficiency. Experimental results demonstrate
the efficiency of large-scale scene reconstruction and high-precision terrain
modeling. This approach provides accurate spatial data, which assists users in
better planning and decision-making based on maps.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19699v1' target='_blank'>Optimal Path Planning and Cost Minimization for a Drone Delivery System
  Via Model Predictive Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhammad Al-Zafar Khan, Jamal Al-Karaki</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 14:27:29</h6>
<p class='card-text'>In this study, we formulate the drone delivery problem as a control problem
and solve it using Model Predictive Control. Two experiments are performed: The
first is on a less challenging grid world environment with lower
dimensionality, and the second is with a higher dimensionality and added
complexity. The MPC method was benchmarked against three popular Multi-Agent
Reinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action
Learners (JAL), and Value-Decomposition Networks (VDN). It was shown that the
MPC method solved the problem quicker and required fewer optimal numbers of
drones to achieve a minimized cost and navigate the optimal path.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19606v1' target='_blank'>Single Shot AI-assisted quantification of KI-67 proliferation index in
  breast cancer</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Deepti Madurai Muthu, Priyanka S, Lalitha Rani N, P. G. Kubendran Amos</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 12:41:45</h6>
<p class='card-text'>Reliable quantification of Ki-67, a key proliferation marker in breast
cancer, is essential for molecular subtyping and informed treatment planning.
Conventional approaches, including visual estimation and manual counting,
suffer from interobserver variability and limited reproducibility. This study
introduces an AI-assisted method using the YOLOv8 object detection framework
for automated Ki-67 scoring. High-resolution digital images (40x magnification)
of immunohistochemically stained tumor sections were captured from Ki-67
hotspot regions and manually annotated by a domain expert to distinguish
Ki-67-positive and negative tumor cells. The dataset was augmented and divided
into training (80%), validation (10%), and testing (10%) subsets. Among the
YOLOv8 variants tested, the Medium model achieved the highest performance, with
a mean Average Precision at 50% Intersection over Union (mAP50) exceeding 85%
for Ki-67-positive cells. The proposed approach offers an efficient, scalable,
and objective alternative to conventional scoring methods, supporting greater
consistency in Ki-67 evaluation. Future directions include developing
user-friendly clinical interfaces and expanding to multi-institutional datasets
to enhance generalizability and facilitate broader adoption in diagnostic
practice.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19584v1' target='_blank'>Multi-agent Application System in Office Collaboration Scenarios</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Songtao Sun, Jingyi Li, Yuanfei Dong, Haoguang Liu, Chenxin Xu, Fuyang Li, Qiang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 12:07:20</h6>
<p class='card-text'>This paper introduces a multi-agent application system designed to enhance
office collaboration efficiency and work quality. The system integrates
artificial intelligence, machine learning, and natural language processing
technologies, achieving functionalities such as task allocation, progress
monitoring, and information sharing. The agents within the system are capable
of providing personalized collaboration support based on team members' needs
and incorporate data analysis tools to improve decision-making quality. The
paper also proposes an intelligent agent architecture that separates Plan and
Solver, and through techniques such as multi-turn query rewriting and business
tool retrieval, it enhances the agent's multi-intent and multi-turn dialogue
capabilities. Furthermore, the paper details the design of tools and multi-turn
dialogue in the context of office collaboration scenarios, and validates the
system's effectiveness through experiments and evaluations. Ultimately, the
system has demonstrated outstanding performance in real business applications,
particularly in query understanding, task planning, and tool calling. Looking
forward, the system is expected to play a more significant role in addressing
complex interaction issues within dynamic environments and large-scale
multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19461v1' target='_blank'>Irradiation Study Using QA Test Pieces of ATLAS18 ITk Strip Sensors with
  80MeV Protons</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Y. Huang, H. Li, B. Crick, V. Cindro, A. Chisholm, M. Cai, H. Deng, V. Fadeyev, S. Hirose, H. Jing, B. Jiang, P. Liu, Y. Liu, W. Lu, H. Liu, I. MandiÄ‡, R. S. Orr, X. Shi, Z. Tan, Y. Unno, M. Ullan, S. Wang, Z. Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 08:50:35</h6>
<p class='card-text'>The ATLAS experiment is planning a complete replacement of its inner
detector(ID) with a new all-silicon inner tracker (ITk) for the ATLAS Inner
Tracker Phase-2 upgrade. The ATLAS18 silicon strip sensors are designed to
operate up to the integrated luminosity of 4000 fb$^{-1}$, which corresponds to
the maximum fluence of $1.6 \times 10^{15} \, \text n_{\text{eq}} /
\text{cm}^2$ (including safety factor). To enhance the quality assurance (QA)
program to monitor the key properties of the sensors, the strip sensor
community is considering to include China Spallation Neutron Source (CSNS) as a
proton irradiation site and Institute of High Energy Physics (IHEP) as a QA
test site. A total of 18 ATLAS18 ITk QA test pieces were irradiated with $6.0
\times 10^{14}$, $1.6 \times 10^{15}$, and $2.6 \times 10^{15} \, \text
n_{\text{eq}} / \text{cm}^2$ protons at CSNS, and measured at IHEP, including
IV (leakage current-voltage), CV (capacitance-voltage) and CCE (charge
collection efficiency) measurements. The upgraded irradiation setup at CSNS and
measurement setup at IHEP are shown in this paper. Irradiated samples were
exchanged between IHEP, Ljubljana and Birmingham to cross-check CCE
measurements.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19405v1' target='_blank'>Multi-modal 3D Pose and Shape Estimation with Computed Tomography</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingxiao Tu, Hoijoon Jung, Alireza Moghadam, Jineel Raythatha, Lachlan Allan, Jeremy Hsu, Andre Kyme, Jinman Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 07:24:58</h6>
<p class='card-text'>In perioperative care, precise in-bed 3D patient pose and shape estimation
(PSE) can be vital in optimizing patient positioning in preoperative planning,
enabling accurate overlay of medical images for augmented reality-based
surgical navigation, and mitigating risks of prolonged immobility during
recovery. Conventional PSE methods relying on modalities such as RGB-D,
infrared, or pressure maps often struggle with occlusions caused by bedding and
complex patient positioning, leading to inaccurate estimation that can affect
clinical outcomes. To address these challenges, we present the first
multi-modal in-bed patient 3D PSE network that fuses detailed geometric
features extracted from routinely acquired computed tomography (CT) scans with
depth maps (mPSE-CT). mPSE-CT incorporates a shape estimation module that
utilizes probabilistic correspondence alignment, a pose estimation module with
a refined neural network, and a final parameters mixing module. This
multi-modal network robustly reconstructs occluded body regions and enhances
the accuracy of the estimated 3D human mesh model. We validated mPSE-CT using
proprietary whole-body rigid phantom and volunteer datasets in clinical
scenarios. mPSE-CT outperformed the best-performing prior method by 23% and
49.16% in pose and shape estimation respectively, demonstrating its potential
for improving clinical outcomes in challenging perioperative environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19395v1' target='_blank'>Quantum molecular dynamics model based on relativistic mean field theory
  for light nucleus fragmentation in hadron therapy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Akihiro Haga, Yoshi-hide Sato, Hana Fujiwara, Dousatsu Sakata, David Bolst, Edward C. Simpson, Susanna Guatelli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 07:01:22</h6>
<p class='card-text'>This study evaluates the accuracy of nuclear fragmentation simulations using
a quantum molecular dynamics (QMD) model based on relativistic mean field (RMF)
theory for an energy range of 50-400 MeV/u, relevant to hadron therapy. A total
of 16 parameter sets within the RMF framework are assessed based on their
ability to reproduce ground-state properties such as the mean squared radius
and binding energy, as obtained in QMD simulations. Among these, the NS2
parameter set is identified as the most suitable for describing stable nuclei
over a wide mass range, with the use of an adaptive Gaussian wave packet width.
Fragmentation cross sections of carbon ion projectiles on light nuclei targets
(H, C, O, Al, Ti, and Cu) are simulated at incident energies of 50, 95, 290,
and 400 MeV/u and compared with experimental data. The results indicate that
the RQMD.RMF model provides superior reproductions for fragmentation at lower
energies (50 and 95 MeV/u) compared to the Light Ion QMD (LIQMD) model
implemented in Geant4 version 11.2. At higher energies (290 and 400 MeV/u), the
RQMD.RMF model performs comparably to the LIQMD. This study demonstrates that
the RQMD.RMF model provides a reliable framework for analyzing nuclear
fragmentation and holds potential for applications in the planning and quality
assurance of hadron therapy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19340v1' target='_blank'>BADGR: Bundle Adjustment Diffusion Conditioned by GRadients for
  Wide-Baseline Floor Plan Reconstruction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuguang Li, Ivaylo Boyadzhiev, Zixuan Liu, Linda Shapiro, Alex Colburn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 04:15:20</h6>
<p class='card-text'>Reconstructing precise camera poses and floor plan layouts from wide-baseline
RGB panoramas is a difficult and unsolved problem. We introduce BADGR, a novel
diffusion model that jointly performs reconstruction and bundle adjustment (BA)
to refine poses and layouts from a coarse state, using 1D floor boundary
predictions from dozens of images of varying input densities. Unlike a guided
diffusion model, BADGR is conditioned on dense per-entity outputs from a
single-step Levenberg Marquardt (LM) optimizer and is trained to predict camera
and wall positions while minimizing reprojection errors for view-consistency.
The objective of layout generation from denoising diffusion process complements
BA optimization by providing additional learned layout-structural constraints
on top of the co-visible features across images. These constraints help BADGR
to make plausible guesses on spatial relations which help constrain pose graph,
such as wall adjacency, collinearity, and learn to mitigate errors from dense
boundary observations with global contexts. BADGR trains exclusively on 2D
floor plans, simplifying data acquisition, enabling robust augmentation, and
supporting variety of input densities. Our experiments and analysis validate
our method, which significantly outperforms the state-of-the-art pose and floor
plan layout reconstruction with different input densities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19316v1' target='_blank'>A Social Dynamical System for Twitter Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiping Xiao, Xinyu Wang, Yifang Qin, Zijie Huang, Mason A. Porter, Yizhou Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 03:25:07</h6>
<p class='card-text'>Understanding the evolution of public opinion is crucial for informed
decision-making in various domains, particularly public affairs. The rapid
growth of social networks, such as Twitter (now rebranded as X), provides an
unprecedented opportunity to analyze public opinion at scale without relying on
traditional surveys. With the rise of deep learning, Graph Neural Networks
(GNNs) have shown great promise in modeling online opinion dynamics. Notably,
classical opinion dynamics models, such as DeGroot, can be reformulated within
a GNN framework.
  We introduce Latent Social Dynamical System (LSDS), a novel framework for
modeling the latent dynamics of social media users' opinions based on textual
content. Since expressed opinions may not fully reflect underlying beliefs,
LSDS first encodes post content into latent representations. It then leverages
a GraphODE framework, using a GNN-based ODE function to predict future
opinions. A decoder subsequently utilizes these predicted latent opinions to
perform downstream tasks, such as interaction prediction, which serve as
benchmarks for model evaluation. Our framework is highly flexible, supporting
various opinion dynamic models as ODE functions, provided they can be adapted
into a GNN-based form. It also accommodates different encoder architectures and
is compatible with diverse downstream tasks.
  To validate our approach, we constructed dynamic datasets from Twitter data.
Experimental results demonstrate the effectiveness of LSDS, highlighting its
potential for future applications. We plan to publicly release our dataset and
code upon the publication of this paper.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19281v1' target='_blank'>CubeRobot: Grounding Language in Rubik's Cube Manipulation via
  Vision-Language Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Feiyang Wang, Xiaomin Yu, Wangyu Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 02:23:47</h6>
<p class='card-text'>Proving Rubik's Cube theorems at the high level represents a notable
milestone in human-level spatial imagination and logic thinking and reasoning.
Traditional Rubik's Cube robots, relying on complex vision systems and fixed
algorithms, often struggle to adapt to complex and dynamic scenarios. To
overcome this limitation, we introduce CubeRobot, a novel vision-language model
(VLM) tailored for solving 3x3 Rubik's Cubes, empowering embodied agents with
multimodal understanding and execution capabilities. We used the CubeCoT image
dataset, which contains multiple-level tasks (43 subtasks in total) that humans
are unable to handle, encompassing various cube states. We incorporate a
dual-loop VisionCoT architecture and Memory Stream, a paradigm for extracting
task-related features from VLM-generated planning queries, thus enabling
CubeRobot to independent planning, decision-making, reflection and separate
management of high- and low-level Rubik's Cube tasks. Furthermore, in low-level
Rubik's Cube restoration tasks, CubeRobot achieved a high accuracy rate of
100%, similar to 100% in medium-level tasks, and achieved an accuracy rate of
80% in high-level tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19216v1' target='_blank'>Towards entropic uncertainty relations for non-regular Hilbert spaces</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alejandro Corichi, Angel Garcia Chung, Federico Zadra</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 23:41:50</h6>
<p class='card-text'>The Entropic Uncertainty Relations (EUR) result from inequalities that are
intrinsic to the Hilbert space and its dual with no direct connection to the
Canonical Commutation Relations. Bialynicky-Mielcisnky obtained them in
\cite{bialynicki1975uncertainty} attending Hilbert spaces with a Lebesgue
measure. The analysis of these EUR in the context of singular Hilbert spaces
has not been addressed. Singular Hilbert spaces are widely used in scenarios
where some discretization of the space (or spacetime) is considered, e.g., loop
quantum gravity, loop quantum cosmology and polymer quantum mechanics. In this
work, we present an overview of the essential literature background and the
road map we plan to follow to obtain the EUR in polymer quantum mechanics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19136v1' target='_blank'>Stochastic Poisson Surface Reconstruction with One Solve using Geometric
  Gaussian Processes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sidhanth Holalkere, David S. Bindel, Silvia SellÃ¡n, Alexander Terenin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 20:47:51</h6>
<p class='card-text'>Poisson Surface Reconstruction is a widely-used algorithm for reconstructing
a surface from an oriented point cloud. To facilitate applications where only
partial surface information is available, or scanning is performed
sequentially, a recent line of work proposes to incorporate uncertainty into
the reconstructed surface via Gaussian process models. The resulting algorithms
first perform Gaussian process interpolation, then solve a set of volumetric
partial differential equations globally in space, resulting in a
computationally expensive two-stage procedure. In this work, we apply
recently-developed techniques from geometric Gaussian processes to combine
interpolation and surface reconstruction into a single stage, requiring only
one linear solve per sample. The resulting reconstructed surface samples can be
queried locally in space, without the use of problem-dependent volumetric
meshes or grids. These capabilities enable one to (a) perform probabilistic
collision detection locally around the region of interest, (b) perform ray
casting without evaluating points not on the ray's trajectory, and (c) perform
next-view planning on a per-slice basis. They also improve reconstruction
quality, by not requiring one to approximate kernel matrix inverses with
diagonal matrices as part of intermediate computations. Results show that our
approach provides a cleaner, more-principled, and more-flexible stochastic
surface reconstruction pipeline.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19135v1' target='_blank'>Cooperative Control of Multi-Quadrotors for Transporting Cable-Suspended
  Payloads: Obstacle-Aware Planning and Event-Based Nonlinear Model Predictive
  Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tohid Kargar Tasooji, Sakineh Khodadadi, Guangjun Liu, Richard Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 20:45:24</h6>
<p class='card-text'>This paper introduces a novel methodology for the cooperative control of
multiple quadrotors transporting cablesuspended payloads, emphasizing
obstacle-aware planning and event-based Nonlinear Model Predictive Control
(NMPC). Our approach integrates trajectory planning with real-time control
through a combination of the A* algorithm for global path planning and NMPC for
local control, enhancing trajectory adaptability and obstacle avoidance. We
propose an advanced event-triggered control system that updates based on events
identified through dynamically generated environmental maps. These maps are
constructed using a dual-camera setup, which includes multi-camera systems for
static obstacle detection and event cameras for high-resolution, low-latency
detection of dynamic obstacles. This design is crucial for addressing
fast-moving and transient obstacles that conventional cameras may overlook,
particularly in environments with rapid motion and variable lighting
conditions. When new obstacles are detected, the A* algorithm recalculates
waypoints based on the updated map, ensuring safe and efficient navigation.
This real-time obstacle detection and map updating integration allows the
system to adaptively respond to environmental changes, markedly improving
safety and navigation efficiency. The system employs SLAM and object detection
techniques utilizing data from multi-cameras, event cameras, and IMUs for
accurate localization and comprehensive environmental mapping. The NMPC
framework adeptly manages the complex dynamics of multiple quadrotors and
suspended payloads, incorporating safety constraints to maintain dynamic
feasibility and stability. Extensive simulations validate the proposed
approach, demonstrating significant enhancements in energy efficiency,
computational resource management, and responsiveness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19104v1' target='_blank'>Forward propagation of a push through a row of people</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sina Feldmann, Juliane Adrian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 19:47:00</h6>
<p class='card-text'>Security plays a crucial role when it comes to planning large events such as
concerts, sporting tournaments, pilgrims, or demonstrations. Monitoring and
controlling pedestrian dynamics can prevent dangerous situations from
occurring. However, little is known about the specific factors that contribute
to harmful situations. For example, the individual response of a person to
external forces in dense crowds is not well studied. In order to address this
gap in knowledge, we conducted a series of experiments to examine how a push
propagates through a row of people and how it affects the participants. We
recorded 2D head trajectories and 3D motion capturing data. To ensure that
different trials can be compared to one another, we measured the force at the
impact. We find that that the propagation distance as well as the propagation
speed of the push are mainly functions of the strength of the push and in
particular the latter depends on the initial arm posture of the pushed
participants. Our results can contribute to a deeper understanding of the
microscopic causes of macroscopic phenomena in large groups, and can be applied
to inform models of pedestrian dynamics or validate them, ultimately improving
crowd safety.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19062v1' target='_blank'>Color Transfer with Modulated Flows</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maria Larchenko, Alexander Lobashev, Dmitry Guskov, Vladimir Vladimirovich Palyulin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 18:39:54</h6>
<p class='card-text'>In this work, we introduce Modulated Flows (ModFlows), a novel approach for
color transfer between images based on rectified flows. The primary goal of the
color transfer is to adjust the colors of a target image to match the color
distribution of a reference image. Our technique is based on optimal transport
and executes color transfer as an invertible transformation within the RGB
color space. The ModFlows utilizes the bijective property of flows, enabling us
to introduce a common intermediate color distribution and build a dataset of
rectified flows. We train an encoder on this dataset to predict the weights of
a rectified model for new images. After training on a set of optimal transport
plans, our approach can generate plans for new pairs of distributions without
additional fine-tuning. We additionally show that the trained encoder provides
an image embedding, associated only with its color style. The presented method
is capable of processing 4K images and achieves the state-of-the-art
performance in terms of content and style similarity. Our source code is
available at https://github.com/maria-larchenko/modflows</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19042v1' target='_blank'>An Upper Limit on the Interstellar Meteoroid Flux at Video Sizes from
  the Global Meteor Network</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Paul Wiegert, Vanessa Tran, Cole Gregg, Denis Vida, Peter Brown</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 18:12:37</h6>
<p class='card-text'>Material arriving at our solar system from the Galaxy may be detected at
Earth in the form of meteors ablating in our atmosphere. Here we report on a
search for interstellar meteors within the highest-quality events in the Global
Meteor Network (GMN) database. No events were detected that were conclusively
hyperbolic with respect to the Sun; however, our search was not exhaustive and
examined only the top 57% of events, with a deeper examination planned for
future work.
  This study's effective meteoroid mass limit is 6.6 +/- 0.8 x 10^{-5} kg (5
millimeter diameter at a density of 1000 kg m^{-3}). Theoretical rates of
interstellar meteors at these sizes range from 3 to 200 events globally per
year. The highest rates can already be largely excluded by this study, while at
the lowest rates GMN would have to observe for 25 more years to be 50%
confident of seeing at least one event. GMN is thus well positioned to provide
substantial constraints on the interstellar population at these sizes over the
coming years. This study's results are statistically compatible with a rate of
interstellar meteors at the Earth at less than 1 per million meteoroid impacts
at Earth at millimeter sizes, or a flux rate of less than 8 +/- 2 x 10^{-11}
per sq. km per hour at the 95% confidence level.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18950v1' target='_blank'>Target-Aware Video Diffusion Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Taeksoo Kim, Hanbyul Joo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 17:59:59</h6>
<p class='card-text'>We present a target-aware video diffusion model that generates videos from an
input image in which an actor interacts with a specified target while
performing a desired action. The target is defined by a segmentation mask and
the desired action is described via a text prompt. Unlike existing controllable
image-to-video diffusion models that often rely on dense structural or motion
cues to guide the actor's movements toward the target, our target-aware model
requires only a simple mask to indicate the target, leveraging the
generalization capabilities of pretrained models to produce plausible actions.
This makes our method particularly effective for human-object interaction (HOI)
scenarios, where providing precise action guidance is challenging, and further
enables the use of video diffusion models for high-level action planning in
applications such as robotics. We build our target-aware model by extending a
baseline model to incorporate the target mask as an additional input. To
enforce target awareness, we introduce a special token that encodes the
target's spatial information within the text prompt. We then fine-tune the
model with our curated dataset using a novel cross-attention loss that aligns
the cross-attention maps associated with this token with the input target mask.
To further improve performance, we selectively apply this loss to the most
semantically relevant transformer blocks and attention regions. Experimental
results show that our target-aware model outperforms existing solutions in
generating videos where actors interact accurately with the specified targets.
We further demonstrate its efficacy in two downstream applications: video
content creation and zero-shot 3D HOI motion synthesis.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18945v2' target='_blank'>Aether: Geometric-Aware Unified World Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Tong He</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 17:59:51</h6>
<p class='card-text'>The integration of geometric reconstruction and generative modeling remains a
critical challenge in developing AI systems capable of human-like spatial
reasoning. This paper proposes Aether, a unified framework that enables
geometry-aware reasoning in world models by jointly optimizing three core
capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video
prediction, and (3) goal-conditioned visual planning. Through task-interleaved
feature learning, Aether achieves synergistic knowledge sharing across
reconstruction, prediction, and planning objectives. Building upon video
generation models, our framework demonstrates unprecedented synthetic-to-real
generalization despite never observing real-world data during training.
Furthermore, our approach achieves zero-shot generalization in both action
following and reconstruction tasks, thanks to its intrinsic geometric modeling.
Remarkably, even without real-world data, its reconstruction performance is
comparable with or even better than that of domain-specific models.
Additionally, Aether employs camera trajectories as geometry-informed action
spaces, enabling effective action-conditioned prediction and visual planning.
We hope our work inspires the community to explore new frontiers in
physically-reasonable world modeling and its applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18938v1' target='_blank'>AdaWorld: Learning Adaptable World Models with Latent Actions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, Chuang Gan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 17:58:15</h6>
<p class='card-text'>World models aim to learn action-controlled prediction models and have proven
essential for the development of intelligent agents. However, most existing
world models rely heavily on substantial action-labeled data and costly
training, making it challenging to adapt to novel environments with
heterogeneous actions through limited interactions. This limitation can hinder
their applicability across broader domains. To overcome this challenge, we
propose AdaWorld, an innovative world model learning approach that enables
efficient adaptation. The key idea is to incorporate action information during
the pretraining of world models. This is achieved by extracting latent actions
from videos in a self-supervised manner, capturing the most critical
transitions between frames. We then develop an autoregressive world model that
conditions on these latent actions. This learning paradigm enables highly
adaptable world models, facilitating efficient transfer and learning of new
actions even with limited interactions and finetuning. Our comprehensive
experiments across multiple environments demonstrate that AdaWorld achieves
superior performance in both simulation quality and visual planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19010v1' target='_blank'>The Forward Physics Facility at the Large Hadron Collider</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Luis A. Anchordoqui, Akitaka Ariga, Tomoko Ariga, Alan J. Barr, Brian Batell, Jianming Bian, Jamie Boyd, Matthew Citron, Albert De Roeck, Milind V. Diwan, Jonathan L. Feng, Christopher S. Hill, Felix Kling, Steven Linden, Toni MÃ¤kelÃ¤, Kostas Mavrokoridis, Josh McFayden, Hidetoshi Otono, Juan Rojo, Dennis Soldin, Anna Stasto, Sebastian Trojanowski, Matteo Vicenzi, Wenjie Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 17:53:19</h6>
<p class='card-text'>The Forward Physics Facility (FPF) is a proposal developed to exploit the
unique scientific potential made possible by the intense hadron beams produced
in the far-forward direction at the high luminosity LHC (HL-LHC). Housed in a
well-shielded cavern 627 m from the LHC interactions, the facility will enable
a broad and deep scientific programme which will greatly extend the physics
capability of the HL-LHC. Instrumented with a suite of four complementary
detectors -- FLArE, FASER$\nu$2, FASER2 and FORMOSA -- the FPF has unique
potential to shed light on neutrino physics, QCD, astroparticle physics, and to
search for dark matter and other new particles. This contribution describes
some of the key scientific drivers for the facility, the engineering and
technical studies that have been made in preparation for it, the design of its
four complementary experiments, and the status of the project's partnerships
and planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18899v1' target='_blank'>Statistical Proof of Execution (SPEX)</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michele Dallachiesa, Antonio Pitasi, David Pinger, Josh Goodbody, Luis Vaello</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 17:13:25</h6>
<p class='card-text'>Many real-world applications are increasingly incorporating automated
decision-making, driven by the widespread adoption of ML/AI inference for
planning and guidance. This study examines the growing need for verifiable
computing in autonomous decision-making. We formalize the problem of verifiable
computing and introduce a sampling-based protocol that is significantly faster,
more cost-effective, and simpler than existing methods. Furthermore, we tackle
the challenges posed by non-determinism, proposing a set of strategies to
effectively manage common scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18871v1' target='_blank'>Bootstrapped Model Predictive Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhang Wang, Hanwei Guo, Sizhe Wang, Long Qian, Xuguang Lan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 16:46:36</h6>
<p class='card-text'>Model Predictive Control (MPC) has been demonstrated to be effective in
continuous control tasks. When a world model and a value function are
available, planning a sequence of actions ahead of time leads to a better
policy. Existing methods typically obtain the value function and the
corresponding policy in a model-free manner. However, we find that such an
approach struggles with complex tasks, resulting in poor policy learning and
inaccurate value estimation. To address this problem, we leverage the strengths
of MPC itself. In this work, we introduce Bootstrapped Model Predictive Control
(BMPC), a novel algorithm that performs policy learning in a bootstrapped
manner. BMPC learns a network policy by imitating an MPC expert, and in turn,
uses this policy to guide the MPC process. Combined with model-based
TD-learning, our policy learning yields better value estimation and further
boosts the efficiency of MPC. We also introduce a lazy reanalyze mechanism,
which enables computationally efficient imitation learning. Our method achieves
superior performance over prior works on diverse continuous control tasks. In
particular, on challenging high-dimensional locomotion tasks, BMPC
significantly improves data efficiency while also enhancing asymptotic
performance and training stability, with comparable training time and smaller
network sizes. Code is available at https://github.com/wertyuilife2/bmpc.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18730v1' target='_blank'>Predicting the Road Ahead: A Knowledge Graph based Foundation Model for
  Scene Understanding in Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongkuan Zhou, Stefan Schmid, Yicong Li, Lavdim Halilaj, Xiangtong Yao, Wei cao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 14:38:25</h6>
<p class='card-text'>The autonomous driving field has seen remarkable advancements in various
topics, such as object recognition, trajectory prediction, and motion planning.
However, current approaches face limitations in effectively comprehending the
complex evolutions of driving scenes over time. This paper proposes FM4SU, a
novel methodology for training a symbolic foundation model (FM) for scene
understanding in autonomous driving. It leverages knowledge graphs (KGs) to
capture sensory observation along with domain knowledge such as road topology,
traffic rules, or complex interactions between traffic participants. A bird's
eye view (BEV) symbolic representation is extracted from the KG for each
driving scene, including the spatio-temporal information among the objects
across the scenes. The BEV representation is serialized into a sequence of
tokens and given to pre-trained language models (PLMs) for learning an inherent
understanding of the co-occurrence among driving scene elements and generating
predictions on the next scenes. We conducted a number of experiments using the
nuScenes dataset and KG in various scenarios. The results demonstrate that
fine-tuned models achieve significantly higher accuracy in all tasks. The
fine-tuned T5 model achieved a next scene prediction accuracy of 86.7%. This
paper concludes that FM4SU offers a promising foundation for developing more
comprehensive models for scene understanding in autonomous driving.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18706v1' target='_blank'>Energy-Efficient Dynamic Training and Inference for GNN-Based Network
  Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chetna Singhal, Yassine Hadjadj-Aoul</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 14:17:57</h6>
<p class='card-text'>Efficient network modeling is essential for resource optimization and network
planning in next-generation large-scale complex networks. Traditional
approaches, such as queuing theory-based modeling and packet-based simulators,
can be inefficient due to the assumption made and the computational expense,
respectively. To address these challenges, we propose an innovative
energy-efficient dynamic orchestration of Graph Neural Networks (GNN) based
model training and inference framework for context-aware network modeling and
predictions. We have developed a low-complexity solution framework, QAG, that
is a Quantum approximation optimization (QAO) algorithm for Adaptive
orchestration of GNN-based network modeling. We leverage the tripartite graph
model to represent a multi-application system with many compute nodes.
Thereafter, we apply the constrained graph-cutting using QAO to find the
feasible energy-efficient configurations of the GNN-based model and deploying
them on the available compute nodes to meet the network modeling application
requirements. The proposed QAG scheme closely matches the optimum and offers
atleast a 50% energy saving while meeting the application requirements with 60%
lower churn-rate.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18572v1' target='_blank'>Identifying and Characterising Higher Order Interactions in Mobility
  Networks Using Hypergraphs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Prathyush Sambaturu, Bernardo Gutierrez, Moritz U. G. Kraemer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 11:29:06</h6>
<p class='card-text'>Understanding human mobility is essential for applications ranging from urban
planning to public health. Traditional mobility models such as flow networks
and colocation matrices capture only pairwise interactions between discrete
locations, overlooking higher-order relationships among locations (i.e.,
mobility flow among two or more locations). To address this, we propose
co-visitation hypergraphs, a model that leverages temporal observation windows
to extract group interactions between locations from individual mobility
trajectory data. Using frequent pattern mining, our approach constructs
hypergraphs that capture dynamic mobility behaviors across different spatial
and temporal scales. We validate our method on a publicly available mobility
dataset and demonstrate its effectiveness in analyzing city-scale mobility
patterns, detecting shifts during external disruptions such as extreme weather
events, and examining how a location's connectivity (degree) relates to the
number of points of interest (POIs) within it. Our results demonstrate that our
hypergraph-based mobility analysis framework is a valuable tool with potential
applications in diverse fields such as public health, disaster resilience, and
urban planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18546v1' target='_blank'>Multi-agent coordination for data gathering with periodic requests and
  deliveries</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaroslav Marchukov, Luis Montano</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 10:59:31</h6>
<p class='card-text'>In this demo work we develop a method to plan and coordinate a multi-agent
team to gather information on demand. The data is periodically requested by a
static Operation Center (OC) from changeable goals locations. The mission of
the team is to reach these locations, taking measurements and delivering the
data to the OC. Due to the limited communication range as well as signal
attenuation because of the obstacles, the agents must travel to the OC, to
upload the data. The agents can play two roles: ones as workers gathering data,
the others as collectors traveling invariant paths for collecting the data of
the workers to re-transmit it to the OC. The refreshing time of the delivered
information depends on the number of available agents as well as of the
scenario. The proposed algorithm finds out the best balance between the number
of collectors-workers and the partition of the scenario into working areas in
the planning phase, which provides the minimum refreshing time and will be the
one executed by the agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18545v1' target='_blank'>Communication-aware planning for robot teams deployment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaroslav Marchukov, Luis Montano</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 10:59:23</h6>
<p class='card-text'>In the present work we address the problem of deploying a team of robots in a
scenario where some locations of interest must be reached. Thus, a planning for
a deployment is required, before sending the robots. The obstacles, the limited
communication range, and the need of communicating to a base station, constrain
the connectivity of the team and the deployment planning. We propose a method
consisting of three algorithms: a distributed path planner to obtain
communication-aware trajectories; a deployment planner providing dual-use of
the robots, visiting primary goals and performing connectivity tasks; and a
clustering algorithm to allocate the tasks to robots, and obtain the best goal
visit order for the mission.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18476v2' target='_blank'>Global-Local Tree Search in VLMs for 3D Indoor Scene Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Deng, Mengshi Qi, Huadong Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 09:21:13</h6>
<p class='card-text'>Large Vision-Language Models (VLMs), such as GPT-4, have achieved remarkable
success across various fields. However, there are few studies on 3D indoor
scene generation with VLMs. This paper considers this task as a planning
problem subject to spatial and layout common sense constraints. To solve the
problem with a VLM, we propose a new global-local tree search algorithm.
Globally, the method places each object sequentially and explores multiple
placements during each placement process, where the problem space is
represented as a tree. To reduce the depth of the tree, we decompose the scene
structure hierarchically, i.e. room level, region level, floor object level,
and supported object level. The algorithm independently generates the floor
objects in different regions and supported objects placed on different floor
objects. Locally, we also decompose the sub-task, the placement of each object,
into multiple steps. The algorithm searches the tree of problem space. To
leverage the VLM model to produce positions of objects, we discretize the
top-down view space as a dense grid and fill each cell with diverse emojis to
make to cells distinct. We prompt the VLM with the emoji grid and the VLM
produces a reasonable location for the object by describing the position with
the name of emojis. The quantitative and qualitative experimental results
illustrate our approach generates more plausible 3D scenes than
state-of-the-art approaches. Our source code is available at
https://github.com/dw-dengwei/TreeSearchGen .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18473v1' target='_blank'>The On-Board Computer of the AcubeSAT Mission</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Konstantinos Tsoupos, Stylianos Tzelepis, Georgios Sklavenitis, Dimitrios Stoupis, Grigorios Pavlakis, Panagiotis Bountzioukas, Christina Athanasiadou, Lily Ha, David Palma, Loris Franchi, Alkis Hatzopoulos</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 09:20:01</h6>
<p class='card-text'>AcubeSAT is an open-source CubeSat mission aiming to explore the effects of
microgravity and radiation on eukaryotic cells using a compact microfluidic
lab-on-a-chip platform. It is developed by SpaceDot, a volunteer,
interdisciplinary student team at the Aristotle University of Thessaloniki and
supported by the "Fly Your Satellite! 3" program of the European Space Agency
(ESA) Education Office.
  The nanosatellite features an in-house designed on-board computer subsystem
responsible for telecommand execution, telemetry fetching, onboard time
synchronization, in-orbit patching, and fault recovery. The subsystem is
designed on one PC/104 standard compatible Printed Circuit Board (PCB) that
hosts the On-board Computer (OBC) on the one side and the Attitude and Orbit
Control Subsystem (AOCS) on the other, and it is compatible with the LibreCube
standard. The hosted subsystems are functionally isolated and feature an ARM
Cortex-M7, radiation-tolerant microcontroller each.
  Before sending anything to space thorough testing is required and
specifically the on-board computer board underwent vibration and thermal
cycling tests to ensure nominal operation in all conditions.
  This paper aims to elucidate the decision-making process, design iterations,
and development stages of the custom board and accompanying in-house software.
Insights garnered from the initial partially successful environmental test
campaign at the ESA CubeSat Support Facility will be shared, along with the
ensuing preparations, results, and lessons learned from subsequent testing
endeavors in April 2024. Furthermore, the current developmental status will be
discussed alongside future electromagnetic compatibility testing, integration
plan on a FlatSat, and prospects for the open-source design as a
cost-effective, and modular solution that can be tailored with little effort
for upcoming missions.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>