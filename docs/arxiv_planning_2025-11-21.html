<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-11-21</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-11-21</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16671v1' target='_blank'>Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, Pheng-Ann Heng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 18:59:52</h6>
<p class='card-text'>Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16635v1' target='_blank'>SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guolin Huang, Wenting Chen, Jiaqi Yang, Xinheng Lyu, Xiaoling Luo, Sen Yang, Xiaohan Xing, Linlin Shen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 18:41:44</h6>
<p class='card-text'>Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16518v1' target='_blank'>MiMo-Embodied: X-Embodied Foundation Model Technical Report</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoshuai Hao, Lei Zhou, Zhijian Huang, Zhiwen Hou, Yingbo Tang, Lingfeng Zhang, Guang Li, Zheng Lu, Shuhuai Ren, Xianhui Meng, Yuchen Zhang, Jing Wu, Jinghui Lu, Chenxu Dang, Jiayi Guan, Jianhua Wu, Zhiyi Hou, Hanbing Li, Shumeng Xia, Mingliang Zhou, Yinan Zheng, Zihao Yue, Shuhao Gu, Hao Tian, Yuannan Shen, Jianwei Cui, Wen Zhang, Shaoqing Xu, Bing Wang, Haiyang Sun, Zeyu Zhu, Yuncheng Jiang, Zibin Guo, Chuhong Gong, Chaofan Zhang, Wenbo Ding, Kun Ma, Guang Chen, Rui Cai, Diyun Xiang, Heng Qu, Fuli Luo, Hangjun Ye, Long Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 16:34:55</h6>
<p class='card-text'>We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16498v1' target='_blank'>Acquisition Time-Informed Breast Tumor Segmentation from Dynamic Contrast-Enhanced MRI</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rui Wang, Yuexi Du, John Lewin, R. Todd Constable, Nicha C. Dvornek</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 16:13:24</h6>
<p class='card-text'>Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays an important role in breast cancer screening, tumor assessment, and treatment planning and monitoring. The dynamic changes in contrast in different tissues help to highlight the tumor in post-contrast images. However, varying acquisition protocols and individual factors result in large variation in the appearance of tissues, even for images acquired in the same phase (e.g., first post-contrast phase), making automated tumor segmentation challenging. Here, we propose a tumor segmentation method that leverages knowledge of the image acquisition time to modulate model features according to the specific acquisition sequence. We incorporate the acquisition times using feature-wise linear modulation (FiLM) layers, a lightweight method for incorporating temporal information that also allows for capitalizing on the full, variables number of images acquired per imaging study. We trained baseline and different configurations for the time-modulated models with varying backbone architectures on a large public multisite breast DCE-MRI dataset. Evaluation on in-domain images and a public out-of-domain dataset showed that incorporating knowledge of phase acquisition time improved tumor segmentation performance and model generalization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16458v1' target='_blank'>A convex approach for Markov chain estimation from aggregate data via inverse optimal transport</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michele Mascherpa, Axel Ringh, Amirhossein Taghvaei, Johan Karlsson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 15:24:44</h6>
<p class='card-text'>We address the problem of identifying the dynamical law governing the evolution of a population of indistinguishable particles, when only aggregate distributions at successive times are observed. Assuming a Markovian evolution on a discrete state space, the task reduces to estimating the underlying transition probability matrix from distributional data. We formulate this inverse problem within the framework of entropic optimal transport, as a joint optimization over the transition matrix and the transport plans connecting successive distributions. This formulation results in a convex optimization problem, and we propose an efficient iterative algorithm based on the entropic proximal method. We illustrate the accuracy and convergence of the method in two numerical setups, considering estimation from independent snapshots and estimation from a time series of aggregate observations, respectively.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16455v1' target='_blank'>[Experiment, Analysis, and Benchmark] Systematic Evaluation of Plan-based Adaptive Query Processing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pei Mu, Anderson Chaves Carniel, Antonio Barbalace, Amir Shaikhha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 15:22:53</h6>
<p class='card-text'>Unreliable cardinality estimation remains a critical performance bottleneck in database management systems (DBMSs). Adaptive Query Processing (AQP) strategies address this limitation by providing a more robust query execution mechanism. Specifically, plan-based AQP achieves this by incrementally refining cardinality using feedback from the execution of sub-plans. However, the actual reason behind the improvements of plan-based AQP, especially across different storage architectures (on-disk vs. in-memory DBMSs), remains unexplored.
  This paper presents the first comprehensive analysis of state-of-the-art plan-based AQP. We implement and evaluate this strategy on both on-disk and in-memory DBMSs across two benchmarks. Our key findings reveal that while plan-based AQP provides overall speedups in both environments, the sources of improvement differ significantly. In the on-disk DBMS, PostgreSQL, performance gains primarily come from the query plan reorderings, but not the cardinality updating mechanism; in fact, updating cardinalities introduces measurable overhead. Conversely, in the in-memory DBMS, DuckDB, cardinality refinement drives significant performance improvements for most queries. We also observe significant performance benefits of the plan-based AQP compared to a state-of-the-art related-based AQP method. These observations provide crucial insights for researchers on when and why plan-based AQP is effective, and ultimately guide database system developers on the tradeoffs between the implementation effort and performance improvements.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16333v1' target='_blank'>Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohammad Areeb Qazi, Maryam Nadeem, Mohammad Yaqub</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 13:11:45</h6>
<p class='card-text'>Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16303v1' target='_blank'>ATLAS: Efficient Atom Rearrangement for Defect-Free Neutral-Atom Quantum Arrays Under Transport Loss</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Otto Savola, Alexandru Paler</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 12:32:35</h6>
<p class='card-text'>Neutral-atom quantum computers encode qubits in individually trapped atoms arranged in optical lattices. Achieving defect-free atom configurations is essential for high-fidelity quantum gates and scalable error correction, yet stochastic loading and atom loss during rearrangement hinder reliable large-scale assembly. This work presents ATLAS, an open-source atom transport algorithm that efficiently converts a randomly loaded $W \times W$ lattice into a defect-free $L \times L$ subarray while accounting for realistic physical constraints, including finite acceleration, transfer time, and per-move loss probability. In the planning phase, optimal batches of parallel moves are computed on a lossless virtual array; during execution, these moves are replayed under probabilistic atom loss to maximize the expected number of retained atoms. Monte Carlo simulations across lattice sizes $W=10$--$100$, loading probabilities $p_{\mathrm{occ}}=0.5$--$0.9$, and loss rates $p_{\mathrm{loss}}=0$--$0.05$ demonstrate fill rates above $99\%$ within six iterations and over $90\%$ atom retention at low loss. The algorithm achieves sublinear move scaling ($\propto M^{0.55}$) and linear growth of required initial size with target dimension, outperforming prior methods in robustness and scalability -- offering a practical path toward larger neutral-atom quantum arrays.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16243v1' target='_blank'>An Agent-Based Simulation of Regularity-Driven Student Attrition: How Institutional Time-to-Live Constraints Create a Dropout Trap in Higher Education</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:H. R. Paz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 11:21:39</h6>
<p class='card-text'>High dropout rates in engineering programmes are conventionally attributed to student deficits: lack of academic preparation or motivation. However, this view neglects the causal role of "normative friction": the complex system of administrative rules, exam validity windows, and prerequisite chains that constrain student progression. This paper introduces "The Regularity Trap," a phenomenon where rigid assessment timelines decouple learning from accreditation. We operationalize the CAPIRE framework into a calibrated Agent-Based Model (ABM) simulating 1,343 student trajectories across a 42-course Civil Engineering curriculum. The model integrates empirical course parameters and thirteen psycho-academic archetypes derived from a 15-year longitudinal dataset. By formalizing the "Regularity Regime" as a decaying validity function, we isolate the effect of administrative time limits on attrition. Results reveal that 86.4% of observed dropouts are driven by normative mechanisms (expiry cascades) rather than purely academic failure (5.3%). While the overall dropout rate stabilized at 32.4%, vulnerability was highly heterogeneous: archetypes with myopic planning horizons faced attrition rates up to 49.0%, compared to 13.2% for strategic agents, despite comparable academic ability. These findings challenge the neutrality of administrative structures, suggesting that rigid validity windows act as an invisible filter that disproportionately penalizes students with lower self-regulatory capital.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16230v1' target='_blank'>When Less is More: A Story of Failing Bayesian Optimization Due to Additional Expert Knowledge</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dorina Weichert, Gunar Ernis, Marvin Worthmann, Peter Ryzko, Lukas Seifert</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 10:56:07</h6>
<p class='card-text'>The compounding of plastics with recycled material remains a practical challenge, as the properties of the processed material is not as easy to control as with completely new raw materials. For a data scientist, it makes sense to plan the necessary experiments in the development of new compounds using Bayesian Optimization, an optimization approach based on a surrogate model that is known for its data efficiency and is therefore well suited for data obtained from costly experiments. Furthermore, if historical data and expert knowledge are available, their inclusion in the surrogate model is expected to accelerate the convergence of the optimization. In this article, we describe a use case in which the addition of data and knowledge has impaired optimization. We also describe the unsuccessful methods that were used to remedy the problem before we found the reasons for the poor performance and achieved a satisfactory result. We conclude with a lesson learned: additional knowledge and data are only beneficial if they do not complicate the underlying optimization goal.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16212v1' target='_blank'>The Fluorescence Camera for the PBR mission</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Francesco Saverio Cafagna</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 10:31:33</h6>
<p class='card-text'>The Probe Of Extreme Multi-Messenger Astrophysics (POEMMA) Balloon with Radio (PBR) is an instrument designed to be borne by a NASA suborbital Super Pressure Balloon (SPB), in a mission planned to last as long as 50 days. The PBR instrument consists of a 1.1 m aperture Schmidt telescope, similar to the POEMMA design, with two cameras in its hybrid focal surface: a Fluorescence Camera (FC) and a Cherenkov Camera (CC), both mounted on a frame that can be tilted to point from nadir up to 13 degrees above the horizon. The FC camera is designed to detect the fluorescence emission of Extensive Air Showers produced by Ultra-High Energy Cosmic Rays from sub-orbital altitudes. This measurement will validate the detection strategy for future space-based missions, such as POEMMA. The FC will be made of 4 Photo Detection Modules (PDMs), each consisting of a 6x6 matrix of 64-channel Multi Anode PhotoMulTipliers (MAPMT), for a grand total of 2304 pixels for each PDM. Custom-designed SPACIROC-3 ASICs perform single photoelectron counting on each pixel as well as charge integration on groups of 8 pixels to measure extremely bright or fast signals, reaching a double pulse resolution in the order of 10 ns for a 1 microsecond acquisition gate. A field flattener lens and a BG3 filter, to match the wavelength range of interest (300-400 nm), are mounted in front of the PDM. The camera will be able to detect showers in a field of view of 24x24 square degrees, with a pixel size on ground corresponding to 115 m. Details on the camera design and implementation will be given, along with the expected performance and the state of the construction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16075v1' target='_blank'>A Hybrid Proactive And Predictive Framework For Edge Cloud Resource Management</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hrikshesh Kumar, Anika Garg, Anshul Gupta, Yashika Agarwal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 06:09:07</h6>
<p class='card-text'>Old cloud edge workload resource management is too reactive. The problem with relying on static thresholds is that we are either overspending for more resources than needed or have reduced performance because of their lack. This is why we work on proactive solutions. A framework developed for it stops reacting to the problems but starts expecting them. We design a hybrid architecture, combining two powerful tools: the CNN LSTM model for time series forecasting and an orchestrator based on multi agent Deep Reinforcement Learning In fact the novelty is in how we combine them as we embed the predictive forecast from the CNN LSTM directly into the DRL agent state space. That is what makes the AI manager smarter it sees the future, which allows it to make better decisions about a long term plan for where to run tasks That means finding that sweet spot between how much money is saved while keeping the system healthy and apps fast for users That is we have given it eyes in order to see down the road so that it does not have to lurch from one problem to another it finds a smooth path forward Our tests show our system easily beats the old methods It is great at solving tough problems like making complex decisions and juggling multiple goals at once like being cheap fast and reliable</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16063v1' target='_blank'>Modeling Pointing, Acquisition, and Tracking Delays in Free-Space Optical Satellite Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jason Gerard, Juan A. Fraire, Sandra Céspedes</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 05:52:23</h6>
<p class='card-text'>Free-space optical inter-satellite links (OISLs) enable high-capacity space communications but require precise Pointing, Acquisition, and Tracking (PAT) between links. Current scheduling approaches often overlook or oversimplify PAT delays, leading to inefficient contact planning and overestimated network capacities. We present a validated model for quantifying retargeting delays, defined as the delay-inducing portion of PAT before data transmission begins, encompassing coarse pointing, fine pointing, and the handover to tracking. The model is grounded in mission data from NASA TBIRD, LLCD, DSOC, and ESA's Lunar Optical Communication Terminal. We find that PAT delays exhibit multimodal distributions based on prior link geometry and scale nonlinearly with initial pointing uncertainty and optical beam width. Integrating these delay models into routing and scheduling algorithms will enable more accurate contact planning and higher utilization in optical networks. The proposed model provides a foundation for evaluating performance and designing algorithms for future large-scale optical satellite networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16048v1' target='_blank'>Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qing Zhang, Jing Huang, Mingyang Xu, Jun Rekimoto</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 05:10:13</h6>
<p class='card-text'>While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately "lo-fi" approach. We present the "Semantic Glitch," a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a "physical glitch" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a "narrative mind" that complements the "weak," historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling "plan to execution" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.16044v1' target='_blank'>Robustness of Online Inventory Balancing to Inventory Shocks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiding Feng, Rad Niazadeh, Amin Saberi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 05:02:32</h6>
<p class='card-text'>In classic adversarial online resource allocation problems such as AdWords, customers arrive online while products are given offline with a fixed initial inventory. To ensure revenue guarantees under uncertainty, the decision maker must balance consumption across products. Based on this, the prevalent policy "inventory balancing (IB)" has proved to be optimal or near-optimal competitive in almost all classic settings. However, these models do not capture various forms of inventory shocks on the supply side, which play an important role in real-world online assortment and can significantly impact the revenue performance of the IB algorithm.
  Motivated by this paradigm, we introduce a variant of online assortment planning with inventory shocks. Our model considers adversarial exogenous shocks (where supply increases unpredictably) and allocation-coupled endogenous shocks (where an inventory reduction is triggered by the algorithms and re-adjusted after a usage duration), whose combination leads to non-monotonic inventory fluctuations. As our main result, we show the robustness of IB-type strategies against such shocks by designing a new family of optimal competitive algorithms called "Batched Inventory Balancing (BIB)." Using a novel randomized primal-dual method, we bound the competitive ratio of BIB against optimal offline. We show that with proper choice of a certain parameter, this competitive ratio is asymptotically optimal and converges to (1-1/e) as initial inventories grow, in contrast to the original IB which no longer achieves the optimal ratio in this new model. Moreover, we characterize BIB's competitive ratio parametric by its penalty function and show that it matches exactly the competitive ratio of IB without shocks. Our refined analysis reduces the dual construction to a combinatorial "interval assignment problem" whose algorithmic solution may be of independent interest.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15995v1' target='_blank'>PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zili Tang, Ying Zhang, Meng Guo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 02:45:46</h6>
<p class='card-text'>Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15981v1' target='_blank'>Molecular resonance identification in complex absorbing potentials via integrated quantum computing and high-throughput computing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingcheng Dai, Atharva Vidwans, Eric H. Wan, Alexander X. Miller, Micheline B. Soley</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-20 02:28:05</h6>
<p class='card-text'>Recent advancements in quantum algorithms have reached a state where we can consider how to capitalize on quantum and classical computational resources to accelerate molecular resonance state identification. Here we identify molecular resonances with a method that combines quantum computing with classical high-throughput computing (HTC). This algorithm, which we term qDRIVE (the quantum deflation resonance identification variational eigensolver) exploits the complex absorbing potential formalism to distill the problem of molecular resonance identification into a network of hybrid quantum-classical variational quantum eigensolver tasks, and harnesses HTC resources to execute these interconnected but independent tasks both asynchronously and in parallel, a strategy that minimizes wall time to completion. We show qDRIVE successfully identifies resonance energies and wavefunctions in simulated quantum processors with current and planned specifications, which bodes well for qDRIVE's ultimate application in disciplines ranging from photocatalysis to quantum control and places a spotlight on the potential offered by integrated heterogenous quantum computing/HTC approaches in computational chemistry.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15932v1' target='_blank'>How Mathematical Forms of Chemotherapy and Radiotherapy Bias Model-Optimized Predictions: Implications for Model Selection</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Changin Oh, Kathleen P. Wilkie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 23:47:58</h6>
<p class='card-text'>The move towards personalized treatment and digital twins for cancer therapy requires a complete understanding of the mathematical models upon which these optimized simulation-based strategies are formulated. This study investigates the influence of mathematical model selection on the optimization of chemotherapy and radiotherapy protocols. By examining three chemotherapy models (log-kill, Norton-Simon, and Emax), and three radiotherapy models (linear-quadratic, proliferation saturation index, and continuous death-rate), we identify similarities and significant differences in the optimized protocols. We demonstrate how the assumptions built into the model formulations heavily influence optimal treatment dosing and sequencing, potentially leading to contradictory results. Further, we demonstrate how different model forms influence predictions in the adaptive therapy setting. As treatment decisions increasingly rely on simulation-based strategies, unexamined model assumptions can introduce bias, leading to model-dependent recommendations that may not be generalizable. This study highlights the importance of basing model selection on a full analysis of bias, sensitivity, practical parameter identifiability and/or inferred parameter posteriors, as a part of the uncertainty quantification process, rather than solely relying on information criterion. Understanding how model choice impacts predictions guiding personalized treatment planning with sufficient uncertainty quantification analysis, will lead to more robust and generalizable predictions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15914v1' target='_blank'>I've Changed My Mind: Robots Adapting to Changing Human Goals during Collaboration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Debasmita Ghose, Oz Gitelson, Ryan Jin, Grace Abawe, Marynel Vazquez, Brian Scassellati</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 22:43:47</h6>
<p class='card-text'>For effective human-robot collaboration, a robot must align its actions with human goals, even as they change mid-task. Prior approaches often assume fixed goals, reducing goal prediction to a one-time inference. However, in real-world scenarios, humans frequently shift goals, making it challenging for robots to adapt without explicit communication. We propose a method for detecting goal changes by tracking multiple candidate action sequences and verifying their plausibility against a policy bank. Upon detecting a change, the robot refines its belief in relevant past actions and constructs Receding Horizon Planning (RHP) trees to actively select actions that assist the human while encouraging Differentiating Actions to reveal their updated goal. We evaluate our approach in a collaborative cooking environment with up to 30 unique recipes and compare it to three comparable human goal prediction algorithms. Our method outperforms all baselines, quickly converging to the correct goal after a switch, reducing task completion time, and improving collaboration efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15852v1' target='_blank'>AI-Enabled Orchestration of Event-Driven Business Processes in Workday ERP for Healthcare Enterprises</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Monu Sharma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 20:18:10</h6>
<p class='card-text'>The adoption of cloud-based Enterprise Resource Planning (ERP) platforms such as Workday has transformed healthcare operations by integrating financial, supply-chain, and workforce processes into a unified ecosystem. However, traditional workflow logic in ERP systems often lacks the adaptability required to manage event-driven and data-intensive healthcare environments.
  This study proposes an AI-enabled event-driven orchestration framework within Workday ERP that intelligently synchronizes financial and supply-chain workflows across distributed healthcare entities. The framework employs machine-learning triggers, anomaly detection, and process mining analytics to anticipate and automate responses to operational events such as inventory depletion, payment delays, or patient demand fluctuations. A multi-organization case analysis demonstrates measurable gains in process efficiency, cost visibility, and decision accuracy.
  Results confirm that embedding AI capabilities into Workday's event-based architecture enhances operational resilience, governance, and scalability. The proposed model contributes to the broader understanding of intelligent ERP integration and establishes a reference for next-generation automation strategies in healthcare enterprises.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15846v1' target='_blank'>The Loss of Control Playbook: Degrees, Dynamics, and Preparedness</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Charlotte Stix, Annika Hallensleben, Alejandro Ortega, Matteo Pistillo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 20:10:39</h6>
<p class='card-text'>This research report addresses the absence of an actionable definition for Loss of Control (LoC) in AI systems by developing a novel taxonomy and preparedness framework. Despite increasing policy and research attention, existing LoC definitions vary significantly in scope and timeline, hindering effective LoC assessment and mitigation. To address this issue, we draw from an extensive literature review and propose a graded LoC taxonomy, based on the metrics of severity and persistence, that distinguishes between Deviation, Bounded LoC, and Strict LoC. We model pathways toward a societal state of vulnerability in which sufficiently advanced AI systems have acquired or could acquire the means to cause Bounded or Strict LoC once a catalyst, either misalignment or pure malfunction, materializes. We argue that this state becomes increasingly likely over time, absent strategic intervention, and propose a strategy to avoid reaching a state of vulnerability. Rather than focusing solely on intervening on AI capabilities and propensities potentially relevant for LoC or on preventing potential catalysts, we introduce a complementary framework that emphasizes three extrinsic factors: Deployment context, Affordances, and Permissions (the DAP framework). Compared to work on intrinsic factors and catalysts, this framework has the unfair advantage of being actionable today. Finally, we put forward a plan to maintain preparedness and prevent the occurrence of LoC outcomes should a state of societal vulnerability be reached, focusing on governance measures (threat modeling, deployment policies, emergency response) and technical controls (pre-deployment testing, control measures, monitoring) that could maintain a condition of perennial suspension.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15840v1' target='_blank'>Comparative Security Performance of Workday Cloud ERP Across Key Dimensions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Monu Sharma, Abhishek Jain</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 19:57:37</h6>
<p class='card-text'>Workday is a cloud-based Enterprise Resource Planning-ERP system that brings HR, Finance, Supply Chain functions , Prism Analytics and Extend custom built in application together under an integrated software as a service SaaS environment. As every organization that undergoes digital transformation, the importance of securing sensitive enterprise data in cloud ERP systems has always been more challeging. To analyze Workday's security architecture, we present a Security analysis in both CIA Triad Enhanced Framework and Zero Trust Security Architecture. The study examines five key dimensions confidentiality, integrity, availability, authentication, and compliance with weighted sub metric analysis and qualitative document review. The results show Workday delivers a composite score of 0.86 with an overall score that closely matches international standards of best practices like GDPR, HIPAA, SOC 2, etc.
  The platform uses encryption protocols, granular access controls, network safeguards, and continuous verification mechanisms to enable least-privilege access and adaptive defense. Security groups and business process access rules provide scalable governance across very large organizational structures.Workday's layered security to tackle everyday cloud security weaknesses. The work concludes that Workday's architecture demonstrates the best practices for secure, scalable, and compliant ERP application-oriented deployment, which can make this a standard for enterprise cloud security management. These insights provide important guidance for organizations that wish to bolster their cloud ERP defenses and stay ahead of changing regulatory expectations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15620v1' target='_blank'>Lost in Vagueness: Towards Context-Sensitive Standards for Robustness Assessment under the EU AI Act</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Roberta Tamponi, Carina Prunkl, Thomas Bäck, Anna V. Kononova</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 17:06:36</h6>
<p class='card-text'>Robustness is a key requirement for high-risk AI systems under the EU Artificial Intelligence Act (AI Act). However, both its definition and assessment methods remain underspecified, leaving providers with little concrete direction on how to demonstrate compliance. This stems from the Act's horizontal approach, which establishes general obligations applicable across all AI systems, but leaves the task of providing technical guidance to harmonised standards. This paper investigates what it means for AI systems to be robust and illustrates the need for context-sensitive standardisation. We argue that robustness is not a fixed property of a system, but depends on which aspects of performance are expected to remain stable ("robustness of what"), the perturbations the system must withstand ("robustness to what") and the operational environment. We identify three contextual drivers--use case, data and model--that shape the relevant perturbations and influence the choice of tests, metrics and benchmarks used to evaluate robustness. The need to provide at least a range of technical options that providers can assess and implement in light of the system's purpose is explicitly recognised by the standardisation request for the AI Act, but planned standards, still focused on horizontal coverage, do not yet offer this level of detail. Building on this, we propose a context-sensitive multi-layered standardisation framework where horizontal standards set common principles and terminology, while domain-specific ones identify risks across the AI lifecycle and guide appropriate practices, organised in a dynamic repository where providers can propose new informative methods and share lessons learned. Such a system reduces the interpretative burden, mitigates arbitrariness and addresses the obsolescence of static standards, ensuring that robustness assessment is both adaptable and operationally meaningful.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15588v1' target='_blank'>Real-Time Optimal Control via Transformer Networks and Bernstein Polynomials</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gage MacLin, Venanzio Cichella, Andrew Patterson, Irene Gregory</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 16:19:12</h6>
<p class='card-text'>In this paper, we propose a Transformer-based framework for approximating solutions to infinite-dimensional optimization problems: calculus of variations problems and optimal control problems. Our approach leverages offline training on data generated by solving a sample of infinite- dimensional optimization problems using composite Bernstein collocation. Once trained, the Transformer efficiently generates near-optimal, feasible trajectories, making it well-suited for real-time applications. In motion planning for autonomous vehicles, for instance, these trajectories can serve to warm- start optimal motion planners or undergo rigorous evaluation to ensure safety. We demonstrate the effectiveness of this method through numerical results on a classical control problem and an online obstacle avoidance task. This data-driven approach offers a promising solution for real-time optimal control of nonlinear, nonconvex systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15532v1' target='_blank'>NMPC-based Motion Planning with Adaptive Weighting for Dynamic Object Interception</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chen Cai, Saksham Kohli, Steven Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 15:31:47</h6>
<p class='card-text'>Catching fast-moving objects serves as a benchmark for robotic agility, posing significant coordination challenges for cooperative manipulator systems holding a catcher, particularly due to inherent closed-chain constraints. This paper presents a nonlinear model predictive control (MPC)-based motion planner that bridges high-level interception planning with real-time joint space control, enabling dynamic object interception for systems comprising two cooperating arms. We introduce an Adaptive- Terminal (AT) MPC formulation featuring cost shaping, which contrasts with a simpler Primitive-Terminal (PT) approach relying heavily on terminal penalties for rapid convergence. The proposed AT formulation is shown to effectively mitigate issues related to actuator power limit violations frequently encountered with the PT strategy, yielding trajectories and significantly reduced control effort. Experimental results on a robotic platform with two cooperative arms, demonstrating excellent real time performance, with an average planner cycle computation time of approximately 19 ms-less than half the 40 ms system sampling time. These results indicate that the AT formulation achieves significantly improved motion quality and robustness with minimal computational overhead compared to the PT baseline, making it well-suited for dynamic, cooperative interception tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15414v1' target='_blank'>RRT*former: Environment-Aware Sampling-Based Motion Planning using Transformer</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingyang Feng, Shaoyuan Li, Xiang Yin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 13:14:10</h6>
<p class='card-text'>We investigate the sampling-based optimal path planning problem for robotics in complex and dynamic environments. Most existing sampling-based algorithms neglect environmental information or the information from previous samples. Yet, these pieces of information are highly informative, as leveraging them can provide better heuristics when sampling the next state. In this paper, we propose a novel sampling-based planning algorithm, called \emph{RRT*former}, which integrates the standard RRT* algorithm with a Transformer network in a novel way. Specifically, the Transformer is used to extract features from the environment and leverage information from previous samples to better guide the sampling process. Our extensive experiments demonstrate that, compared to existing sampling-based approaches such as RRT*, Neural RRT*, and their variants, our algorithm achieves considerable improvements in both the optimality of the path and sampling efficiency. The code for our implementation is available on https://github.com/fengmingyang666/RRTformer.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15412v1' target='_blank'>Spatially Consistent Air-to-Ground Channel Modeling and Simulation via 3D Shadow Projections</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Evgenii Vinogradov, Aymen Fakhreddine, Abdul Saboor, Sergi Abadal, Sofie Pollin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 13:08:54</h6>
<p class='card-text'>We present an approach for spatially-consistent semi-deterministic Air-to-Ground (A2G) channel modeling in Unmanned Aerial Vehicle-assisted networks. We use efficient 3D building shadow projections to determine Line-of-Sight (LOS) regions, enabling fast generation of LOS maps. By integrating LOS-aware deterministic path loss with stochastic shadow fading, the approach produces spatially consistent A2G radio maps suitable for environment- and mobility-aware channel evaluation and performance prediction. Simulation results in ITU-compliant Manhattan grid environments demonstrate the model's ability to reflect key urban propagation characteristics, such as LOS blockage patterns and outage behavior. The proposed approach provides an efficient alternative to ray tracing or fully stochastic models, with particular relevance for user mobility, link planning, and radio map generation in 6G non-terrestrial networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15369v1' target='_blank'>IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gihwan Kim, Jemin Lee, Hyungshin Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 11:56:16</h6>
<p class='card-text'>Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\%p (avg. 1.78\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15321v1' target='_blank'>Techno-Economic Modelling and Component Sizing in Renewable Energy Communities: A Participant Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vishal Kachhad, Amit Joshi, Luigi Glielmo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 10:38:18</h6>
<p class='card-text'>This article proposes an optimization problem formulation to find the optimal sizes of Photovoltaics (PV) and Battery Energy Storage Systems (BESS) for individual participants within the context of the Renewable Energy Community (REC). An optimization problem considered the dynamic nature of electricity pricing, solar irradiation levels, financial aspects such as capital investment, and operational and maintenance expenditures of PV and BESS. The analysis also considered replacement costs and the efficiency of charging and discharging the BESS unit. We employed Mixed-Integer Non-Linear Programming (MINLP) to determine the optimal system size that maximizes the Net Present Value (NPV) of individual participants. Furthermore, in this study, we used daily representative signals for each season of the year to reduce simulation runtime. The Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) was used to extract these signals. Then, these representative signals obtained were used in the optimization problem formulation to reduce simulation time and extend our analysis to a wider planning horizon. In addition, the study introduced fairness by applying the individual marginal contribution method to distribute incentives equitably among REC participants, ensuring that each member benefited from their contribution. A simulation study was conducted using a real demand dataset of five houses located in Roseto Valfortore, a small town and commune in the Foggia Province of the Apulia region in southern Italy, to demonstrate the practical relevance and usefulness of the ideas discussed. Ultimately, the goal of the article was to empower the REC with the knowledge necessary to make informed decisions and shape the future of sustainable energy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.15293v1' target='_blank'>A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jia Li, Zhi Jin, Kechi Zhang, Huangzhao Zhang, Jiaru Qian, Tiankuo Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-19 09:57:49</h6>
<p class='card-text'>Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>