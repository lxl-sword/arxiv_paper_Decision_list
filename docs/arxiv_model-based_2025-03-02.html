<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>model-based - 2025-03-02</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>model-based - 2025-03-02</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20168v1' target='_blank'>Accelerating Model-Based Reinforcement Learning with State-Space World
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maria Krinner, Elie Aljalbout, Angel Romero, Davide Scaramuzza</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 15:05:25</h6>
<p class='card-text'>Reinforcement learning (RL) is a powerful approach for robot learning.
However, model-free RL (MFRL) requires a large number of environment
interactions to learn successful control policies. This is due to the noisy RL
training updates and the complexity of robotic systems, which typically involve
highly non-linear dynamics and noisy sensor signals. In contrast, model-based
RL (MBRL) not only trains a policy but simultaneously learns a world model that
captures the environment's dynamics and rewards. The world model can either be
used for planning, for data collection, or to provide first-order policy
gradients for training. Leveraging a world model significantly improves sample
efficiency compared to model-free RL. However, training a world model alongside
the policy increases the computational complexity, leading to longer training
times that are often intractable for complex real-world scenarios. In this
work, we propose a new method for accelerating model-based RL using state-space
world models. Our approach leverages state-space models (SSMs) to parallelize
the training of the dynamics model, which is typically the main computational
bottleneck. Additionally, we propose an architecture that provides privileged
information to the world model during training, which is particularly relevant
for partially observable environments. We evaluate our method in several
real-world agile quadrotor flight tasks, involving complex dynamics, for both
fully and partially observable environments. We demonstrate a significant
speedup, reducing the world model training time by up to 10 times, and the
overall MBRL training time by up to 4 times. This benefit comes without
compromising performance, as our method achieves similar sample efficiency and
task rewards to state-of-the-art MBRL methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11480v1' target='_blank'>Enhancing Offline Model-Based RL via Active Model Selection: A Bayesian
  Optimization Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu-Wei Yang, Yun-Ming Chan, Wei Hung, Xi Liu, Ping-Chun Hsieh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-17 06:34:58</h6>
<p class='card-text'>Offline model-based reinforcement learning (MBRL) serves as a competitive
framework that can learn well-performing policies solely from pre-collected
data with the help of learned dynamics models. To fully unleash the power of
offline MBRL, model selection plays a pivotal role in determining the dynamics
model utilized for downstream policy learning. However, offline MBRL
conventionally relies on validation or off-policy evaluation, which are rather
inaccurate due to the inherent distribution shift in offline RL. To tackle
this, we propose BOMS, an active model selection framework that enhances model
selection in offline MBRL with only a small online interaction budget, through
the lens of Bayesian optimization (BO). Specifically, we recast model selection
as BO and enable probabilistic inference in BOMS by proposing a novel
model-induced kernel, which is theoretically grounded and computationally
efficient. Through extensive experiments, we show that BOMS improves over the
baseline methods with a small amount of online interaction comparable to only
$1\%$-$2.5\%$ of offline training data on various RL tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.10077v1' target='_blank'>Towards Empowerment Gain through Causal Structure Learning in
  Model-Based RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongye Cao, Fan Feng, Meng Fang, Shaokang Dong, Tianpei Yang, Jing Huo, Yang Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-14 10:59:09</h6>
<p class='card-text'>In Model-Based Reinforcement Learning (MBRL), incorporating causal structures
into dynamics models provides agents with a structured understanding of the
environments, enabling efficient decision. Empowerment as an intrinsic
motivation enhances the ability of agents to actively control their
environments by maximizing the mutual information between future states and
actions. We posit that empowerment coupled with causal understanding can
improve controllability, while enhanced empowerment gain can further facilitate
causal reasoning in MBRL. To improve learning efficiency and controllability,
we propose a novel framework, Empowerment through Causal Learning (ECL), where
an agent with the awareness of causal dynamics models achieves
empowerment-driven exploration and optimizes its causal structure for task
learning. Specifically, ECL operates by first training a causal dynamics model
of the environment based on collected data. We then maximize empowerment under
the causal structure for exploration, simultaneously using data gathered
through exploration to update causal dynamics model to be more controllable
than dense dynamics model without causal structure. In downstream task
learning, an intrinsic curiosity reward is included to balance the causality,
mitigating overfitting. Importantly, ECL is method-agnostic and is capable of
integrating various causal discovery methods. We evaluate ECL combined with 3
causal discovery methods across 6 environments including pixel-based tasks,
demonstrating its superior performance compared to other causal MBRL methods,
in terms of causal discovery, sample efficiency, and asymptotic performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05595v1' target='_blank'>Data efficient Robotic Object Throwing with Model-Based Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Niccol√≤ Turcato, Giulio Giacomuzzo, Matteo Terreran, Davide Allegro, Ruggero Carli, Alberto Dalla Libera</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 14:43:42</h6>
<p class='card-text'>Pick-and-place (PnP) operations, featuring object grasping and trajectory
planning, are fundamental in industrial robotics applications. Despite many
advancements in the field, PnP is limited by workspace constraints, reducing
flexibility. Pick-and-throw (PnT) is a promising alternative where the robot
throws objects to target locations, leveraging extrinsic resources like gravity
to improve efficiency and expand the workspace. However, PnT execution is
complex, requiring precise coordination of high-speed movements and object
dynamics. Solutions to the PnT problem are categorized into analytical and
learning-based approaches. Analytical methods focus on system modeling and
trajectory generation but are time-consuming and offer limited generalization.
Learning-based solutions, in particular Model-Free Reinforcement Learning
(MFRL), offer automation and adaptability but require extensive interaction
time. This paper introduces a Model-Based Reinforcement Learning (MBRL)
framework, MC-PILOT, which combines data-driven modeling with policy
optimization for efficient and accurate PnT tasks. MC-PILOT accounts for model
uncertainties and release errors, demonstrating superior performance in
simulations and real-world tests with a Franka Emika Panda manipulator. The
proposed approach generalizes rapidly to new targets, offering advantages over
analytical and Model-Free methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01591v1' target='_blank'>Improving Transformer World Models for Data-Efficient RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Antoine Dedieu, Joseph Ortiz, Xinghua Lou, Carter Wendelken, Wolfgang Lehrach, J Swaroop Guntupalli, Miguel Lazaro-Gredilla, Kevin Patrick Murphy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-03 18:25:17</h6>
<p class='card-text'>We present an approach to model-based RL that achieves a new state of the art
performance on the challenging Craftax-classic benchmark, an open-world 2D
survival game that requires agents to exhibit a wide range of general abilities
-- such as strong generalization, deep exploration, and long-term reasoning.
With a series of careful design choices aimed at improving sample efficiency,
our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps,
significantly outperforming DreamerV3, which achieves 53.2%, and, for the first
time, exceeds human performance of 65.0%. Our method starts by constructing a
SOTA model-free baseline, using a novel policy architecture that combines CNNs
and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna
with warmup", which trains the policy on real and imaginary data, (b) "nearest
neighbor tokenizer" on image patches, which improves the scheme to create the
transformer world model (TWM) inputs, and (c) "block teacher forcing", which
allows the TWM to reason jointly about the future tokens of the next timestep.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.16918v1' target='_blank'>On Rollouts in Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bernd Frauenknecht, Devdutt Subhasish, Friedrich Solowjow, Sebastian Trimpe</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-28 13:02:52</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) seeks to enhance data efficiency by
learning a model of the environment and generating synthetic rollouts from it.
However, accumulated model errors during these rollouts can distort the data
distribution, negatively impacting policy learning and hindering long-term
planning. Thus, the accumulation of model errors is a key bottleneck in current
MBRL methods. We propose Infoprop, a model-based rollout mechanism that
separates aleatoric from epistemic model uncertainty and reduces the influence
of the latter on the data distribution. Further, Infoprop keeps track of
accumulated model errors along a model rollout and provides termination
criteria to limit data corruption. We demonstrate the capabilities of Infoprop
in the Infoprop-Dyna algorithm, reporting state-of-the-art performance in
Dyna-style MBRL on common MuJoCo benchmark tasks while substantially increasing
rollout length and data quality.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.16733v1' target='_blank'>Dream to Drive with Predictive Individual World Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yinfeng Gao, Qichao Zhang, Da-wei Ding, Dongbin Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-28 06:18:29</h6>
<p class='card-text'>It is still a challenging topic to make reactive driving behaviors in complex
urban environments as road users' intentions are unknown. Model-based
reinforcement learning (MBRL) offers great potential to learn a reactive policy
by constructing a world model that can provide informative states and
imagination training. However, a critical limitation in relevant research lies
in the scene-level reconstruction representation learning, which may overlook
key interactive vehicles and hardly model the interactive features among
vehicles and their long-term intentions. Therefore, this paper presents a novel
MBRL method with a predictive individual world model (PIWM) for autonomous
driving. PIWM describes the driving environment from an individual-level
perspective and captures vehicles' interactive relations and their intentions
via trajectory prediction task. Meanwhile, a behavior policy is learned jointly
with PIWM. It is trained in PIWM's imagination and effectively navigates in the
urban driving scenes leveraging intention-aware latent states. The proposed
method is trained and evaluated on simulation environments built upon
real-world challenging interactive scenarios. Compared with popular model-free
and state-of-the-art model-based reinforcement learning methods, experimental
results show that the proposed method achieves the best performance in terms of
safety and efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.16443v1' target='_blank'>Objects matter: object-centric world models improve reinforcement
  learning in visually complex environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weipu Zhang, Adam Jelley, Trevor McInroe, Amos Storkey</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-27 19:07:06</h6>
<p class='card-text'>Deep reinforcement learning has achieved remarkable success in learning
control policies from pixels across a wide range of tasks, yet its application
remains hindered by low sample efficiency, requiring significantly more
environment interactions than humans to reach comparable performance.
Model-based reinforcement learning (MBRL) offers a solution by leveraging
learnt world models to generate simulated experience, thereby improving sample
efficiency. However, in visually complex environments, small or dynamic
elements can be critical for decision-making. Yet, traditional MBRL methods in
pixel-based environments typically rely on auto-encoding with an $L_2$ loss,
which is dominated by large areas and often fails to capture decision-relevant
details. To address these limitations, we propose an object-centric MBRL
pipeline, which integrates recent advances in computer vision to allow agents
to focus on key decision-related elements. Our approach consists of four main
steps: (1) annotating key objects related to rewards and goals with
segmentation masks, (2) extracting object features using a pre-trained, frozen
foundation vision model, (3) incorporating these object features with the raw
observations to predict environmental dynamics, and (4) training the policy
using imagined trajectories generated by this object-centric world model.
Building on the efficient MBRL algorithm STORM, we call this pipeline OC-STORM.
We demonstrate OC-STORM's practical value in overcoming the limitations of
conventional MBRL approaches on both Atari games and the visually complex game
Hollow Knight.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.11949v1' target='_blank'>GLAM: Global-Local Variation Awareness in Mamba-based World Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qian He, Wenqi Liang, Chunhui Hao, Gan Sun, Jiandong Tian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-21 07:47:03</h6>
<p class='card-text'>Mimicking the real interaction trajectory in the inference of the world model
has been shown to improve the sample efficiency of model-based reinforcement
learning (MBRL) algorithms. Many methods directly use known state sequences for
reasoning. However, this approach fails to enhance the quality of reasoning by
capturing the subtle variation between states. Much like how humans infer
trends in event development from this variation, in this work, we introduce
Global-Local variation Awareness Mamba-based world model (GLAM) that improves
reasoning quality by perceiving and predicting variation between states. GLAM
comprises two Mambabased parallel reasoning modules, GMamba and LMamba, which
focus on perceiving variation from global and local perspectives, respectively,
during the reasoning process. GMamba focuses on identifying patterns of
variation between states in the input sequence and leverages these patterns to
enhance the prediction of future state variation. LMamba emphasizes reasoning
about unknown information, such as rewards, termination signals, and visual
representations, by perceiving variation in adjacent states. By integrating the
strengths of the two modules, GLAM accounts for highervalue variation in
environmental changes, providing the agent with more efficient
imagination-based training. We demonstrate that our method outperforms existing
methods in normalized human scores on the Atari 100k benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.09611v1' target='_blank'>EVaDE : Event-Based Variational Thompson Sampling for Model-Based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Siddharth Aravindan, Dixant Mittal, Wee Sun Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-16 15:35:48</h6>
<p class='card-text'>Posterior Sampling for Reinforcement Learning (PSRL) is a well-known
algorithm that augments model-based reinforcement learning (MBRL) algorithms
with Thompson sampling. PSRL maintains posterior distributions of the
environment transition dynamics and the reward function, which are intractable
for tasks with high-dimensional state and action spaces. Recent works show that
dropout, used in conjunction with neural networks, induces variational
distributions that can approximate these posteriors. In this paper, we propose
Event-based Variational Distributions for Exploration (EVaDE), which are
variational distributions that are useful for MBRL, especially when the
underlying domain is object-based. We leverage the general domain knowledge of
object-based domains to design three types of event-based convolutional layers
to direct exploration. These layers rely on Gaussian dropouts and are inserted
between the layers of the deep neural network model to help facilitate
variational Thompson sampling. We empirically show the effectiveness of
EVaDE-equipped Simulated Policy Learning (EVaDE-SimPLe) on the 100K Atari game
suite.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.06635v1' target='_blank'>A Reduced Order Iterative Linear Quadratic Regulator (ILQR) Technique
  for the Optimal Control of Nonlinear Partial Differential Equations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aayushman Sharma, Suman Chakravorty</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-11 20:53:33</h6>
<p class='card-text'>In this paper, we introduce a reduced order model-based reinforcement
learning (MBRL) approach, utilizing the Iterative Linear Quadratic Regulator
(ILQR) algorithm for the optimal control of nonlinear partial differential
equations (PDEs). The approach proposes a novel modification of the ILQR
technique: it uses the Method of Snapshots to identify a reduced order Linear
Time Varying (LTV) approximation of the nonlinear PDE dynamics around a current
estimate of the optimal trajectory, utilizes the identified LTV model to solve
a time-varying reduced order LQR problem to obtain an improved estimate of the
optimal trajectory along with a new reduced basis, and iterates till
convergence. The convergence behavior of the reduced order approach is analyzed
and the algorithm is shown to converge to a limit set that is dependent on the
truncation error in the reduction. The proposed approach is tested on the
viscous Burger's equation and two phase-field models for microstructure
evolution in materials, and the results show that there is a significant
reduction in the computational burden over the standard ILQR approach, without
significantly sacrificing performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.02774v1' target='_blank'>Learn A Flexible Exploration Model for Parameterized Action Markov
  Decision Processes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijian Wang, Bin Wang, Mingwen Shao, Hongbo Dou, Boxiang Tao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-06 05:33:09</h6>
<p class='card-text'>Hybrid action models are widely considered an effective approach to
reinforcement learning (RL) modeling. The current mainstream method is to train
agents under Parameterized Action Markov Decision Processes (PAMDPs), which
performs well in specific environments. Unfortunately, these models either
exhibit drastic low learning efficiency in complex PAMDPs or lose crucial
information in the conversion between raw space and latent space. To enhance
the learning efficiency and asymptotic performance of the agent, we propose a
model-based RL (MBRL) algorithm, FLEXplore. FLEXplore learns a
parameterized-action-conditioned dynamics model and employs a modified Model
Predictive Path Integral control. Unlike conventional MBRL algorithms, we
carefully design the dynamics loss function and reward smoothing process to
learn a loose yet flexible model. Additionally, we use the variational lower
bound to maximize the mutual information between the state and the hybrid
action, enhancing the exploration effectiveness of the agent. We theoretically
demonstrate that FLEXplore can reduce the regret of the rollout trajectory
through the Wasserstein Metric under given Lipschitz conditions. Our empirical
results on several standard benchmarks show that FLEXplore has outstanding
learning efficiency and asymptotic performance compared to other baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.05766v1' target='_blank'>Policy-shaped prediction: avoiding distractions in model-based
  reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Miles Hutson, Isaac Kauvar, Nick Haber</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-08 00:21:37</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) is a promising route to
sample-efficient policy optimization. However, a known vulnerability of
reconstruction-based MBRL consists of scenarios in which detailed aspects of
the world are highly predictable, but irrelevant to learning a good policy.
Such scenarios can lead the model to exhaust its capacity on meaningless
content, at the cost of neglecting important environment dynamics. While
existing approaches attempt to solve this problem, we highlight its continuing
impact on leading MBRL methods -- including DreamerV3 and DreamerPro -- with a
novel environment where background distractions are intricate, predictable, and
useless for planning future actions. To address this challenge we develop a
method for focusing the capacity of the world model through synergy of a
pretrained segmentation model, a task-aware reconstruction loss, and
adversarial learning. Our method outperforms a variety of other approaches
designed to reduce the impact of distractors, and is an advance towards robust
model-based reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.19639v1' target='_blank'>RMIO: A Model-Based MARL Framework for Scenarios with Observation Loss
  in Some Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zifeng Shi, Meiqin Liu, Senlin Zhang, Ronghao Zheng, Shanling Dong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-29 11:45:21</h6>
<p class='card-text'>In recent years, model-based reinforcement learning (MBRL) has emerged as a
solution to address sample complexity in multi-agent reinforcement learning
(MARL) by modeling agent-environment dynamics to improve sample efficiency.
However, most MBRL methods assume complete and continuous observations from
each agent during the inference stage, which can be overly idealistic in
practical applications. A novel model-based MARL approach called RMIO is
introduced to address this limitation, specifically designed for scenarios
where observation is lost in some agent. RMIO leverages the world model to
reconstruct missing observations, and further reduces reconstruction errors
through inter-agent information integration to ensure stable multi-agent
decision-making. Secondly, unlike CTCE methods such as MAMBA, RMIO adopts the
CTDE paradigm in standard environment, and enabling limited communication only
when agents lack observation data, thereby reducing reliance on communication.
Additionally, RMIO improves asymptotic performance through strategies such as
reward smoothing, a dual-layer experience replay buffer, and an RNN-augmented
policy model, surpassing previous work. Our experiments conducted in both the
SMAC and MaMuJoCo environments demonstrate that RMIO outperforms current
state-of-the-art approaches in terms of asymptotic convergence performance and
policy robustness, both in standard mission settings and in scenarios involving
observation loss.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.16019v1' target='_blank'>M3: Mamba-assisted Multi-Circuit Optimization via MBRL with Effective
  Scheduling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Youngmin Oh, Jinje Park, Seunggeun Kim, Taejin Paik, David Pan, Bosun Hwang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-25 00:30:49</h6>
<p class='card-text'>Recent advancements in reinforcement learning (RL) for analog circuit
optimization have demonstrated significant potential for improving sample
efficiency and generalization across diverse circuit topologies and target
specifications. However, there are challenges such as high computational
overhead, the need for bespoke models for each circuit. To address them, we
propose M3, a novel Model-based RL (MBRL) method employing the Mamba
architecture and effective scheduling. The Mamba architecture, known as a
strong alternative to the transformer architecture, enables multi-circuit
optimization with distinct parameters and target specifications. The effective
scheduling strategy enhances sample efficiency by adjusting crucial MBRL
training parameters. To the best of our knowledge, M3 is the first method for
multi-circuit optimization by leveraging both the Mamba architecture and a MBRL
with effective scheduling. As a result, it significantly improves sample
efficiency compared to existing RL methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.10175v2' target='_blank'>The Surprising Ineffectiveness of Pre-Trained Visual Representations for
  Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Moritz Schneider, Robert Krug, Narunas Vaskevicius, Luigi Palmieri, Joschka Boedecker</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-15 13:21:26</h6>
<p class='card-text'>Visual Reinforcement Learning (RL) methods often require extensive amounts of
data. As opposed to model-free RL, model-based RL (MBRL) offers a potential
solution with efficient data utilization through planning. Additionally, RL
lacks generalization capabilities for real-world tasks. Prior work has shown
that incorporating pre-trained visual representations (PVRs) enhances sample
efficiency and generalization. While PVRs have been extensively studied in the
context of model-free RL, their potential in MBRL remains largely unexplored.
In this paper, we benchmark a set of PVRs on challenging control tasks in a
model-based RL setting. We investigate the data efficiency, generalization
capabilities, and the impact of different properties of PVRs on the performance
of model-based agents. Our results, perhaps surprisingly, reveal that for MBRL
current PVRs are not more sample efficient than learning representations from
scratch, and that they do not generalize better to out-of-distribution (OOD)
settings. To explain this, we analyze the quality of the trained dynamics
model. Furthermore, we show that data diversity and network architecture are
the most important contributors to OOD generalization performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.11234v1' target='_blank'>Bayes Adaptive Monte Carlo Tree Search for Offline Model-based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiayu Chen, Wentse Chen, Jeff Schneider</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-15 03:36:43</h6>
<p class='card-text'>Offline reinforcement learning (RL) is a powerful approach for data-driven
decision-making and control. Compared to model-free methods, offline
model-based reinforcement learning (MBRL) explicitly learns world models from a
static dataset and uses them as surrogate simulators, improving the data
efficiency and enabling the learned policy to potentially generalize beyond the
dataset support. However, there could be various MDPs that behave identically
on the offline dataset and so dealing with the uncertainty about the true MDP
can be challenging. In this paper, we propose modeling offline MBRL as a Bayes
Adaptive Markov Decision Process (BAMDP), which is a principled framework for
addressing model uncertainty. We further introduce a novel Bayes Adaptive
Monte-Carlo planning algorithm capable of solving BAMDPs in continuous state
and action spaces with stochastic transitions. This planning process is based
on Monte Carlo Tree Search and can be integrated into offline MBRL as a policy
improvement operator in policy iteration. Our ``RL + Search" framework follows
in the footsteps of superhuman AIs like AlphaZero, improving on current offline
MBRL methods by incorporating more computation input. The proposed algorithm
significantly outperforms state-of-the-art model-based and model-free offline
RL methods on twelve D4RL MuJoCo benchmark tasks and three target tracking
tasks in a challenging, stochastic tokamak control simulator.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09972v1' target='_blank'>Make the Pertinent Salient: Task-Relevant Reconstruction for Visual
  Control with Distractions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kyungmin Kim, JB Lanier, Pierre Baldi, Charless Fowlkes, Roy Fox</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-13 19:24:07</h6>
<p class='card-text'>Recent advancements in Model-Based Reinforcement Learning (MBRL) have made it
a powerful tool for visual control tasks. Despite improved data efficiency, it
remains challenging to train MBRL agents with generalizable perception.
Training in the presence of visual distractions is particularly difficult due
to the high variation they introduce to representation learning. Building on
DREAMER, a popular MBRL method, we propose a simple yet effective auxiliary
task to facilitate representation learning in distracting environments. Under
the assumption that task-relevant components of image observations are
straightforward to identify with prior knowledge in a given task, we use a
segmentation mask on image observations to only reconstruct task-relevant
components. In doing so, we greatly reduce the complexity of representation
learning by removing the need to encode task-irrelevant objects in the latent
representation. Our method, Segmentation Dreamer (SD), can be used either with
ground-truth masks easily accessible in simulation or by leveraging potentially
imperfect segmentation foundation models. The latter is further improved by
selectively applying the reconstruction loss to avoid providing misleading
learning signals due to mask prediction errors. In modified DeepMind Control
suite (DMC) and Meta-World tasks with added visual distractions, SD achieves
significantly better sample efficiency and greater final performance than prior
work. We find that SD is especially helpful in sparse reward tasks otherwise
unsolvable by prior work, enabling the training of visually robust agents
without the need for extensive reward engineering.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09163v2' target='_blank'>Learning to Walk from Three Minutes of Real-World Data with
  Semi-structured Dynamics Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jacob Levy, Tyler Westenbroek, David Fridovich-Keil</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-11 18:11:21</h6>
<p class='card-text'>Traditionally, model-based reinforcement learning (MBRL) methods exploit
neural networks as flexible function approximators to represent $\textit{a
priori}$ unknown environment dynamics. However, training data are typically
scarce in practice, and these black-box models often fail to generalize.
Modeling architectures that leverage known physics can substantially reduce the
complexity of system-identification, but break down in the face of complex
phenomena such as contact. We introduce a novel framework for learning
semi-structured dynamics models for contact-rich systems which seamlessly
integrates structured first principles modeling techniques with black-box
auto-regressive models. Specifically, we develop an ensemble of probabilistic
models to estimate external forces, conditioned on historical observations and
actions, and integrate these predictions using known Lagrangian dynamics. With
this semi-structured approach, we can make accurate long-horizon predictions
with substantially less data than prior methods. We leverage this capability
and propose Semi-Structured Reinforcement Learning ($\texttt{SSRL}$) a simple
model-based learning framework which pushes the sample complexity boundary for
real-world learning. We validate our approach on a real-world Unitree Go1
quadruped robot, learning dynamic gaits -- from scratch -- on both hard and
soft surfaces with just a few minutes of real-world data. Video and code are
available at: https://sites.google.com/utexas.edu/ssrl</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.14685v1' target='_blank'>Model-Based Reinforcement Learning for Control of Strongly-Disturbed
  Unsteady Aerodynamic Flows</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhecheng Liu, Diederik Beckers, Jeff D. Eldredge</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-26 23:21:44</h6>
<p class='card-text'>The intrinsic high dimension of fluid dynamics is an inherent challenge to
control of aerodynamic flows, and this is further complicated by a flow's
nonlinear response to strong disturbances. Deep reinforcement learning, which
takes advantage of the exploratory aspects of reinforcement learning (RL) and
the rich nonlinearity of a deep neural network, provides a promising approach
to discover feasible control strategies. However, the typical model-free
approach to reinforcement learning requires a significant amount of interaction
between the flow environment and the RL agent during training, and this high
training cost impedes its development and application. In this work, we propose
a model-based reinforcement learning (MBRL) approach by incorporating a novel
reduced-order model as a surrogate for the full environment. The model consists
of a physics-augmented autoencoder, which compresses high-dimensional CFD flow
field snaphsots into a three-dimensional latent space, and a latent dynamics
model that is trained to accurately predict the long-time dynamics of
trajectories in the latent space in response to action sequences. The
robustness and generalizability of the model is demonstrated in two distinct
flow environments, a pitching airfoil in a highly disturbed environment and a
vertical-axis wind turbine in a disturbance-free environment. Based on the
trained model in the first problem, we realize an MBRL strategy to mitigate
lift variation during gust-airfoil encounters. We demonstrate that the policy
learned in the reduced-order environment translates to an effective control
strategy in the full CFD environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.14232v1' target='_blank'>Efficient Active Flow Control Strategy for Confined Square Cylinder Wake
  Using Deep Learning-Based Surrogate Model and Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Meng Zhang, Mustafa Z. Yousif, Minze Xu, Haifeng Zhou, Linqi Yu, HeeChang Lim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-26 12:48:10</h6>
<p class='card-text'>This study presents a deep learning model-based reinforcement learning
(DL-MBRL) approach for active control of two-dimensional (2D) wake flow past a
square cylinder using antiphase jets. The DL-MBRL framework alternates between
interacting with a deep learning surrogate model (DL-SM) and computational
fluid dynamics (CFD) simulations to suppress wake vortex shedding,
significantly reducing computational costs. The DL-SM, which combines a
Transformer and a multiscale enhanced super-resolution generative adversarial
network (MS-ESRGAN), effectively models complex flow dynamics, efficiently
emulating the CFD environment. Trained on 2D direct numerical simulation (DNS)
data, the Transformer and MS-ESRGAN demonstrated excellent agreement with DNS
results, validating the DL-SM's accuracy. Error analysis suggests replacing the
DL-SM with CFD every five interactions to maintain reliability. While DL-MBRL
showed less robust convergence than model-free reinforcement learning (MFRL)
during training, it reduced training time by 49.2%, from 41.87 hours to 20.62
hours. Both MFRL and DL-MBRL achieved a 98% reduction in shedding energy and a
95% reduction in the standard deviation of the lift coefficient (C_L). However,
MFRL exhibited a nonzero mean lift coefficient due to insufficient exploration,
whereas DL-MBRL improved exploration by leveraging the randomness of the DL-SM,
resolving the nonzero mean C_L issue. This study demonstrates that DL-MBRL is
not only comparably effective but also superior to MFRL in flow stabilization,
with significantly reduced training time, highlighting the potential of
combining deep reinforcement learning with DL-SM for enhanced active flow
control.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.10713v1' target='_blank'>Offline Model-Based Reinforcement Learning with Anti-Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Padmanaba Srinivasan, William Knottenbelt</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-20 10:29:21</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) algorithms learn a dynamics model
from collected data and apply it to generate synthetic trajectories to enable
faster learning. This is an especially promising paradigm in offline
reinforcement learning (RL) where data may be limited in quantity, in addition
to being deficient in coverage and quality. Practical approaches to offline
MBRL usually rely on ensembles of dynamics models to prevent exploitation of
any individual model and to extract uncertainty estimates that penalize values
in states far from the dataset support. Uncertainty estimates from ensembles
can vary greatly in scale, making it challenging to generalize hyperparameters
well across even similar tasks. In this paper, we present Morse Model-based
offline RL (MoMo), which extends the anti-exploration paradigm found in offline
model-free RL to the model-based space. We develop model-free and model-based
variants of MoMo and show how the model-free version can be extended to detect
and deal with out-of-distribution (OOD) states using explicit uncertainty
estimation without the need for large ensembles. MoMo performs offline MBRL
using an anti-exploration bonus to counteract value overestimation in
combination with a policy constraint, as well as a truncation function to
terminate synthetic rollouts that are excessively OOD. Experimentally, we find
that both model-free and model-based MoMo perform well, and the latter
outperforms prior model-based and model-free baselines on the majority of D4RL
datasets tested.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.09807v3' target='_blank'>Reset-free Reinforcement Learning with World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhao Yang, Thomas M. Moerland, Mike Preuss, Aske Plaat, Edward S. Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-19 08:56:00</h6>
<p class='card-text'>Reinforcement learning (RL) is an appealing paradigm for training intelligent
agents, enabling policy acquisition from the agent's own autonomously acquired
experience. However, the training process of RL is far from automatic,
requiring extensive human effort to reset the agent and environments. To tackle
the challenging reset-free setting, we first demonstrate the superiority of
model-based (MB) RL methods in such setting, showing that a straightforward
adaptation of MBRL can outperform all the prior state-of-the-art methods while
requiring less supervision. We then identify limitations inherent to this
direct extension and propose a solution called model-based reset-free
(MoReFree) agent, which further enhances the performance. MoReFree adapts two
key mechanisms, exploration and policy learning, to handle reset-free tasks by
prioritizing task-relevant states. It exhibits superior data-efficiency across
various reset-free tasks without access to environmental reward or
demonstrations while significantly outperforming privileged baselines that
require supervision. Our findings suggest model-based methods hold significant
promise for reducing human effort in RL. Website:
https://yangzhao-666.github.io/morefree</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.12195v2' target='_blank'>A Safe and Data-efficient Model-based Reinforcement Learning System for
  HVAC Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xianzhong Ding, Zhiyu An, Arya Rathee, Wan Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-16 21:43:09</h6>
<p class='card-text'>Model-Based Reinforcement Learning (MBRL) has been widely studied for
Heating, Ventilation, and Air Conditioning (HVAC) control in buildings. One of
the critical challenges is the large amount of data required to effectively
train neural networks for modeling building dynamics. This paper presents CLUE,
an MBRL system for HVAC control in buildings. CLUE optimizes HVAC operations by
integrating a Gaussian Process (GP) model to model building dynamics with
uncertainty awareness. CLUE utilizes GP to predict state transitions as
Gaussian distributions, effectively capturing prediction uncertainty and
enhancing decision-making under sparse data conditions. Our approach employs a
meta-kernel learning technique to efficiently set GP kernel hyperparameters
using domain knowledge from diverse buildings. This drastically reduces the
data requirements typically associated with GP models in HVAC applications.
Additionally, CLUE incorporates these uncertainty estimates into a Model
Predictive Path Integral (MPPI) algorithm, enabling the selection of safe,
energy-efficient control actions. This uncertainty-aware control strategy
evaluates and selects action trajectories based on their predicted impact on
energy consumption and human comfort, optimizing operations even under
uncertain conditions. Extensive simulations in a five-zone office building
demonstrate that CLUE reduces the required training data from hundreds of days
to just seven while maintaining robust control performance. It reduces comfort
violations by an average of 12.07% compared to existing MBRL methods, without
compromising on energy efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.10967v1' target='_blank'>BECAUSE: Bilinear Causal Representation for Generalizable Offline
  Model-based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haohong Lin, Wenhao Ding, Jian Chen, Laixi Shi, Jiacheng Zhu, Bo Li, Ding Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-15 17:59:23</h6>
<p class='card-text'>Offline model-based reinforcement learning (MBRL) enhances data efficiency by
utilizing pre-collected datasets to learn models and policies, especially in
scenarios where exploration is costly or infeasible. Nevertheless, its
performance often suffers from the objective mismatch between model and policy
learning, resulting in inferior performance despite accurate model predictions.
This paper first identifies the primary source of this mismatch comes from the
underlying confounders present in offline data for MBRL. Subsequently, we
introduce \textbf{B}ilin\textbf{E}ar \textbf{CAUS}al
r\textbf{E}presentation~(BECAUSE), an algorithm to capture causal
representation for both states and actions to reduce the influence of the
distribution shift, thus mitigating the objective mismatch problem.
Comprehensive evaluations on 18 tasks that vary in data quality and environment
context demonstrate the superior performance of BECAUSE over existing offline
RL algorithms. We show the generalizability and robustness of BECAUSE under
fewer samples or larger numbers of confounders. Additionally, we offer
theoretical analysis of BECAUSE to prove its error bound and sample efficiency
when integrating causal representation into offline MBRL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.09249v2' target='_blank'>Graph Neural Networks with Model-based Reinforcement Learning for
  Multi-agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanxiao Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-12 13:21:35</h6>
<p class='card-text'>Multi-agent systems (MAS) constitute a significant role in exploring machine
intelligence and advanced applications. In order to deeply investigate
complicated interactions within MAS scenarios, we originally propose "GNN for
MBRL" model, which utilizes a state-spaced Graph Neural Networks with
Model-based Reinforcement Learning to address specific MAS missions (e.g.,
Billiard-Avoidance, Autonomous Driving Cars). In detail, we firstly used GNN
model to predict future states and trajectories of multiple agents, then
applied the Cross-Entropy Method (CEM) optimized Model Predictive Control to
assist the ego-agent planning actions and successfully accomplish certain MAS
tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.07069v1' target='_blank'>Optimal Gait Control for a Tendon-driven Soft Quadruped Robot by
  Model-based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuezhi Niu, Kaige Tan, Lei Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-11 08:56:08</h6>
<p class='card-text'>This study presents an innovative approach to optimal gait control for a soft
quadruped robot enabled by four Compressible Tendon-driven Soft Actuators
(CTSAs). Improving our previous studies of using model-free reinforcement
learning for gait control, we employ model-based reinforcement learning (MBRL)
to further enhance the performance of the gait controller. Compared to rigid
robots, the proposed soft quadruped robot has better safety, less weight, and a
simpler mechanism for fabrication and control. However, the primary challenge
lies in developing sophisticated control algorithms to attain optimal gait
control for fast and stable locomotion. The research employs a multi-stage
methodology, including state space restriction, data-driven model training, and
reinforcement learning algorithm development. Compared to benchmark methods,
the proposed MBRL algorithm, combined with post-training, significantly
improves the efficiency and performance of gait control policies. The developed
policy is both robust and adaptable to the robot's deformable morphology. The
study concludes by highlighting the practical applicability of these findings
in real-world scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.06714v2' target='_blank'>Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach
  For Adaptive Brain Stimulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michelle Pan, Mariah Schrum, Vivek Myers, Erdem Bƒ±yƒ±k, Anca Dragan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-10 18:23:03</h6>
<p class='card-text'>Adaptive brain stimulation can treat neurological conditions such as
Parkinson's disease and post-stroke motor deficits by influencing abnormal
neural activity. Because of patient heterogeneity, each patient requires a
unique stimulation policy to achieve optimal neural responses. Model-free
reinforcement learning (MFRL) holds promise in learning effective policies for
a variety of similar control tasks, but is limited in domains like brain
stimulation by a need for numerous costly environment interactions. In this
work we introduce Coprocessor Actor Critic, a novel, model-based reinforcement
learning (MBRL) approach for learning neural coprocessor policies for brain
stimulation. Our key insight is that coprocessor policy learning is a
combination of learning how to act optimally in the world and learning how to
induce optimal actions in the world through stimulation of an injured brain. We
show that our approach overcomes the limitations of traditional MFRL methods in
terms of sample efficiency and task success and outperforms baseline MBRL
approaches in a neurologically realistic model of an injured brain.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.02616v5' target='_blank'>Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A
  Model-Based Reinforcement Learning Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxuan Chen, Rongpeng Li, Xiaoxue Yu, Zhifeng Zhao, Honggang Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-03 09:41:42</h6>
<p class='card-text'>Optimizing the deployment of large language models (LLMs) in edge computing
environments is critical for enhancing privacy and computational efficiency.
Toward efficient wireless LLM inference in edge computing, this study
comprehensively analyzes the impact of different splitting points in mainstream
open-source LLMs. On this basis, this study introduces a framework taking
inspiration from model-based reinforcement learning (MBRL) to determine the
optimal splitting point across the edge and user equipment (UE). By
incorporating a reward surrogate model, our approach significantly reduces the
computational cost of frequent performance evaluations. Extensive simulations
demonstrate that this method effectively balances inference performance and
computational load under varying network conditions, providing a robust
solution for LLM deployment in decentralized settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.00483v1' target='_blank'>Exploring the limits of Hierarchical World Models in Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Robin Schiewer, Anand Subramoney, Laurenz Wiskott</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-01 16:29:03</h6>
<p class='card-text'>Hierarchical model-based reinforcement learning (HMBRL) aims to combine the
benefits of better sample efficiency of model based reinforcement learning
(MBRL) with the abstraction capability of hierarchical reinforcement learning
(HRL) to solve complex tasks efficiently. While HMBRL has great potential, it
still lacks wide adoption. In this work we describe a novel HMBRL framework and
evaluate it thoroughly. To complement the multi-layered decision making idiom
characteristic for HRL, we construct hierarchical world models that simulate
environment dynamics at various levels of temporal abstraction. These models
are used to train a stack of agents that communicate in a top-down manner by
proposing goals to their subordinate agents. A significant focus of this study
is the exploration of a static and environment agnostic temporal abstraction,
which allows concurrent training of models and agents throughout the hierarchy.
Unlike most goal-conditioned H(MB)RL approaches, it also leads to comparatively
low dimensional abstract actions. Although our HMBRL approach did not
outperform traditional methods in terms of final episode returns, it
successfully facilitated decision making across two levels of abstraction using
compact, low dimensional abstract actions. A central challenge in enhancing our
method's performance, as uncovered through comprehensive experimentation, is
model exploitation on the abstract level of our world model stack. We provide
an in depth examination of this issue, discussing its implications for the
field and suggesting directions for future research to overcome this challenge.
By sharing these findings, we aim to contribute to the broader discourse on
refining HMBRL methodologies and to assist in the development of more effective
autonomous learning systems for complex decision-making environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.19014v3' target='_blank'>Trust the Model Where It Trusts Itself -- Model-Based Actor-Critic with
  Uncertainty-Aware Rollout Adaption</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bernd Frauenknecht, Artur Eisele, Devdutt Subhasish, Friedrich Solowjow, Sebastian Trimpe</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-29 11:53:07</h6>
<p class='card-text'>Dyna-style model-based reinforcement learning (MBRL) combines model-free
agents with predictive transition models through model-based rollouts. This
combination raises a critical question: 'When to trust your model?'; i.e.,
which rollout length results in the model providing useful data? Janner et al.
(2019) address this question by gradually increasing rollout lengths throughout
the training. While theoretically tempting, uniform model accuracy is a fallacy
that collapses at the latest when extrapolating. Instead, we propose asking the
question 'Where to trust your model?'. Using inherent model uncertainty to
consider local accuracy, we obtain the Model-Based Actor-Critic with
Uncertainty-Aware Rollout Adaption (MACURA) algorithm. We propose an
easy-to-tune rollout mechanism and demonstrate substantial improvements in data
efficiency and performance compared to state-of-the-art deep MBRL methods on
the MuJoCo benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.17784v2' target='_blank'>Adaptive Horizon Actor-Critic for Policy Learning in Contact-Rich
  Differentiable Simulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ignat Georgiev, Krishnan Srinivasan, Jie Xu, Eric Heiden, Animesh Garg</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-28 03:28:00</h6>
<p class='card-text'>Model-Free Reinforcement Learning (MFRL), leveraging the policy gradient
theorem, has demonstrated considerable success in continuous control tasks.
However, these approaches are plagued by high gradient variance due to
zeroth-order gradient estimation, resulting in suboptimal policies. Conversely,
First-Order Model-Based Reinforcement Learning (FO-MBRL) methods employing
differentiable simulation provide gradients with reduced variance but are
susceptible to sampling error in scenarios involving stiff dynamics, such as
physical contact. This paper investigates the source of this error and
introduces Adaptive Horizon Actor-Critic (AHAC), an FO-MBRL algorithm that
reduces gradient error by adapting the model-based horizon to avoid stiff
dynamics. Empirical findings reveal that AHAC outperforms MFRL baselines,
attaining 40% more reward across a set of locomotion tasks and efficiently
scaling to high-dimensional control environments with improved wall-clock-time
efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.16184v1' target='_blank'>Safe Deep Model-Based Reinforcement Learning with Lyapunov Functions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Harry Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-25 11:21:12</h6>
<p class='card-text'>Model-based Reinforcement Learning (MBRL) has shown many desirable properties
for intelligent control tasks. However, satisfying safety and stability
constraints during training and rollout remains an open question. We propose a
new Model-based RL framework to enable efficient policy learning with unknown
dynamics based on learning model predictive control (LMPC) framework with
mathematically provable guarantees of stability. We introduce and explore a
novel method for adding safety constraints for model-based RL during training
and policy learning. The new stability-augmented framework consists of a
neural-network-based learner that learns to construct a Lyapunov function, and
a model-based RL agent to consistently complete the tasks while satisfying
user-specified constraints given only sub-optimal demonstrations and
sparse-cost feedback. We demonstrate the capability of the proposed framework
through simulated experiments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.15616v1' target='_blank'>Neuromorphic dreaming: A pathway to efficient learning in artificial
  agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ingo Blakowski, Dmitrii Zendrikov, Cristiano Capone, Giacomo Indiveri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-24 15:03:56</h6>
<p class='card-text'>Achieving energy efficiency in learning is a key challenge for artificial
intelligence (AI) computing platforms. Biological systems demonstrate
remarkable abilities to learn complex skills quickly and efficiently. Inspired
by this, we present a hardware implementation of model-based reinforcement
learning (MBRL) using spiking neural networks (SNNs) on mixed-signal
analog/digital neuromorphic hardware. This approach leverages the energy
efficiency of mixed-signal neuromorphic chips while achieving high sample
efficiency through an alternation of online learning, referred to as the
"awake" phase, and offline learning, known as the "dreaming" phase. The model
proposed includes two symbiotic networks: an agent network that learns by
combining real and simulated experiences, and a learned world model network
that generates the simulated experiences. We validate the model by training the
hardware implementation to play the Atari game Pong. We start from a baseline
consisting of an agent network learning without a world model and dreaming,
which successfully learns to play the game. By incorporating dreaming, the
number of required real game experiences are reduced significantly compared to
the baseline. The networks are implemented using a mixed-signal neuromorphic
processor, with the readout layers trained using a computer in-the-loop, while
the other layers remain fixed. These results pave the way toward
energy-efficient neuromorphic learning systems capable of rapid learning in
real world applications and use-cases.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.11778v1' target='_blank'>Efficient Multi-agent Reinforcement Learning by Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qihan Liu, Jianing Ye, Xiaoteng Ma, Jun Yang, Bin Liang, Chongjie Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-20 04:36:02</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) algorithms have accomplished
remarkable breakthroughs in solving large-scale decision-making tasks.
Nonetheless, most existing MARL algorithms are model-free, limiting sample
efficiency and hindering their applicability in more challenging scenarios. In
contrast, model-based reinforcement learning (MBRL), particularly algorithms
integrating planning, such as MuZero, has demonstrated superhuman performance
with limited data in many tasks. Hence, we aim to boost the sample efficiency
of MARL by adopting model-based approaches. However, incorporating planning and
search methods into multi-agent systems poses significant challenges. The
expansive action space of multi-agent systems often necessitates leveraging the
nearly-independent property of agents to accelerate learning. To tackle this
issue, we propose the MAZero algorithm, which combines a centralized model with
Monte Carlo Tree Search (MCTS) for policy search. We design a novel network
structure to facilitate distributed execution and parameter sharing. To enhance
search efficiency in deterministic environments with sizable action spaces, we
introduce two novel techniques: Optimistic Search Lambda (OS($\lambda$)) and
Advantage-Weighted Policy Optimization (AWPO). Extensive experiments on the
SMAC benchmark demonstrate that MAZero outperforms model-free approaches in
terms of sample efficiency and provides comparable or better performance than
existing model-based methods in terms of both sample and computational
efficiency. Our code is available at https://github.com/liuqh16/MAZero.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.06263v2' target='_blank'>Learning Latent Dynamic Robust Representations for World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruixiang Sun, Hongyu Zang, Xin Li, Riashat Islam</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-10 06:28:42</h6>
<p class='card-text'>Visual Model-Based Reinforcement Learning (MBRL) promises to encapsulate
agent's knowledge about the underlying dynamics of the environment, enabling
learning a world model as a useful planner. However, top MBRL agents such as
Dreamer often struggle with visual pixel-based inputs in the presence of
exogenous or irrelevant noise in the observation space, due to failure to
capture task-specific features while filtering out irrelevant spatio-temporal
details. To tackle this problem, we apply a spatio-temporal masking strategy, a
bisimulation principle, combined with latent reconstruction, to capture
endogenous task-specific aspects of the environment for world models,
effectively eliminating non-essential information. Joint training of
representations, dynamics, and policy often leads to instabilities. To further
address this issue, we develop a Hybrid Recurrent State-Space Model (HRSSM)
structure, enhancing state representation robustness for effective policy
learning. Our empirical evaluation demonstrates significant performance
improvements over existing methods in a range of visually complex control tasks
such as Maniskill \cite{gu2023maniskill2} with exogenous distractors from the
Matterport environment. Our code is avaliable at
https://github.com/bit1029public/HRSSM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.15908v1' target='_blank'>Deep Gaussian Covariance Network with Trajectory Sampling for
  Data-Efficient Policy Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Can Bogoclu, Robert Vosshall, Kevin Cremanns, Dirk Roos</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-23 18:42:22</h6>
<p class='card-text'>Probabilistic world models increase data efficiency of model-based
reinforcement learning (MBRL) by guiding the policy with their epistemic
uncertainty to improve exploration and acquire new samples. Moreover, the
uncertainty-aware learning procedures in probabilistic approaches lead to
robust policies that are less sensitive to noisy observations compared to
uncertainty unaware solutions. We propose to combine trajectory sampling and
deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL
problems in an optimal control setting. We compare trajectory sampling with
density-based approximation for uncertainty propagation using three different
probabilistic world models; Gaussian processes, Bayesian neural networks, and
DGCNs. We provide empirical evidence using four different well-known test
environments, that our method improves the sample-efficiency over other
combinations of uncertainty propagation methods and probabilistic models.
During our tests, we place particular emphasis on the robustness of the learned
policies with respect to noisy initial states.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.14860v1' target='_blank'>Robust Model Based Reinforcement Learning Using $\mathcal{L}_1$ Adaptive
  Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Minjun Sung, Sambhu H. Karumanchi, Aditya Gahlawat, Naira Hovakimyan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-21 22:15:09</h6>
<p class='card-text'>We introduce $\mathcal{L}_1$-MBRL, a control-theoretic augmentation scheme
for Model-Based Reinforcement Learning (MBRL) algorithms. Unlike model-free
approaches, MBRL algorithms learn a model of the transition function using data
and use it to design a control input. Our approach generates a series of
approximate control-affine models of the learned transition function according
to the proposed switching law. Using the approximate model, control input
produced by the underlying MBRL is perturbed by the $\mathcal{L}_1$ adaptive
control, which is designed to enhance the robustness of the system against
uncertainties. Importantly, this approach is agnostic to the choice of MBRL
algorithm, enabling the use of the scheme with various MBRL algorithms. MBRL
algorithms with $\mathcal{L}_1$ augmentation exhibit enhanced performance and
sample efficiency across multiple MuJoCo environments, outperforming the
original MBRL algorithms, both with and without system noise.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.04253v1' target='_blank'>Mastering Memory Tasks with World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, Sarath Chandar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-07 06:35:59</h6>
<p class='card-text'>Current model-based reinforcement learning (MBRL) agents struggle with
long-term dependencies. This limits their ability to effectively solve tasks
involving extended time gaps between actions and outcomes, or tasks demanding
the recalling of distant observations to inform current actions. To improve
temporal coherence, we integrate a new family of state space models (SSMs) in
world models of MBRL agents to present a new method, Recall to Imagine (R2I).
This integration aims to enhance both long-term memory and long-horizon credit
assignment. Through a diverse set of illustrative tasks, we systematically
demonstrate that R2I not only establishes a new state-of-the-art for
challenging memory and credit assignment RL tasks, such as BSuite and POPGym,
but also showcases superhuman performance in the complex memory domain of
Memory Maze. At the same time, it upholds comparable performance in classic RL
tasks, such as Atari and DMC, suggesting the generality of our method. We also
show that R2I is faster than the state-of-the-art MBRL method, DreamerV3,
resulting in faster wall-time convergence.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.00172v1' target='_blank'>Go Beyond Black-box Policies: Rethinking the Design of Learning Agent
  for Interpretable and Verifiable HVAC Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiyu An, Xianzhong Ding, Wan Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-29 22:42:23</h6>
<p class='card-text'>Recent research has shown the potential of Model-based Reinforcement Learning
(MBRL) to enhance energy efficiency of Heating, Ventilation, and Air
Conditioning (HVAC) systems. However, existing methods rely on black-box
thermal dynamics models and stochastic optimizers, lacking reliability
guarantees and posing risks to occupant health. In this work, we overcome the
reliability bottleneck by redesigning HVAC controllers using decision trees
extracted from existing thermal dynamics models and historical data. Our
decision tree-based policies are deterministic, verifiable, interpretable, and
more energy-efficient than current MBRL methods. First, we introduce a novel
verification criterion for RL agents in HVAC control based on domain knowledge.
Second, we develop a policy extraction procedure that produces a verifiable
decision tree policy. We found that the high dimensionality of the thermal
dynamics model input hinders the efficiency of policy extraction. To tackle the
dimensionality challenge, we leverage importance sampling conditioned on
historical data distributions, significantly improving policy extraction
efficiency. Lastly, we present an offline verification algorithm that
guarantees the reliability of a control policy. Extensive experiments show that
our method saves 68.4% more energy and increases human comfort gain by 14.8%
compared to the state-of-the-art method, in addition to an 1127x reduction in
computation overhead. Our code and data are available at
https://github.com/ryeii/Veri_HVAC</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.18866v2' target='_blank'>Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hany Hamed, Subin Kim, Dongyeong Kim, Jaesik Yoon, Sungjin Ahn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-29 05:34:05</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) has been a primary approach to
ameliorating the sample efficiency issue as well as to make a generalist agent.
However, there has not been much effort toward enhancing the strategy of
dreaming itself. Therefore, it is a question whether and how an agent can
"dream better" in a more structured and strategic way. In this paper, inspired
by the observation from cognitive science suggesting that humans use a spatial
divide-and-conquer strategy in planning, we propose a new MBRL agent, called
Dr. Strategy, which is equipped with a novel Dreaming Strategy. The proposed
agent realizes a version of divide-and-conquer-like strategy in dreaming. This
is achieved by learning a set of latent landmarks and then utilizing these to
learn a landmark-conditioned highway policy. With the highway policy, the agent
can first learn in the dream to move to a landmark, and from there it tackles
the exploration and achievement task in a more focused way. In experiments, we
show that the proposed model outperforms prior pixel-based MBRL methods in
various visually complex and partially observable navigation tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.13034v4' target='_blank'>Locality Sensitive Sparse Encoding for Learning World Models Online</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zichen Liu, Chao Du, Wee Sun Lee, Min Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-23 19:00:02</h6>
<p class='card-text'>Acquiring an accurate world model online for model-based reinforcement
learning (MBRL) is challenging due to data nonstationarity, which typically
causes catastrophic forgetting for neural networks (NNs). From the online
learning perspective, a Follow-The-Leader (FTL) world model is desirable, which
optimally fits all previous experiences at each round. Unfortunately, NN-based
models need re-training on all accumulated data at every interaction step to
achieve FTL, which is computationally expensive for lifelong agents. In this
paper, we revisit models that can achieve FTL with incremental updates.
Specifically, our world model is a linear regression model supported by
nonlinear random features. The linear part ensures efficient FTL update while
the nonlinear random feature empowers the fitting of complex environments. To
best trade off model capacity and computation efficiency, we introduce a
locality sensitive sparse encoding, which allows us to conduct efficient sparse
updates even with very high dimensional nonlinear features. We validate the
representation power of our encoding and verify that it allows efficient online
learning under data covariate shift. We also show, in the Dyna MBRL setting,
that our world models learned online using a single pass of trajectory data
either surpass or match the performance of deep world models trained with
replay and other continual learning methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.00242v1' target='_blank'>Laboratory Experiments of Model-based Reinforcement Learning for
  Adaptive Optics Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jalo Nousiainen, Byron Engler, Markus Kasper, Chang Rajani, Tapio Helin, C√©dric T. Heritier, Sascha P. Quanz, Adrian M. Glauser</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-30 14:11:43</h6>
<p class='card-text'>Direct imaging of Earth-like exoplanets is one of the most prominent
scientific drivers of the next generation of ground-based telescopes.
Typically, Earth-like exoplanets are located at small angular separations from
their host stars, making their detection difficult. Consequently, the adaptive
optics (AO) system's control algorithm must be carefully designed to
distinguish the exoplanet from the residual light produced by the host star.
  A new promising avenue of research to improve AO control builds on
data-driven control methods such as Reinforcement Learning (RL). RL is an
active branch of the machine learning research field, where control of a system
is learned through interaction with the environment. Thus, RL can be seen as an
automated approach to AO control, where its usage is entirely a turnkey
operation. In particular, model-based reinforcement learning (MBRL) has been
shown to cope with both temporal and misregistration errors. Similarly, it has
been demonstrated to adapt to non-linear wavefront sensing while being
efficient in training and execution.
  In this work, we implement and adapt an RL method called Policy Optimization
for AO (PO4AO) to the GHOST test bench at ESO headquarters, where we
demonstrate a strong performance of the method in a laboratory environment. Our
implementation allows the training to be performed parallel to inference, which
is crucial for on-sky operation. In particular, we study the predictive and
self-calibrating aspects of the method. The new implementation on GHOST running
PyTorch introduces only around 700 microseconds in addition to hardware,
pipeline, and Python interface latency. We open-source well-documented code for
the implementation and specify the requirements for the RTC pipeline. We also
discuss the important hyperparameters of the method, the source of the latency,
and the possible paths for a lower latency implementation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.13910v3' target='_blank'>Multi-Agent Probabilistic Ensembles with Trajectory Sampling for
  Connected Autonomous Vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruoqi Wen, Jiahao Huang, Rongpeng Li, Guoru Ding, Zhifeng Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-21 14:55:21</h6>
<p class='card-text'>Autonomous Vehicles (AVs) have attracted significant attention in recent
years and Reinforcement Learning (RL) has shown remarkable performance in
improving the autonomy of vehicles. In that regard, the widely adopted
Model-Free RL (MFRL) promises to solve decision-making tasks in connected AVs
(CAVs), contingent on the readiness of a significant amount of data samples for
training. Nevertheless, it might be infeasible in practice and possibly lead to
learning instability. In contrast, Model-Based RL (MBRL) manifests itself in
sample-efficient learning, but the asymptotic performance of MBRL might lag
behind the state-of-the-art MFRL algorithms. Furthermore, most studies for CAVs
are limited to the decision-making of a single AV only, thus underscoring the
performance due to the absence of communications. In this study, we try to
address the decision-making problem of multiple CAVs with limited
communications and propose a decentralized Multi-Agent Probabilistic Ensembles
with Trajectory Sampling algorithm MA-PETS. In particular, in order to better
capture the uncertainty of the unknown environment, MA-PETS leverages
Probabilistic Ensemble (PE) neural networks to learn from communicated samples
among neighboring CAVs. Afterwards, MA-PETS capably develops Trajectory
Sampling (TS)-based model-predictive control for decision-making. On this
basis, we derive the multi-agent group regret bound affected by the number of
agents within the communication range and mathematically validate that
incorporating effective information exchange among agents into the multi-agent
learning scheme contributes to reducing the group regret bound in the worst
case. Finally, we empirically demonstrate the superiority of MA-PETS in terms
of the sample efficiency comparable to MFBL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.01450v2' target='_blank'>DreamSmooth: Improving Model-based Reinforcement Learning via Reward
  Smoothing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vint Lee, Pieter Abbeel, Youngwoon Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-02 17:57:38</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) has gained much attention for its
ability to learn complex behaviors in a sample-efficient way: planning actions
by generating imaginary trajectories with predicted rewards. Despite its
success, we found that surprisingly, reward prediction is often a bottleneck of
MBRL, especially for sparse rewards that are challenging (or even ambiguous) to
predict. Motivated by the intuition that humans can learn from rough reward
estimates, we propose a simple yet effective reward smoothing approach,
DreamSmooth, which learns to predict a temporally-smoothed reward, instead of
the exact reward at the given timestep. We empirically show that DreamSmooth
achieves state-of-the-art performance on long-horizon sparse-reward tasks both
in sample efficiency and final performance without losing performance on common
benchmarks, such as Deepmind Control Suite and Atari benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.15017v3' target='_blank'>Mind the Model, Not the Agent: The Primacy Bias in Model-based RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhongjian Qiao, Jiafei Lyu, Xiu Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-23 15:12:20</h6>
<p class='card-text'>The primacy bias in model-free reinforcement learning (MFRL), which refers to
the agent's tendency to overfit early data and lose the ability to learn from
new data, can significantly decrease the performance of MFRL algorithms.
Previous studies have shown that employing simple techniques, such as resetting
the agent's parameters, can substantially alleviate the primacy bias in MFRL.
However, the primacy bias in model-based reinforcement learning (MBRL) remains
unexplored. In this work, we focus on investigating the primacy bias in MBRL.
We begin by observing that resetting the agent's parameters harms its
performance in the context of MBRL. We further find that the primacy bias in
MBRL is more closely related to the primacy bias of the world model instead of
the primacy bias of the agent. Based on this finding, we propose \textit{world
model resetting}, a simple yet effective technique to alleviate the primacy
bias in MBRL. We apply our method to two different MBRL algorithms, MBPO and
DreamerV2. We validate the effectiveness of our method on multiple continuous
control tasks on MuJoCo and DeepMind Control Suite, as well as discrete control
tasks on Atari 100k benchmark. The experimental results show that \textit{world
model resetting} can significantly alleviate the primacy bias in the
model-based setting and improve the algorithm's performance. We also give a
guide on how to perform \textit{world model resetting} effectively.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.06253v2' target='_blank'>A Unified View on Solving Objective Mismatch in Model-Based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ran Wei, Nathan Lambert, Anthony McDonald, Alfredo Garcia, Roberto Calandra</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-10 01:58:38</h6>
<p class='card-text'>Model-based Reinforcement Learning (MBRL) aims to make agents more
sample-efficient, adaptive, and explainable by learning an explicit model of
the environment. While the capabilities of MBRL agents have significantly
improved in recent years, how to best learn the model is still an unresolved
question. The majority of MBRL algorithms aim at training the model to make
accurate predictions about the environment and subsequently using the model to
determine the most rewarding actions. However, recent research has shown that
model predictive accuracy is often not correlated with action quality, tracing
the root cause to the objective mismatch between accurate dynamics model
learning and policy optimization of rewards. A number of interrelated solution
categories to the objective mismatch problem have emerged as MBRL continues to
mature as a research area. In this work, we provide an in-depth survey of these
solution categories and propose a taxonomy to foster future research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.05672v2' target='_blank'>Multi-timestep models for Model-based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abdelhakim Benechehab, Giuseppe Paolo, Albert Thomas, Maurizio Filippone, Bal√°zs K√©gl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-09 12:42:39</h6>
<p class='card-text'>In model-based reinforcement learning (MBRL), most algorithms rely on
simulating trajectories from one-step dynamics models learned on data. A
critical challenge of this approach is the compounding of one-step prediction
errors as length of the trajectory grows. In this paper we tackle this issue by
using a multi-timestep objective to train one-step models. Our objective is a
weighted sum of a loss function (e.g., negative log-likelihood) at various
future horizons. We explore and test a range of weights profiles. We find that
exponentially decaying weights lead to models that significantly improve the
long-horizon R2 score. This improvement is particularly noticeable when the
models were evaluated on noisy data. Finally, using a soft actor-critic (SAC)
agent in pure batch reinforcement learning (RL) and iterated batch RL
scenarios, we found that our multi-timestep models outperform or match standard
one-step models. This was especially evident in a noisy variant of the
considered environment, highlighting the potential of our approach in
real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.05422v1' target='_blank'>Reward-Consistent Dynamics Models are Strongly Generalizable for Offline
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fan-Ming Luo, Tian Xu, Xingchen Cao, Yang Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-09 05:37:58</h6>
<p class='card-text'>Learning a precise dynamics model can be crucial for offline reinforcement
learning, which, unfortunately, has been found to be quite challenging.
Dynamics models that are learned by fitting historical transitions often
struggle to generalize to unseen transitions. In this study, we identify a
hidden but pivotal factor termed dynamics reward that remains consistent across
transitions, offering a pathway to better generalization. Therefore, we propose
the idea of reward-consistent dynamics models: any trajectory generated by the
dynamics model should maximize the dynamics reward derived from the data. We
implement this idea as the MOREC (Model-based Offline reinforcement learning
with Reward Consistency) method, which can be seamlessly integrated into
previous offline model-based reinforcement learning (MBRL) methods. MOREC
learns a generalizable dynamics reward function from offline data, which is
subsequently employed as a transition filter in any offline MBRL method: when
generating transitions, the dynamics model generates a batch of transitions and
selects the one with the highest dynamics reward value. On a synthetic task, we
visualize that MOREC has a strong generalization ability and can surprisingly
recover some distant unseen transitions. On 21 offline tasks in D4RL and NeoRL
benchmarks, MOREC improves the previous state-of-the-art performance by a
significant margin, i.e., 4.6% on D4RL tasks and 25.9% on NeoRL tasks. Notably,
MOREC is the first method that can achieve above 95% online RL performance in 6
out of 12 D4RL tasks and 3 out of 9 NeoRL tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.00344v3' target='_blank'>HarmonyDream: Task Harmonization Inside World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoyu Ma, Jialong Wu, Ningya Feng, Chenjun Xiao, Dong Li, Jianye Hao, Jianmin Wang, Mingsheng Long</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-30 11:38:13</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) holds the promise of
sample-efficient learning by utilizing a world model, which models how the
environment works and typically encompasses components for two tasks:
observation modeling and reward modeling. In this paper, through a dedicated
empirical investigation, we gain a deeper understanding of the role each task
plays in world models and uncover the overlooked potential of sample-efficient
MBRL by mitigating the domination of either observation or reward modeling. Our
key insight is that while prevalent approaches of explicit MBRL attempt to
restore abundant details of the environment via observation models, it is
difficult due to the environment's complexity and limited model capacity. On
the other hand, reward models, while dominating implicit MBRL and adept at
learning compact task-centric dynamics, are inadequate for sample-efficient
learning without richer learning signals. Motivated by these insights and
discoveries, we propose a simple yet effective approach, HarmonyDream, which
automatically adjusts loss coefficients to maintain task harmonization, i.e. a
dynamic equilibrium between the two tasks in world model learning. Our
experiments show that the base MBRL method equipped with HarmonyDream gains
10%-69% absolute performance boosts on visual robotic tasks and sets a new
state-of-the-art result on the Atari 100K benchmark. Code is available at
https://github.com/thuml/HarmonyDream.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.14236v2' target='_blank'>MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Patrick Lancaster, Nicklas Hansen, Aravind Rajeswaran, Vikash Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-25 15:51:29</h6>
<p class='card-text'>Robotic systems that aspire to operate in uninstrumented real-world
environments must perceive the world directly via onboard sensing. Vision-based
learning systems aim to eliminate the need for environment instrumentation by
building an implicit understanding of the world based on raw pixels, but
navigating the contact-rich high-dimensional search space from solely sparse
visual reward signals significantly exacerbates the challenge of exploration.
The applicability of such systems is thus typically restricted to simulated or
heavily engineered environments since agent exploration in the real-world
without the guidance of explicit state estimation and dense rewards can lead to
unsafe behavior and safety faults that are catastrophic. In this study, we
isolate the root causes behind these limitations to develop a system, called
MoDem-V2, capable of learning contact-rich manipulation directly in the
uninstrumented real world. Building on the latest algorithmic advancements in
model-based reinforcement learning (MBRL), demo-bootstrapping, and effective
exploration, MoDem-V2 can acquire contact-rich dexterous manipulation skills
directly in the real world. We identify key ingredients for leveraging
demonstrations in model learning while respecting real-world safety
considerations -- exploration centering, agency handover, and actor-critic
ensembles. We empirically demonstrate the contribution of these ingredients in
four complex visuo-motor manipulation problems in both simulation and the real
world. To the best of our knowledge, our work presents the first successful
system for demonstration-augmented visual MBRL trained directly in the real
world. Visit https://sites.google.com/view/modem-v2 for videos and more
details.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.12671v2' target='_blank'>How to Fine-tune the Model: Unified Model Shift and Model Bias Policy
  Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hai Zhang, Hang Yu, Junqiao Zhao, Di Zhang, Chang Huang, Hongtu Zhou, Xiao Zhang, Chen Ye</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-22 07:27:32</h6>
<p class='card-text'>Designing and deriving effective model-based reinforcement learning (MBRL)
algorithms with a performance improvement guarantee is challenging, mainly
attributed to the high coupling between model learning and policy optimization.
Many prior methods that rely on return discrepancy to guide model learning
ignore the impacts of model shift, which can lead to performance deterioration
due to excessive model updates. Other methods use performance difference bound
to explicitly consider model shift. However, these methods rely on a fixed
threshold to constrain model shift, resulting in a heavy dependence on the
threshold and a lack of adaptability during the training process. In this
paper, we theoretically derive an optimization objective that can unify model
shift and model bias and then formulate a fine-tuning process. This process
adaptively adjusts the model updates to get a performance improvement guarantee
while avoiding model overfitting. Based on these, we develop a straightforward
algorithm USB-PO (Unified model Shift and model Bias Policy Optimization).
Empirical results show that USB-PO achieves state-of-the-art performance on
several challenging benchmark tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.11089v1' target='_blank'>Practical Probabilistic Model-based Deep Reinforcement Learning by
  Integrating Dropout Uncertainty and Trajectory Sampling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenjun Huang, Yunduan Cui, Huiyun Li, Xinyu Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-20 06:39:19</h6>
<p class='card-text'>This paper addresses the prediction stability, prediction accuracy and
control capability of the current probabilistic model-based reinforcement
learning (MBRL) built on neural networks. A novel approach dropout-based
probabilistic ensembles with trajectory sampling (DPETS) is proposed where the
system uncertainty is stably predicted by combining the Monte-Carlo dropout and
trajectory sampling in one framework. Its loss function is designed to correct
the fitting error of neural networks for more accurate prediction of
probabilistic models. The state propagation in its policy is extended to filter
the aleatoric uncertainty for superior control capability. Evaluated by several
Mujoco benchmark control tasks under additional disturbances and one practical
robot arm manipulation task, DPETS outperforms related MBRL approaches in both
average return and convergence velocity while achieving superior performance
than well-known model-free baselines with significant sample efficiency. The
open source code of DPETS is available at https://github.com/mrjun123/DPETS.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.06590v2' target='_blank'>Value-Distributional Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, Jan Peters</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-12 14:59:19</h6>
<p class='card-text'>Quantifying uncertainty about a policy's long-term performance is important
to solve sequential decision-making tasks. We study the problem from a
model-based Bayesian reinforcement learning perspective, where the goal is to
learn the posterior distribution over value functions induced by parameter
(epistemic) uncertainty of the Markov decision process. Previous work restricts
the analysis to a few moments of the distribution over values or imposes a
particular distribution shape, e.g., Gaussians. Inspired by distributional
reinforcement learning, we introduce a Bellman operator whose fixed-point is
the value distribution function. Based on our theory, we propose Epistemic
Quantile-Regression (EQR), a model-based algorithm that learns a value
distribution function. We combine EQR with soft actor-critic (SAC) for policy
optimization with an arbitrary differentiable objective function of the learned
value distribution. Evaluation across several continuous-control tasks shows
performance benefits with respect to both model-based and model-free
algorithms. The code is available at
https://github.com/boschresearch/dist-mbrl.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.02150v1' target='_blank'>Learning to Shape by Grinding: Cutting-surface-aware Model-based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Takumi Hachimine, Jun Morimoto, Takamitsu Matsubara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-04 06:14:13</h6>
<p class='card-text'>Object shaping by grinding is a crucial industrial process in which a
rotating grinding belt removes material. Object-shape transition models are
essential to achieving automation by robots; however, learning such a complex
model that depends on process conditions is challenging because it requires a
significant amount of data, and the irreversible nature of the removal process
makes data collection expensive. This paper proposes a cutting-surface-aware
Model-Based Reinforcement Learning (MBRL) method for robotic grinding. Our
method employs a cutting-surface-aware model as the object's shape transition
model, which in turn is composed of a geometric cutting model and a
cutting-surface-deviation model, based on the assumption that the robot action
can specify the cutting surface made by the tool. Furthermore, according to the
grinding resistance theory, the cutting-surface-deviation model does not
require raw shape information, making the model's dimensions smaller and easier
to learn than a naive shape transition model directly mapping the shapes.
Through evaluation and comparison by simulation and real robot experiments, we
confirm that our MBRL method can achieve high data efficiency for learning
object shaping by grinding and also provide generalization capability for
initial and target shapes that differ from the training data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.02064v2' target='_blank'>Facing Off World Model Backbones: RNNs, Transformers, and S4</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fei Deng, Junyeong Park, Sungjin Ahn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-05 07:00:31</h6>
<p class='card-text'>World models are a fundamental component in model-based reinforcement
learning (MBRL). To perform temporally extended and consistent simulations of
the future in partially observable environments, world models need to possess
long-term memory. However, state-of-the-art MBRL agents, such as Dreamer,
predominantly employ recurrent neural networks (RNNs) as their world model
backbone, which have limited memory capacity. In this paper, we seek to explore
alternative world model backbones for improving long-term memory. In
particular, we investigate the effectiveness of Transformers and Structured
State Space Sequence (S4) models, motivated by their remarkable ability to
capture long-range dependencies in low-dimensional sequences and their
complementary strengths. We propose S4WM, the first world model compatible with
parallelizable SSMs including S4 and its variants. By incorporating latent
variable modeling, S4WM can efficiently generate high-dimensional image
sequences through latent imagination. Furthermore, we extensively compare RNN-,
Transformer-, and S4-based world models across four sets of environments, which
we have tailored to assess crucial memory capabilities of world models,
including long-term imagination, context-dependent recall, reward prediction,
and memory-based reasoning. Our findings demonstrate that S4WM outperforms
Transformer-based world models in terms of long-term memory, while exhibiting
greater efficiency during training and imagination. These results pave the way
for the development of stronger MBRL agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.00840v4' target='_blank'>What model does MuZero learn?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinke He, Thomas M. Moerland, Joery A. de Vries, Frans A. Oliehoek</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-01 16:01:23</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) has drawn considerable interest in
recent years, given its promise to improve sample efficiency. Moreover, when
using deep-learned models, it is possible to learn compact and generalizable
models from data. In this work, we study MuZero, a state-of-the-art deep
model-based reinforcement learning algorithm that distinguishes itself from
existing algorithms by learning a value-equivalent model. Despite MuZero's
success and impact in the field of MBRL, existing literature has not thoroughly
addressed why MuZero performs so well in practice. Specifically, there is a
lack of in-depth investigation into the value-equivalent model learned by
MuZero and its effectiveness in model-based credit assignment and policy
improvement, which is vital for achieving sample efficiency in MBRL. To fill
this gap, we explore two fundamental questions through our empirical analysis:
1) to what extent does MuZero achieve its learning objective of a
value-equivalent model, and 2) how useful are these models for policy
improvement? Our findings reveal that MuZero's model struggles to generalize
when evaluating unseen policies, which limits its capacity for additional
policy improvement. However, MuZero's incorporation of the policy prior in MCTS
alleviates this problem, which biases the search towards actions where the
model is more accurate.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.18499v2' target='_blank'>Pre-training Contextualized World Models with In-the-wild Videos for
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jialong Wu, Haoyu Ma, Chaoyi Deng, Mingsheng Long</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-29 14:29:12</h6>
<p class='card-text'>Unsupervised pre-training methods utilizing large and diverse datasets have
achieved tremendous success across a range of domains. Recent work has
investigated such unsupervised pre-training methods for model-based
reinforcement learning (MBRL) but is limited to domain-specific or simulated
data. In this paper, we study the problem of pre-training world models with
abundant in-the-wild videos for efficient learning of downstream visual control
tasks. However, in-the-wild videos are complicated with various contextual
factors, such as intricate backgrounds and textured appearance, which precludes
a world model from extracting shared world knowledge to generalize better. To
tackle this issue, we introduce Contextualized World Models (ContextWM) that
explicitly separate context and dynamics modeling to overcome the complexity
and diversity of in-the-wild videos and facilitate knowledge transfer between
distinct scenes. Specifically, a contextualized extension of the latent
dynamics model is elaborately realized by incorporating a context encoder to
retain contextual information and empower the image decoder, which encourages
the latent dynamics model to concentrate on essential temporal variations. Our
experiments show that in-the-wild video pre-training equipped with ContextWM
can significantly improve the sample efficiency of MBRL in various domains,
including robotic manipulation, locomotion, and autonomous driving. Code is
available at this repository: https://github.com/thuml/ContextWM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.16571v1' target='_blank'>Digital Twin-Based 3D Map Management for Edge-Assisted Mobile Augmented
  Reality</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Conghao Zhou, Jie Gao, Mushu Li, Nan Cheng, Xuemin Shen, Weihua Zhuang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-26 01:38:45</h6>
<p class='card-text'>In this paper, we design a 3D map management scheme for edge-assisted mobile
augmented reality (MAR) to support the pose estimation of individual MAR
device, which uploads camera frames to an edge server. Our objective is to
minimize the pose estimation uncertainty of the MAR device by periodically
selecting a proper set of camera frames for uploading to update the 3D map. To
address the challenges of the dynamic uplink data rate and the time-varying
pose of the MAR device, we propose a digital twin (DT)-based approach to 3D map
management. First, a DT is created for the MAR device, which emulates 3D map
management based on predicting subsequent camera frames. Second, a model-based
reinforcement learning (MBRL) algorithm is developed, utilizing the data
collected from both the actual and the emulated data to manage the 3D map. With
extensive emulated data provided by the DT, the MBRL algorithm can quickly
provide an adaptive map management policy in a highly dynamic environment.
Simulation results demonstrate that the proposed DT-based 3D map management
outperforms benchmark schemes by achieving lower pose estimation uncertainty
and higher data efficiency in dynamic environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.12663v1' target='_blank'>TOM: Learning Policy-Aware Models for Model-Based Reinforcement Learning
  via Transition Occupancy Matching</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yecheng Jason Ma, Kausik Sivakumar, Jason Yan, Osbert Bastani, Dinesh Jayaraman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-22 03:06:09</h6>
<p class='card-text'>Standard model-based reinforcement learning (MBRL) approaches fit a
transition model of the environment to all past experience, but this wastes
model capacity on data that is irrelevant for policy improvement. We instead
propose a new "transition occupancy matching" (TOM) objective for MBRL model
learning: a model is good to the extent that the current policy experiences the
same distribution of transitions inside the model as in the real environment.
We derive TOM directly from a novel lower bound on the standard reinforcement
learning objective. To optimize TOM, we show how to reduce it to a form of
importance weighted maximum-likelihood estimation, where the automatically
computed importance weights identify policy-relevant past experiences from a
replay buffer, enabling stable optimization. TOM thus offers a plug-and-play
model learning sub-routine that is compatible with any backbone MBRL algorithm.
On various Mujoco continuous robotic control tasks, we show that TOM
successfully focuses model learning on policy-relevant experience and drives
policies faster to higher task rewards than alternative model learning
approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.04750v1' target='_blank'>Sense, Imagine, Act: Multimodal Perception Improves Model-Based
  Reinforcement Learning for Head-to-Head Autonomous Racing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Elena Shrestha, Chetan Reddy, Hanxi Wan, Yulun Zhuang, Ram Vasudevan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-08 14:49:02</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) techniques have recently yielded
promising results for real-world autonomous racing using high-dimensional
observations. MBRL agents, such as Dreamer, solve long-horizon tasks by
building a world model and planning actions by latent imagination. This
approach involves explicitly learning a model of the system dynamics and using
it to learn the optimal policy for continuous control over multiple timesteps.
As a result, MBRL agents may converge to sub-optimal policies if the world
model is inaccurate. To improve state estimation for autonomous racing, this
paper proposes a self-supervised sensor fusion technique that combines
egocentric LiDAR and RGB camera observations collected from the F1TENTH Gym.
The zero-shot performance of MBRL agents is empirically evaluated on unseen
tracks and against a dynamic obstacle. This paper illustrates that multimodal
perception improves robustness of the world model without requiring additional
training data. The resulting multimodal Dreamer agent safely avoided collisions
and won the most races compared to other tested baselines in zero-shot
head-to-head autonomous racing.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2304.03365v3' target='_blank'>Decision-Focused Model-based Reinforcement Learning for Reward Transfer</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abhishek Sharma, Sonali Parbhoo, Omer Gottesman, Finale Doshi-Velez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-04-06 20:47:09</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) provides a way to learn a
transition model of the environment, which can then be used to plan
personalized policies for different patient cohorts and to understand the
dynamics involved in the decision-making process. However, standard MBRL
algorithms are either sensitive to changes in the reward function or achieve
suboptimal performance on the task when the transition model is restricted.
Motivated by the need to use simple and interpretable models in critical
domains such as healthcare, we propose a novel robust decision-focused (RDF)
algorithm that learns a transition model that achieves high returns while being
robust to changes in the reward function. We demonstrate our RDF algorithm can
be used with several model classes and planning algorithms. We also provide
theoretical and empirical evidence, on a variety of simulators and real patient
data, that RDF can learn simple yet effective models that can be used to plan
personalized policies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.12410v2' target='_blank'>EDGI: Equivariant Diffusion for Planning with Embodied Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Johann Brehmer, Joey Bose, Pim de Haan, Taco Cohen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-22 09:19:39</h6>
<p class='card-text'>Embodied agents operate in a structured world, often solving tasks with
spatial, temporal, and permutation symmetries. Most algorithms for planning and
model-based reinforcement learning (MBRL) do not take this rich geometric
structure into account, leading to sample inefficiency and poor generalization.
We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an
algorithm for MBRL and planning that is equivariant with respect to the product
of the spatial symmetry group SE(3), the discrete-time translation group Z, and
the object permutation group Sn. EDGI follows the Diffuser framework (Janner et
al., 2022) in treating both learning a world model and planning in it as a
conditional generative modeling problem, training a diffusion model on an
offline trajectory dataset. We introduce a new SE(3)xZxSn-equivariant diffusion
model that supports multiple representations. We integrate this model in a
planning loop, where conditioning and classifier guidance let us softly break
the symmetry for specific tasks as needed. On object manipulation and
navigation tasks, EDGI is substantially more sample efficient and generalizes
better across the symmetry group than non-equivariant models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.08690v2' target='_blank'>Replay Buffer with Local Forgetting for Adapting to Local Environment
  Changes in Deep Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ali Rahimi-Kalahroudi, Janarthanan Rajendran, Ida Momennejad, Harm van Seijen, Sarath Chandar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-15 15:21:26</h6>
<p class='card-text'>One of the key behavioral characteristics used in neuroscience to determine
whether the subject of study -- be it a rodent or a human -- exhibits
model-based learning is effective adaptation to local changes in the
environment, a particular form of adaptivity that is the focus of this work. In
reinforcement learning, however, recent work has shown that modern deep
model-based reinforcement-learning (MBRL) methods adapt poorly to local
environment changes. An explanation for this mismatch is that MBRL methods are
typically designed with sample-efficiency on a single task in mind and the
requirements for effective adaptation are substantially higher, both in terms
of the learned world model and the planning routine. One particularly
challenging requirement is that the learned world model has to be sufficiently
accurate throughout relevant parts of the state-space. This is challenging for
deep-learning-based world models due to catastrophic forgetting. And while a
replay buffer can mitigate the effects of catastrophic forgetting, the
traditional first-in-first-out replay buffer precludes effective adaptation due
to maintaining stale data. In this work, we show that a conceptually simple
variation of this traditional replay buffer is able to overcome this
limitation. By removing only samples from the buffer from the local
neighbourhood of the newly observed samples, deep world models can be built
that maintain their accuracy across the state-space, while also being able to
effectively adapt to local changes in the reward function. We demonstrate this
by applying our replay-buffer variation to a deep version of the classical Dyna
method, as well as to recent methods such as PlaNet and DreamerV2,
demonstrating that deep model-based methods can adapt effectively as well to
local changes in the environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.06572v1' target='_blank'>Predictive Experience Replay for Continual Visual Control and
  Forecasting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wendong Zhang, Geng Chen, Xiangming Zhu, Siyu Gao, Yunbo Wang, Xiaokang Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-12 05:08:03</h6>
<p class='card-text'>Learning physical dynamics in a series of non-stationary environments is a
challenging but essential task for model-based reinforcement learning (MBRL)
with visual inputs. It requires the agent to consistently adapt to novel tasks
without forgetting previous knowledge. In this paper, we present a new
continual learning approach for visual dynamics modeling and explore its
efficacy in visual control and forecasting. The key assumption is that an ideal
world model can provide a non-forgetting environment simulator, which enables
the agent to optimize the policy in a multi-task learning manner based on the
imagined trajectories from the world model. To this end, we first propose the
mixture world model that learns task-specific dynamics priors with a mixture of
Gaussians, and then introduce a new training strategy to overcome catastrophic
forgetting, which we call predictive experience replay. Finally, we extend
these methods to continual RL and further address the value estimation problems
with the exploratory-conservative behavior learning approach. Our model
remarkably outperforms the naive combinations of existing continual learning
and visual RL algorithms on DeepMind Control and Meta-World benchmarks with
continual visual control tasks. It is also shown to effectively alleviate the
forgetting of spatiotemporal dynamics in video prediction datasets with
evolving domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.05458v1' target='_blank'>Beware of Instantaneous Dependence in Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhengmao Zhu, Yuren Liu, Honglong Tian, Yang Yu, Kun Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-09 17:57:36</h6>
<p class='card-text'>Playing an important role in Model-Based Reinforcement Learning (MBRL),
environment models aim to predict future states based on the past. Existing
works usually ignore instantaneous dependence in the state, that is, assuming
that the future state variables are conditionally independent given the past
states. However, instantaneous dependence is prevalent in many RL environments.
For instance, in the stock market, instantaneous dependence can exist between
two stocks because the fluctuation of one stock can quickly affect the other
and the resolution of price change is lower than that of the effect. In this
paper, we prove that with few exceptions, ignoring instantaneous dependence can
result in suboptimal policy learning in MBRL. To address the suboptimality
problem, we propose a simple plug-and-play method to enable existing MBRL
algorithms to take instantaneous dependence into account. Through experiments
on two benchmarks, we (1) confirm the existence of instantaneous dependence
with visualization; (2) validate our theoretical findings that ignoring
instantaneous dependence leads to suboptimal policy; (3) verify that our method
effectively enables reinforcement learning with instantaneous dependence and
improves policy performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.03787v2' target='_blank'>Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method
  and Contrastive Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mostafa Kotb, Cornelius Weber, Stefan Wermter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-07 10:48:20</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) with real-time planning has shown
great potential in locomotion and manipulation control tasks. However, the
existing planning methods, such as the Cross-Entropy Method (CEM), do not scale
well to complex high-dimensional environments. One of the key reasons for
underperformance is the lack of exploration, as these planning methods only aim
to maximize the cumulative extrinsic reward over the planning horizon.
Furthermore, planning inside the compact latent space in the absence of
observations makes it challenging to use curiosity-based intrinsic motivation.
We propose Curiosity CEM (CCEM), an improved version of the CEM algorithm for
encouraging exploration via curiosity. Our proposed method maximizes the sum of
state-action Q values over the planning horizon, in which these Q values
estimate the future extrinsic and intrinsic reward, hence encouraging reaching
novel observations. In addition, our model uses contrastive representation
learning to efficiently learn latent representations. Experiments on
image-based continuous control tasks from the DeepMind Control suite show that
CCEM is by a large margin more sample-efficient than previous MBRL algorithms
and compares favorably with the best model-free RL methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2303.00694v1' target='_blank'>The Virtues of Laziness in Model-based RL: A Unified Objective and
  Algorithms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anirudh Vemula, Yuda Song, Aarti Singh, J. Andrew Bagnell, Sanjiban Choudhury</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-03-01 17:42:26</h6>
<p class='card-text'>We propose a novel approach to addressing two fundamental challenges in
Model-based Reinforcement Learning (MBRL): the computational expense of
repeatedly finding a good policy in the learned model, and the objective
mismatch between model fitting and policy computation. Our "lazy" method
leverages a novel unified objective, Performance Difference via Advantage in
Model, to capture the performance difference between the learned policy and
expert policy under the true dynamics. This objective demonstrates that
optimizing the expected policy advantage in the learned model under an
exploration distribution is sufficient for policy computation, resulting in a
significant boost in computational efficiency compared to traditional planning
methods. Additionally, the unified objective uses a value moment matching term
for model fitting, which is aligned with the model's usage during policy
computation. We present two no-regret algorithms to optimize the proposed
objective, and demonstrate their statistical and computational gains compared
to existing MBRL methods through simulated benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2302.00725v1' target='_blank'>Multi-zone HVAC Control with Model-Based Deep Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xianzhong Ding, Alberto Cerpa, Wan Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-02-01 19:41:03</h6>
<p class='card-text'>In this paper, we conduct a set of experiments to analyze the limitations of
current MBRL-based HVAC control methods, in terms of model uncertainty and
controller effectiveness. Using the lessons learned, we develop MB2C, a novel
MBRL-based HVAC control system that can achieve high control performance with
excellent sample efficiency. MB2C learns the building dynamics by employing an
ensemble of environment-conditioned neural networks. It then applies a new
control method, Model Predictive Path Integral (MPPI), for HVAC control. It
produces candidate action sequences by using an importance sampling weighted
algorithm that scales better to high state and action dimensions of multi-zone
buildings. We evaluate MB2C using EnergyPlus simulations in a five-zone office
building. The results show that MB2C can achieve 8.23% more energy savings
compared to the state-of-the-art MBRL solution while maintaining similar
thermal comfort. MB2C can reduce the training data set by an order of magnitude
(10.52x) while achieving comparable performance to MFRL approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2301.13183v1' target='_blank'>Learning Control from Raw Position Measurements</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fabio Amadio, Alberto Dalla Libera, Daniel Nikovski, Ruggero Carli, Diego Romeres</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-01-30 18:50:37</h6>
<p class='card-text'>We propose a Model-Based Reinforcement Learning (MBRL) algorithm named
VF-MC-PILCO, specifically designed for application to mechanical systems where
velocities cannot be directly measured. This circumstance, if not adequately
considered, can compromise the success of MBRL approaches. To cope with this
problem, we define a velocity-free state formulation which consists of the
collection of past positions and inputs. Then, VF-MC-PILCO uses Gaussian
Process Regression to model the dynamics of the velocity-free state and
optimizes the control policy through a particle-based policy gradient approach.
We compare VF-MC-PILCO with our previous MBRL algorithm, MC-PILCO4PMS, which
handles the lack of direct velocity measurements by modeling the presence of
velocity estimators. Results on both simulated (cart-pole and UR5 robot) and
real mechanical systems (Furuta pendulum and a ball-and-plate rig) show that
the two algorithms achieve similar results. Conveniently, VF-MC-PILCO does not
require the design and implementation of state estimators, which can be a
challenging and time-consuming activity to be performed by an expert user.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2301.08502v1' target='_blank'>Plan To Predict: Learning an Uncertainty-Foreseeing Model for
  Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zifan Wu, Chao Yu, Chen Chen, Jianye Hao, Hankz Hankui Zhuo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-01-20 10:17:22</h6>
<p class='card-text'>In Model-based Reinforcement Learning (MBRL), model learning is critical
since an inaccurate model can bias policy learning via generating misleading
samples. However, learning an accurate model can be difficult since the policy
is continually updated and the induced distribution over visited states used
for model learning shifts accordingly. Prior methods alleviate this issue by
quantifying the uncertainty of model-generated samples. However, these methods
only quantify the uncertainty passively after the samples were generated,
rather than foreseeing the uncertainty before model trajectories fall into
those highly uncertain regions. The resulting low-quality samples can induce
unstable learning targets and hinder the optimization of the policy. Moreover,
while being learned to minimize one-step prediction errors, the model is
generally used to predict for multiple steps, leading to a mismatch between the
objectives of model learning and model usage. To this end, we propose
\emph{Plan To Predict} (P2P), an MBRL framework that treats the model rollout
process as a sequential decision making problem by reversely considering the
model as a decision maker and the current policy as the dynamics. In this way,
the model can quickly adapt to the current policy and foresee the multi-step
future uncertainty when generating trajectories. Theoretically, we show that
the performance of P2P can be guaranteed by approximately optimizing a lower
bound of the true environment return. Empirical results demonstrate that P2P
achieves state-of-the-art performance on several challenging benchmark tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2301.03142v1' target='_blank'>Exploration in Model-based Reinforcement Learning with Randomized Reward</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lingxiao Wang, Ping Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-01-09 01:50:55</h6>
<p class='card-text'>Model-based Reinforcement Learning (MBRL) has been widely adapted due to its
sample efficiency. However, existing worst-case regret analysis typically
requires optimistic planning, which is not realistic in general. In contrast,
motivated by the theory, empirical study utilizes ensemble of models, which
achieve state-of-the-art performance on various testing environments. Such
deviation between theory and empirical study leads us to question whether
randomized model ensemble guarantee optimism, and hence the optimal worst-case
regret? This paper partially answers such question from the perspective of
reward randomization, a scarcely explored direction of exploration with MBRL.
We show that under the kernelized linear regulator (KNR) model, reward
randomization guarantees a partial optimism, which further yields a
near-optimal worst-case regret in terms of the number of interactions. We
further extend our theory to generalized function approximation and identified
conditions for reward randomization to attain provably efficient exploration.
Correspondingly, we propose concrete examples of efficient reward
randomization. To the best of our knowledge, our analysis establishes the first
worst-case regret analysis on randomized MBRL with function approximation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2212.08235v1' target='_blank'>A Simple Decentralized Cross-Entropy Method</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zichen Zhang, Jun Jin, Martin Jagersand, Jun Luo, Dale Schuurmans</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-12-16 02:00:55</h6>
<p class='card-text'>Cross-Entropy Method (CEM) is commonly used for planning in model-based
reinforcement learning (MBRL) where a centralized approach is typically
utilized to update the sampling distribution based on only the top-$k$
operation's results on samples. In this paper, we show that such a centralized
approach makes CEM vulnerable to local optima, thus impairing its sample
efficiency. To tackle this issue, we propose Decentralized CEM (DecentCEM), a
simple but effective improvement over classical CEM, by using an ensemble of
CEM instances running independently from one another, and each performing a
local improvement of its own sampling distribution. We provide both theoretical
and empirical analysis to demonstrate the effectiveness of this simple
decentralized approach. We empirically show that, compared to the classical
centralized approach using either a single or even a mixture of Gaussian
distributions, our DecentCEM finds the global optimum much more consistently
thus improves the sample efficiency. Furthermore, we plug in our DecentCEM in
the planning problem of MBRL, and evaluate our approach in several continuous
control environments, with comparison to the state-of-art CEM based MBRL
approaches (PETS and POPLIN). Results show sample efficiency improvement by
simply replacing the classical CEM module with our DecentCEM module, while only
sacrificing a reasonable amount of computational cost. Lastly, we conduct
ablation studies for more in-depth analysis. Code is available at
https://github.com/vincentzhang/decentCEM</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2210.15185v3' target='_blank'>SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via
  Differentiable Physics-Based Simulation and Rendering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jun Lv, Yunhai Feng, Cheng Zhang, Shuang Zhao, Lin Shao, Cewu Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-10-27 05:30:43</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) is recognized with the potential to
be significantly more sample-efficient than model-free RL. How an accurate
model can be developed automatically and efficiently from raw sensory inputs
(such as images), especially for complex environments and tasks, is a
challenging problem that hinders the broad application of MBRL in the real
world. In this work, we propose a sensing-aware model-based reinforcement
learning system called SAM-RL. Leveraging the differentiable physics-based
simulation and rendering, SAM-RL automatically updates the model by comparing
rendered images with real raw images and produces the policy efficiently. With
the sensing-aware learning pipeline, SAM-RL allows a robot to select an
informative viewpoint to monitor the task process. We apply our framework to
real world experiments for accomplishing three manipulation tasks: robotic
assembly, tool manipulation, and deformable object manipulation. We demonstrate
the effectiveness of SAM-RL via extensive experiments. Videos are available on
our project webpage at https://sites.google.com/view/rss-sam-rl.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2210.08349v4' target='_blank'>When to Update Your Model: Constrained Model-based Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianying Ji, Yu Luo, Fuchun Sun, Mingxuan Jing, Fengxiang He, Wenbing Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-10-15 17:57:43</h6>
<p class='card-text'>Designing and analyzing model-based RL (MBRL) algorithms with guaranteed
monotonic improvement has been challenging, mainly due to the interdependence
between policy optimization and model learning. Existing discrepancy bounds
generally ignore the impacts of model shifts, and their corresponding
algorithms are prone to degrade performance by drastic model updating. In this
work, we first propose a novel and general theoretical scheme for a
non-decreasing performance guarantee of MBRL. Our follow-up derived bounds
reveal the relationship between model shifts and performance improvement. These
discoveries encourage us to formulate a constrained lower-bound optimization
problem to permit the monotonicity of MBRL. A further example demonstrates that
learning models from a dynamically-varying number of explorations benefit the
eventual returns. Motivated by these analyses, we design a simple but effective
algorithm CMLO (Constrained Model-shift Lower-bound Optimization), by
introducing an event-triggered mechanism that flexibly determines when to
update the model. Experiments show that CMLO surpasses other state-of-the-art
methods and produces a boost when various policy optimization methods are
employed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2210.05922v1' target='_blank'>A Unified Framework for Alternating Offline Model Training and Policy
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shentao Yang, Shujian Zhang, Yihao Feng, Mingyuan Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-10-12 04:58:51</h6>
<p class='card-text'>In offline model-based reinforcement learning (offline MBRL), we learn a
dynamic model from historically collected data, and subsequently utilize the
learned model and fixed datasets for policy learning, without further
interacting with the environment. Offline MBRL algorithms can improve the
efficiency and stability of policy learning over the model-free algorithms.
However, in most of the existing offline MBRL algorithms, the learning
objectives for the dynamic models and the policies are isolated from each
other. Such an objective mismatch may lead to inferior performance of the
learned agents. In this paper, we address this issue by developing an iterative
offline MBRL framework, where we maximize a lower bound of the true expected
return, by alternating between dynamic-model training and policy learning. With
the proposed unified model-policy learning framework, we achieve competitive
performance on a wide range of continuous-control offline reinforcement
learning datasets. Source code is publicly released.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2209.08169v2' target='_blank'>Value Summation: A Novel Scoring Function for MPC-based Model-based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mehran Raisi, Amirhossein Noohian, Luc Mccutcheon, Saber Fallah</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-09-16 20:52:39</h6>
<p class='card-text'>This paper proposes a novel scoring function for the planning module of
MPC-based reinforcement learning methods to address the inherent bias of using
the reward function to score trajectories. The proposed method enhances the
learning efficiency of existing MPC-based MBRL methods using the discounted sum
of values. The method utilizes optimal trajectories to guide policy learning
and updates its state-action value function based on real-world and augmented
onboard data. The learning efficiency of the proposed method is evaluated in
selected MuJoCo Gym environments as well as in learning locomotion skills for a
simulated model of the Cassie robot. The results demonstrate that the proposed
method outperforms the current state-of-the-art algorithms in terms of learning
efficiency and average reward return.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2209.07676v1' target='_blank'>Conservative Dual Policy Optimization for Efficient Model-Based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shenao Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-09-16 02:27:01</h6>
<p class='card-text'>Provably efficient Model-Based Reinforcement Learning (MBRL) based on
optimism or posterior sampling (PSRL) is ensured to attain the global
optimality asymptotically by introducing the complexity measure of the model.
However, the complexity might grow exponentially for the simplest nonlinear
models, where global convergence is impossible within finite iterations. When
the model suffers a large generalization error, which is quantitatively
measured by the model complexity, the uncertainty can be large. The sampled
model that current policy is greedily optimized upon will thus be unsettled,
resulting in aggressive policy updates and over-exploration. In this work, we
propose Conservative Dual Policy Optimization (CDPO) that involves a
Referential Update and a Conservative Update. The policy is first optimized
under a reference model, which imitates the mechanism of PSRL while offering
more stability. A conservative range of randomness is guaranteed by maximizing
the expectation of model value. Without harmful sampling procedures, CDPO can
still achieve the same regret as PSRL. More importantly, CDPO enjoys monotonic
policy improvement and global optimality simultaneously. Empirical results also
validate the exploration efficiency of CDPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2209.01956v1' target='_blank'>Moderately-Balanced Representation Learning for Treatment Effects with
  Orthogonality Information</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiyan Huang, Cheuk Hang Leung, Shumin Ma, Qi Wu, Dongdong Wang, Zhixiang Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-09-05 13:20:12</h6>
<p class='card-text'>Estimating the average treatment effect (ATE) from observational data is
challenging due to selection bias. Existing works mainly tackle this challenge
in two ways. Some researchers propose constructing a score function that
satisfies the orthogonal condition, which guarantees that the established ATE
estimator is "orthogonal" to be more robust. The others explore representation
learning models to achieve a balanced representation between the treated and
the controlled groups. However, existing studies fail to 1) discriminate
treated units from controlled ones in the representation space to avoid the
over-balanced issue; 2) fully utilize the "orthogonality information". In this
paper, we propose a moderately-balanced representation learning (MBRL)
framework based on recent covariates balanced representation learning methods
and orthogonal machine learning theory. This framework protects the
representation from being over-balanced via multi-task learning.
Simultaneously, MBRL incorporates the noise orthogonality information in the
training and validation stages to achieve a better ATE estimation. The
comprehensive experiments on benchmark and simulated datasets show the
superiority and robustness of our method on treatment effect estimations
compared with existing state-of-the-art methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2208.14407v3' target='_blank'>An Analysis of Model-Based Reinforcement Learning From Abstracted
  Observations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rolf A. N. Starre, Marco Loog, Elena Congeduti, Frans A. Oliehoek</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-08-30 17:19:26</h6>
<p class='card-text'>Many methods for Model-based Reinforcement learning (MBRL) in Markov decision
processes (MDPs) provide guarantees for both the accuracy of the model they can
deliver and the learning efficiency. At the same time, state abstraction
techniques allow for a reduction of the size of an MDP while maintaining a
bounded loss with respect to the original problem. Therefore, it may come as a
surprise that no such guarantees are available when combining both techniques,
i.e., where MBRL merely observes abstract states. Our theoretical analysis
shows that abstraction can introduce a dependence between samples collected
online (e.g., in the real world). That means that, without taking this
dependence into account, results for MBRL do not directly extend to this
setting. Our result shows that we can use concentration inequalities for
martingales to overcome this problem. This result makes it possible to extend
the guarantees of existing MBRL algorithms to the setting with abstraction. We
illustrate this by combining R-MAX, a prototypical MBRL algorithm, with
abstraction, thus producing the first performance guarantees for model-based
'RL from Abstracted Observations': model-based reinforcement learning with an
abstract model.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2206.13452v1' target='_blank'>Causal Dynamics Learning for Task-Independent State Abstraction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zizhao Wang, Xuesu Xiao, Zifan Xu, Yuke Zhu, Peter Stone</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-06-27 17:02:53</h6>
<p class='card-text'>Learning dynamics models accurately is an important goal for Model-Based
Reinforcement Learning (MBRL), but most MBRL methods learn a dense dynamics
model which is vulnerable to spurious correlations and therefore generalizes
poorly to unseen states. In this paper, we introduce Causal Dynamics Learning
for Task-Independent State Abstraction (CDL), which first learns a
theoretically proved causal dynamics model that removes unnecessary
dependencies between state variables and the action, thus generalizing well to
unseen states. A state abstraction can then be derived from the learned
dynamics, which not only improves sample efficiency but also applies to a wider
range of tasks than existing state abstraction methods. Evaluated on two
simulated environments and downstream tasks, both the dynamics model and
policies learned by the proposed method generalize well to unseen states and
the derived state abstraction improves sample efficiency compared to learning
without it.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2206.09328v1' target='_blank'>A Survey on Model-based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fan-Ming Luo, Tian Xu, Hang Lai, Xiong-Hui Chen, Weinan Zhang, Yang Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-06-19 05:28:03</h6>
<p class='card-text'>Reinforcement learning (RL) solves sequential decision-making problems via a
trial-and-error process interacting with the environment. While RL achieves
outstanding success in playing complex video games that allow huge
trial-and-error, making errors is always undesired in the real world. To
improve the sample efficiency and thus reduce the errors, model-based
reinforcement learning (MBRL) is believed to be a promising direction, which
builds environment models in which the trial-and-errors can take place without
real costs. In this survey, we take a review of MBRL with a focus on the recent
progress in deep RL. For non-tabular environments, there is always a
generalization error between the learned environment model and the real
environment. As such, it is of great importance to analyze the discrepancy
between policy training in the environment model and that in the real
environment, which in turn guides the algorithm design for better model
learning, model usage, and policy training. Besides, we also discuss the recent
advances of model-based techniques in other forms of RL, including offline RL,
goal-conditioned RL, multi-agent RL, and meta-RL. Moreover, we discuss the
applicability and advantages of MBRL in real-world tasks. Finally, we end this
survey by discussing the promising prospects for the future development of
MBRL. We think that MBRL has great potential and advantages in real-world
applications that were overlooked, and we hope this survey could attract more
research on MBRL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2206.04551v1' target='_blank'>A Relational Intervention Approach for Unsupervised Dynamics
  Generalization in Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jixian Guo, Mingming Gong, Dacheng Tao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-06-09 15:01:36</h6>
<p class='card-text'>The generalization of model-based reinforcement learning (MBRL) methods to
environments with unseen transition dynamics is an important yet challenging
problem. Existing methods try to extract environment-specified information $Z$
from past transition segments to make the dynamics prediction model
generalizable to different dynamics. However, because environments are not
labelled, the extracted information inevitably contains redundant information
unrelated to the dynamics in transition segments and thus fails to maintain a
crucial property of $Z$: $Z$ should be similar in the same environment and
dissimilar in different ones. As a result, the learned dynamics prediction
function will deviate from the true one, which undermines the generalization
ability. To tackle this problem, we introduce an interventional prediction
module to estimate the probability of two estimated $\hat{z}_i, \hat{z}_j$
belonging to the same environment. Furthermore, by utilizing the $Z$'s
invariance within a single environment, a relational head is proposed to
enforce the similarity between $\hat{{Z}}$ from the same environment. As a
result, the redundant information will be reduced in $\hat{Z}$. We empirically
show that $\hat{{Z}}$ estimated by our method enjoy less redundant information
than previous methods, and such $\hat{{Z}}$ can significantly reduce dynamics
prediction errors and improve the performance of model-based RL methods on
zero-shot new environments with unseen dynamics. The codes of this method are
available at \url{https://github.com/CR-Gjx/RIA}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2206.01162v2' target='_blank'>Posterior Coreset Construction with Kernelized Stein Discrepancy for
  Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Souradip Chakraborty, Amrit Singh Bedi, Alec Koppel, Brian M. Sadler, Furong Huang, Pratap Tokekar, Dinesh Manocha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-06-02 17:27:49</h6>
<p class='card-text'>Model-based approaches to reinforcement learning (MBRL) exhibit favorable
performance in practice, but their theoretical guarantees in large spaces are
mostly restricted to the setting when transition model is Gaussian or
Lipschitz, and demands a posterior estimate whose representational complexity
grows unbounded with time. In this work, we develop a novel MBRL method (i)
which relaxes the assumptions on the target transition model to belong to a
generic family of mixture models; (ii) is applicable to large-scale training by
incorporating a compression step such that the posterior estimate consists of a
Bayesian coreset of only statistically significant past state-action pairs; and
(iii) exhibits a sublinear Bayesian regret. To achieve these results, we adopt
an approach based upon Stein's method, which, under a smoothness condition on
the constructed posterior and target, allows distributional distance to be
evaluated in closed form as the kernelized Stein discrepancy (KSD). The
aforementioned compression step is then computed in terms of greedily retaining
only those samples which are more than a certain KSD away from the previous
model estimate. Experimentally, we observe that this approach is competitive
with several state-of-the-art RL methodologies, and can achieve up-to 50
percent reduction in wall clock time in some continuous control environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2205.15056v1' target='_blank'>Stock Trading Optimization through Model-based Reinforcement Learning
  with Resistance Support Relative Strength</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huifang Huang, Ting Gao, Yi Gui, Jin Guo, Peng Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-05-30 12:36:48</h6>
<p class='card-text'>Reinforcement learning (RL) is gaining attention by more and more researchers
in quantitative finance as the agent-environment interaction framework is
aligned with decision making process in many business problems. Most of the
current financial applications using RL algorithms are based on model-free
method, which still faces stability and adaptivity challenges. As lots of
cutting-edge model-based reinforcement learning (MBRL) algorithms mature in
applications such as video games or robotics, we design a new approach that
leverages resistance and support (RS) level as regularization terms for action
in MBRL, to improve the algorithm's efficiency and stability. From the
experiment results, we can see RS level, as a market timing technique, enhances
the performance of pure MBRL models in terms of various measurements and
obtains better profit gain with less riskiness. Besides, our proposed method
even resists big drop (less maximum drawdown) during COVID-19 pandemic period
when the financial market got unpredictable crisis. Explanations on why control
of resistance and support level can boost MBRL is also investigated through
numerical experiments, such as loss of actor-critic network and prediction
error of the transition dynamical model. It shows that RS indicators indeed
help the MBRL algorithms to converge faster at early stage and obtain smaller
critic loss as training episodes increase.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2205.15023v1' target='_blank'>Scalable Multi-Agent Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vladimir Egorov, Aleksei Shpilman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-05-25 08:35:00</h6>
<p class='card-text'>Recent Multi-Agent Reinforcement Learning (MARL) literature has been largely
focused on Centralized Training with Decentralized Execution (CTDE) paradigm.
CTDE has been a dominant approach for both cooperative and mixed environments
due to its capability to efficiently train decentralized policies. While in
mixed environments full autonomy of the agents can be a desirable outcome,
cooperative environments allow agents to share information to facilitate
coordination. Approaches that leverage this technique are usually referred as
communication methods, as full autonomy of agents is compromised for better
performance. Although communication approaches have shown impressive results,
they do not fully leverage this additional information during training phase.
In this paper, we propose a new method called MAMBA which utilizes Model-Based
Reinforcement Learning (MBRL) to further leverage centralized training in
cooperative environments. We argue that communication between agents is enough
to sustain a world model for each agent during execution phase while imaginary
rollouts can be used for training, removing the necessity to interact with the
environment. These properties yield sample efficient algorithm that can scale
gracefully with the number of agents. We empirically confirm that MAMBA
achieves good performance while reducing the number of interactions with the
environment up to an orders of magnitude compared to Model-Free
state-of-the-art approaches in challenging domains of SMAC and Flatland.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2205.10736v1' target='_blank'>Should Models Be Accurate?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Esra'a Saleh, John D. Martin, Anna Koop, Arash Pourzarabi, Michael Bowling</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-05-22 04:23:54</h6>
<p class='card-text'>Model-based Reinforcement Learning (MBRL) holds promise for data-efficiency
by planning with model-generated experience in addition to learning with
experience from the environment. However, in complex or changing environments,
models in MBRL will inevitably be imperfect, and their detrimental effects on
learning can be difficult to mitigate. In this work, we question whether the
objective of these models should be the accurate simulation of environment
dynamics at all. We focus our investigations on Dyna-style planning in a
prediction setting. First, we highlight and support three motivating points: a
perfectly accurate model of environment dynamics is not practically achievable,
is not necessary, and is not always the most useful anyways. Second, we
introduce a meta-learning algorithm for training models with a focus on their
usefulness to the learner instead of their accuracy in modelling the
environment. Our experiments show that in a simple non-stationary environment,
our algorithm enables faster learning than even using an accurate model built
with domain-specific knowledge of the non-stationarity.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2204.01464v2' target='_blank'>Value Gradient weighted Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Claas Voelcker, Victor Liao, Animesh Garg, Amir-massoud Farahmand</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-04-04 13:28:31</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) is a sample efficient technique to
obtain control policies, yet unavoidable modeling errors often lead performance
deterioration. The model in MBRL is often solely fitted to reconstruct
dynamics, state observations in particular, while the impact of model error on
the policy is not captured by the training objective. This leads to a mismatch
between the intended goal of MBRL, enabling good policy and value learning, and
the target of the loss function employed in practice, future state prediction.
Naive intuition would suggest that value-aware model learning would fix this
problem and, indeed, several solutions to this objective mismatch problem have
been proposed based on theoretical analysis. However, they tend to be inferior
in practice to commonly used maximum likelihood (MLE) based approaches. In this
paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel
method for value-aware model learning which improves the performance of MBRL in
challenging settings, such as small model capacity and the presence of
distracting state dimensions. We analyze both MLE and value-aware approaches
and demonstrate how they fail to account for exploration and the behavior of
function approximation when learning value-aware models and highlight the
additional goals that must be met to stabilize optimization in the deep
learning setting. We verify our analysis by showing that our loss function is
able to achieve high returns on the Mujoco benchmark suite while being more
robust than maximum likelihood based approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2203.09637v1' target='_blank'>Investigating Compounding Prediction Errors in Learned Dynamics Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nathan Lambert, Kristofer Pister, Roberto Calandra</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-03-17 22:24:38</h6>
<p class='card-text'>Accurately predicting the consequences of agents' actions is a key
prerequisite for planning in robotic control. Model-based reinforcement
learning (MBRL) is one paradigm which relies on the iterative learning and
prediction of state-action transitions to solve a task. Deep MBRL has become a
popular candidate, using a neural network to learn a dynamics model that
predicts with each pass from high-dimensional states to actions. These
"one-step" predictions are known to become inaccurate over longer horizons of
composed prediction - called the compounding error problem. Given the
prevalence of the compounding error problem in MBRL and related fields of
data-driven control, we set out to understand the properties of and conditions
causing these long-horizon errors. In this paper, we explore the effects of
subcomponents of a control problem on long term prediction error: including
choosing a system, collecting data, and training a model. These detailed
quantitative studies on simulated and real-world data show that the underlying
dynamics of a system are the strongest factor determining the shape and
magnitude of prediction error. Given a clearer understanding of compounding
prediction error, researchers can implement new types of models beyond
"one-step" that are more useful for control.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2202.09481v2' target='_blank'>TransDreamer: Reinforcement Learning with Transformer World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chang Chen, Yi-Fu Wu, Jaesik Yoon, Sungjin Ahn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-02-19 00:30:52</h6>
<p class='card-text'>The Dreamer agent provides various benefits of Model-Based Reinforcement
Learning (MBRL) such as sample efficiency, reusable knowledge, and safe
planning. However, its world model and policy networks inherit the limitations
of recurrent neural networks and thus an important question is how an MBRL
framework can benefit from the recent advances of transformers and what the
challenges are in doing so. In this paper, we propose a transformer-based MBRL
agent, called TransDreamer. We first introduce the Transformer State-Space
Model, a world model that leverages a transformer for dynamics predictions. We
then share this world model with a transformer-based policy network and obtain
stability in training a transformer-based RL agent. In experiments, we apply
the proposed model to 2D visual RL and 3D first-person visual RL tasks both
requiring long-range memory access for memory-based reasoning. We show that the
proposed model outperforms Dreamer in these complex tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2112.02817v2' target='_blank'>ED2: Environment Dynamics Decomposition World Models for Continuous
  Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianye Hao, Yifu Yuan, Cong Wang, Zhen Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-12-06 07:11:19</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) achieves significant sample
efficiency in practice in comparison to model-free RL, but its performance is
often limited by the existence of model prediction error. To reduce the model
error, standard MBRL approaches train a single well-designed network to fit the
entire environment dynamics, but this wastes rich information on multiple
sub-dynamics which can be modeled separately, allowing us to construct the
world model more accurately. In this paper, we propose the Environment Dynamics
Decomposition (ED2), a novel world model construction framework that models the
environment in a decomposing manner. ED2 contains two key components:
sub-dynamics discovery (SD2) and dynamics decomposition prediction (D2P). SD2
discovers the sub-dynamics in an environment automatically and then D2P
constructs the decomposed world model following the sub-dynamics. ED2 can be
easily combined with existing MBRL algorithms and empirical results show that
ED2 significantly reduces the model error, increases the sample efficiency, and
achieves higher asymptotic performance when combined with the state-of-the-art
MBRL algorithms on various continuous control tasks. Our code is open source
and available at https://github.com/ED2-source-code/ED2.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2110.14565v1' target='_blank'>DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with
  Prototypical Representations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fei Deng, Ingook Jang, Sungjin Ahn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-10-27 16:35:00</h6>
<p class='card-text'>Top-performing Model-Based Reinforcement Learning (MBRL) agents, such as
Dreamer, learn the world model by reconstructing the image observations. Hence,
they often fail to discard task-irrelevant details and struggle to handle
visual distractions. To address this issue, previous work has proposed to
contrastively learn the world model, but the performance tends to be inferior
in the absence of distractions. In this paper, we seek to enhance robustness to
distractions for MBRL agents. Specifically, we consider incorporating
prototypical representations, which have yielded more accurate and robust
results than contrastive approaches in computer vision. However, it remains
elusive how prototypical representations can benefit temporal dynamics learning
in MBRL, since they treat each image independently without capturing temporal
structures. To this end, we propose to learn the prototypes from the recurrent
states of the world model, thereby distilling temporal structures from past
observations and actions into the prototypes. The resulting model, DreamerPro,
successfully combines Dreamer with prototypes, making large performance gains
on the DeepMind Control suite both in the standard setting and when there are
complex background distractions. Code available at
https://github.com/fdeng18/dreamer-pro .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2110.13241v1' target='_blank'>Multitask Adaptation by Retrospective Exploration with Learned World
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Artem Zholus, Aleksandr I. Panov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-10-25 20:02:57</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) allows solving complex tasks in a
sample-efficient manner. However, no information is reused between the tasks.
In this work, we propose a meta-learned addressing model called RAMa that
provides training samples for the MBRL agent taken from continuously growing
task-agnostic storage. The model is trained to maximize the expected agent's
performance by selecting promising trajectories solving prior tasks from the
storage. We show that such retrospective exploration can accelerate the
learning process of the MBRL agent by better informing learned dynamics and
prompting agent with exploratory trajectories. We test the performance of our
approach on several domains from the DeepMind control suite, from Metaworld
multitask benchmark, and from our bespoke environment implemented with a
robotic NVIDIA Isaac simulator to test the ability of the model to act in a
photorealistic, ray-traced environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2110.03363v1' target='_blank'>Evaluating model-based planning and planner amortization for continuous
  control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arunkumar Byravan, Leonard Hasenclever, Piotr Trochim, Mehdi Mirza, Alessandro Davide Ialongo, Yuval Tassa, Jost Tobias Springenberg, Abbas Abdolmaleki, Nicolas Heess, Josh Merel, Martin Riedmiller</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-10-07 12:00:40</h6>
<p class='card-text'>There is a widespread intuition that model-based control methods should be
able to surpass the data efficiency of model-free approaches. In this paper we
attempt to evaluate this intuition on various challenging locomotion tasks. We
take a hybrid approach, combining model predictive control (MPC) with a learned
model and model-free policy learning; the learned policy serves as a proposal
for MPC. We find that well-tuned model-free agents are strong baselines even
for high DoF control problems but MPC with learned proposals and models
(trained on the fly or transferred from related tasks) can significantly
improve performance and data efficiency in hard multi-task/multi-goal settings.
Finally, we show that it is possible to distil a model-based planner into a
policy that amortizes the planning computation without any loss of performance.
Videos of agents performing different tasks can be seen at
https://sites.google.com/view/mbrl-amortization/home.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2108.00128v1' target='_blank'>Physics-informed Dyna-Style Model-Based Deep Reinforcement Learning for
  Dynamic Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xin-Yang Liu, Jian-Xun Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-07-31 02:19:36</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) is believed to have much higher
sample efficiency compared to model-free algorithms by learning a predictive
model of the environment. However, the performance of MBRL highly relies on the
quality of the learned model, which is usually built in a black-box manner and
may have poor predictive accuracy outside of the data distribution. The
deficiencies of the learned model may prevent the policy from being fully
optimized. Although some uncertainty analysis-based remedies have been proposed
to alleviate this issue, model bias still poses a great challenge for MBRL. In
this work, we propose to leverage the prior knowledge of underlying physics of
the environment, where the governing laws are (partially) known. In particular,
we developed a physics-informed MBRL framework, where governing equations and
physical constraints are utilized to inform the model learning and policy
search. By incorporating the prior information of the environment, the quality
of the learned model can be notably improved, while the required interactions
with the environment are significantly reduced, leading to better sample
efficiency and learning performance. The effectiveness and merit have been
demonstrated over a handful of classic control problems, where the environments
are governed by canonical ordinary/partial differential equations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2107.13790v1' target='_blank'>Non-Markovian Reinforcement Learning using Fractional Dynamics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gaurav Gupta, Chenzhong Yin, Jyotirmoy V. Deshmukh, Paul Bogdan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-07-29 07:35:13</h6>
<p class='card-text'>Reinforcement learning (RL) is a technique to learn the control policy for an
agent that interacts with a stochastic environment. In any given state, the
agent takes some action, and the environment determines the probability
distribution over the next state as well as gives the agent some reward. Most
RL algorithms typically assume that the environment satisfies Markov
assumptions (i.e. the probability distribution over the next state depends only
on the current state). In this paper, we propose a model-based RL technique for
a system that has non-Markovian dynamics. Such environments are common in many
real-world applications such as in human physiology, biological systems,
material science, and population dynamics. Model-based RL (MBRL) techniques
typically try to simultaneously learn a model of the environment from the data,
as well as try to identify an optimal policy for the learned model. We propose
a technique where the non-Markovianity of the system is modeled through a
fractional dynamical system. We show that we can quantify the difference in the
performance of an MBRL algorithm that uses bounded horizon model predictive
control from the optimal policy. Finally, we demonstrate our proposed framework
on a pharmacokinetic model of human blood glucose dynamics and show that our
fractional models can capture distant correlations on real-world datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2107.11587v1' target='_blank'>Model-based micro-data reinforcement learning: what are the crucial
  model properties and which model to choose?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bal√°zs K√©gl, Gabriel Hurtado, Albert Thomas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-07-24 11:38:25</h6>
<p class='card-text'>We contribute to micro-data model-based reinforcement learning (MBRL) by
rigorously comparing popular generative models using a fixed (random shooting)
control agent. We find that on an environment that requires multimodal
posterior predictives, mixture density nets outperform all other models by a
large margin. When multimodality is not required, our surprising finding is
that we do not need probabilistic posterior predictives: deterministic models
are on par, in fact they consistently (although non-significantly) outperform
their probabilistic counterparts. We also found that heteroscedasticity at
training time, perhaps acting as a regularizer, improves predictions at longer
horizons. At the methodological side, we design metrics and an experimental
protocol which can be used to evaluate the various models, predicting their
asymptotic performance when using them on the control problem. Using this
framework, we improve the state-of-the-art sample complexity of MBRL on Acrobot
by two to four folds, using an aggressive training schedule which is outside of
the hyperparameter interval usually considered</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2106.12194v2' target='_blank'>Uncertainty-Aware Model-Based Reinforcement Learning with Application to
  Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingda Wu, Zhiyu Huang, Chen Lv</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-06-23 06:55:14</h6>
<p class='card-text'>To further improve the learning efficiency and performance of reinforcement
learning (RL), in this paper we propose a novel uncertainty-aware model-based
RL (UA-MBRL) framework, and then implement and validate it in autonomous
driving under various task scenarios. First, an action-conditioned ensemble
model with the ability of uncertainty assessment is established as the virtual
environment model. Then, a novel uncertainty-aware model-based RL framework is
developed based on the adaptive truncation approach, providing virtual
interactions between the agent and environment model, and improving RL's
training efficiency and performance. The developed algorithms are then
implemented in end-to-end autonomous vehicle control tasks, validated and
compared with state-of-the-art methods under various driving scenarios. The
validation results suggest that the proposed UA-MBRL method surpasses the
existing model-based and model-free RL approaches, in terms of learning
efficiency and achieved performance. The results also demonstrate the good
ability of the proposed method with respect to the adaptiveness and robustness,
under various autonomous driving scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2106.07156v1' target='_blank'>Temporal Predictive Coding For Model-Based Planning In Latent Space</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tung Nguyen, Rui Shu, Tuan Pham, Hung Bui, Stefano Ermon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-06-14 04:31:15</h6>
<p class='card-text'>High-dimensional observations are a major challenge in the application of
model-based reinforcement learning (MBRL) to real-world environments. To handle
high-dimensional sensory inputs, existing approaches use representation
learning to map high-dimensional observations into a lower-dimensional latent
space that is more amenable to dynamics estimation and planning. In this work,
we present an information-theoretic approach that employs temporal predictive
coding to encode elements in the environment that can be predicted across time.
Since this approach focuses on encoding temporally-predictable information, we
implicitly prioritize the encoding of task-relevant components over nuisance
information within the environment that are provably task-irrelevant. By
learning this representation in conjunction with a recurrent state space model,
we can then perform planning in latent space. We evaluate our model on a
challenging modification of standard DMControl tasks where the background is
replaced with natural videos that contain complex but irrelevant information to
the planning task. Our experiments show that our model is superior to existing
methods in the challenging complex-background setting while remaining
competitive with current state-of-the-art models in the standard setting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2105.05716v6' target='_blank'>Acting upon Imagination: when to trust imagined trajectories in model
  based reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Adrian Remonda, Eduardo Veas, Granit Luzhnica</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-05-12 15:04:07</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) aims to learn model(s) of the
environment dynamics that can predict the outcome of its actions. Forward
application of the model yields so called imagined trajectories (sequences of
action, predicted state-reward) used to optimize the set of candidate actions
that maximize expected reward. The outcome, an ideal imagined trajectory or
plan, is imperfect and typically MBRL relies on model predictive control (MPC)
to overcome this by continuously re-planning from scratch, incurring thus major
computational cost and increasing complexity in tasks with longer receding
horizon. We propose uncertainty estimation methods for online evaluation of
imagined trajectories to assess whether further planned actions can be trusted
to deliver acceptable reward. These methods include comparing the error after
performing the last action with the standard expected error and using model
uncertainty to assess the deviation from expected outcomes. Additionally, we
introduce methods that exploit the forward propagation of the dynamics model to
evaluate if the remainder of the plan aligns with expected results and assess
the remainder of the plan in terms of the expected reward. Our experiments
demonstrate the effectiveness of the proposed uncertainty estimation methods by
applying them to avoid unnecessary trajectory replanning in a shooting MBRL
setting. Results highlight significant reduction on computational costs without
sacrificing performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2104.13685v1' target='_blank'>Adaptive Optics control using Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jalo Nousiainen, Chang Rajani, Markus Kasper, Tapio Helin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-04-28 10:13:45</h6>
<p class='card-text'>Reinforcement Learning (RL) presents a new approach for controlling Adaptive
Optics (AO) systems for Astronomy. It promises to effectively cope with some
aspects often hampering AO performance such as temporal delay or calibration
errors. We formulate the AO control loop as a model-based RL problem (MBRL) and
apply it in numerical simulations to a simple Shack-Hartmann Sensor (SHS) based
AO system with 24 resolution elements across the aperture. The simulations show
that MBRL controlled AO predicts the temporal evolution of turbulence and
adjusts to mis-registration between deformable mirror and SHS which is a
typical calibration issue in AO. The method learns continuously on timescales
of some seconds and is therefore capable of automatically adjusting to changing
conditions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2104.10159v1' target='_blank'>MBRL-Lib: A Modular Library for Model-based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Luis Pineda, Brandon Amos, Amy Zhang, Nathan O. Lambert, Roberto Calandra</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-04-20 17:58:22</h6>
<p class='card-text'>Model-based reinforcement learning is a compelling framework for
data-efficient learning of agents that interact with the world. This family of
algorithms has many subcomponents that need to be carefully selected and tuned.
As a result the entry-bar for researchers to approach the field and to deploy
it in real-world tasks can be daunting. In this paper, we present MBRL-Lib -- a
machine learning library for model-based reinforcement learning in continuous
state-action spaces based on PyTorch. MBRL-Lib is designed as a platform for
both researchers, to easily develop, debug and compare new algorithms, and
non-expert user, to lower the entry-bar of deploying state-of-the-art
algorithms. MBRL-Lib is open-source at
https://github.com/facebookresearch/mbrl-lib.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2104.08543v1' target='_blank'>Planning with Expectation Models for Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Katya Kudashkina, Yi Wan, Abhishek Naik, Richard S. Sutton</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-04-17 13:37:14</h6>
<p class='card-text'>In model-based reinforcement learning (MBRL), Wan et al. (2019) showed
conditions under which the environment model could produce the expectation of
the next feature vector rather than the full distribution, or a sample thereof,
with no loss in planning performance. Such expectation models are of interest
when the environment is stochastic and non-stationary, and the model is
approximate, such as when it is learned using function approximation. In these
cases a full distribution model may be impractical and a sample model may be
either more expensive computationally or of high variance. Wan et al.
considered only planning for prediction to evaluate a fixed policy. In this
paper, we treat the control case - planning to improve and find a good
approximate policy. We prove that planning with an expectation model must
update a state-value function, not an action-value function as previously
suggested (e.g., Sorg & Singh, 2010). This opens the question of how planning
influences action selections. We consider three strategies for this and present
general MBRL algorithms for each. We identify the strengths and weaknesses of
these algorithms in computational experiments. Our algorithms and experiments
are the first to treat MBRL with expectation models in a general setting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2104.08171v4' target='_blank'>Safe Exploration in Model-based Reinforcement Learning using Control
  Barrier Functions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Max H. Cohen, Calin Belta</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-04-16 15:29:58</h6>
<p class='card-text'>This paper develops a model-based reinforcement learning (MBRL) framework for
learning online the value function of an infinite-horizon optimal control
problem while obeying safety constraints expressed as control barrier functions
(CBFs). Our approach is facilitated by the development of a novel class of
CBFs, termed Lyapunov-like CBFs (LCBFs), that retain the beneficial properties
of CBFs for developing minimally-invasive safe control policies while also
possessing desirable Lyapunov-like qualities such as positive
semi-definiteness. We show how these LCBFs can be used to augment a
learning-based control policy to guarantee safety and then leverage this
approach to develop a safe exploration framework in a MBRL setting. We
demonstrate that our approach can handle more general safety constraints than
comparative methods via numerical examples.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2103.12999v2' target='_blank'>Discriminator Augmented Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Behzad Haghgoo, Allan Zhou, Archit Sharma, Chelsea Finn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-03-24 06:01:55</h6>
<p class='card-text'>By planning through a learned dynamics model, model-based reinforcement
learning (MBRL) offers the prospect of good performance with little environment
interaction. However, it is common in practice for the learned model to be
inaccurate, impairing planning and leading to poor performance. This paper aims
to improve planning with an importance sampling framework that accounts and
corrects for discrepancy between the true and learned dynamics. This framework
also motivates an alternative objective for fitting the dynamics model: to
minimize the variance of value estimation during planning. We derive and
implement this objective, which encourages better prediction on trajectories
with larger returns. We observe empirically that our approach improves the
performance of current MBRL algorithms on two stochastic control problems, and
provide a theoretical basis for our method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2103.10369v1' target='_blank'>Combining Pessimism with Optimism for Robust and Efficient Model-Based
  Deep Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sebastian Curi, Ilija Bogunovic, Andreas Krause</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-03-18 16:50:17</h6>
<p class='card-text'>In real-world tasks, reinforcement learning (RL) agents frequently encounter
situations that are not present during training time. To ensure reliable
performance, the RL agents need to exhibit robustness against worst-case
situations. The robust RL framework addresses this challenge via a worst-case
optimization between an agent and an adversary. Previous robust RL algorithms
are either sample inefficient, lack robustness guarantees, or do not scale to
large problems. We propose the Robust Hallucinated Upper-Confidence RL
(RH-UCRL) algorithm to provably solve this problem while attaining near-optimal
sample complexity guarantees. RH-UCRL is a model-based reinforcement learning
(MBRL) algorithm that effectively distinguishes between epistemic and aleatoric
uncertainty and efficiently explores both the agent and adversary decision
spaces during policy learning. We scale RH-UCRL to complex tasks via neural
networks ensemble models as well as neural network policies. Experimentally, we
demonstrate that RH-UCRL outperforms other robust deep RL algorithms in a
variety of adversarial environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2102.13651v1' target='_blank'>On the Importance of Hyperparameter Optimization for Model-based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Baohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, Andr√© Biedenkapp, Kurtland Chua, Frank Hutter, Roberto Calandra</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-02-26 18:57:47</h6>
<p class='card-text'>Model-based Reinforcement Learning (MBRL) is a promising framework for
learning control in a data-efficient manner. MBRL algorithms can be fairly
complex due to the separate dynamics modeling and the subsequent planning
algorithm, and as a result, they often possess tens of hyperparameters and
architectural choices. For this reason, MBRL typically requires significant
human expertise before it can be applied to new problems and domains. To
alleviate this problem, we propose to use automatic hyperparameter optimization
(HPO). We demonstrate that this problem can be tackled effectively with
automated HPO, which we demonstrate to yield significantly improved performance
compared to human experts. In addition, we show that tuning of several MBRL
hyperparameters dynamically, i.e. during the training itself, further improves
the performance compared to using static hyperparameters which are kept fixed
for the whole training. Finally, our experiments provide valuable insights into
the effects of several hyperparameters, such as plan horizon or learning rate
and their influence on the stability of training and resulting rewards.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2102.09850v2' target='_blank'>Model-Invariant State Abstractions for Model-Based Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Manan Tomar, Amy Zhang, Roberto Calandra, Matthew E. Taylor, Joelle Pineau</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-02-19 10:37:54</h6>
<p class='card-text'>Accuracy and generalization of dynamics models is key to the success of
model-based reinforcement learning (MBRL). As the complexity of tasks
increases, so does the sample inefficiency of learning accurate dynamics
models. However, many complex tasks also exhibit sparsity in the dynamics,
i.e., actions have only a local effect on the system dynamics. In this paper,
we exploit this property with a causal invariance perspective in the
single-task setting, introducing a new type of state abstraction called
\textit{model-invariance}. Unlike previous forms of state abstractions, a
model-invariance state abstraction leverages causal sparsity over state
variables. This allows for compositional generalization to unseen states,
something that non-factored forms of state abstractions cannot do. We prove
that an optimal policy can be learned over this model-invariance state
abstraction and show improved generalization in a simple toy domain. Next, we
propose a practical method to approximately learn a model-invariant
representation for complex domains and validate our approach by showing
improved modelling performance over standard maximum likelihood approaches on
challenging tasks, such as the MuJoCo-based Humanoid. Finally, within the MBRL
setting we show strong performance gains with respect to sample efficiency
across a host of other continuous control tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2102.04764v3' target='_blank'>Continuous-Time Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:√áaƒüatay Yƒ±ldƒ±z, Markus Heinonen, Harri L√§hdesm√§ki</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-02-09 11:30:19</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) approaches rely on discrete-time
state transition models whereas physical systems and the vast majority of
control tasks operate in continuous-time. To avoid time-discretization
approximation of the underlying process, we propose a continuous-time MBRL
framework based on a novel actor-critic method. Our approach also infers the
unknown state evolution differentials with Bayesian neural ordinary
differential equations (ODE) to account for epistemic uncertainty. We implement
and test our method on a new ODE-RL suite that explicitly solves
continuous-time control systems. Our experiments illustrate that the model is
robust against irregular and noisy data, is sample-efficient, and can solve
control problems which pose challenges to discrete-time MBRL methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2101.12115v4' target='_blank'>Model-Based Policy Search Using Monte Carlo Gradient Estimation with
  Real Systems Application</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fabio Amadio, Alberto Dalla Libera, Riccardo Antonello, Daniel Nikovski, Ruggero Carli, Diego Romeres</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-01-28 17:01:15</h6>
<p class='card-text'>In this paper, we present a Model-Based Reinforcement Learning (MBRL)
algorithm named \emph{Monte Carlo Probabilistic Inference for Learning COntrol}
(MC-PILCO). The algorithm relies on Gaussian Processes (GPs) to model the
system dynamics and on a Monte Carlo approach to estimate the policy gradient.
This defines a framework in which we ablate the choice of the following
components: (i) the selection of the cost function, (ii) the optimization of
policies using dropout, (iii) an improved data efficiency through the use of
structured kernels in the GP models. The combination of the aforementioned
aspects affects dramatically the performance of MC-PILCO. Numerical comparisons
in a simulated cart-pole environment show that MC-PILCO exhibits better data
efficiency and control performance w.r.t. state-of-the-art GP-based MBRL
algorithms. Finally, we apply MC-PILCO to real systems, considering in
particular systems with partially measurable states. We discuss the importance
of modeling both the measurement system and the state estimators during policy
optimization. The effectiveness of the proposed solutions has been tested in
simulation and on two real systems, a Furuta pendulum and a ball-and-plate rig.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2101.08740v1' target='_blank'>Model-based Policy Search for Partially Measurable Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fabio Amadio, Alberto Dalla Libera, Ruggero Carli, Daniel Nikovski, Diego Romeres</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-01-21 17:39:22</h6>
<p class='card-text'>In this paper, we propose a Model-Based Reinforcement Learning (MBRL)
algorithm for Partially Measurable Systems (PMS), i.e., systems where the state
can not be directly measured, but must be estimated through proper state
observers. The proposed algorithm, named Monte Carlo Probabilistic Inference
for Learning COntrol for Partially Measurable Systems (MC-PILCO4PMS), relies on
Gaussian Processes (GPs) to model the system dynamics, and on a Monte Carlo
approach to update the policy parameters. W.r.t. previous GP-based MBRL
algorithms, MC-PILCO4PMS models explicitly the presence of state observers
during policy optimization, allowing to deal PMS. The effectiveness of the
proposed algorithm has been tested both in simulation and in two real systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2012.04603v1' target='_blank'>Models, Pixels, and Rewards: Evaluating Design Trade-offs in Visual
  Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohammad Babaeizadeh, Mohammad Taghi Saffar, Danijar Hafner, Harini Kannan, Chelsea Finn, Sergey Levine, Dumitru Erhan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-12-08 18:03:21</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) methods have shown strong sample
efficiency and performance across a variety of tasks, including when faced with
high-dimensional visual observations. These methods learn to predict the
environment dynamics and expected reward from interaction and use this
predictive model to plan and perform the task. However, MBRL methods vary in
their fundamental design choices, and there is no strong consensus in the
literature on how these design decisions affect performance. In this paper, we
study a number of design decisions for the predictive model in visual MBRL
algorithms, focusing specifically on methods that use a predictive model for
planning. We find that a range of design decisions that are often considered
crucial, such as the use of latent spaces, have little effect on task
performance. A big exception to this finding is that predicting future
observations (i.e., images) leads to significant task performance improvement
compared to only predicting rewards. We also empirically find that image
prediction accuracy, somewhat surprisingly, correlates more strongly with
downstream task performance than reward prediction accuracy. We show how this
phenomenon is related to exploration and how some of the lower-scoring models
on standard benchmarks (that require exploration) will perform the same as the
best-performing models when trained on the same training data. Simultaneously,
in the absence of exploration, models that fit the data better usually perform
better on the downstream task as well, but surprisingly, these are often not
the same models that perform the best when learning and exploring from scratch.
These findings suggest that performance and exploration place important and
potentially contradictory requirements on the model.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2011.10605v2' target='_blank'>Nested Mixture of Experts: Cooperative and Competitive Learning of
  Hybrid Dynamical System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junhyeok Ahn, Luis Sentis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-11-20 19:36:45</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) algorithms can attain significant
sample efficiency but require an appropriate network structure to represent
system dynamics. Current approaches include white-box modeling using analytic
parameterizations and black-box modeling using deep neural networks. However,
both can suffer from a bias-variance trade-off in the learning process, and
neither provides a structured method for injecting domain knowledge into the
network. As an alternative, gray-box modeling leverages prior knowledge in
neural network training but only for simple systems. In this paper, we devise a
nested mixture of experts (NMOE) for representing and learning hybrid dynamical
systems. An NMOE combines both white-box and black-box models while optimizing
bias-variance trade-off. Moreover, an NMOE provides a structured method for
incorporating various types of prior knowledge by training the associative
experts cooperatively or competitively. The prior knowledge includes
information on robots' physical contacts with the environments as well as their
kinematic and dynamic properties. In this paper, we demonstrate how to
incorporate prior knowledge into our NMOE in various continuous control
domains, including hybrid dynamical systems. We also show the effectiveness of
our method in terms of data-efficiency, generalization to unseen data, and
bias-variance trade-off. Finally, we evaluate our NMOE using an MBRL setup,
where the model is integrated with a model-based controller and trained online.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2011.04021v2' target='_blank'>On the role of planning in model-based deep reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jessica B. Hamrick, Abram L. Friesen, Feryal Behbahani, Arthur Guez, Fabio Viola, Sims Witherspoon, Thomas Anthony, Lars Buesing, Petar Veliƒçkoviƒá, Th√©ophane Weber</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-11-08 16:55:16</h6>
<p class='card-text'>Model-based planning is often thought to be necessary for deep, careful
reasoning and generalization in artificial agents. While recent successes of
model-based reinforcement learning (MBRL) with deep function approximation have
strengthened this hypothesis, the resulting diversity of model-based methods
has also made it difficult to track which components drive success and why. In
this paper, we seek to disentangle the contributions of recent methods by
focusing on three questions: (1) How does planning benefit MBRL agents? (2)
Within planning, what choices drive performance? (3) To what extent does
planning improve generalization? To answer these questions, we study the
performance of MuZero (Schrittwieser et al., 2019), a state-of-the-art MBRL
algorithm with strong connections and overlapping components with many other
MBRL algorithms. We perform a number of interventions and ablations of MuZero
across a wide range of environments, including control tasks, Atari, and 9x9
Go. Our results suggest the following: (1) Planning is most useful in the
learning process, both for policy updates and for providing a more useful data
distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as
performant as more complex methods, except in the most difficult reasoning
tasks. (3) Planning alone is insufficient to drive strong generalization. These
results indicate where and how to utilize planning in reinforcement learning
settings, and highlight a number of open questions for future MBRL research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2011.03615v1' target='_blank'>Single and Multi-Agent Deep Reinforcement Learning for AI-Enabled
  Wireless Networks: A Tutorial</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amal Feriani, Ekram Hossain</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-11-06 22:12:40</h6>
<p class='card-text'>Deep Reinforcement Learning (DRL) has recently witnessed significant advances
that have led to multiple successes in solving sequential decision-making
problems in various domains, particularly in wireless communications. The
future sixth-generation (6G) networks are expected to provide scalable,
low-latency, ultra-reliable services empowered by the application of
data-driven Artificial Intelligence (AI). The key enabling technologies of
future 6G networks, such as intelligent meta-surfaces, aerial networks, and AI
at the edge, involve more than one agent which motivates the importance of
multi-agent learning techniques. Furthermore, cooperation is central to
establishing self-organizing, self-sustaining, and decentralized networks. In
this context, this tutorial focuses on the role of DRL with an emphasis on deep
Multi-Agent Reinforcement Learning (MARL) for AI-enabled 6G networks. The first
part of this paper will present a clear overview of the mathematical frameworks
for single-agent RL and MARL. The main idea of this work is to motivate the
application of RL beyond the model-free perspective which was extensively
adopted in recent years. Thus, we provide a selective description of RL
algorithms such as Model-Based RL (MBRL) and cooperative MARL and we highlight
their potential applications in 6G wireless networks. Finally, we overview the
state-of-the-art of MARL in fields such as Mobile Edge Computing (MEC),
Unmanned Aerial Vehicles (UAV) networks, and cell-free massive MIMO, and
identify promising future research directions. We expect this tutorial to
stimulate more research endeavors to build scalable and decentralized systems
based on MARL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2011.01734v1' target='_blank'>Differentiable Physics Models for Real-world Offline Model-based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michael Lutter, Johannes Silberbauer, Joe Watson, Jan Peters</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-11-03 14:37:53</h6>
<p class='card-text'>A limitation of model-based reinforcement learning (MBRL) is the exploitation
of errors in the learned models. Black-box models can fit complex dynamics with
high fidelity, but their behavior is undefined outside of the data
distribution.Physics-based models are better at extrapolating, due to the
general validity of their informed structure, but underfit in the real world
due to the presence of unmodeled phenomena. In this work, we demonstrate
experimentally that for the offline model-based reinforcement learning setting,
physics-based models can be beneficial compared to high-capacity function
approximators if the mechanical structure is known. Physics-based models can
learn to perform the ball in a cup (BiC) task on a physical manipulator using
only 4 minutes of sampled data using offline MBRL. We find that black-box
models consistently produce unviable policies for BiC as all predicted
trajectories diverge to physically impossible state, despite having access to
more data than the physics-based model. In addition, we generalize the approach
of physics parameter identification from modeling holonomic multi-body systems
to systems with nonholonomic dynamics using end-to-end automatic
differentiation.
  Videos: https://sites.google.com/view/ball-in-a-cup-in-4-minutes/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2010.12914v3' target='_blank'>Planning with Exploration: Addressing Dynamics Bottleneck in Model-based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiyao Wang, Junge Zhang, Wenzhen Huang, Qiyue Yin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-10-24 15:29:02</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) is believed to have higher sample
efficiency compared with model-free reinforcement learning (MFRL). However,
MBRL is plagued by dynamics bottleneck dilemma. Dynamics bottleneck dilemma is
the phenomenon that the performance of the algorithm falls into the local
optimum instead of increasing when the interaction step with the environment
increases, which means more data can not bring better performance. In this
paper, we find that the trajectory reward estimation error is the main reason
that causes dynamics bottleneck dilemma through theoretical analysis. We give
an upper bound of the trajectory reward estimation error and point out that
increasing the agent's exploration ability is the key to reduce trajectory
reward estimation error, thereby alleviating dynamics bottleneck dilemma.
Motivated by this, a model-based control method combined with exploration named
MOdel-based Progressive Entropy-based Exploration (MOPE2) is proposed. We
conduct experiments on several complex continuous control benchmark tasks. The
results verify that MOPE2 can effectively alleviate dynamics bottleneck dilemma
and have higher sample efficiency than previous MBRL and MFRL algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2010.09832v1' target='_blank'>Dream and Search to Control: Latent Space Planning for Continuous
  Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anurag Koul, Varun V. Kumar, Alan Fern, Somdeb Majumdar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-10-19 20:10:51</h6>
<p class='card-text'>Learning and planning with latent space dynamics has been shown to be useful
for sample efficiency in model-based reinforcement learning (MBRL) for discrete
and continuous control tasks. In particular, recent work, for discrete action
spaces, demonstrated the effectiveness of latent-space planning via Monte-Carlo
Tree Search (MCTS) for bootstrapping MBRL during learning and at test time.
However, the potential gains from latent-space tree search have not yet been
demonstrated for environments with continuous action spaces. In this work, we
propose and explore an MBRL approach for continuous action spaces based on
tree-based planning over learned latent dynamics. We show that it is possible
to demonstrate the types of bootstrapping benefits as previously shown for
discrete spaces. In particular, the approach achieves improved sample
efficiency and performance on a majority of challenging continuous-control
benchmarks compared to the state-of-the-art.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2010.08169v3' target='_blank'>Uncertainty-aware Contact-safe Model-based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Cheng-Yu Kuo, Andreas Schaarschmidt, Yunduan Cui, Tamim Asfour, Takamitsu Matsubara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-10-16 05:11:25</h6>
<p class='card-text'>This letter presents contact-safe Model-based Reinforcement Learning (MBRL)
for robot applications that achieves contact-safe behaviors in the learning
process. In typical MBRL, we cannot expect the data-driven model to generate
accurate and reliable policies to the intended robotic tasks during the
learning process due to sample scarcity. Operating these unreliable policies in
a contact-rich environment could cause damage to the robot and its
surroundings. To alleviate the risk of causing damage through unexpected
intensive physical contacts, we present the contact-safe MBRL that associates
the probabilistic Model Predictive Control's (pMPC) control limits with the
model uncertainty so that the allowed acceleration of controlled behavior is
adjusted according to learning progress. Control planning with such
uncertainty-aware control limits is formulated as a deterministic MPC problem
using a computation-efficient approximated GP dynamics and an approximated
inference technique. Our approach's effectiveness is evaluated through bowl
mixing tasks with simulated and real robots, scooping tasks with a real robot
as examples of contact-rich manipulation skills. (video:
https://youtu.be/sdhHP3NhYi0)</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2009.11997v2' target='_blank'>Continual Model-Based Reinforcement Learning with Hypernetworks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yizhou Huang, Kevin Xie, Homanga Bharadhwaj, Florian Shkurti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-09-25 01:46:26</h6>
<p class='card-text'>Effective planning in model-based reinforcement learning (MBRL) and
model-predictive control (MPC) relies on the accuracy of the learned dynamics
model. In many instances of MBRL and MPC, this model is assumed to be
stationary and is periodically re-trained from scratch on state transition
experience collected from the beginning of environment interactions. This
implies that the time required to train the dynamics model - and the pause
required between plan executions - grows linearly with the size of the
collected experience. We argue that this is too slow for lifelong robot
learning and propose HyperCRL, a method that continually learns the encountered
dynamics in a sequence of tasks using task-conditional hypernetworks. Our
method has three main attributes: first, it includes dynamics learning sessions
that do not revisit training data from previous tasks, so it only needs to
store the most recent fixed-size portion of the state transition experience;
second, it uses fixed-capacity hypernetworks to represent non-stationary and
task-aware dynamics; third, it outperforms existing continual learning
alternatives that rely on fixed-capacity networks, and does competitively with
baselines that remember an ever increasing coreset of past experience. We show
that HyperCRL is effective in continual model-based reinforcement learning in
robot locomotion and manipulation scenarios, such as tasks involving pushing
and door opening. Our project website with videos is at this link
https://rvl.cs.toronto.edu/blog/2020/hypercrl</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2007.14535v2' target='_blank'>Dreaming: Model-based Reinforcement Learning by Latent Imagination
  without Reconstruction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Masashi Okada, Tadahiro Taniguchi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-07-29 00:14:40</h6>
<p class='card-text'>In the present paper, we propose a decoder-free extension of Dreamer, a
leading model-based reinforcement learning (MBRL) method from pixels. Dreamer
is a sample- and cost-efficient solution to robot learning, as it is used to
train latent state-space models based on a variational autoencoder and to
conduct policy optimization by latent trajectory imagination. However, this
autoencoding based approach often causes object vanishing, in which the
autoencoder fails to perceives key objects for solving control tasks, and thus
significantly limiting Dreamer's potential. This work aims to relieve this
Dreamer's bottleneck and enhance its performance by means of removing the
decoder. For this purpose, we firstly derive a likelihood-free and InfoMax
objective of contrastive learning from the evidence lower bound of Dreamer.
Secondly, we incorporate two components, (i) independent linear dynamics and
(ii) the random crop data augmentation, to the learning scheme so as to improve
the training performance. In comparison to Dreamer and other recent model-free
reinforcement learning methods, our newly devised Dreamer with InfoMax and
without generative decoder (Dreaming) achieves the best scores on 5 difficult
simulated robotics tasks, in which Dreamer suffers from object vanishing.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2006.09234v1' target='_blank'>Model Embedding Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoyu Tan, Chao Qu, Junwu Xiong, James Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-06-16 15:10:28</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) has shown its advantages in
sample-efficiency over model-free reinforcement learning (MFRL). Despite the
impressive results it achieves, it still faces a trade-off between the ease of
data generation and model bias. In this paper, we propose a simple and elegant
model-embedding model-based reinforcement learning (MEMB) algorithm in the
framework of the probabilistic reinforcement learning. To balance the
sample-efficiency and model bias, we exploit both real and imaginary data in
the training. In particular, we embed the model in the policy update and learn
$Q$ and $V$ functions from the real data set. We provide the theoretical
analysis of MEMB with the Lipschitz continuity assumption on the model and
policy. At last, we evaluate MEMB on several benchmarks and demonstrate our
algorithm can achieve state-of-the-art performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2004.07804v2' target='_blank'>A Game Theoretic Framework for Model Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aravind Rajeswaran, Igor Mordatch, Vikash Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-04-16 17:51:45</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) has recently gained immense
interest due to its potential for sample efficiency and ability to incorporate
off-policy data. However, designing stable and efficient MBRL algorithms using
rich function approximators have remained challenging. To help expose the
practical challenges in MBRL and simplify algorithm design from the lens of
abstraction, we develop a new framework that casts MBRL as a game between: (1)
a policy player, which attempts to maximize rewards under the learned model;
(2) a model player, which attempts to fit the real-world data collected by the
policy player. For algorithm development, we construct a Stackelberg game
between the two players, and show that it can be solved with approximate
bi-level optimization. This gives rise to two natural families of algorithms
for MBRL based on which player is chosen as the leader in the Stackelberg game.
Together, they encapsulate, unify, and generalize many previous MBRL
algorithms. Furthermore, our framework is consistent with and provides a clear
basis for heuristics known to be important in practice from prior works.
Finally, through experiments we validate that our proposed algorithms are
highly sample efficient, match the asymptotic performance of model-free policy
gradient, and scale gracefully to high-dimensional tasks like dexterous hand
manipulation. Additional details and code can be obtained from the project page
at https://sites.google.com/view/mbrl-game</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2003.04663v2' target='_blank'>Fast Online Adaptation in Robotics through Meta-Learning Embeddings of
  Simulated Priors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rituraj Kaushik, Timoth√©e Anne, Jean-Baptiste Mouret</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-03-10 12:37:52</h6>
<p class='card-text'>Meta-learning algorithms can accelerate the model-based reinforcement
learning (MBRL) algorithms by finding an initial set of parameters for the
dynamical model such that the model can be trained to match the actual dynamics
of the system with only a few data-points. However, in the real world, a robot
might encounter any situation starting from motor failures to finding itself in
a rocky terrain where the dynamics of the robot can be significantly different
from one another. In this paper, first, we show that when meta-training
situations (the prior situations) have such diverse dynamics, using a single
set of meta-trained parameters as a starting point still requires a large
number of observations from the real system to learn a useful model of the
dynamics. Second, we propose an algorithm called FAMLE that mitigates this
limitation by meta-training several initial starting points (i.e., initial
parameters) for training the model and allows the robot to select the most
suitable starting point to adapt the model to the current situation with only a
few gradient steps. We compare FAMLE to MBRL, MBRL with a meta-trained model
with MAML, and model-free policy search algorithm PPO for various simulated and
real robotic tasks, and show that FAMLE allows the robots to adapt to novel
damages in significantly fewer time-steps than the baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2003.00370v1' target='_blank'>PlaNet of the Bayesians: Reconsidering and Improving Deep Planning
  Network by Incorporating Bayesian Inference</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Masashi Okada, Norio Kosaka, Tadahiro Taniguchi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-03-01 00:46:36</h6>
<p class='card-text'>In the present paper, we propose an extension of the Deep Planning Network
(PlaNet), also referred to as PlaNet of the Bayesians (PlaNet-Bayes). There has
been a growing demand in model predictive control (MPC) in partially observable
environments in which complete information is unavailable because of, for
example, lack of expensive sensors. PlaNet is a promising solution to realize
such latent MPC, as it is used to train state-space models via model-based
reinforcement learning (MBRL) and to conduct planning in the latent space.
However, recent state-of-the-art strategies mentioned in MBRR literature, such
as involving uncertainty into training and planning, have not been considered,
significantly suppressing the training performance. The proposed extension is
to make PlaNet uncertainty-aware on the basis of Bayesian inference, in which
both model and action uncertainty are incorporated. Uncertainty in latent
models is represented using a neural network ensemble to approximately infer
model posteriors. The ensemble of optimal action candidates is also employed to
capture multimodal uncertainty in the optimality. The concept of the action
ensemble relies on a general variational inference MPC (VI-MPC) framework and
its instance, probabilistic action ensemble with trajectory sampling (PaETS).
In this paper, we extend VI-MPC and PaETS, which have been originally
introduced in previous literature, to address partially observable cases. We
experimentally compare the performances on continuous control tasks, and
conclude that our method can consistently improve the asymptotic performance
compared with PlaNet.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2003.00030v2' target='_blank'>Policy-Aware Model Learning for Policy Gradient Methods</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Romina Abachi, Mohammad Ghavamzadeh, Amir-massoud Farahmand</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-02-28 19:18:18</h6>
<p class='card-text'>This paper considers the problem of learning a model in model-based
reinforcement learning (MBRL). We examine how the planning module of an MBRL
algorithm uses the model, and propose that the model learning module should
incorporate the way the planner is going to use the model. This is in contrast
to conventional model learning approaches, such as those based on maximum
likelihood estimate, that learn a predictive model of the environment without
explicitly considering the interaction of the model and the planner. We focus
on policy gradient type of planning algorithms and derive new loss functions
for model learning that incorporate how the planner uses the model. We call
this approach Policy-Aware Model Learning (PAML). We theoretically analyze a
generic model-based policy gradient algorithm and provide a convergence
guarantee for the optimized policy. We also empirically evaluate PAML on some
benchmark problems, showing promising results.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2002.04523v3' target='_blank'>Objective Mismatch in Model-based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nathan Lambert, Brandon Amos, Omry Yadan, Roberto Calandra</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-02-11 16:26:07</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) has been shown to be a powerful
framework for data-efficiently learning control of continuous tasks. Recent
work in MBRL has mostly focused on using more advanced function approximators
and planning schemes, with little development of the general framework. In this
paper, we identify a fundamental issue of the standard MBRL framework -- what
we call the objective mismatch issue. Objective mismatch arises when one
objective is optimized in the hope that a second, often uncorrelated, metric
will also be optimized. In the context of MBRL, we characterize the objective
mismatch between training the forward dynamics model w.r.t.~the likelihood of
the one-step ahead prediction, and the overall goal of improving performance on
a downstream control task. For example, this issue can emerge with the
realization that dynamics models effective for a specific task do not
necessarily need to be globally accurate, and vice versa globally accurate
models might not be sufficiently accurate locally to obtain good control
performance on a specific task. In our experiments, we study this objective
mismatch issue and demonstrate that the likelihood of one-step ahead
predictions is not always correlated with control performance. This observation
highlights a critical limitation in the MBRL framework which will require
further research to be fully understood and addressed. We propose an initial
method to mitigate the mismatch issue by re-weighting dynamics model training.
Building on it, we conclude with a discussion about other potential directions
of research for addressing this issue.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2002.02693v1' target='_blank'>Ready Policy One: World Building Through Active Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Philip Ball, Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, Stephen Roberts</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-02-07 09:57:53</h6>
<p class='card-text'>Model-Based Reinforcement Learning (MBRL) offers a promising direction for
sample efficient learning, often achieving state of the art results for
continuous control tasks. However, many existing MBRL methods rely on combining
greedy policies with exploration heuristics, and even those which utilize
principled exploration bonuses construct dual objectives in an ad hoc fashion.
In this paper we introduce Ready Policy One (RP1), a framework that views MBRL
as an active learning problem, where we aim to improve the world model in the
fewest samples possible. RP1 achieves this by utilizing a hybrid objective
function, which crucially adapts during optimization, allowing the algorithm to
trade off reward v.s. exploration at different stages of learning. In addition,
we introduce a principled mechanism to terminate sample collection once we have
a rich enough trajectory batch to improve the model. We rigorously evaluate our
method on a variety of continuous control tasks, and demonstrate statistically
significant gains over existing approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2001.08092v1' target='_blank'>Local Policy Optimization for Trajectory-Centric Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Patrik Kolaric, Devesh K. Jha, Arvind U. Raghunathan, Frank L. Lewis, Mouhacine Benosman, Diego Romeres, Daniel Nikovski</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-01-22 15:56:00</h6>
<p class='card-text'>The goal of this paper is to present a method for simultaneous trajectory and
local stabilizing policy optimization to generate local policies for
trajectory-centric model-based reinforcement learning (MBRL). This is motivated
by the fact that global policy optimization for non-linear systems could be a
very challenging problem both algorithmically and numerically. However, a lot
of robotic manipulation tasks are trajectory-centric, and thus do not require a
global model or policy. Due to inaccuracies in the learned model estimates, an
open-loop trajectory optimization process mostly results in very poor
performance when used on the real system. Motivated by these problems, we try
to formulate the problem of trajectory optimization and local policy synthesis
as a single optimization problem. It is then solved simultaneously as an
instance of nonlinear programming. We provide some results for analysis as well
as achieved performance of the proposed technique under some simplifying
assumptions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1912.13007v1' target='_blank'>World Programs for Model-Based Learning and Planning in Compositional
  State and Action Spaces</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marwin H. S. Segler</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-12-30 17:03:16</h6>
<p class='card-text'>Some of the most important tasks take place in environments which lack cheap
and perfect simulators, thus hampering the application of model-free
reinforcement learning (RL). While model-based RL aims to learn a dynamics
model, in a more general case the learner does not know a priori what the
action space is. Here we propose a formalism where the learner induces a world
program by learning a dynamics model and the actions in graph-based
compositional environments by observing state-state transition examples. Then,
the learner can perform RL with the world program as the simulator for complex
planning tasks. We highlight a recent application, and propose a challenge for
the community to assess world program-based planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1909.11821v3' target='_blank'>Model Imitation for Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yueh-Hua Wu, Ting-Han Fan, Peter J. Ramadge, Hao Su</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-09-25 23:52:30</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) aims to learn a dynamic model to
reduce the number of interactions with real-world environments. However, due to
estimation error, rollouts in the learned model, especially those of long
horizons, fail to match the ones in real-world environments. This mismatching
has seriously impacted the sample complexity of MBRL. The phenomenon can be
attributed to the fact that previous works employ supervised learning to learn
the one-step transition models, which has inherent difficulty ensuring the
matching of distributions from multi-step rollouts. Based on the claim, we
propose to learn the transition model by matching the distributions of
multi-step rollouts sampled from the transition model and the real ones via
WGAN. We theoretically show that matching the two can minimize the difference
of cumulative rewards between the real transition and the learned one. Our
experiments also show that the proposed Model Imitation method can compete or
outperform the state-of-the-art in terms of sample complexity and average
return.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1909.04915v3' target='_blank'>Data-efficient Model Learning and Prediction for Contact-rich
  Manipulation Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shahbaz Abdul Khader, Hang Yin, Pietro Falco, Danica Kragic</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-09-11 08:36:20</h6>
<p class='card-text'>In this letter, we investigate learning forward dynamics models and
multi-step prediction of state variables (long-term prediction) for
contact-rich manipulation. The problems are formulated in the context of
model-based reinforcement learning (MBRL). We focus on two
aspects-discontinuous dynamics and data-efficiency-both of which are important
in the identified scope and pose significant challenges to State-of-the-Art
methods. We contribute to closing this gap by proposing a method that
explicitly adopts a specific hybrid structure for the model while leveraging
the uncertainty representation and data-efficiency of Gaussian process. Our
experiments on an illustrative moving block task and a 7-DOF robot demonstrate
a clear advantage when compared to popular baselines in low data regimes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1908.06012v1' target='_blank'>Model-based Lookahead Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhang-Wei Hong, Joni Pajarinen, Jan Peters</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-08-15 04:10:13</h6>
<p class='card-text'>Model-based Reinforcement Learning (MBRL) allows data-efficient learning
which is required in real world applications such as robotics. However, despite
the impressive data-efficiency, MBRL does not achieve the final performance of
state-of-the-art Model-free Reinforcement Learning (MFRL) methods. We leverage
the strengths of both realms and propose an approach that obtains high
performance with a small amount of data. In particular, we combine MFRL and
Model Predictive Control (MPC). While MFRL's strength in exploration allows us
to train a better forward dynamics model for MPC, MPC improves the performance
of the MFRL policy by sampling-based planning. The experimental results in
standard continuous control benchmarks show that our approach can achieve
MFRL`s level of performance while being as data-efficient as MBRL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1907.04202v2' target='_blank'>Variational Inference MPC for Bayesian Model-based Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Masashi Okada, Tadahiro Taniguchi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-07-08 01:54:08</h6>
<p class='card-text'>In recent studies on model-based reinforcement learning (MBRL), incorporating
uncertainty in forward dynamics is a state-of-the-art strategy to enhance
learning performance, making MBRLs competitive to cutting-edge model free
methods, especially in simulated robotics tasks. Probabilistic ensembles with
trajectory sampling (PETS) is a leading type of MBRL, which employs Bayesian
inference to dynamics modeling and model predictive control (MPC) with
stochastic optimization via the cross entropy method (CEM). In this paper, we
propose a novel extension to the uncertainty-aware MBRL. Our main contributions
are twofold: Firstly, we introduce a variational inference MPC, which
reformulates various stochastic methods, including CEM, in a Bayesian fashion.
Secondly, we propose a novel instance of the framework, called probabilistic
action ensembles with trajectory sampling (PaETS). As a result, our Bayesian
MBRL can involve multimodal uncertainties both in dynamics and optimal
trajectories. In comparison to PETS, our method consistently improves
asymptotic performance on several challenging locomotion tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1907.02057v1' target='_blank'>Benchmarking Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-07-03 17:53:02</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) is widely seen as having the
potential to be significantly more sample efficient than model-free RL.
However, research in model-based RL has not been very standardized. It is
fairly common for authors to experiment with self-designed environments, and
there are several separate lines of research, which are sometimes
closed-sourced or not reproducible. Accordingly, it is an open question how
these various existing MBRL algorithms perform relative to each other. To
facilitate research in MBRL, in this paper we gather a wide collection of MBRL
algorithms and propose over 18 benchmarking environments specially designed for
MBRL. We benchmark these algorithms with unified problem settings, including
noisy environments. Beyond cataloguing performance, we explore and unify the
underlying algorithmic differences across MBRL algorithms. We characterize
three key research challenges for future MBRL research: the dynamics
bottleneck, the planning horizon dilemma, and the early-termination dilemma.
Finally, to maximally facilitate future research on MBRL, we open-source our
benchmark in http://www.cs.toronto.edu/~tingwuwang/mbrl.html.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1907.01657v2' target='_blank'>Dynamics-Aware Unsupervised Discovery of Skills</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-07-02 21:32:19</h6>
<p class='card-text'>Conventionally, model-based reinforcement learning (MBRL) aims to learn a
global model for the dynamics of the environment. A good model can potentially
enable planning algorithms to generate a large variety of behaviors and solve
diverse tasks. However, learning an accurate model for complex dynamical
systems is difficult, and even then, the model might not generalize well
outside the distribution of states on which it was trained. In this work, we
combine model-based learning with model-free learning of primitives that make
model-based planning easy. To that end, we aim to answer the question: how can
we discover skills whose outcomes are easy to predict? We propose an
unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS),
which simultaneously discovers predictable behaviors and learns their dynamics.
Our method can leverage continuous skill spaces, theoretically, allowing us to
learn infinitely many behaviors even for high-dimensional state-spaces. We
demonstrate that zero-shot planning in the learned latent space significantly
outperforms standard MBRL and model-free goal-conditioned RL, can handle
sparse-reward tasks, and substantially improves over prior hierarchical RL
methods for unsupervised skill discovery.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1906.08649v1' target='_blank'>Exploring Model-based Planning with Policy Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tingwu Wang, Jimmy Ba</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-06-20 14:13:12</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) with model-predictive control or
online planning has shown great potential for locomotion control tasks in terms
of both sample efficiency and asymptotic performance. Despite their initial
successes, the existing planning methods search from candidate sequences
randomly generated in the action space, which is inefficient in complex
high-dimensional environments. In this paper, we propose a novel MBRL
algorithm, model-based policy planning (POPLIN), that combines policy networks
with online planning. More specifically, we formulate action planning at each
time-step as an optimization problem using neural networks. We experiment with
both optimization w.r.t. the action sequences initialized from the policy
network, and also online optimization directly w.r.t. the parameters of the
policy network. We show that POPLIN obtains state-of-the-art performance in the
MuJoCo benchmarking environments, being about 3x more sample efficient than the
state-of-the-art algorithms, such as PETS, TD3 and SAC. To explain the
effectiveness of our algorithm, we show that the optimization surface in
parameter space is smoother than in action space. Further more, we found the
distilled policy network can be effectively applied without the expansive model
predictive control during test time for some environments such as Cheetah. Code
is released in https://github.com/WilsonWangTHU/POPLIN.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1904.10762v4' target='_blank'>Baconian: A Unified Open-source Framework for Model-Based Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Linsen Dong, Guanyu Gao, Xinyi Zhang, Liangyu Chen, Yonggang Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-04-23 05:35:50</h6>
<p class='card-text'>Model-Based Reinforcement Learning (MBRL) is one category of Reinforcement
Learning (RL) algorithms which can improve sampling efficiency by modeling and
approximating system dynamics. It has been widely adopted in the research of
robotics, autonomous driving, etc. Despite its popularity, there still lacks
some sophisticated and reusable open-source frameworks to facilitate MBRL
research and experiments. To fill this gap, we develop a flexible and
modularized framework, Baconian, which allows researchers to easily implement a
MBRL testbed by customizing or building upon our provided modules and
algorithms. Our framework can free users from re-implementing popular MBRL
algorithms from scratch thus greatly save users' efforts on MBRL experiments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1904.06786v2' target='_blank'>Curious iLQR: Resolving Uncertainty in Model-based RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sarah Bechtle, Yixin Lin, Akshara Rai, Ludovic Righetti, Franziska Meier</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-04-15 00:02:27</h6>
<p class='card-text'>Curiosity as a means to explore during reinforcement learning problems has
recently become very popular. However, very little progress has been made in
utilizing curiosity for learning control. In this work, we propose a
model-based reinforcement learning (MBRL) framework that combines Bayesian
modeling of the system dynamics with curious iLQR, an iterative LQR approach
that considers model uncertainty. During trajectory optimization the curious
iLQR attempts to minimize both the task-dependent cost and the uncertainty in
the dynamics model. We demonstrate the approach on reaching tasks with 7-DoF
manipulators in simulation and on a real robot. Our experiments show that MBRL
with curious iLQR reaches desired end-effector targets more reliably and with
less system rollouts when learning a new task from scratch, and that the
learned model generalizes better to new reaching tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1904.01191v4' target='_blank'>Planning with Expectation Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yi Wan, Zaheer Abbas, Adam White, Martha White, Richard S. Sutton</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-04-02 03:25:25</h6>
<p class='card-text'>Distribution and sample models are two popular model choices in model-based
reinforcement learning (MBRL). However, learning these models can be
intractable, particularly when the state and action spaces are large.
Expectation models, on the other hand, are relatively easier to learn due to
their compactness and have also been widely used for deterministic
environments. For stochastic environments, it is not obvious how expectation
models can be used for planning as they only partially characterize a
distribution. In this paper, we propose a sound way of using approximate
expectation models for MBRL. In particular, we 1) show that planning with an
expectation model is equivalent to planning with a distribution model if the
state value function is linear in state features, 2) analyze two common
parametrization choices for approximating the expectation: linear and
non-linear expectation models, 3) propose a sound model-based policy evaluation
algorithm and present its convergence results, and 4) empirically demonstrate
the effectiveness of the proposed planning algorithm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1903.02078v2' target='_blank'>Output-feedback online optimal control for a class of nonlinear systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ryan Self, Michael Harlan, Rushikesh Kamalapurkar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-03-05 22:02:37</h6>
<p class='card-text'>In this paper an output-feedback model-based reinforcement learning (MBRL)
method for a class of second-order nonlinear systems is developed. The control
technique uses exact model knowledge and integrates a dynamic state estimator
within the model-based reinforcement learning framework to achieve
output-feedback MBRL. Simulation results demonstrate the efficacy of the
developed method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1901.10251v3' target='_blank'>Multi-Agent Reinforcement Learning with Multi-Step Generative Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Orr Krupnik, Igor Mordatch, Aviv Tamar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-01-29 12:29:20</h6>
<p class='card-text'>We consider model-based reinforcement learning (MBRL) in 2-agent,
high-fidelity continuous control problems -- an important domain for robots
interacting with other agents in the same workspace. For non-trivial dynamical
systems, MBRL typically suffers from accumulating errors. Several recent
studies have addressed this problem by learning latent variable models for
trajectory segments and optimizing over behavior in the latent space. In this
work, we investigate whether this approach can be extended to 2-agent
competitive and cooperative settings. The fundamental challenge is how to learn
models that capture interactions between agents, yet are disentangled to allow
for optimization of each agent behavior separately. We propose such models
based on a disentangled variational auto-encoder, and demonstrate our approach
on a simulated 2-robot manipulation task, where one robot can either help or
distract the other. We show that our approach has better sample efficiency than
a strong model-free RL baseline, and can learn both cooperative and adversarial
behavior from the same data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1901.03737v2' target='_blank'>Low Level Control of a Quadrotor with Deep Model-Based Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nathan O. Lambert, Daniel S. Drew, Joseph Yaconelli, Roberto Calandra, Sergey Levine, Kristofer S. J. Pister</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-01-11 20:30:42</h6>
<p class='card-text'>Designing effective low-level robot controllers often entail
platform-specific implementations that require manual heuristic parameter
tuning, significant system knowledge, or long design times. With the rising
number of robotic and mechatronic systems deployed across areas ranging from
industrial automation to intelligent toys, the need for a general approach to
generating low-level controllers is increasing. To address the challenge of
rapidly generating low-level controllers, we argue for using model-based
reinforcement learning (MBRL) trained on relatively small amounts of
automatically generated (i.e., without system simulation) data. In this paper,
we explore the capabilities of MBRL on a Crazyflie centimeter-scale quadrotor
with rapid dynamics to predict and control at <50Hz. To our knowledge, this is
the first use of MBRL for controlled hover of a quadrotor using only on-board
sensors, direct motor input signals, and no initial dynamics knowledge. Our
controller leverages rapid simulation of a neural network forward dynamics
model on a GPU-enabled base station, which then transmits the best current
action to the quadrotor firmware via radio. In our experiments, the quadrotor
achieved hovering capability of up to 6 seconds with 3 minutes of experimental
training data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1806.01825v3' target='_blank'>The Effect of Planning Shape on Dyna-style Planning in High-dimensional
  State Spaces</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:G. Zacharias Holland, Erin J. Talvitie, Michael Bowling</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-06-05 17:31:02</h6>
<p class='card-text'>Dyna is a fundamental approach to model-based reinforcement learning (MBRL)
that interleaves planning, acting, and learning in an online setting. In the
most typical application of Dyna, the dynamics model is used to generate
one-step transitions from selected start states from the agent's history, which
are used to update the agent's value function or policy as if they were real
experiences. In this work, one-step Dyna was applied to several games from the
Arcade Learning Environment (ALE). We found that the model-based updates
offered surprisingly little benefit over simply performing more updates with
the agent's existing experience, even when using a perfect model. We
hypothesize that to get the most from planning, the model must be used to
generate unfamiliar experience. To test this, we experimented with the "shape"
of planning in multiple different concrete instantiations of Dyna, performing
fewer, longer rollouts, rather than many short rollouts. We found that planning
shape has a profound impact on the efficacy of Dyna for both perfect and
learned models. In addition to these findings regarding Dyna in general, our
results represent, to our knowledge, the first time that a learned dynamics
model has been successfully used for planning in the ALE, suggesting that Dyna
may be a viable approach to MBRL in the ALE and other high-dimensional
problems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1805.09496v6' target='_blank'>Intelligent Trainer for Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuanlong Li, Linsen Dong, Xin Zhou, Yonggang Wen, Kyle Guan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-05-24 03:08:40</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) has been proposed as a promising
alternative solution to tackle the high sampling cost challenge in the
canonical reinforcement learning (RL), by leveraging a learned model to
generate synthesized data for policy training purpose. The MBRL framework,
nevertheless, is inherently limited by the convoluted process of jointly
learning control policy and configuring hyper-parameters (e.g., global/local
models, real and synthesized data, etc). The training process could be tedious
and prohibitively costly. In this research, we propose an "reinforcement on
reinforcement" (RoR) architecture to decompose the convoluted tasks into two
layers of reinforcement learning. The inner layer is the canonical model-based
RL training process environment (TPE), which learns the control policy for the
underlying system and exposes interfaces to access states, actions and rewards.
The outer layer presents an RL agent, called as AI trainer, to learn an optimal
hyper-parameter configuration for the inner TPE. This decomposition approach
provides a desirable flexibility to implement different trainer designs, called
as "train the trainer". In our research, we propose and optimize two
alternative trainer designs: 1) a uni-head trainer and 2) a multi-head trainer.
Our proposed RoR framework is evaluated for five tasks in the OpenAI gym (i.e.,
Pendulum, Mountain Car, Reacher, Half Cheetah and Swimmer). Compared to three
other baseline algorithms, our proposed Train-the-Trainer algorithm has a
competitive performance in auto-tuning capability, with upto 56% expected
sampling cost saving without knowing the best parameter setting in advance. The
proposed trainer framework can be easily extended to other cases in which the
hyper-parameter tuning is costly.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1612.06018v2' target='_blank'>Self-Correcting Models for Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Erik Talvitie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2016-12-19 01:09:23</h6>
<p class='card-text'>When an agent cannot represent a perfectly accurate model of its
environment's dynamics, model-based reinforcement learning (MBRL) can fail
catastrophically. Planning involves composing the predictions of the model;
when flawed predictions are composed, even minor errors can compound and render
the model useless for planning. Hallucinated Replay (Talvitie 2014) trains the
model to "correct" itself when it produces errors, substantially improving MBRL
with flawed models. This paper theoretically analyzes this approach,
illuminates settings in which it is likely to be effective or ineffective, and
presents a novel error bound, showing that a model's ability to self-correct is
more tightly related to MBRL performance than one-step prediction error. These
results inspire an MBRL algorithm for deterministic MDPs with performance
guarantees that are robust to model class limitations.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>