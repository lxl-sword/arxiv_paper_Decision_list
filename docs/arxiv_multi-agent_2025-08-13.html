<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>multi-agent - 2025-08-13</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>multi-agent - 2025-08-13</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08800v1' target='_blank'>Fault Tolerant Multi-Agent Learning with Adversarial Budget Constraints</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:David Mguni, Yaqi Sun, Haojun Chen, Amir Darabi, Larry Olanrewaju Orimoloye, Yaodong Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 09:57:05</h6>
<p class='card-text'>In multi-agent systems, the safe and reliable execution of tasks often
depends on agents correctly coordinating their actions. However, in real-world
deployments, failures of computational components are inevitable, presenting a
critical challenge: ensuring that multi-agent reinforcement learning (MARL)
policies remain effective even when some agents malfunction. We propose the
Multi-Agent Robust Training Algorithm (MARTA), a plug-and-play framework for
training MARL agents to be resilient to potentially severe faults. MARTA
operates in cooperative multi-agent settings where agents may lose the ability
to execute their intended actions. It learns to identify failure scenarios that
are especially detrimental to system performance and equips agents with
strategies to mitigate their impact. At the heart of MARTA is a novel
adversarial Markov game in which an adversary -- modelled via \emph{Markov
switching controls} -- learns to disable agents in high-risk state regions,
while the remaining agents are trained to \emph{jointly} best-respond to such
targeted malfunctions. To ensure practicality, MARTA enforces a malfunction
budget, constraining the adversary to a fixed number of failures and learning
robust policies accordingly. We provide theoretical guarantees that MARTA
converges to a Markov perfect equilibrium, ensuring agents optimally counteract
worst-case faults. Empirically, we show that MARTA achieves state-of-the-art
fault-tolerant performance across benchmark environments, including Multi-Agent
Particle World and Level-Based Foraging.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08555v1' target='_blank'>Traffic Load-Aware Resource Management Strategy for Underwater Wireless
  Sensor Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tong Zhang, Yu Gou, Jun Liu, Jun-Hong Cui</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 01:50:33</h6>
<p class='card-text'>Underwater Wireless Sensor Networks (UWSNs) represent a promising technology
that enables diverse underwater applications through acoustic communication.
However, it encounters significant challenges including harsh communication
environments, limited energy supply, and restricted signal transmission. This
paper aims to provide efficient and reliable communication in underwater
networks with limited energy and communication resources by optimizing the
scheduling of communication links and adjusting transmission parameters (e.g.,
transmit power and transmission rate). The efficient and reliable communication
multi-objective optimization problem (ERCMOP) is formulated as a decentralized
partially observable Markov decision process (Dec-POMDP). A Traffic Load-Aware
Resource Management (TARM) strategy based on deep multi-agent reinforcement
learning (MARL) is presented to address this problem. Specifically, a traffic
load-aware mechanism that leverages the overhear information from neighboring
nodes is designed to mitigate the disparity between partial observations and
global states. Moreover, by incorporating a solution space optimization
algorithm, the number of candidate solutions for the deep MARL-based
decision-making model can be effectively reduced, thereby optimizing the
computational complexity. Simulation results demonstrate the adaptability of
TARM in various scenarios with different transmission demands and collision
probabilities, while also validating the effectiveness of the proposed approach
in supporting efficient and reliable communication in underwater networks with
limited resources.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.07679v1' target='_blank'>Joint link scheduling and power allocation in imperfect and
  energy-constrained underwater wireless sensor networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tong Zhang, Yu Gou, Jun Liu, Shanshan Song, Tingting Yang, Jun-Hong Cui</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 06:55:11</h6>
<p class='card-text'>Underwater wireless sensor networks (UWSNs) stand as promising technologies
facilitating diverse underwater applications. However, the major design issues
of the considered system are the severely limited energy supply and unexpected
node malfunctions. This paper aims to provide fair, efficient, and reliable
(FER) communication to the imperfect and energy-constrained UWSNs (IC-UWSNs).
Therefore, we formulate a FER-communication optimization problem (FERCOP) and
propose ICRL-JSA to solve the formulated problem. ICRL-JSA is a deep
multi-agent reinforcement learning (MARL)-based optimizer for IC-UWSNs through
joint link scheduling and power allocation, which automatically learns
scheduling algorithms without human intervention. However, conventional RL
methods are unable to address the challenges posed by underwater environments
and IC-UWSNs. To construct ICRL-JSA, we integrate deep Q-network into IC-UWSNs
and propose an advanced training mechanism to deal with complex acoustic
channels, limited energy supplies, and unexpected node malfunctions. Simulation
results demonstrate the superiority of the proposed ICRL-JSA scheme with an
advanced training mechanism compared to various benchmark algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.07578v1' target='_blank'>Achieving Fair-Effective Communications and Robustness in Underwater
  Acoustic Sensor Networks: A Semi-Cooperative Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu Gou, Tong Zhang, Jun Liu, Tingting Yang, Shanshan Song, Jun-Hong Cui</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-11 03:20:36</h6>
<p class='card-text'>This paper investigates the fair-effective communication and robustness in
imperfect and energy-constrained underwater acoustic sensor networks
(IC-UASNs). Specifically, we investigate the impact of unexpected node
malfunctions on the network performance under the time-varying acoustic
channels. Each node is expected to satisfy Quality of Service (QoS)
requirements. However, achieving individual QoS requirements may interfere with
other concurrent communications. Underwater nodes rely excessively on the
rationality of other underwater nodes when guided by fully cooperative
approaches, making it difficult to seek a trade-off between individual QoS and
global fair-effective communications under imperfect conditions. Therefore,
this paper presents a SEmi-COoperative Power Allocation approach (SECOPA) that
achieves fair-effective communication and robustness in IC-UASNs. The approach
is distributed multi-agent reinforcement learning (MARL)-based, and the
objectives are twofold. On the one hand, each intelligent node individually
decides the transmission power to simultaneously optimize individual and global
performance. On the other hand, advanced training algorithms are developed to
provide imperfect environments for training robust models that can adapt to the
time-varying acoustic channels and handle unexpected node failures in the
network. Numerical results are presented to validate our proposed approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.07001v1' target='_blank'>Consensus-based Decentralized Multi-agent Reinforcement Learning for
  Random Access Network Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Myeung Suk Oh, Zhiyao Zhang, FNU Hairi, Alvaro Velasquez, Jia Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-09 14:39:27</h6>
<p class='card-text'>With wireless devices increasingly forming a unified smart network for
seamless, user-friendly operations, random access (RA) medium access control
(MAC) design is considered a key solution for handling unpredictable data
traffic from multiple terminals. However, it remains challenging to design an
effective RA-based MAC protocol to minimize collisions and ensure transmission
fairness across the devices. While existing multi-agent reinforcement learning
(MARL) approaches with centralized training and decentralized execution (CTDE)
have been proposed to optimize RA performance, their reliance on centralized
training and the significant overhead required for information collection can
make real-world applications unrealistic. In this work, we adopt a fully
decentralized MARL architecture, where policy learning does not rely on
centralized tasks but leverages consensus-based information exchanges across
devices. We design our MARL algorithm over an actor-critic (AC) network and
propose exchanging only local rewards to minimize communication overhead.
Furthermore, we provide a theoretical proof of global convergence for our
approach. Numerical experiments show that our proposed MARL algorithm can
significantly improve RA network performance compared to other baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.06836v1' target='_blank'>Multi-level Advantage Credit Assignment for Cooperative Multi-Agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xutong Zhao, Yaqi Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-09 05:36:08</h6>
<p class='card-text'>Cooperative multi-agent reinforcement learning (MARL) aims to coordinate
multiple agents to achieve a common goal. A key challenge in MARL is credit
assignment, which involves assessing each agent's contribution to the shared
reward. Given the diversity of tasks, agents may perform different types of
coordination, with rewards attributed to diverse and often overlapping agent
subsets. In this work, we formalize the credit assignment level as the number
of agents cooperating to obtain a reward, and address scenarios with multiple
coexisting levels. We introduce a multi-level advantage formulation that
performs explicit counterfactual reasoning to infer credits across distinct
levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures
agent contributions at multiple levels by integrating advantage functions that
reason about individual, joint, and correlated actions. Utilizing an
attention-based framework, MACA identifies correlated agent relationships and
constructs multi-level advantages to guide policy learning. Comprehensive
experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior
performance, underscoring its efficacy in complex credit assignment scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.06767v1' target='_blank'>PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in
  Digital Twin Ecosystems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arman Dogru, R. Irem Bor-Yaliniz, Nimal Gamini Senarath</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-09 00:59:55</h6>
<p class='card-text'>Digital Twins (DTs) are transforming industries through advanced data
processing and analysis, positioning the world of DTs, Digital World, as a
cornerstone of nextgeneration technologies including embodied AI. As robotics
and automated systems scale, efficient data-sharing frameworks and robust
algorithms become critical. We explore the pivotal role of data handling in
next-gen networks, focusing on dynamics between application and network
providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with
Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)
based multi-agent path finding (MAPF). By adopting a Centralized Training with
Decentralized Execution (CTDE) framework and asynchronous actor-learner
architectures, PANAMA accelerates training while enabling autonomous task
execution by embodied AI. Our approach demonstrates superior pathfinding
performance in accuracy, speed, and scalability compared to existing
benchmarks. Through simulations, we highlight optimized data-sharing strategies
for scalable, automated systems, ensuring resilience in complex, real-world
environments. PANAMA bridges the gap between network-aware decision-making and
robust multi-agent coordination, advancing the synergy between DTs, wireless
networks, and AI-driven automation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.06269v1' target='_blank'>OM2P: Offline Multi-Agent Mean-Flow Policy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuoran Li, Xun Wang, Hai Zhong, Longbo Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-08 12:38:56</h6>
<p class='card-text'>Generative models, especially diffusion and flow-based models, have been
promising in offline multi-agent reinforcement learning. However, integrating
powerful generative models into this framework poses unique challenges. In
particular, diffusion and flow-based policies suffer from low sampling
efficiency due to their iterative generation processes, making them impractical
in time-sensitive or resource-constrained settings. To tackle these
difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel
offline MARL algorithm to achieve efficient one-step action sampling. To
address the misalignment between generative objectives and reward maximization,
we introduce a reward-aware optimization scheme that integrates a
carefully-designed mean-flow matching loss with Q-function supervision.
Additionally, we design a generalized timestep distribution and a
derivative-free estimation strategy to reduce memory overhead and improve
training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo
benchmarks demonstrate that OM2P achieves superior performance, with up to a
3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.
Our approach represents the first to successfully integrate mean-flow model
into offline MARL, paving the way for practical and scalable generative
policies in cooperative multi-agent settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.06061v1' target='_blank'>Policy Optimization in Multi-Agent Settings under Partially Observable
  Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ainur Zhaikhan, Malek Khammassi, Ali H. Sayed</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-08 06:45:43</h6>
<p class='card-text'>This work leverages adaptive social learning to estimate partially observable
global states in multi-agent reinforcement learning (MARL) problems. Unlike
existing methods, the proposed approach enables the concurrent operation of
social learning and reinforcement learning. Specifically, it alternates between
a single step of social learning and a single step of MARL, eliminating the
need for the time- and computation-intensive two-timescale learning frameworks.
Theoretical guarantees are provided to support the effectiveness of the
proposed method. Simulation results verify that the performance of the proposed
methodology can approach that of reinforcement learning when the true state is
known.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04652v1' target='_blank'>LLM Collaboration With Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuo Liu, Zeyu Liang, Xueguang Lyu, Christopher Amato</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 17:18:25</h6>
<p class='card-text'>A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.03864v1' target='_blank'>Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for
  Internalized Safety</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenyu Pan, Yiting Zhang, Yutong Zhang, Jianshu Zhang, Haozheng Luo, Yuwei Han, Dennis Wu, Hong-Yu Chen, Philip S. Yu, Manling Li, Han Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-05 19:26:55</h6>
<p class='card-text'>Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.02912v1' target='_blank'>Engineered over Emergent Communication in MARL for Scalable and
  Sample-Efficient Cooperative Task Allocation in a Partially Observable Grid</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Brennen A. Hill, Mant Koh En Wei, Thangavel Jishnuanandh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-04 21:29:07</h6>
<p class='card-text'>We compare the efficacy of learned versus engineered communication strategies
in a cooperative multi-agent reinforcement learning (MARL) environment. For the
learned approach, we introduce Learned Direct Communication (LDC), where agents
generate messages and actions concurrently via a neural network. Our engineered
approach, Intention Communication, employs an Imagined Trajectory Generation
Module (ITGM) and a Message Generation Network (MGN) to formulate messages
based on predicted future states. Both strategies are evaluated on their
success rates in cooperative tasks under fully and partially observable
conditions. Our findings indicate that while emergent communication is viable,
the engineered approach demonstrates superior performance and scalability,
particularly as environmental complexity increases.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.02027v1' target='_blank'>An Evolving Scenario Generation Method based on Dual-modal Driver Model
  Trained by Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinzheng Wu, Junyi Chen, Shaolingfeng Ye, Wei Jiang, Yong Shen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-04 03:42:30</h6>
<p class='card-text'>In the autonomous driving testing methods based on evolving scenarios, the
construction method of the driver model, which determines the driving maneuvers
of background vehicles (BVs) in the scenario, plays a critical role in
generating safety-critical scenarios. In particular, the cooperative
adversarial driving characteristics between BVs can contribute to the efficient
generation of safety-critical scenarios with high testing value. In this paper,
a multi-agent reinforcement learning (MARL) method is used to train and
generate a dual-modal driver model (Dual-DM) with non-adversarial and
adversarial driving modalities. The model is then connected to a continuous
simulated traffic environment to generate complex, diverse and strong
interactive safety-critical scenarios through evolving scenario generation
method. After that, the generated evolving scenarios are evaluated in terms of
fidelity, test efficiency, complexity and diversity. Results show that without
performance degradation in scenario fidelity (>85% similarity to real-world
scenarios) and complexity (complexity metric: 0.45, +32.35% and +12.5% over two
baselines), Dual-DM achieves a substantial enhancement in the efficiency of
generating safety-critical scenarios (efficiency metric: 0.86, +195% over two
baselines). Furthermore, statistical analysis and case studies demonstrate the
diversity of safety-critical evolving scenarios generated by Dual-DM in terms
of the adversarial interaction patterns. Therefore, Dual-DM can greatly improve
the performance of the generation of safety-critical scenarios through evolving
scenario generation method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.01522v1' target='_blank'>Decentralized Aerial Manipulation of a Cable-Suspended Load using
  Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jack Zeng, Andreu Matoses Gimenez, Eugene Vinitsky, Javier Alonso-Mora, Sihao Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-02 23:52:33</h6>
<p class='card-text'>This paper presents the first decentralized method to enable real-world 6-DoF
manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles
(MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train
an outer-loop control policy for each MAV. Unlike state-of-the-art controllers
that utilize a centralized scheme, our policy does not require global states,
inter-MAV communications, nor neighboring MAV information. Instead, agents
communicate implicitly through load pose observations alone, which enables high
scalability and flexibility. It also significantly reduces computing costs
during inference time, enabling onboard deployment of the policy. In addition,
we introduce a new action space design for the MAVs using linear acceleration
and body rates. This choice, combined with a robust low-level controller,
enables reliable sim-to-real transfer despite significant uncertainties caused
by cable tension during dynamic 3D motion. We validate our method in various
real-world experiments, including full-pose control under load model
uncertainties, showing setpoint tracking performance comparable to the
state-of-the-art centralized method. We also demonstrate cooperation amongst
agents with heterogeneous control policies, and robustness to the complete
in-flight loss of one MAV. Videos of experiments:
https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.01049v1' target='_blank'>Centralized Adaptive Sampling for Reliable Co-Training of Independent
  Multi-Agent Policies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nicholas E. Corrado, Josiah P. Hanna</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-01 20:07:25</h6>
<p class='card-text'>Independent on-policy policy gradient algorithms are widely used for
multi-agent reinforcement learning (MARL) in cooperative and no-conflict games,
but they are known to converge suboptimally when each agent's policy gradient
points toward a suboptimal equilibrium. In this work, we identify a subtler
failure mode that arises \textit{even when the expected policy gradients of all
agents point toward an optimal solution.} After collecting a finite set of
trajectories, stochasticity in independent action sampling can cause the joint
data distribution to deviate from the expected joint on-policy distribution.
This \textit{sampling error} w.r.t. the joint on-policy distribution produces
inaccurate gradient estimates that can lead agents to converge suboptimally. In
this paper, we investigate if joint sampling error can be reduced through
coordinated action selection and whether doing so improves the reliability of
policy gradient learning in MARL. Toward this end, we introduce an adaptive
action sampling approach to reduce joint sampling error. Our method,
Multi-Agent Proximal Robust On-Policy Sampling (MA-PROPS), uses a centralized
behavior policy that we continually adapt to place larger probability on joint
actions that are currently under-sampled w.r.t. the current joint policy. We
empirically evaluate MA-PROPS in a diverse range of multi-agent games and
demonstrate that (1) MA-PROPS reduces joint sampling error more efficiently
than standard on-policy sampling and (2) improves the reliability of
independent policy gradient algorithms, increasing the fraction of training
runs that converge to an optimal joint policy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.23604v1' target='_blank'>Hierarchical Message-Passing Policies for Multi-Agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tommaso Marzi, Cesare Alippi, Andrea Cini</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-31 14:42:12</h6>
<p class='card-text'>Decentralized Multi-Agent Reinforcement Learning (MARL) methods allow for
learning scalable multi-agent policies, but suffer from partial observability
and induced non-stationarity. These challenges can be addressed by introducing
mechanisms that facilitate coordination and high-level planning. Specifically,
coordination and temporal abstraction can be achieved through communication
(e.g., message passing) and Hierarchical Reinforcement Learning (HRL)
approaches to decision-making. However, optimization issues limit the
applicability of hierarchical policies to multi-agent systems. As such, the
combination of these approaches has not been fully explored. To fill this void,
we propose a novel and effective methodology for learning multi-agent
hierarchies of message-passing policies. We adopt the feudal HRL framework and
rely on a hierarchical graph structure for planning and coordination among
agents. Agents at lower levels in the hierarchy receive goals from the upper
levels and exchange messages with neighboring agents at the same level. To
learn hierarchical multi-agent policies, we design a novel reward-assignment
method based on training the lower-level policies to maximize the advantage
function associated with the upper levels. Results on relevant benchmarks show
that our method performs favorably compared to the state of the art.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.21638v1' target='_blank'>Assistax: A Hardware-Accelerated Reinforcement Learning Benchmark for
  Assistive Robotics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Leonard Hinckeldey, Elliot Fosong, Elle Miller, Rimvydas Rubavicius, Trevor McInroe, Patricia Wollstadt, Christiane B. Wiebel-Herboth, Subramanian Ramamoorthy, Stefano V. Albrecht</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-29 09:49:11</h6>
<p class='card-text'>The development of reinforcement learning (RL) algorithms has been largely
driven by ambitious challenge tasks and benchmarks. Games have dominated RL
benchmarks because they present relevant challenges, are inexpensive to run and
easy to understand. While games such as Go and Atari have led to many
breakthroughs, they often do not directly translate to real-world embodied
applications. In recognising the need to diversify RL benchmarks and addressing
complexities that arise in embodied interaction scenarios, we introduce
Assistax: an open-source benchmark designed to address challenges arising in
assistive robotics tasks. Assistax uses JAX's hardware acceleration for
significant speed-ups for learning in physics-based simulations. In terms of
open-loop wall-clock time, Assistax runs up to $370\times$ faster when
vectorising training runs compared to CPU-based alternatives. Assistax
conceptualises the interaction between an assistive robot and an active human
patient using multi-agent RL to train a population of diverse partner agents
against which an embodied robotic agent's zero-shot coordination capabilities
can be tested. Extensive evaluation and hyperparameter tuning for popular
continuous control RL and MARL algorithms provide reliable baselines and
establish Assistax as a practical benchmark for advancing RL research for
assistive robotics. The code is available at:
https://github.com/assistive-autonomy/assistax.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.20143v1' target='_blank'>Concept Learning for Cooperative Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhonghan Ge, Yuanyang Zhu, Chunlin Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-27 06:22:24</h6>
<p class='card-text'>Despite substantial progress in applying neural networks (NN) to multi-agent
reinforcement learning (MARL) areas, they still largely suffer from a lack of
transparency and interoperability. However, its implicit cooperative mechanism
is not yet fully understood due to black-box networks. In this work, we study
an interpretable value decomposition framework via concept bottleneck models,
which promote trustworthiness by conditioning credit assignment on an
intermediate level of human-like cooperation concepts. To address this problem,
we propose a novel value-based method, named Concepts learning for Multi-agent
Q-learning (CMQ), that goes beyond the current performance-vs-interpretability
trade-off by learning interpretable cooperation concepts. CMQ represents each
cooperation concept as a supervised vector, as opposed to existing models where
the information flowing through their end-to-end mechanism is concept-agnostic.
Intuitively, using individual action value conditioning on global state
embeddings to represent each concept allows for extra cooperation
representation capacity. Empirical evaluations on the StarCraft II
micromanagement challenge and level-based foraging (LBF) show that CMQ achieves
superior performance compared with the state-of-the-art counterparts. The
results also demonstrate that CMQ provides more cooperation concept
representation capturing meaningful cooperation modes, and supports test-time
concept interventions for detecting potential biases of cooperation mode and
identifying spurious artifacts that impact cooperation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.19151v2' target='_blank'>ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for
  Multi-Agent Coordination</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michael Amir, Guang Yang, Zhan Gao, Keisuke Okumura, Heedo Woo, Amanda Prorok</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-25 10:47:39</h6>
<p class='card-text'>Constraint-based optimization is a cornerstone of robotics, enabling the
design of controllers that reliably encode task and safety requirements such as
collision avoidance or formation adherence. However, handcrafted constraints
can fail in multi-agent settings that demand complex coordination. We introduce
ReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid
framework that merges the reliability of optimization-based controllers with
the adaptability of multi-agent reinforcement learning. Rather than discarding
expert controllers, ReCoDe improves them by learning additional, dynamic
constraints that capture subtler behaviors, for example, by constraining agent
movements to prevent congestion in cluttered scenarios. Through local
communication, agents collectively constrain their allowed actions to
coordinate more effectively under changing conditions. In this work, we focus
on applications of ReCoDe to multi-agent navigation tasks requiring intricate,
context-based movements and consensus, where we show that it outperforms purely
handcrafted controllers, other hybrid approaches, and standard MARL baselines.
We give empirical (real robot) and theoretical evidence that retaining a
user-defined controller, even when it is imperfect, is more efficient than
learning from scratch, especially because ReCoDe can dynamically change the
degree to which it relies on this controller.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.19050v1' target='_blank'>Large Language Model-Based Task Offloading and Resource Allocation for
  Digital Twin Edge Computing Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qiong Wu, Yu Xie, Pingyi Fan, Dong Qin, Kezhi Wang, Nan Cheng, Khaled B. Letaief</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-25 08:11:09</h6>
<p class='card-text'>In this paper, we propose a general digital twin edge computing network
comprising multiple vehicles and a server. Each vehicle generates multiple
computing tasks within a time slot, leading to queuing challenges when
offloading tasks to the server. The study investigates task offloading
strategies, queue stability, and resource allocation. Lyapunov optimization is
employed to transform long-term constraints into tractable short-term
decisions. To solve the resulting problem, an in-context learning approach
based on large language model (LLM) is adopted, replacing the conventional
multi-agent reinforcement learning (MARL) framework. Experimental results
demonstrate that the LLM-based method achieves comparable or even superior
performance to MARL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.18867v1' target='_blank'>Learning Individual Intrinsic Reward in Multi-Agent Reinforcement
  Learning via Incorporating Generalized Human Expertise</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuefei Wu, Xiao Yin, Yuanyang Zhu, Chunlin Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-25 00:59:10</h6>
<p class='card-text'>Efficient exploration in multi-agent reinforcement learning (MARL) is a
challenging problem when receiving only a team reward, especially in
environments with sparse rewards. A powerful method to mitigate this issue
involves crafting dense individual rewards to guide the agents toward efficient
exploration. However, individual rewards generally rely on manually engineered
shaping-reward functions that lack high-order intelligence, thus it behaves
ineffectively than humans regarding learning and generalization in complex
problems. To tackle these issues, we combine the above two paradigms and
propose a novel framework, LIGHT (Learning Individual Intrinsic reward via
Incorporating Generalized Human experTise), which can integrate human knowledge
into MARL algorithms in an end-to-end manner. LIGHT guides each agent to avoid
unnecessary exploration by considering both individual action distribution and
human expertise preference distribution. Then, LIGHT designs individual
intrinsic rewards for each agent based on actionable representational
transformation relevant to Q-learning so that the agents align their action
preferences with the human expertise while maximizing the joint action value.
Experimental results demonstrate the superiority of our method over
representative baselines regarding performance and better knowledge reusability
across different sparse-reward tasks on challenging scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.18333v1' target='_blank'>Remembering the Markov Property in Cooperative MARL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kale-ab Abebe Tessera, Leonard Hinckeldey, Riccardo Zamboni, David Abel, Amos Storkey</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-24 11:59:42</h6>
<p class='card-text'>Cooperative multi-agent reinforcement learning (MARL) is typically formalised
as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),
where agents must reason about the environment and other agents' behaviour. In
practice, current model-free MARL algorithms use simple recurrent function
approximators to address the challenge of reasoning about others using partial
information. In this position paper, we argue that the empirical success of
these methods is not due to effective Markov signal recovery, but rather to
learning simple conventions that bypass environment observations and memory.
Through a targeted case study, we show that co-adapting agents can learn
brittle conventions, which then fail when partnered with non-adaptive agents.
Crucially, the same models can learn grounded policies when the task design
necessitates it, revealing that the issue is not a fundamental limitation of
the learning models but a failure of the benchmark design. Our analysis also
suggests that modern MARL environments may not adequately test the core
assumptions of Dec-POMDPs. We therefore advocate for new cooperative
environments built upon two core principles: (1) behaviours grounded in
observations and (2) memory-based reasoning about other agents, ensuring
success requires genuine skill rather than fragile, co-adapted agreements.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.18059v1' target='_blank'>Multi-Agent Guided Policy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yueheng Li, Guangming Xie, Zongqing Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-24 03:22:21</h6>
<p class='card-text'>Due to practical constraints such as partial observability and limited
communication, Centralized Training with Decentralized Execution (CTDE) has
become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning
(MARL). However, existing CTDE methods often underutilize centralized training
or lack theoretical guarantees. We propose Multi-Agent Guided Policy
Optimization (MAGPO), a novel framework that better leverages centralized
training by integrating centralized guidance with decentralized execution.
MAGPO uses an auto-regressive joint policy for scalable, coordinated
exploration and explicitly aligns it with decentralized policies to ensure
deployability under partial observability. We provide theoretical guarantees of
monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across
6 diverse environments. Results show that MAGPO consistently outperforms strong
CTDE baselines and matches or surpasses fully centralized approaches, offering
a principled and practical solution for decentralized multi-agent learning. Our
code and experimental data can be found in https://github.com/liyheng/MAGPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.16796v1' target='_blank'>Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading
  with Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mian Ibad Ali Shah, Enda Barrett, Karl Mason</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-22 17:46:28</h6>
<p class='card-text'>This paper presents a novel framework for Peer-to-Peer (P2P) energy trading
that integrates uncertainty-aware prediction with multi-agent reinforcement
learning (MARL), addressing a critical gap in current literature. In contrast
to previous works relying on deterministic forecasts, the proposed approach
employs a heteroscedastic probabilistic transformer-based prediction model
called Knowledge Transformer with Uncertainty (KTU) to explicitly quantify
prediction uncertainty, which is essential for robust decision-making in the
stochastic environment of P2P energy trading. The KTU model leverages
domain-specific features and is trained with a custom loss function that
ensures reliable probabilistic forecasts and confidence intervals for each
prediction. Integrating these uncertainty-aware forecasts into the MARL
framework enables agents to optimize trading strategies with a clear
understanding of risk and variability. Experimental results show that the
uncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to
5.7% without P2P trading and 3.2% with P2P trading, while increasing
electricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak
hour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These
improvements are even more pronounced when P2P trading is enabled, highlighting
the synergy between advanced forecasting and market mechanisms for resilient,
economically efficient energy communities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.16479v1' target='_blank'>Arbitrage Tactics in the Local Markets via Hierarchical Multi-agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoyang Zhang, Mina Montazeri, Philipp Heer, Koen Kok, Nikolaos G. Paterakis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-22 11:35:37</h6>
<p class='card-text'>Strategic bidding tactics employed by prosumers in local markets, including
the Local Electricity Market (LEM) and Local Flexibility Market (LFM), have
attracted significant attention due to their potential to enhance economic
benefits for market participants through optimized energy management and
bidding. While existing research has explored strategic bidding in a single
market with multi-agent reinforcement learning (MARL) algorithms, arbitrage
opportunities across local markets remain unexplored. This paper introduces a
hierarchical MARL (HMARL) algorithm designed to enable aggregator arbitrage
across multiple local markets. The strategic behavior of these aggregators in
local markets is modeled as a two-stage Markov game: the first stage involves
the LEM, while the second stage encompasses both the LFM and the balancing
market. To solve this two-stage Markov game, the HMARL framework assigns two
sub-agents to each aggregator, a primary sub-agent and a secondary sub-agent.
Without the arbitrage strategy, these sub-agents operate in silos, with the
primary sub-agent focusing on first-stage profits and the secondary sub-agent
on second-stage profits, each employing independent MARLs. On the contrary,
when implementing the arbitrage strategy with the proposed HMARL, the
sub-agents communicate and coordinate to perform arbitrage across multiple
local markets, enhancing overall efficiency. The case study, conducted under a
scenario where all aggregators employ the arbitrage strategy, shows that
despite higher initial costs in the LEM, this strategy generates substantial
savings in the LFM and the balancing market, resulting in a total profit
increase of $40.6\%$ on average. This highlights the capability of the proposed
HMARL to address the two-stage Markov game and facilitate arbitrage across
local markets, thereby enhancing profitability for participants.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.16382v1' target='_blank'>Application of LLM Guided Reinforcement Learning in Formation Control
  with Collision Avoidance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenhao Yao, Zike Yuan, Xiaoxu Liu, Chi Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-22 09:26:00</h6>
<p class='card-text'>Multi-Agent Systems (MAS) excel at accomplishing complex objectives through
the collaborative efforts of individual agents. Among the methodologies
employed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of
the most efficacious algorithms. However, when confronted with the complex
objective of Formation Control with Collision Avoidance (FCCA): designing an
effective reward function that facilitates swift convergence of the policy
network to an optimal solution. In this paper, we introduce a novel framework
that aims to overcome this challenge. By giving large language models (LLMs) on
the prioritization of tasks and the observable information available to each
agent, our framework generates reward functions that can be dynamically
adjusted online based on evaluation outcomes by employing more advanced
evaluation metrics rather than the rewards themselves. This mechanism enables
the MAS to simultaneously achieve formation control and obstacle avoidance in
dynamic environments with enhanced efficiency, requiring fewer iterations to
reach superior performance levels. Our empirical studies, conducted in both
simulation and real-world settings, validate the practicality and effectiveness
of our proposed approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.16306v1' target='_blank'>COMPASS: Cooperative Multi-Agent Persistent Monitoring using
  Spatio-Temporal Attention Network</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xingjian Zhang, Yizhuo Wang, Guillaume Sartoretti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-22 07:44:08</h6>
<p class='card-text'>Persistent monitoring of dynamic targets is essential in real-world
applications such as disaster response, environmental sensing, and wildlife
conservation, where mobile agents must continuously gather information under
uncertainty. We propose COMPASS, a multi-agent reinforcement learning (MARL)
framework that enables decentralized agents to persistently monitor multiple
moving targets efficiently. We model the environment as a graph, where nodes
represent spatial locations and edges capture topological proximity, allowing
agents to reason over structured layouts and revisit informative regions as
needed. Each agent independently selects actions based on a shared
spatio-temporal attention network that we design to integrate historical
observations and spatial context. We model target dynamics using Gaussian
Processes (GPs), which support principled belief updates and enable
uncertainty-aware planning. We train COMPASS using centralized value estimation
and decentralized policy execution under an adaptive reward setting. Our
extensive experiments demonstrate that COMPASS consistently outperforms strong
baselines in uncertainty reduction, target coverage, and coordination
efficiency across dynamic multi-target scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.16249v1' target='_blank'>Multi-Agent Reinforcement Learning for Sample-Efficient Deep Neural
  Network Mapping</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Srivatsan Krishnan, Jason Jabbour, Dan Zhang, Natasha Jaques, Aleksandra Faust, Shayegan Omidshafiei, Vijay Janapa Reddi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-22 05:51:07</h6>
<p class='card-text'>Mapping deep neural networks (DNNs) to hardware is critical for optimizing
latency, energy consumption, and resource utilization, making it a cornerstone
of high-performance accelerator design. Due to the vast and complex mapping
space, reinforcement learning (RL) has emerged as a promising approach-but its
effectiveness is often limited by sample inefficiency. We present a
decentralized multi-agent reinforcement learning (MARL) framework designed to
overcome this challenge. By distributing the search across multiple agents, our
framework accelerates exploration. To avoid inefficiencies from training
multiple agents in parallel, we introduce an agent clustering algorithm that
assigns similar mapping parameters to the same agents based on correlation
analysis. This enables a decentralized, parallelized learning process that
significantly improves sample efficiency. Experimental results show our MARL
approach improves sample efficiency by 30-300x over standard single-agent RL,
achieving up to 32.61x latency reduction and 16.45x energy-delay product (EDP)
reduction under iso-sample conditions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.15351v1' target='_blank'>One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step
  Policy Optimization for Order Dispatch on Ride-Sharing Platforms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijian Zhao, Sen Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-21 08:04:31</h6>
<p class='card-text'>On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.15174v1' target='_blank'>Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in
  Multi-Agent Traffic Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Justin Turnau, Longchao Da, Khoa Vo, Ferdous Al Rafi, Shreyas Bachiraju, Tiejin Chen, Hua Wei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-21 01:33:59</h6>
<p class='card-text'>Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>