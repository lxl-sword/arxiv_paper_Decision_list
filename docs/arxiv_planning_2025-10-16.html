<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-10-16</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-10-16</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.13686v1' target='_blank'>Hierarchical Discrete Lattice Assembly: An Approach for the Digital
  Fabrication of Scalable Macroscale Structures</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Miana Smith, Paul Arthur Richard, Alexander Htet Kyaw, Neil Gershenfeld</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-15 15:43:43</h6>
<p class='card-text'>Although digital fabrication processes at the desktop scale have become
proficient and prolific, systems aimed at producing larger-scale structures are
still typically complex, expensive, and unreliable. In this work, we present an
approach for the fabrication of scalable macroscale structures using simple
robots and interlocking lattice building blocks. A target structure is first
voxelized so that it can be populated with an architected lattice. These voxels
are then grouped into larger interconnected blocks, which are produced using
standard digital fabrication processes, leveraging their capability to produce
highly complex geometries at a small scale. These blocks, on the size scale of
tens of centimeters, are then fed to mobile relative robots that are able to
traverse over the structure and place new blocks to form structures on the
meter scale. To facilitate the assembly of large structures, we introduce a
live digital twin simulation tool for controlling and coordinating assembly
robots that enables both global planning for a target structure and live user
design, interaction, or intervention. To improve assembly throughput, we
introduce a new modular assembly robot, designed for hierarchical voxel
handling. We validate this system by demonstrating the voxelization,
hierarchical blocking, path planning, and robotic fabrication of a set of
meter-scale objects.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.13638v1' target='_blank'>Challenges, Advances, and Evaluation Metrics in Medical Image
  Enhancement: A Systematic Literature Review</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chun Wai Chin, Haniza Yazid, Hoi Leong Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-15 15:01:58</h6>
<p class='card-text'>Medical image enhancement is crucial for improving the quality and
interpretability of diagnostic images, ultimately supporting early detection,
accurate diagnosis, and effective treatment planning. Despite advancements in
imaging technologies such as X-ray, CT, MRI, and ultrasound, medical images
often suffer from challenges like noise, artifacts, and low contrast, which
limit their diagnostic potential. Addressing these challenges requires robust
preprocessing, denoising algorithms, and advanced enhancement methods, with
deep learning techniques playing an increasingly significant role. This
systematic literature review, following the PRISMA approach, investigates the
key challenges, recent advancements, and evaluation metrics in medical image
enhancement. By analyzing findings from 39 peer-reviewed studies, this review
provides insights into the effectiveness of various enhancement methods across
different imaging modalities and the importance of evaluation metrics in
assessing their impact. Key issues like low contrast and noise are identified
as the most frequent, with MRI and multi-modal imaging receiving the most
attention, while specialized modalities such as histopathology, endoscopy, and
bone scintigraphy remain underexplored. Out of the 39 studies, 29 utilize
conventional mathematical methods, 9 focus on deep learning techniques, and 1
explores a hybrid approach. In terms of image quality assessment, 18 studies
employ both reference-based and non-reference-based metrics, 9 rely solely on
reference-based metrics, and 12 use only non-reference-based metrics, with a
total of 65 IQA metrics introduced, predominantly non-reference-based. This
review highlights current limitations, research gaps, and potential future
directions for advancing medical image enhancement.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.13391v1' target='_blank'>Going with the Flow: Approximating Banzhaf Values via Graph Neural
  Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Benjamin Kempinski, Tal Kachman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-15 10:40:33</h6>
<p class='card-text'>Computing the Banzhaf value in network flow games is fundamental for
quantifying agent influence in multi-agent systems, with applications ranging
from cybersecurity to infrastructure planning. However, exact computation is
intractable for systems with more than $\sim20$ agents due to exponential
complexity $\mathcal{O}(2^m)$. While Monte Carlo sampling methods provide
statistical estimates, they suffer from high sample complexity and cannot
transfer knowledge across different network configurations, making them
impractical for large-scale or dynamic systems. We present a novel
learning-based approach using Graph Neural Networks (GNNs) to approximate
Banzhaf values in cardinal network flow games. By framing the problem as a
graph-level prediction task, our method learns generalisable patterns of agent
influence directly from network topology and control structure. We conduct a
comprehensive empirical study comparing three state-of-the-art GNN
architectures-Graph Attention Networks (GAT), Graph Isomorphism Networks with
Edge features (GINE), and EdgeConv-on a large-scale synthetic dataset of
200,000 graphs per configuration, varying in size (20-100 nodes), agent count
(5-20), and edge probability (0.5-1.0). Our results demonstrate that trained
GNN models achieve high-fidelity Banzhaf value approximation with
order-of-magnitude speedups compared to exact and sampling-based methods. Most
significantly, we show strong zero-shot generalisation: models trained on
graphs of a specific size and topology accurately predict Banzhaf values for
entirely new networks with different structural properties, without requiring
retraining. This work establishes GNNs as a practical tool for scalable
cooperative game-theoretic analysis of complex networked systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.13300v1' target='_blank'>Five Years of Clinical Application of independent Monte Carlo-Based
  Patient-Specific Quality Assurance at the Maastro Proton Therapy Center</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ilaria Rinaldi, Giorgio Cartechini, Angelo Schiavi, Jan Gajewski, Nils Krah, Antoni Rucinski, Gloria Vilches Freixas, Vincenzo Patera, Sebastiaan Nijsten</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-15 08:40:04</h6>
<p class='card-text'>At the Maastro Proton Therapy Center in Maastricht, patient-specific quality
assurance (PSQA) using an independent GPU-accelerated Monte Carlo (MC)
calculation has fully replaced conventional measurements, which are
time-consuming and have limited sensitivity to clinically relevant errors.
  A fully automated and robust pipeline was developed, integrating two clinical
workflows based on the fast MC code Fred. The system is fully operational, and
automatic verification reports are part of daily clinical practice.
  The first workflow performs a pre-treatment dose recalculation in Fred using
the planning CT and clinical plan. The second uses Fred with machine log files
to verify the actually delivered dose. Both generate automatic reports for
clinical review.
  Over five years, this workflow has become part of routine clinical
operations, providing robust 3D dosimetric verification in heterogeneous
anatomies. So far, Fred has recalculated more than 6000 pre-treatment plans and
3513 log file-based PSQA cases, saving an estimated 4090 hours of QA work. The
pipeline identified true negatives and detected two planning-related failures
that would have been missed by conventional measurements. No false positives or
negatives were observed, confirming high accuracy and reliability.
  The MC-based PSQA pipeline offers an efficient, sensitive, and clinically
meaningful alternative to measurement-based QA in pencil beam scanning proton
therapy. By eliminating routine measurements, it saves resources while
improving patient safety and treatment quality. Five years of experience
confirm that measurement-less MC-based PSQA is a viable and superior approach,
providing full 3D verification and early error detection - a practical
blueprint for other proton therapy centres.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.13265v1' target='_blank'>Unstable optimal transport maps</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Cyril Letrouit</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-15 08:10:09</h6>
<p class='card-text'>The stability of optimal transport maps with respect to perturbations of the
marginals is a question of interest for several reasons, ranging from the
justification of the linearized optimal transport framework to numerical
analysis and statistics. Under various assumptions on the source measure, it is
known that optimal transport maps are stable with respect to variations of the
target measure. In this note, we focus on the mechanisms that can, on the
contrary, lead to instability. We identify two of them, which we illustrate
through examples of absolutely continuous source measures $\rho$ in
$\mathbb{R}^d$ for which optimal transport maps are less stable, or even very
unstable. We first show that instability may arise from the unboundedness of
the density: we exhibit a source density on the unit ball of $\mathbb{R}^d$
which blows up superpolynomially at two points of the boundary and for which
optimal transport maps are highly unstable. Then we prove that even for uniform
densities on bounded open sets, optimal transport maps can be rather unstable
close enough to configurations where uniqueness of optimal plans is lost.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.13250v1' target='_blank'>Real-Time Crowd Counting for Embedded Systems with Lightweight
  Architecture</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiyuan Zhao, Yubin Wen, Siyu Yang, Lichen Ning, Yuandong Liu, Junyu Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-15 07:58:46</h6>
<p class='card-text'>Crowd counting is a task of estimating the number of the crowd through
images, which is extremely valuable in the fields of intelligent security,
urban planning, public safety management, and so on. However, the existing
counting methods have some problems in practical application on embedded
systems for these fields, such as excessive model parameters, abundant complex
calculations, etc. The practical application of embedded systems requires the
model to be real-time, which means that the model is fast enough. Considering
the aforementioned problems, we design a super real-time model with a
stem-encoder-decoder structure for crowd counting tasks, which achieves the
fastest inference compared with state-of-the-arts. Firstly, large convolution
kernels in the stem network are used to enlarge the receptive field, which
effectively extracts detailed head information. Then, in the encoder part, we
use conditional channel weighting and multi-branch local fusion block to merge
multi-scale features with low computational consumption. This part is crucial
to the super real-time performance of the model. Finally, the feature pyramid
networks are added to the top of the encoder to alleviate its incomplete fusion
problems. Experiments on three benchmarks show that our network is suitable for
super real-time crowd counting on embedded systems, ensuring competitive
accuracy. At the same time, the proposed network reasoning speed is the
fastest. Specifically, the proposed network achieves 381.7 FPS on NVIDIA GTX
1080Ti and 71.9 FPS on NVIDIA Jetson TX1.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.13149v1' target='_blank'>RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional
  Generalization in Long-Horizon Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yangtao Chen, Zixuan Chen, Nga Teng Chan, Junting Chen, Junhui Yin, Jieqi Shi, Yang Gao, Yong-Lu Li, Jing Huo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-15 04:58:13</h6>
<p class='card-text'>Enabling robots to flexibly schedule and compose learned skills for novel
long-horizon manipulation under diverse perturbations remains a core challenge.
Early explorations with end-to-end VLA models show limited success, as these
models struggle to generalize beyond the training distribution. Hierarchical
approaches, where high-level planners generate subgoals for low-level policies,
bring certain improvements but still suffer under complex perturbations,
revealing limited capability in skill composition. However, existing benchmarks
primarily emphasize task completion in long-horizon settings, offering little
insight into compositional generalization, robustness, and the interplay
between planning and execution. To systematically investigate these gaps, we
propose RoboHiMan, a hierarchical evaluation paradigm for compositional
generalization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench,
a benchmark of atomic and compositional tasks under diverse perturbations,
supported by a multi-level training dataset for analyzing progressive data
scaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled)
that probe the necessity of skill composition and reveal bottlenecks in
hierarchical architectures. Experiments highlight clear capability gaps across
representative models and architectures, pointing to directions for advancing
models better suited to real-world long-horizon manipulation tasks. Videos and
open-source code can be found on our project website:
https://chenyt31.github.io/robo-himan.github.io/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.13100v1' target='_blank'>Decision-dependent Robust Charging Infrastructure Planning for
  Light-duty Truck Electrification at Industrial Sites: Scheduling and
  Abandonment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yifu Ding, Ruicheng Ao, Pablo Duenas-Martinez, Thomas Magnanti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-15 02:44:00</h6>
<p class='card-text'>Many industrial sites rely on diesel-powered light-duty trucks to transport
workers and small-scale facilities, which has resulted in a significant amount
of greenhouse emissions (GHGs). To address this, we developed a two-stage
robust charging infrastructure planning model for electrifying light-duty
trucks at industrial sites. The model is formulated as a mixed-integer linear
programming (MILP) that optimizes the charging infrastructure, selected from
multiple charger types and potential locations, and determines opportunity
charging schedules for each truck based on the chosen infrastructure. Given the
strict stopping points and schedules at industrial sites, we introduced a
scheduling problem with abandonment, where trucks forgo charging if their
waiting times exceed a maximum threshold. We also further incorporated the
impacts of overnight charging and range anxiety on waiting and abandonment
behaviors. To represent the stochastic and heterogeneous parking durations of
trucks, we constructed a decision-dependent robust uncertainty set in which
parking time variability flexibly depends on charging choices. We applied the
model in a case study of an open-pit mining site, which plans charger
installations in eight zones and schedules a fleet of around 200 trucks. By
decomposing the problem into monthly subproblems and using heuristic
approaches, for the whole-year dataset, the model achieves an optimality gap of
less than 0.1 % within a reasonable computation time under diverse uncertainty
scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.13083v1' target='_blank'>Average-case thresholds for exact regularization of linear programs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michael P. Friedlander, Sharvaj Kubal, Yaniv Plan, Matthew S. Scott</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-15 01:55:01</h6>
<p class='card-text'>Small regularizers can preserve linear programming solutions exactly. This
paper provides the first average-case analysis of exact regularization: with a
standard Gaussian cost vector and fixed constraint set, bounds are established
for the probability that exact regularization succeeds as a function of
regularization strength. Failure is characterized via the Gaussian measure of
inner cones, controlled by novel two-sided bounds on the measure of shifted
cones. Results reveal dimension-dependent scaling laws and connect exact
regularization of linear programs to their polyhedral geometry via the normal
fan and the Gaussian (solid-angle) measure of its cones. Computable bounds are
obtained in several canonical settings, including regularized optimal
transport. Numerical experiments corroborate the predicted scalings and
thresholds.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.13062v1' target='_blank'>Towards Human-Centric Intelligent Treatment Planning for Radiation
  Therapy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Adnan Jafar, Xun Jia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-15 01:04:48</h6>
<p class='card-text'>Current radiation therapy treatment planning is limited by suboptimal plan
quality, inefficiency, and high costs. This perspective paper explores the
complexity of treatment planning and introduces Human-Centric Intelligent
Treatment Planning (HCITP), an AI-driven framework under human oversight, which
integrates clinical guidelines, automates plan generation, and enables direct
interactions with operators. We expect that HCITP will enhance efficiency,
potentially reducing planning time to minutes, and will deliver personalized,
high-quality plans. Challenges and potential solutions are discussed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12992v1' target='_blank'>UNCAP: Uncertainty-Guided Planning Using Natural Language Communication
  for Cooperative Autonomous Vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Neel P. Bhatt, Po-han Li, Kushagra Gupta, Rohan Siva, Daniel Milan, Alexander T. Hogue, Sandeep P. Chinchali, David Fridovich-Keil, Zhangyang Wang, Ufuk Topcu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 21:09:09</h6>
<p class='card-text'>Safe large-scale coordination of multiple cooperative connected autonomous
vehicles (CAVs) hinges on communication that is both efficient and
interpretable. Existing approaches either rely on transmitting high-bandwidth
raw sensor data streams or neglect perception and planning uncertainties
inherent in shared data, resulting in systems that are neither scalable nor
safe. To address these limitations, we propose Uncertainty-Guided Natural
Language Cooperative Autonomous Planning (UNCAP), a vision-language model-based
planning approach that enables CAVs to communicate via lightweight natural
language messages while explicitly accounting for perception uncertainty in
decision-making. UNCAP features a two-stage communication protocol: (i) an ego
CAV first identifies the subset of vehicles most relevant for information
exchange, and (ii) the selected CAVs then transmit messages that quantitatively
express their perception uncertainty. By selectively fusing messages that
maximize mutual information, this strategy allows the ego vehicle to integrate
only the most relevant signals into its decision-making, improving both the
scalability and reliability of cooperative planning. Experiments across diverse
driving scenarios show a 63% reduction in communication bandwidth with a 31%
increase in driving safety score, a 61% reduction in decision uncertainty, and
a four-fold increase in collision distance margin during near-miss events.
Project website: https://uncap-project.github.io/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12986v1' target='_blank'>Surrogate Models to Predict Wave Hydrodynamics on Evolving Landscapes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohammad Ahmadi Gharehtoragh, David R Johnson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 20:56:28</h6>
<p class='card-text'>Coastal planners using probabilistic risk assessments to evaluate structural
flood risk reduction projects may wish to simulate the hydrodynamics associated
with large suites of tropical cyclones in large ensembles of landscapes: with
and without projects' implementation; over decades of their useful lifetimes;
and under multiple scenarios reflecting uncertainty about sea level rise, land
subsidence, and other factors. Wave action can be a substantial contributor to
flood losses and overtopping of structural features like levees and floodwalls,
but numerical methods solving for wave dynamics are computationally expensive,
potentially limiting budget-constrained planning efforts. In this study, we
present and evaluate the performance of deep learning-based surrogate models
for predicting peak significant wave heights under a variety of relevant use
cases: predicting waves with or without modeled peak storm surge as a feature,
predicting wave heights while simultaneously predicting peak storm surge, or
using storm surge predicted by another surrogate model as an input feature. All
models incorporate landscape morphological elements (e.g., elevation,
roughness, canopy) and global boundary conditions (e.g., sea level) in addition
to tropical cyclone characteristics as predictive features to improve accuracy
as landscapes evolve over time. Using simulations from Louisiana's 2023 Coastal
Master Plan as a case study, we demonstrate suitable accuracy of surrogate
models for planning-level studies, with a two-sided Kolmogorov-Smirnov test
indicating no significant difference between significant wave heights generated
by the Simulating Waves Nearshore model and those predicted by our surrogate
models in approximately 89% of grid cells and landscapes evaluated in the
study, with performance varying by landscape and model. On average, the models
produced a root mean squared error of 0.05-0.06 m.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12962v1' target='_blank'>Enhancing Sampling-based Planning with a Library of Paths</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michal Minařík, Vojtěch Vonásek, Robert Pěnička</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 20:13:34</h6>
<p class='card-text'>Path planning for 3D solid objects is a challenging problem, requiring a
search in a six-dimensional configuration space, which is, nevertheless,
essential in many robotic applications such as bin-picking and assembly. The
commonly used sampling-based planners, such as Rapidly-exploring Random Trees,
struggle with narrow passages where the sampling probability is low, increasing
the time needed to find a solution. In scenarios like robotic bin-picking,
various objects must be transported through the same environment. However,
traditional planners start from scratch each time, losing valuable information
gained during the planning process. We address this by using a library of past
solutions, allowing the reuse of previous experiences even when planning for a
new, previously unseen object. Paths for a set of objects are stored, and when
planning for a new object, we find the most similar one in the library and use
its paths as approximate solutions, adjusting for possible mutual
transformations. The configuration space is then sampled along the approximate
paths. Our method is tested in various narrow passage scenarios and compared
with state-of-the-art methods from the OMPL library. Results show significant
speed improvements (up to 85% decrease in the required time) of our method,
often finding a solution in cases where the other planners fail. Our
implementation of the proposed method is released as an open-source package.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12961v1' target='_blank'>Competitive EV charging station location with queues</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:The Minh Nguyen, Nagisa Sugishita, Margarida Carvalho, Amira Dems</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 20:12:27</h6>
<p class='card-text'>Electric vehicle (EV) public charging infrastructure planning faces
significant challenges in competitive markets, where multiple service providers
affect congestion and user behavior. This work extends existing modeling
frameworks by incorporating the presence of competitors' stations and more
realistic queueing systems.
  First, we analyze three finite queueing systems, M/M/1/K, M/M/s/K, and
M/Er/s/K, with varying numbers of servers (charging outlets) and service time
distributions, deriving analytic expressions for user behavior metrics. Second,
we embed the queueing-based user behavior model into a bilevel program, where
the upper level locates new charging stations to maximize accessibility
(throughput), and the lower level captures users' station choices via a user
equilibrium. Third, we apply a reformulation from competitive congested
user-choice facility location models to approximately solve the bilevel problem
and introduce a surrogate-based heuristic to enhance scalability. Fourth, we
showcase our methodology on a real-world case study of an urban area in
Montreal (Canada), offering managerial insights into how user-choice behavior
assumptions and competition affect throughput and location decisions. The
results demonstrate that our model yields (re)location strategies that
outperform the existing network. More broadly, this approach provides a tool
for incorporating charging service quality-through queueing metrics-and
existing competition into station planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12891v1' target='_blank'>Polarization dependency in Resonant Inelastic X-Ray Scattering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michelangelo Tagliavini, Fabian Wenzel, Maurits W. Haverkort</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 18:05:47</h6>
<p class='card-text'>Resonant Inelastic X-Ray Scattering (RIXS) is a well-established tool for
probing excitations in a wide range of materials. The measured spectra strongly
depend on the scattering geometry, via its influence on the polarization of the
incoming and outgoing light. By employing a tensor representation of the
4-point response function that governs the RIXS intensity, we disentangle the
experimental geometry from the intrinsic material properties. In dipole-dipole
RIXS processes and low-symmetry crystals, up to 81 linearly independent
fundamental spectra can be measured as a function of light polarization.
However, for crystals or molecules with symmetry, the number of independent
fundamental spectra that define the RIXS tensor is significantly reduced.
  This work presents a systematic framework for determining the number of
fundamental spectra and expressing the RIXS tensor in terms of these
fundamental components. Given a specific experimental geometry, the measured
spectrum can be represented as a linear combination of these fundamental
spectra. To validate our approach, we performed calculations for different
point group symmetries, both with and without an applied magnetic field. Within
the same framework, we derived expressions for powder spectra in
momentum-independent processes and spectra obtained using Bragg spectrometers.
This formalism provides a valuable toolkit for optimizing experiment planning,
data interpretation, and RIXS simulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12733v1' target='_blank'>HYPE: Hybrid Planning with Ego Proposal-Conditioned Predictions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hang Yu, Julian Jordan, Julian Schmidt, Silvan Lindner, Alessandro Canevaro, Wilhelm Stork</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 17:11:04</h6>
<p class='card-text'>Safe and interpretable motion planning in complex urban environments needs to
reason about bidirectional multi-agent interactions. This reasoning requires to
estimate the costs of potential ego driving maneuvers. Many existing planners
generate initial trajectories with sampling-based methods and refine them by
optimizing on learned predictions of future environment states, which requires
a cost function that encodes the desired vehicle behavior. Designing such a
cost function can be very challenging, especially if a wide range of complex
urban scenarios has to be considered. We propose HYPE: HYbrid Planning with Ego
proposal-conditioned predictions, a planner that integrates multimodal
trajectory proposals from a learned proposal model as heuristic priors into a
Monte Carlo Tree Search (MCTS) refinement. To model bidirectional interactions,
we introduce an ego-conditioned occupancy prediction model, enabling
consistent, scene-aware reasoning. Our design significantly simplifies cost
function design in refinement by considering proposal-driven guidance,
requiring only minimalistic grid-based cost terms. Evaluations on large-scale
real-world benchmarks nuPlan and DeepUrban show that HYPE effectively achieves
state-of-the-art performance, especially in safety and adaptability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12693v1' target='_blank'>ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning
  and Online Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanyang Chen, Mark Zhao, Rui Yang, Qinwei Ma, Ke Yang, Jiarui Yao, Kangrui Wang, Hao Bai, Zhenhailong Wang, Rui Pan, Mengchao Zhang, Jose Barreiros, Aykut Onol, ChengXiang Zhai, Heng Ji, Manling Li, Huan Zhang, Tong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 16:25:46</h6>
<p class='card-text'>Recent advances in embodied AI highlight the potential of vision language
models (VLMs) as agents capable of perception, reasoning, and interaction in
complex environments. However, top-performing systems rely on large-scale
models that are costly to deploy, while smaller VLMs lack the necessary
knowledge and skills to succeed. To bridge this gap, we present
\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates
prior knowledge learning and online reinforcement learning (RL). The first
stage, \textit{Embodied Prior Learning}, distills foundational knowledge from
three types of data: (1) Trajectory-Augmented Priors, which enrich existing
trajectory data with structured reasoning generated by stronger models; (2)
Environment-Anchored Priors, which provide in-environment knowledge and
grounding supervision; and (3) External Knowledge Priors, which transfer
general knowledge from out-of-environment datasets. In the second stage, we
develop an online RL pipeline that builds on these priors to further enhance
agent performance. To overcome the inherent challenges in agent RL, including
long horizons, sparse rewards, and training instability, we introduce three key
designs: self-summarization for context management, dense reward shaping, and
turn-level policy optimization. Extensive experiments on both high-level
planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate
that ERA-3B surpasses both prompting-based large models and previous
training-based baselines. Specifically, it achieves overall improvements of
8.4\% on EB-ALFRED and 19.4\% on EB-Manipulation over GPT-4o, and exhibits
strong generalization to unseen tasks. Overall, ERA offers a practical path
toward scalable embodied intelligence, providing methodological insights for
future embodied AI systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12662v1' target='_blank'>Maximal Adaptation, Minimal Guidance: Permissive Reactive Robot Task
  Planning with Humans in the Loop</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Oz Gitelson, Satya Prakash Nayak, Ritam Raha, Anne-Kathrin Schmuck</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 15:58:42</h6>
<p class='card-text'>We present a novel framework for human-robot \emph{logical} interaction that
enables robots to reliably satisfy (infinite horizon) temporal logic tasks
while effectively collaborating with humans who pursue independent and unknown
tasks. The framework combines two key capabilities: (i) \emph{maximal
adaptation} enables the robot to adjust its strategy \emph{online} to exploit
human behavior for cooperation whenever possible, and (ii) \emph{minimal
tunable feedback} enables the robot to request cooperation by the human online
only when necessary to guarantee progress. This balance minimizes human-robot
interference, preserves human autonomy, and ensures persistent robot task
satisfaction even under conflicting human goals. We validate the approach in a
real-world block-manipulation task with a Franka Emika Panda robotic arm and in
the Overcooked-AI benchmark, demonstrating that our method produces rich,
\emph{emergent} cooperative behaviors beyond the reach of existing approaches,
while maintaining strong formal guarantees.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12642v1' target='_blank'>Aixel: A Unified, Adaptive and Extensible System for AI-powered Data
  Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Meihui Zhang, Liming Wang, Chi Zhang, Zhaojing Luo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 15:34:35</h6>
<p class='card-text'>A growing trend in modern data analysis is the integration of data management
with learning, guided by accuracy, latency, and cost requirements. In practice,
applications draw data of different formats from many sources. In the
meanwhile, the objectives and budgets change over time. Existing systems handle
these applications across databases, analysis libraries, and tuning services.
Such fragmentation leads to complex user interaction, limited adaptability,
suboptimal performance, and poor extensibility across components. To address
these challenges, we present Aixel, a unified, adaptive, and extensible system
for AI-powered data analysis. The system organizes work across four layers:
application, task, model, and data. The task layer provides a declarative
interface to capture user intent, which is parsed into an executable operator
plan. An optimizer compiles and schedules this plan to meet specified goals in
accuracy, latency, and cost. The task layer coordinates the execution of data
and model operators, with built-in support for reuse and caching to improve
efficiency. The model layer offers versioned storage for index, metadata,
tensors, and model artifacts. It supports adaptive construction, task-aligned
drift detection, and safe updates that reuse shared components. The data layer
provides unified data management capabilities, including indexing,
constraint-aware discovery, task-aligned selection, and comprehensive feature
management. With the above designed layers, Aixel delivers a user friendly,
adaptive, efficient, and extensible system.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12585v1' target='_blank'>OCTOPUS: A Versatile, User-Friendly, and Extensible Public Code for
  General-Relativistic Ray-Tracing in Spherically Symmetric and Static
  Spacetimes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shiyang Hu, Shijie Tan, Dan Li, Lina Zhang, Chen Deng, Wenfu Cao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 14:40:57</h6>
<p class='card-text'>This paper presents OCTOPUS, a relativistic ray-tracing algorithm developed
within a Fortran-based, OpenMP-accelerated framework, designed for
asymptotically flat, spherically symmetric curved spacetimes. The code
efficiently and accurately computes key relativistic features -- including the
black hole event horizon, photon rings, critical curves, and innermost stable
circular orbits -- and simulates black hole shadows, redshift factor
distributions, accretion disk images, toroidal images, as well as gravitational
lensing, light curves, and gravitational radiation from hot-spots. OCTOPUS
provides an automated, modular solution for qualitative studies of black hole
observables and multi-messenger correlations between electromagnetic and
gravitational signals in curved spacetime. Its implementation requires only the
metric potential and its first-, second-, and third-order radial derivatives as
input, ensuring low user barriers while remaining highly extensible and
adaptable. Using a Schwarzschild black hole surrounded by a Dehnen-type dark
matter halo, we thoroughly validate the algorithm's precision, efficiency, and
functionality, and investigate how dark matter halo parameters affect
observational signatures. Our results demonstrate that increasing the scale and
density of the dark matter halo strengthens the spacetime's gravitational
field, an effect clearly reflected in black hole images and supported by
hot-spot light curve signatures. A future version of OCTOPUS, with expanded
capabilities for axisymmetric spacetimes, is planned for release.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12519v1' target='_blank'>Trading robustness: a scenario-free approach to robust Multi-Criteria
  Optimization for Treatment Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Remo Cristoforetti, Philipp Süss, Tobias Becher, Niklas Wahl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 13:43:39</h6>
<p class='card-text'>Treatment planning in radiotherapy is inherently a multi-criteria
optimization (MCO) problem. Traditionally, the treatment's robustness is not
formulated as a part of this decision making problem, but dealt with separately
through margins or robust optimization. This work facilitates integration of
robustness into multi-criteria optimization using a recently proposed efficient
scenario-free (s-f) robust optimization approach: The s-f approach relies on
the fast evaluation of the expected dose distribution and mean variance during
optimization. This is achieved by precomputation of probabilistic quantities,
which can then be used for repeated solving of subproblems in the two explored
MCO approaches: Lexicographic Ordering (LO) and Pareto Front (PF)
approximation. Different prioritization strategies within the LO approach are
used to assess the impact of variance reduction while a 3-objective PF
approximation, including a variance reduction objective, is generated to
visualize and analyze trade-offs between the competing objectives. The robust
optimization is performed including 100 scenarios modeling setup and range
errors, as well as organ motion, on 3D- and 4DCT lung cancer patient datasets.
Robustness analysis is performed to assess and explore the efficacy of all
optimization strategies. The s-f approach enabled robust optimization in MCO
with computational times comparable to nominal MCO. Both MCO strategies
highlighted the interplay between dosimetric and variance reduction objectives.
The LO approach showed how prioritization affects plan quality and robustness,
while the PF analysis revealed a clear trade-off between robustness and
organ-at-risk sparing. The reported analysis highlighted the conflicting
trade-off nature of plan robustness and dosimetric quality, demonstrating how
robust MCO supports a more informed and flexible decision-making process in
treatment planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12509v1' target='_blank'>Automated Behavior Planning for Fruit Tree Pruning via Redundant Robot
  Manipulators: Addressing the Behavior Planning Challenge</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gaoyuan Liu, Bas Boom, Naftali Slob, Yuri Durodié, Ann Nowé, Bram Vanderborght</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 13:40:40</h6>
<p class='card-text'>Pruning is an essential agricultural practice for orchards. Proper pruning
can promote healthier growth and optimize fruit production throughout the
orchard's lifespan. Robot manipulators have been developed as an automated
solution for this repetitive task, which typically requires seasonal labor with
specialized skills. While previous research has primarily focused on the
challenges of perception, the complexities of manipulation are often
overlooked. These challenges involve planning and control in both joint and
Cartesian spaces to guide the end-effector through intricate, obstructive
branches. Our work addresses the behavior planning challenge for a robotic
pruning system, which entails a multi-level planning problem in environments
with complex collisions. In this paper, we formulate the planning problem for a
high-dimensional robotic arm in a pruning scenario, investigate the system's
intrinsic redundancies, and propose a comprehensive pruning workflow that
integrates perception, modeling, and holistic planning. In our experiments, we
demonstrate that more comprehensive planning methods can significantly enhance
the performance of the robotic manipulator. Finally, we implement the proposed
workflow on a real-world robot. As a result, this work complements previous
efforts on robotic pruning and motivates future research and development in
planning for pruning applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12477v1' target='_blank'>A Task-Efficient Reinforcement Learning Task-Motion Planner for Safe
  Human-Robot Cooperation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gaoyuan Liu, Joris de Winter, Kelly Merckaert, Denis Steckelmacher, Ann Nowe, Bram Vanderborght</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 13:11:09</h6>
<p class='card-text'>In a Human-Robot Cooperation (HRC) environment, safety and efficiency are the
two core properties to evaluate robot performance. However, safety mechanisms
usually hinder task efficiency since human intervention will cause backup
motions and goal failures of the robot. Frequent motion replanning will
increase the computational load and the chance of failure. In this paper, we
present a hybrid Reinforcement Learning (RL) planning framework which is
comprised of an interactive motion planner and a RL task planner. The RL task
planner attempts to choose statistically safe and efficient task sequences
based on the feedback from the motion planner, while the motion planner keeps
the task execution process collision-free by detecting human arm motions and
deploying new paths when the previous path is not valid anymore. Intuitively,
the RL agent will learn to avoid dangerous tasks, while the motion planner
ensures that the chosen tasks are safe. The proposed framework is validated on
the cobot in both simulation and the real world, we compare the planner with
hard-coded task motion planning methods. The results show that our planning
framework can 1) react to uncertain human motions at both joint and task
levels; 2) reduce the times of repeating failed goal commands; 3) reduce the
total number of replanning requests.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12435v2' target='_blank'>The value of storage in electricity distribution: The role of markets</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dirk Lauinger, Deepjyoti Deka, Sungho Shin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 12:13:58</h6>
<p class='card-text'>Electricity distribution companies deploy battery storage to defer grid
upgrades by reducing peak demand. In deregulated jurisdictions, such storage
often sits idle because regulatory constraints bar participation in electricity
markets. Here, we develop an optimization framework that, to our knowledge,
provides the first formal model of market participation constraints within
storage investment and operation planning. Applying the framework to a
Massachusetts case study, we find that market participation could deliver
similar savings as peak demand reduction. Under current conditions, market
participation does not increase storage investment, but at very low storage
costs, could incentivize deployment beyond local distribution needs. This might
run contrary to the separation of distribution from generation in deregulated
markets. Our framework can identify investment levels appropriate for local
distribution needs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12434v1' target='_blank'>PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for
  Retrieval-Augmented Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiangjun Zai, Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Wenjie Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 12:13:23</h6>
<p class='card-text'>Knowledge Hypergraphs (KHs) have recently emerged as a knowledge
representation for retrieval-augmented generation (RAG), offering a paradigm to
model multi-entity relations into a structured form. However, existing KH-based
RAG methods suffer from three major limitations: static retrieval planning,
non-adaptive retrieval execution, and superficial use of KH structure and
semantics, which constrain their ability to perform effective multi-hop
question answering. To overcome these limitations, we propose PRoH, a dynamic
Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates
three core innovations: (i) a context-aware planning module that sketches the
local KH neighborhood to guide structurally grounded reasoning plan generation;
(ii) a structured question decomposition process that organizes subquestions as
a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,
multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided
reasoning path retrieval algorithm that prioritizes semantically coherent
hyperedge traversals. Experiments across multiple domains demonstrate that PRoH
achieves state-of-the-art performance, surpassing the prior SOTA model
HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation
(G-E) score, while maintaining strong robustness in long-range multi-hop
reasoning tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12346v1' target='_blank'>PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair
  Climbing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bingquan Li, Ning Wang, Tianwei Zhang, Zhicheng He, Yucong Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 10:00:05</h6>
<p class='card-text'>Recently, biped robot walking technology has been significantly developed,
mainly in the context of a bland walking scheme. To emulate human walking,
robots need to step on the positions they see in unknown spaces accurately. In
this paper, we present PolyMap, a perception-based locomotion planning
framework for humanoid robots to climb stairs. Our core idea is to build a
real-time polygonal staircase plane semantic map, followed by a footstep planar
using these polygonal plane segments. These plane segmentation and visual
odometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs). The
proposed framework is deployed on a NVIDIA Orin, which performs 20-30 Hz
whole-body motion planning output. Both indoor and outdoor real-scene
experiments indicate that our method is efficient and robust for humanoid robot
stair climbing.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12253v1' target='_blank'>Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and
  Development</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Changfu Xu, Jianxiong Guo, Yuzhu Liang, Haiyang Huang, Haodong Zou, Xi Zheng, Shui Yu, Xiaowen Chu, Jiannong Cao, Tian Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 08:03:46</h6>
<p class='card-text'>Diffusion Models (DMs), as a leading class of generative models, offer key
advantages for reinforcement learning (RL), including multi-modal
expressiveness, stable training, and trajectory-level planning. This survey
delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We
first provide an overview of RL, highlighting its challenges, and then
introduce the fundamental concepts of DMs, investigating how they are
integrated into RL frameworks to address key challenges in this research field.
We establish a dual-axis taxonomy that organizes the field along two orthogonal
dimensions: a function-oriented taxonomy that clarifies the roles DMs play
within the RL pipeline, and a technique-oriented taxonomy that situates
implementations across online versus offline learning regimes. We also provide
a comprehensive examination of this progression from single-agent to
multi-agent domains, thereby forming several frameworks for DM-RL integration
and highlighting their practical utility. Furthermore, we outline several
categories of successful applications of diffusion-based RL across diverse
domains, discuss open research issues of current methodologies, and highlight
key directions for future research to advance the field. Finally, we summarize
the survey to identify promising future development directions. We are actively
maintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for
papers and other related resources to apply DMs for RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12200v1' target='_blank'>HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application
  Vulnerabilities</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoxue Ren, Penghao Jiang, Kaixin Li, Zhiyong Huang, Xiaoning Du, Jiaojiao Jiang, Zhenchang Xing, Jiamou Sun, Terry Yue Zhuo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 06:52:15</h6>
<p class='card-text'>Web applications are prime targets for cyberattacks as gateways to critical
services and sensitive data. Traditional penetration testing is costly and
expertise-intensive, making it difficult to scale with the growing web
ecosystem. While language model agents show promise in cybersecurity, modern
web applications demand visual understanding, dynamic content handling, and
multi-step interactions that only computer-use agents (CUAs) can perform. Yet,
their ability to discover and exploit vulnerabilities through graphical
interfaces remains largely unexplored. We present HackWorld, the first
framework for systematically evaluating CUAs' capabilities to exploit web
application vulnerabilities via visual interaction. Unlike sanitized
benchmarks, HackWorld includes 36 real-world applications across 11 frameworks
and 7 languages, featuring realistic flaws such as injection vulnerabilities,
authentication bypasses, and unsafe input handling. Using a Capture-the-Flag
(CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses
while navigating complex web interfaces. Evaluation of state-of-the-art CUAs
reveals concerning trends: exploitation rates below 12% and low cybersecurity
awareness. CUAs often fail at multi-step attack planning and misuse security
tools. These results expose the current limitations of CUAs in web security
contexts and highlight opportunities for developing more security-aware agents
capable of effective vulnerability detection and exploitation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12194v1' target='_blank'>ResearStudio: A Human-Intervenable Framework for Building Controllable
  Deep-Research Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Linyi Yang, Yixuan Weng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 06:40:11</h6>
<p class='card-text'>Current deep-research agents run in a ''fire-and-forget'' mode: once started,
they give users no way to fix errors or add expert knowledge during execution.
We present ResearStudio, the first open-source framework that places real-time
human control at its core. The system follows a Collaborative Workshop design.
A hierarchical Planner-Executor writes every step to a live
''plan-as-document,'' a fast communication layer streams each action, file
change, and tool call to a web interface. At any moment, the user can pause the
run, edit the plan or code, run custom commands, and resume -- switching
smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In
fully autonomous mode, ResearStudio achieves state-of-the-art results on the
GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These
results show that strong automated performance and fine-grained human control
can coexist. The full code, protocol, and evaluation scripts are available at
https://github.com/ResearAI/ResearStudio. We will continue to update the
repository to encourage further work on safe and controllable research agents.
Our live demo is publicly accessible at http://ai-researcher.net:3000/. We
support the development of DeepScientist, which can be accessed at
https://github.com/ResearAI/DeepScientist.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.12169v2' target='_blank'>Hybrid Terrain-Aware Path Planning: Integrating VD-RRT* Exploration and
  VD-D* Lite Repair</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Akshay Naik, William R. Norris, Dustin Nottage, Ahmet Soylemezoglu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-14 05:54:46</h6>
<p class='card-text'>Autonomous ground vehicles operating off-road must plan curvature-feasible
paths while accounting for spatially varying soil strength and slope hazards in
real time. We present a continuous state--cost metric that combines a Bekker
pressure--sinkage model with elevation-derived slope and attitude penalties.
The resulting terrain cost field is analytic, bounded, and monotonic in soil
modulus and slope, ensuring well-posed discretization and stable updates under
sensor noise. This metric is evaluated on a lattice with exact steering
primitives: Dubins and Reeds--Shepp motions for differential drive and
time-parameterized bicycle arcs for Ackermann steering. Global exploration is
performed using Vehicle-Dynamics RRT\(^{*}\), while local repair is managed by
Vehicle-Dynamics D\(^{*}\) Lite, enabling millisecond-scale replanning without
heuristic smoothing. By separating the terrain--vehicle model from the planner,
the framework provides a reusable basis for deterministic, sampling-based, or
learning-driven planning in deformable terrain. Hardware trials on an off-road
platform demonstrate real-time navigation across soft soil and slope
transitions, supporting reliable autonomy in unstructured environments.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>