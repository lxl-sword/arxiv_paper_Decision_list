<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2026-02-03</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2026-02-03</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.02468v1' target='_blank'>Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aiden Yiliu Li, Xinyue Hao, Shilong Liu, Mengdi Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 18:50:07</h6>
<p class='card-text'>Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.02437v1' target='_blank'>UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dianyi Wang, Chaofan Ma, Feng Han, Size Wu, Wei Song, Yibin Wang, Zhixiong Zhang, Tianhang Wang, Siyuan Wang, Zhongyu Wei, Jiaqi Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 18:34:35</h6>
<p class='card-text'>Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.02411v1' target='_blank'>Multi-Agent Monte Carlo Tree Search for Makespan-Efficient Object Rearrangement in Cluttered Spaces</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanwen Ren, Junyong Kim, Aathman Tharmasanthiran, Ahmed H. Qureshi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 18:10:45</h6>
<p class='card-text'>Object rearrangement planning in complex, cluttered environments is a common challenge in warehouses, households, and rescue sites. Prior studies largely address monotone instances, whereas real-world tasks are often non-monotone-objects block one another and must be temporarily relocated to intermediate positions before reaching their final goals. In such settings, effective multi-agent collaboration can substantially reduce the time required to complete tasks. This paper introduces Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a novel framework for general-purpose makespan-efficient object rearrangement planning in challenging environments. CAM-MCTS combines centralized task assignment-where agents remain aware of each other's intended actions to facilitate globally optimized planning-with an asynchronous task execution strategy that enables agents to take on new tasks at appropriate time steps, rather than waiting for others, guided by a one-step look-ahead cost estimate. This design minimizes idle time, prevents unnecessary synchronization delays, and enhances overall system efficiency. We evaluate CAM-MCTS across a diverse set of monotone and non-monotone tasks in cluttered environments, demonstrating consistent reductions in makespan compared to strong baselines. Finally, we validate our approach on a real-world multi-agent system under different configurations, further confirming its effectiveness and robustness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.02347v1' target='_blank'>Modelling Socio-Psychological Drivers of Land Management Intensity</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ronja Hotz, Calum Brown, Yongchao Zeng, Thomas Schmitt, Mark Rounsevell</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 17:09:45</h6>
<p class='card-text'>Land management intensity shapes ecosystem service provision, socio-ecological resilience and is central to sustainable transformation. Yet most land use models emphasise economic and biophysical drivers, while socio-psychological factors influencing land managers' decisions remain underrepresented despite increasing evidence that they shape land management choices. To address this gap, we develop a generic behavioural extension for agent-based land use models, guided by the Theory of Planned Behaviour as an overarching conceptual framework. The extension integrates environmental attitudes, descriptive social norms and behavioural inertia into land managers' decisions on land management intensity. To demonstrate applicability, the extension is coupled to an existing land use modelling framework and explored in stylised settings to isolate behavioural mechanisms. Results show that socio-psychological drivers can significantly alter land management intensity shares, landscape configuration, and ecosystem service provision. Nonlinear feedbacks between these drivers, spatial resource heterogeneity, and ecosystem service demand lead to emergent dynamics that are sometimes counter-intuitive and can diverge from the agent-level decision rules. Increasing the influence of social norms generates spatial clustering and higher landscape connectivity, while feedbacks between behavioural factors can lead to path dependence, lock-in effects, and the emergence of multiple stable regimes with sharp transitions. The proposed framework demonstrates how even low levels of behavioural diversity and social interactions can reshape system-level land use outcomes and provides a reusable modelling component for incorporating socio-psychological processes into land use simulations. The approach can be integrated into other agent-based land use models and parameterised empirically in future work.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.02308v1' target='_blank'>Chasing Long-Lived Doubly Charged Scalars at Future Lepton Colliders</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nandini Das, Dilip Kumar Ghosh, Nivedita Ghosh, Ritesh K. Singh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 16:40:06</h6>
<p class='card-text'>We come up with a novel search strategy for long-lived doubly charged scalars at future proposed lepton colliders. The doubly charged scalar studied in this work belongs to an $SU(2)_L$ complex scalar triplet that accounts for tiny neutrino masses via the Type-II Seesaw mechanism. For scalar masses $\lesssim 200 $ GeV and appropriate values of the triplet vacuum expectation value, this state can be long-lived and decay predominantly into like-sign muon pairs (e.g. $μ^+μ^+ $ or $μ^-μ^-$), producing distinctive displaced-vertex signals. We investigate the pair production of these scalars at the International Linear Collider (ILC) and a prospective muon collider, considering their planned center-of-mass energies. Incorporating theoretical and experimental constraints, we study the resulting signature of four leptons accompanied by missing transverse energy. Displaced vertices offer direct evidence of the scalar's long lifetime, while we further show that the invariant mass distribution of same-sign dilepton pairs serves as a powerful complementary probe for discovering doubly charged Higgs bosons at both the ILC and muon collider.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.02235v1' target='_blank'>Agent-Based Software Artifact Evaluation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhaonan Wu, Yanjie Zhao, Zhenpeng Chen, Zheng Wang, Haoyu Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 15:41:16</h6>
<p class='card-text'>Artifact evaluation has been adopted in the Software Engineering (SE) research community for 15 years, substantially improving research reproducibility across major SE conferences. However, this success has introduced a growing scalability challenge, as artifact evaluation relies heavily on reviewers' manual execution and debugging, leading to escalating human effort amid rapidly increasing paper submissions. To address this problem, we investigate automated artifact evaluation. We first conduct a preliminary study on artifacts from top-tier SE conferences and identify three key challenges: perceiving execution states, maintaining stable execution environments, and recovering from execution errors. Inspired by these findings, we propose ArtifactCopilot, the first end-to-end agent-based framework for automated artifact evaluation. ArtifactCopilot automates environment construction, instruction execution, and error recovery by combining an execution normalization strategy to ensure environment stability with an artifact evaluation graph that transforms README documents into dependency-aware command graphs, enabling structured execution planning, execution-state tracking, and error recovery. Evaluation on 48 real-world artifacts shows that ArtifactCopilot matches human artifact evaluation outcomes for 85.42% of the artifacts, outperforming Claude Code by 52.09 percentage points, while costing only \$0.091 per artifact on average and requiring zero human intervention for 45 out of 48 artifacts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.02179v1' target='_blank'>SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marina Mastroleo, Alberto Archetti, Federico Mastroleo, Matteo Matteucci</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 14:49:14</h6>
<p class='card-text'>Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.02158v1' target='_blank'>Traffic-Aware Navigation in Road Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sarah Nassar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 14:35:49</h6>
<p class='card-text'>This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.02110v1' target='_blank'>An Empirical Study of World Model Quantization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhongqian Fu, Tianyi Zhao, Kai Han, Hang Zhou, Xinghao Chen, Yunhe Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 13:54:03</h6>
<p class='card-text'>World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.02084v1' target='_blank'>Closing the Loop: Universal Repository Representation with RPG-Encoder</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jane Luo, Chengyu Yin, Xin Zhang, Qingtao Li, Steven Liu, Yiming Huang, Jie Wu, Hao Liu, Yangyu Huang, Yu Kang, Fangkai Yang, Ying Xin, Scarlett Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 13:30:00</h6>
<p class='card-text'>Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.02009v1' target='_blank'>Logic-Guided Vector Fields for Constrained Generative Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ali Baheri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 12:09:42</h6>
<p class='card-text'>Neuro-symbolic systems aim to combine the expressive structure of symbolic logic with the flexibility of neural learning; yet, generative models typically lack mechanisms to enforce declarative constraints at generation time. We propose Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework that injects symbolic knowledge, specified as differentiable relaxations of logical constraints, into flow matching generative models. LGVF couples two complementary mechanisms: (1) a training-time logic loss that penalizes constraint violations along continuous flow trajectories, with weights that emphasize correctness near the target distribution; and (2) an inference-time adjustment that steers sampling using constraint gradients, acting as a lightweight, logic-informed correction to the learned dynamics. We evaluate LGVF on three constrained generation case studies spanning linear, nonlinear, and multi-region feasibility constraints. Across all settings, LGVF reduces constraint violations by 59-82% compared to standard flow matching and achieves the lowest violation rates in each case. In the linear and ring settings, LGVF also improves distributional fidelity as measured by MMD, while in the multi-obstacle setting, we observe a satisfaction-fidelity trade-off, with improved feasibility but increased MMD. Beyond quantitative gains, LGVF yields constraint-aware vector fields exhibiting emergent obstacle-avoidance behavior, routing samples around forbidden regions without explicit path planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01972v1' target='_blank'>Heat load measurements for the PIP-II pHB650 cryomodule</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:D. Porwisiak, M. J. White, V. Roger, J. Bernardini, B. J. Hansen, J. P. Holzbauer, J. Makara, J. Ozelis, D. Passarelli, S. Yoon, J. Subedi, S. Ranpariya, V. Patel, J. Dong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 11:25:46</h6>
<p class='card-text'>Phase-3 testing of the pHB650 cryomodule at the PIP-II Injector Test Facility was conducted to evaluate the effectiveness of heat load mitigations performed after earlier phases of testing and to continue pinpointing any sources of unexpectedly high heat loads.. The programme measured HTTS, LTTS, and 2 K isothermal/non-isothermal loads under "standard", "linac", and "simulated dynamic" operating modes, recording data both inside the cryomodule and across the bayonet can circuits. Thermal-acoustic oscillations were eliminated by replacing the original G10 cooldown-valve stem with a stainless-steel stem fitted with wipers. A newly developed Python script automated acquisition of ACNET data, performed real-time heat-load calculations, and generated plots and tables that were posted to the electronic logbook within minutes, vastly reducing manual effort and accelerating feedback between SRF and cryogenics teams. Analysis showed that JT heat-exchanger effectiveness and temperature stratification in the two-phase and relief piping strongly influence the observed loads and helped isolate sources of excess heat. The campaign demonstrates that rigorous pre-test planning, real-time diagnostics, and automated reporting can improve both accuracy and efficiency, providing a template for future PIP-II cryomodule tests and for implementing targeted heat-load mitigations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01960v1' target='_blank'>Grounding Generated Videos in Feasible Plans via World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Christos Ziakas, Amir Bar, Alessandra Russo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 11:04:47</h6>
<p class='card-text'>Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01942v1' target='_blank'>Human Society-Inspired Approaches to Agentic AI Security: The 4C Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alsharif Abuadbba, Nazatul Sultan, Surya Nepal, Sanjay Jha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 10:45:16</h6>
<p class='card-text'>AI is moving from domain-specific autonomy in closed, predictable settings to large-language-model-driven agents that plan and act in open, cross-organizational environments. As a result, the cybersecurity risk landscape is changing in fundamental ways. Agentic AI systems can plan, act, collaborate, and persist over time, functioning as participants in complex socio-technical ecosystems rather than as isolated software components. Although recent work has strengthened defenses against model and pipeline level vulnerabilities such as prompt injection, data poisoning, and tool misuse, these system centric approaches may fail to capture risks that arise from autonomy, interaction, and emergent behavior. This article introduces the 4C Framework for multi-agent AI security, inspired by societal governance. It organizes agentic risks across four interdependent dimensions: Core (system, infrastructure, and environmental integrity), Connection (communication, coordination, and trust), Cognition (belief, goal, and reasoning integrity), and Compliance (ethical, legal, and institutional governance). By shifting AI security from a narrow focus on system-centric protection to the broader preservation of behavioral integrity and intent, the framework complements existing AI security strategies and offers a principled foundation for building agentic AI systems that are trustworthy, governable, and aligned with human values.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01862v1' target='_blank'>Coordinated planning of European charging infrastructure and energy system for optimal V1G and V2G deployment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Francesco Sanvito, Francesco Lombardi, Stefan Pfenninger-Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 09:34:14</h6>
<p class='card-text'>Vehicle charging infrastructure targets in Europe currently rely on uniform benchmarks and overlook the flexibility that could be provided by future smart charging (V1G) and vehicle to grid operation (V2G). To address this gap, we explicitly represent charging infrastructure and its costs in a cost minimizing European energy system model, allowing uncontrolled charging, V1G, and V2G to compete. We find that V1G captures the majority of system cost savings, amounting to 19 to 42 billion euros per year, or 2.2 to 4.5 percent, and substantially reduces infrastructure requirements. V2G provides more limited system cost savings of up to 2.5 billion euros per year, but generates substantial balancing market revenues of around 6.4 billion euros per year. V2G deployment is most cost effective in photovoltaic dominated systems and in scenarios with limited grid expansion, where combined solar and wind generation is relatively scarce. Charging infrastructure requirements vary across countries, reflecting either utilization maximization or flexibility maximization. This indicates that uniform EU targets risk overestimating infrastructure needs in some regions while constraining the benefits of smart charging in others.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01825v1' target='_blank'>Learning Sequential Decisions from Multiple Sources via Group-Robust Markov Decision Processes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingyuan Xu, Zongqi Xia, Tianxi Cai, Doudou Zhou, Nian Si</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 08:58:55</h6>
<p class='card-text'>We often collect data from multiple sites (e.g., hospitals) that share common structure but also exhibit heterogeneity. This paper aims to learn robust sequential decision-making policies from such offline, multi-site datasets. To model cross-site uncertainty, we study distributionally robust MDPs with a group-linear structure: all sites share a common feature map, and both the transition kernels and expected reward functions are linear in these shared features. We introduce feature-wise (d-rectangular) uncertainty sets, which preserve tractable robust Bellman recursions while maintaining key cross-site structure. Building on this, we then develop an offline algorithm based on pessimistic value iteration that includes: (i) per-site ridge regression for Bellman targets, (ii) feature-wise worst-case (row-wise minimization) aggregation, and (iii) a data-dependent pessimism penalty computed from the diagonals of the inverse design matrices. We further propose a cluster-level extension that pools similar sites to improve sample efficiency, guided by prior knowledge of site similarity. Under a robust partial coverage assumption, we prove a suboptimality bound for the resulting policy. Overall, our framework addresses multi-site learning with heterogeneous data sources and provides a principled approach to robust planning without relying on strong state-action rectangularity assumptions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01780v1' target='_blank'>DDP-WM: Disentangled Dynamics Prediction for Efficient World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shicheng Yin, Kaixuan Yin, Weixing Chen, Yang Liu, Guanbin Li, Liang Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 08:04:25</h6>
<p class='card-text'>World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01776v1' target='_blank'>Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingyue Cheng, Xiaoyu Tao, Qi Liu, Ze Guo, Enhong Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 08:01:11</h6>
<p class='card-text'>Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01725v1' target='_blank'>SafePred: A Predictive Guardrail for Computer-Using Agents via World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yurun Chen, Zeyi Liao, Ping Yin, Taotao Xie, Keting Yin, Shengyu Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 07:04:06</h6>
<p class='card-text'>With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01693v1' target='_blank'>GSR: Learning Structured Reasoning for Embodied Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kewei Hu, Michael Zhang, Wei Ying, Tianhao Liu, Guoqiang Hao, Zimeng Li, Wanchan Yu, Jiajian Jing, Fangwen Chen, Hanwen Kang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 06:07:42</h6>
<p class='card-text'>Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01691v1' target='_blank'>Locally sparse estimation for simultaneous functional quantile regression</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Boyi Hu, Jiguo Cao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 06:07:14</h6>
<p class='card-text'>Motivated by the study of how daily temperature affects soybean yield, this article proposes a simultaneous functional quantile regression (FQR) model featuring a locally sparse bivariate slope function indexed by both quantile and time and linked to a functional predictor. The slope function's local sparsity means it holds non-zero values only in certain segments of its domain, remaining zero elsewhere. These zero-slope regions, which vary by quantile, indicate times when the functional predictor has no discernible impact on the response variable. This feature boosts the model's interpretability. Unlike traditional FQR models, which fit one quantile at a time and have several limitations, our proposed method can handle a spectrum of quantiles simultaneously. We tested the new approach through simulation studies, demonstrating its clear advantages over standard techniques. To validate its practical use, we applied the method to soybean yield data, pinpointing the time periods when daily temperature doesn't affect yield. This insight could be crucial for agricultural planning and crop management.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01608v1' target='_blank'>Reasoning with Autoregressive-Diffusion Collaborative Thoughts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 03:54:15</h6>
<p class='card-text'>Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01568v1' target='_blank'>Efficiently Solving Mixed-Hierarchy Games with Quasi-Policy Approximations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hamzah Khan, Dong Ho Lee, Jingqi Li, Tianyu Qiu, Christian Ellis, Jesse Milzman, Wesley Suttle, David Fridovich-Keil</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 03:03:29</h6>
<p class='card-text'>Multi-robot coordination often exhibits hierarchical structure, with some robots' decisions depending on the planned behaviors of others. While game theory provides a principled framework for such interactions, existing solvers struggle to handle mixed information structures that combine simultaneous (Nash) and hierarchical (Stackelberg) decision-making. We study N-robot forest-structured mixed-hierarchy games, in which each robot acts as a Stackelberg leader over its subtree while robots in different branches interact via Nash equilibria. We derive the Karush-Kuhn-Tucker (KKT) first-order optimality conditions for this class of games and show that they involve increasingly high-order derivatives of robots' best-response policies as the hierarchy depth grows, rendering a direct solution intractable. To overcome this challenge, we introduce a quasi-policy approximation that removes higher-order policy derivatives and develop an inexact Newton method for efficiently solving the resulting approximated KKT systems. We prove local exponential convergence of the proposed algorithm for games with non-quadratic objectives and nonlinear constraints. The approach is implemented in a highly optimized Julia library (MixedHierarchyGames.jl) and evaluated in simulated experiments, demonstrating real-time convergence for complex mixed-hierarchy information structures.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01538v1' target='_blank'>Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu, Sen Liang, Guozhen Zhang, Ziqiao Peng, Shunkai Li, Yi Chen, Zixiang Zhou, Yuan Zhou, Qinglin Lu, Xiu Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 02:12:09</h6>
<p class='card-text'>Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01536v1' target='_blank'>UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuai Liu, Siheng Ren, Xiaoyao Zhu, Quanmin Liang, Zefeng Li, Qiang Li, Xin Hu, Kai Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 02:10:51</h6>
<p class='card-text'>Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01502v1' target='_blank'>Optimal Sizing of Charging Energy Hubs for Heavy-Duty Electric Transport through Co-Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:M. Izadi, D. Fernandez Zapico, M. Salazar, T. Hofman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-02 00:32:16</h6>
<p class='card-text'>Electrification of heavy-duty vehicles places substantial stress on distribution grids, and Charging Energy Hubs (CEHs) mitigate these impacts by integrating charging infrastructure with renewable energy sources and battery storage. Optimal sizing of CEH components is therefore a critical investment decision, yet challenging because design choices depend strongly on operational dynamics. This work presents a mixed-integer linear programming model for the optimal sizing of CEH components, using a co-design approach that jointly optimizes component sizing and operational decisions. A case study for a heavy-duty fleet demonstrates the effectiveness of the method for cost-efficient, scalable, and grid-compliant CEH planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01475v1' target='_blank'>Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Brij Malhotra, Shivvrat Arya, Tahrima Rahman, Vibhav Giridhar Gogate</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-01 22:43:28</h6>
<p class='card-text'>Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs) is a fundamental yet computationally challenging problem arising in domains such as diagnosis, planning, and structured prediction. In many practical settings, the graphical model remains fixed while inference must be performed repeatedly for varying evidence patterns. Stochastic Local Search (SLS) algorithms scale to large models but rely on myopic best-improvement rule that prioritizes immediate likelihood gains and often stagnate in poor local optima. Heuristics such as Guided Local Search (GLS+) partially alleviate this limitation by modifying the search landscape, but their guidance cannot be reused effectively across multiple inference queries on the same model. We propose a neural amortization framework for improving local search in this repeated-query regime. Exploiting the fixed graph structure, we train an attention-based network to score local moves by predicting their ability to reduce Hamming distance to a near-optimal solution. Our approach integrates seamlessly with existing local search procedures, using this signal to balance short-term likelihood gains with long-term promise during neighbor selection. We provide theoretical intuition linking distance-reducing move selection to improved convergence behavior, and empirically demonstrate consistent improvements over SLS and GLS+ on challenging high-treewidth benchmarks in the amortized inference setting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01463v1' target='_blank'>On three recent questions of Bourin and Lee on quadratic symmetric modulus and Euler-operator identity</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Teng Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-01 22:14:56</h6>
<p class='card-text'>We answer three questions posed by Bourin and Lee on symmetric moduli and related orbit inequalities in \cite{BL26}. First, we show that the exponent $2$ in their unitary-orbit estimate for the quadratic symmetric modulus is optimal in every dimension $n\ge2$ by an explicit $2\times2$ counterexample for $p>2$.Second, we construct a compact operator for which the corresponding singular-value inequality fails for every $p>2$ (indeed for all $p>0$ at a fixed pair of indices).Finally, we obtain isometry-orbit refinements of Euler's quadrilateral identity for matrices and derive several Clarkson--McCarthy type inequalities for Schatten $p$-norms and related consequences.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01419v1' target='_blank'>Semi-supervised CAPP Transformer Learning via Pseudo-labeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dennis Gross, Helge Spieker, Arnaud Gotlieb, Emmanuel Stathatos, Panorios Benardos, George-Christopher Vosniakos</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-01 19:51:39</h6>
<p class='card-text'>High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer models without manual labeling. An oracle, trained on available transformer behaviour data, filters correct predictions from unseen parts, which are then used for one-shot retraining. Experiments on small-scale datasets with simulated ground truth across the full data distribution show consistent accuracy gains over baselines, demonstrating the method's effectiveness in data-scarce manufacturing environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2602.01367v1' target='_blank'>Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pinar Erbil, Alberto Archetti, Eugenio Lomurno, Matteo Matteucci</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2026-02-01 18:07:40</h6>
<p class='card-text'>Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>