<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>multi-agent - 2025-03-20</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>multi-agent - 2025-03-20</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.15172v1' target='_blank'>Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic
  Spectrum Access Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:George Stamatelis, Angelos-Nikolaos Kanatas, George C. Alexandropoulos</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-19 12:56:23</h6>
<p class='card-text'>Multi-Agent Deep Reinforcement Learning (MADRL) has emerged as a powerful
tool for optimizing decentralized decision-making systems in complex settings,
such as Dynamic Spectrum Access (DSA). However, deploying deep learning models
on resource-constrained edge devices remains challenging due to their high
computational cost. To address this challenge, in this paper, we present a
novel sparse recurrent MARL framework integrating gradual neural network
pruning into the independent actor global critic paradigm. Additionally, we
introduce a harmonic annealing sparsity scheduler, which achieves comparable,
and in certain cases superior, performance to standard linear and polynomial
pruning schedulers at large sparsities. Our experimental investigation
demonstrates that the proposed DSA framework can discover superior policies,
under diverse training conditions, outperforming conventional DSA, MADRL
baselines, and state-of-the-art pruning techniques.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.14576v1' target='_blank'>SocialJax: An Evaluation Suite for Multi-agent Reinforcement Learning in
  Sequential Social Dilemmas</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zihao Guo, Richard Willis, Shuqing Shi, Tristan Tomilin, Joel Z. Leibo, Yali Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-18 16:03:59</h6>
<p class='card-text'>Social dilemmas pose a significant challenge in the field of multi-agent
reinforcement learning (MARL). Melting Pot is an extensive framework designed
to evaluate social dilemma environments, providing an evaluation protocol that
measures generalization to new social partners across various test scenarios.
However, running reinforcement learning algorithms in the official Melting Pot
environments demands substantial computational resources. In this paper, we
introduce SocialJax, a suite of sequential social dilemma environments
implemented in JAX. JAX is a high-performance numerical computing library for
Python that enables significant improvements in the operational efficiency of
SocialJax on GPUs and TPUs. Our experiments demonstrate that the training
pipeline of SocialJax achieves a 50\texttimes{} speedup in real-time
performance compared to Melting Pot's RLlib baselines. Additionally, we
validate the effectiveness of baseline algorithms within the SocialJax
environments. Finally, we use Schelling diagrams to verify the social dilemma
properties of these environments, ensuring they accurately capture the dynamics
of social dilemmas.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.14555v1' target='_blank'>A Generalist Hanabi Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arjun V Sudhakar, Hadi Nekoei, Mathieu Reymond, Miao Liu, Janarthanan Rajendran, Sarath Chandar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-17 22:25:15</h6>
<p class='card-text'>Traditional multi-agent reinforcement learning (MARL) systems can develop
cooperative strategies through repeated interactions. However, these systems
are unable to perform well on any other setting than the one they have been
trained on, and struggle to successfully cooperate with unfamiliar
collaborators. This is particularly visible in the Hanabi benchmark, a popular
2-to-5 player cooperative card-game which requires complex reasoning and
precise assistance to other agents. Current MARL agents for Hanabi can only
learn one specific game-setting (e.g., 2-player games), and play with the same
algorithmic agents. This is in stark contrast to humans, who can quickly adjust
their strategies to work with unfamiliar partners or situations. In this paper,
we introduce Recurrent Replay Relevance Distributed DQN (R3D2), a generalist
agent for Hanabi, designed to overcome these limitations. We reformulate the
task using text, as language has been shown to improve transfer. We then
propose a distributed MARL algorithm that copes with the resulting dynamic
observation- and action-space. In doing so, our agent is the first that can
play all game settings concurrently, and extend strategies learned from one
setting to other ones. As a consequence, our agent also demonstrates the
ability to collaborate with different algorithmic agents -- agents that are
themselves unable to do so. The implementation code is available at:
$\href{https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent}{R3D2-A-Generalist-Hanabi-Agent}$</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.13415v1' target='_blank'>A Comprehensive Survey on Multi-Agent Cooperative Decision-Making:
  Scenarios, Approaches, Challenges and Perspectives</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weiqiang Jin, Hongyang Du, Biao Zhao, Xingwu Tian, Bohang Shi, Guang Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-17 17:45:46</h6>
<p class='card-text'>With the rapid development of artificial intelligence, intelligent
decision-making techniques have gradually surpassed human levels in various
human-machine competitions, especially in complex multi-agent cooperative task
scenarios. Multi-agent cooperative decision-making involves multiple agents
working together to complete established tasks and achieve specific objectives.
These techniques are widely applicable in real-world scenarios such as
autonomous driving, drone navigation, disaster rescue, and simulated military
confrontations. This paper begins with a comprehensive survey of the leading
simulation environments and platforms used for multi-agent cooperative
decision-making. Specifically, we provide an in-depth analysis for these
simulation environments from various perspectives, including task formats,
reward allocation, and the underlying technologies employed. Subsequently, we
provide a comprehensive overview of the mainstream intelligent decision-making
approaches, algorithms and models for multi-agent systems (MAS).
Theseapproaches can be broadly categorized into five types: rule-based
(primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep
multi-agent reinforcement learning (MARL)-based, and large language
models(LLMs)reasoning-based. Given the significant advantages of MARL
andLLMs-baseddecision-making methods over the traditional rule, game theory,
and evolutionary algorithms, this paper focuses on these multi-agent methods
utilizing MARL and LLMs-based techniques. We provide an in-depth discussion of
these approaches, highlighting their methodology taxonomies, advantages, and
drawbacks. Further, several prominent research directions in the future and
potential challenges of multi-agent cooperative decision-making are also
detailed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.13553v1' target='_blank'>LLM-Mediated Guidance of MARL Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Philipp D. Siedler, Ian Gemp</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-16 20:16:13</h6>
<p class='card-text'>In complex multi-agent environments, achieving efficient learning and
desirable behaviours is a significant challenge for Multi-Agent Reinforcement
Learning (MARL) systems. This work explores the potential of combining MARL
with Large Language Model (LLM)-mediated interventions to guide agents toward
more desirable behaviours. Specifically, we investigate how LLMs can be used to
interpret and facilitate interventions that shape the learning trajectories of
multiple agents. We experimented with two types of interventions, referred to
as controllers: a Natural Language (NL) Controller and a Rule-Based (RB)
Controller. The NL Controller, which uses an LLM to simulate human-like
interventions, showed a stronger impact than the RB Controller. Our findings
indicate that agents particularly benefit from early interventions, leading to
more efficient training and higher performance. Both intervention types
outperform the baseline without interventions, highlighting the potential of
LLM-mediated guidance to accelerate training and enhance MARL performance in
challenging environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.13547v1' target='_blank'>Adaptive AUV Hunting Policy with Covert Communication via Diffusion
  Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xu Guo, Xiangwang Hou, Minrui Xu, Jianrui Chen, Jingjing Wang, Jun Du, Yong Ren</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-16 13:35:53</h6>
<p class='card-text'>Collaborative underwater target hunting, facilitated by multiple autonomous
underwater vehicles (AUVs), plays a significant role in various domains,
especially military missions. Existing research predominantly focuses on
designing efficient and high-success-rate hunting policy, particularly
addressing the target's evasion capabilities. However, in real-world scenarios,
the target can not only adjust its evasion policy based on its observations and
predictions but also possess eavesdropping capabilities. If communication among
hunter AUVs, such as hunting policy exchanges, is intercepted by the target, it
can adapt its escape policy accordingly, significantly reducing the success
rate of the hunting mission. To address this challenge, we propose a covert
communication-guaranteed collaborative target hunting framework, which ensures
efficient hunting in complex underwater environments while defending against
the target's eavesdropping. To the best of our knowledge, this is the first
study to incorporate the confidentiality of inter-agent communication into the
design of target hunting policy. Furthermore, given the complexity of
coordinating multiple AUVs in dynamic and unpredictable environments, we
propose an adaptive multi-agent diffusion policy (AMADP), which incorporates
the strong generative ability of diffusion models into the multi-agent
reinforcement learning (MARL) algorithm. Experimental results demonstrate that
AMADP achieves faster convergence and higher hunting success rates while
maintaining covertness constraints.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.12122v1' target='_blank'>ICCO: Learning an Instruction-conditioned Coordinator for
  Language-guided Task-aligned Multi-robot Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yoshiki Yano, Kazuki Shibata, Maarten Kokshoorn, Takamitsu Matsubara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-15 13:03:20</h6>
<p class='card-text'>Recent advances in Large Language Models (LLMs) have permitted the
development of language-guided multi-robot systems, which allow robots to
execute tasks based on natural language instructions. However, achieving
effective coordination in distributed multi-agent environments remains
challenging due to (1) misalignment between instructions and task requirements
and (2) inconsistency in robot behaviors when they independently interpret
ambiguous instructions. To address these challenges, we propose
Instruction-Conditioned Coordinator (ICCO), a Multi-Agent Reinforcement
Learning (MARL) framework designed to enhance coordination in language-guided
multi-robot systems. ICCO consists of a Coordinator agent and multiple Local
Agents, where the Coordinator generates Task-Aligned and Consistent
Instructions (TACI) by integrating language instructions with environmental
states, ensuring task alignment and behavioral consistency. The Coordinator and
Local Agents are jointly trained to optimize a reward function that balances
task efficiency and instruction following. A Consistency Enhancement Term is
added to the learning objective to maximize mutual information between
instructions and robot behaviors, further improving coordination. Simulation
and real-world experiments validate the effectiveness of ICCO in achieving
language-guided task-aligned multi-robot control. The demonstration can be
found at https://yanoyoshiki.github.io/ICCO/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.11488v1' target='_blank'>Unicorn: A Universal and Collaborative Reinforcement Learning Approach
  Towards Generalizable Network-Wide Traffic Signal Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yifeng Zhang, Yilin Liu, Ping Gong, Peizhuo Li, Mingfeng Fan, Guillaume Sartoretti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-14 15:13:42</h6>
<p class='card-text'>Adaptive traffic signal control (ATSC) is crucial in reducing congestion,
maximizing throughput, and improving mobility in rapidly growing urban areas.
Recent advancements in parameter-sharing multi-agent reinforcement learning
(MARL) have greatly enhanced the scalable and adaptive optimization of complex,
dynamic flows in large-scale homogeneous networks. However, the inherent
heterogeneity of real-world traffic networks, with their varied intersection
topologies and interaction dynamics, poses substantial challenges to achieving
scalable and effective ATSC across different traffic scenarios. To address
these challenges, we present Unicorn, a universal and collaborative MARL
framework designed for efficient and adaptable network-wide ATSC. Specifically,
we first propose a unified approach to map the states and actions of
intersections with varying topologies into a common structure based on traffic
movements. Next, we design a Universal Traffic Representation (UTR) module with
a decoder-only network for general feature extraction, enhancing the model's
adaptability to diverse traffic scenarios. Additionally, we incorporate an
Intersection Specifics Representation (ISR) module, designed to identify key
latent vectors that represent the unique intersection's topology and traffic
dynamics through variational inference techniques. To further refine these
latent representations, we employ a contrastive learning approach in a
self-supervised manner, which enables better differentiation of
intersection-specific features. Moreover, we integrate the state-action
dependencies of neighboring agents into policy optimization, which effectively
captures dynamic agent interactions and facilitates efficient regional
collaboration. Our results show that Unicorn outperforms other methods across
various evaluation metrics, highlighting its potential in complex, dynamic
traffic networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.11726v1' target='_blank'>SPECTra: Scalable Multi-Agent Reinforcement Learning with
  Permutation-Free Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hyunwoo Park, Baekryun Seong, Sang-Ki Ko</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-14 04:26:51</h6>
<p class='card-text'>In cooperative multi-agent reinforcement learning (MARL), the permutation
problem where the state space grows exponentially with the number of agents
reduces sample efficiency. Additionally, many existing architectures struggle
with scalability, relying on a fixed structure tied to a specific number of
agents, limiting their applicability to environments with a variable number of
entities. While approaches such as graph neural networks (GNNs) and
self-attention mechanisms have progressed in addressing these challenges, they
have significant limitations as dense GNNs and self-attention mechanisms incur
high computational costs. To overcome these limitations, we propose a novel
agent network and a non-linear mixing network that ensure
permutation-equivariance and scalability, allowing them to generalize to
environments with various numbers of agents. Our agent network significantly
reduces computational complexity, and our scalable hypernetwork enables
efficient weight generation for non-linear mixing. Additionally, we introduce
curriculum learning to improve training efficiency. Experiments on SMACv2 and
Google Research Football (GRF) demonstrate that our approach achieves superior
learning performance compared to existing methods. By addressing both
permutation-invariance and scalability in MARL, our work provides a more
efficient and adaptable framework for cooperative MARL. Our code is available
at https://github.com/funny-rl/SPECTra.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10907v1' target='_blank'>H2-MARL: Multi-Agent Reinforcement Learning for Pareto Optimality in
  Hospital Capacity Strain and Human Mobility during Epidemic</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xueting Luo, Hao Deng, Jihong Yang, Yao Shen, Huanhuan Guo, Zhiyuan Sun, Mingqing Liu, Jiming Wei, Shengjie Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 21:40:07</h6>
<p class='card-text'>The necessity of achieving an effective balance between minimizing the losses
associated with restricting human mobility and ensuring hospital capacity has
gained significant attention in the aftermath of COVID-19. Reinforcement
learning (RL)-based strategies for human mobility management have recently
advanced in addressing the dynamic evolution of cities and epidemics; however,
they still face challenges in achieving coordinated control at the township
level and adapting to cities of varying scales. To address the above issues, we
propose a multi-agent RL approach that achieves Pareto optimality in managing
hospital capacity and human mobility (H2-MARL), applicable across cities of
different scales. We first develop a township-level infection model with
online-updatable parameters to simulate disease transmission and construct a
city-wide dynamic spatiotemporal epidemic simulator. On this basis, H2-MARL is
designed to treat each division as an agent, with a trade-off dual-objective
reward function formulated and an experience replay buffer enriched with expert
knowledge built. To evaluate the effectiveness of the model, we construct a
township-level human mobility dataset containing over one billion records from
four representative cities of varying scales. Extensive experiments demonstrate
that H2-MARL has the optimal dual-objective trade-off capability, which can
minimize hospital capacity strain while minimizing human mobility restriction
loss. Meanwhile, the applicability of the proposed model to epidemic control in
cities of varying scales is verified, which showcases its feasibility and
versatility in practical applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10049v1' target='_blank'>Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based
  Planner and Graph-based Policy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziqi Jia, Junjie Li, Xiaoyang Qu, Jianzong Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 05:02:49</h6>
<p class='card-text'>Multi-agent systems (MAS) have shown great potential in executing complex
tasks, but coordination and safety remain significant challenges. Multi-Agent
Reinforcement Learning (MARL) offers a promising framework for agent
collaboration, but it faces difficulties in handling complex tasks and
designing reward functions. The introduction of Large Language Models (LLMs)
has brought stronger reasoning and cognitive abilities to MAS, but existing
LLM-based systems struggle to respond quickly and accurately in dynamic
environments. To address these challenges, we propose LLM-based Graph
Collaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and
MARL. This framework decomposes complex tasks into executable subtasks and
achieves efficient collaboration among multiple agents through graph-based
coordination. Specifically, LGC-MARL consists of two main components: an LLM
planner and a graph-based collaboration meta policy. The LLM planner transforms
complex task instructions into a series of executable subtasks, evaluates the
rationality of these subtasks using a critic model, and generates an action
dependency graph. The graph-based collaboration meta policy facilitates
communication and collaboration among agents based on the action dependency
graph, and adapts to new task environments through meta-learning. Experimental
results on the AI2-THOR simulation platform demonstrate the superior
performance and scalability of LGC-MARL in completing various complex tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09501v2' target='_blank'>ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 16:05:31</h6>
<p class='card-text'>Recent research on Reasoning of Large Language Models (LLMs) has sought to
further enhance their performance by integrating meta-thinking -- enabling
models to monitor, evaluate, and control their reasoning processes for more
adaptive and effective problem-solving. However, current single-agent work
lacks a specialized design for acquiring meta-thinking, resulting in low
efficacy. To address this challenge, we introduce Reinforced Meta-thinking
Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement
Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think
about thinking. ReMA decouples the reasoning process into two hierarchical
agents: a high-level meta-thinking agent responsible for generating strategic
oversight and plans, and a low-level reasoning agent for detailed executions.
Through iterative reinforcement learning with aligned objectives, these agents
explore and learn collaboration, leading to improved generalization and
robustness. Experimental results demonstrate that ReMA outperforms single-agent
RL baselines on complex reasoning tasks, including competitive-level
mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation
studies further illustrate the evolving dynamics of each distinct agent,
providing valuable insights into how the meta-thinking reasoning process
enhances the reasoning capabilities of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08740v1' target='_blank'>Cooperative Bearing-Only Target Pursuit via Multiagent Reinforcement
  Learning: Design and Experiment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianan Li, Zhikun Wang, Susheng Ding, Shiliang Guo, Shiyu Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 08:21:35</h6>
<p class='card-text'>This paper addresses the multi-robot pursuit problem for an unknown target,
encompassing both target state estimation and pursuit control. First, in state
estimation, we focus on using only bearing information, as it is readily
available from vision sensors and effective for small, distant targets.
Challenges such as instability due to the nonlinearity of bearing measurements
and singularities in the two-angle representation are addressed through a
proposed uniform bearing-only information filter. This filter integrates
multiple 3D bearing measurements, provides a concise formulation, and enhances
stability and resilience to target loss caused by limited field of view (FoV).
Second, in target pursuit control within complex environments, where challenges
such as heterogeneity and limited FoV arise, conventional methods like
differential games or Voronoi partitioning often prove inadequate. To address
these limitations, we propose a novel multiagent reinforcement learning (MARL)
framework, enabling multiple heterogeneous vehicles to search, localize, and
follow a target while effectively handling those challenges. Third, to bridge
the sim-to-real gap, we propose two key techniques: incorporating adjustable
low-level control gains in training to replicate the dynamics of real-world
autonomous ground vehicles (AGVs), and proposing spectral-normalized RL
algorithms to enhance policy smoothness and robustness. Finally, we demonstrate
the successful zero-shot transfer of the MARL controllers to AGVs, validating
the effectiveness and practical feasibility of our approach. The accompanying
video is available at https://youtu.be/HO7FJyZiJ3E.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08728v1' target='_blank'>Enhancing Traffic Signal Control through Model-based Reinforcement
  Learning and Policy Reuse</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yihong Li, Chengwei Zhang, Furui Zhan, Wanting Liu, Kailing Zhou, Longji Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 01:21:13</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) has shown significant potential in
traffic signal control (TSC). However, current MARL-based methods often suffer
from insufficient generalization due to the fixed traffic patterns and road
network conditions used during training. This limitation results in poor
adaptability to new traffic scenarios, leading to high retraining costs and
complex deployment. To address this challenge, we propose two algorithms:
PLight and PRLight. PLight employs a model-based reinforcement learning
approach, pretraining control policies and environment models using predefined
source-domain traffic scenarios. The environment model predicts the state
transitions, which facilitates the comparison of environmental features.
PRLight further enhances adaptability by adaptively selecting pre-trained
PLight agents based on the similarity between the source and target domains to
accelerate the learning process in the target domain. We evaluated the
algorithms through two transfer settings: (1) adaptability to different traffic
scenarios within the same road network, and (2) generalization across different
road networks. The results show that PRLight significantly reduces the
adaptation time compared to learning from scratch in new TSC scenarios,
achieving optimal performance using similarities between available and target
scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07678v1' target='_blank'>Using a single actor to output personalized policy for different
  intersections</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kailing Zhou, Chengwei Zhang, Furui Zhan, Wanting Liu, Yihong Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 07:55:33</h6>
<p class='card-text'>Recently, with the development of Multi-agent reinforcement learning (MARL),
adaptive traffic signal control (ATSC) has achieved satisfactory results. In
traffic scenarios with multiple intersections, MARL treats each intersection as
an agent and optimizes traffic signal control strategies through learning and
real-time decision-making. Considering that observation distributions of
intersections might be different in real-world scenarios, shared parameter
methods might lack diversity and thus lead to high generalization requirements
in the shared-policy network. A typical solution is to increase the size of
network parameters. However, simply increasing the scale of the network does
not necessarily improve policy generalization, which is validated in our
experiments. Accordingly, an approach that considers both the personalization
of intersections and the efficiency of parameter sharing is required. To this
end, we propose Hyper-Action Multi-Head Proximal Policy Optimization
(HAMH-PPO), a Centralized Training with Decentralized Execution (CTDE) MARL
method that utilizes a shared PPO policy network to deliver personalized
policies for intersections with non-iid observation distributions. The
centralized critic in HAMH-PPO uses graph attention units to calculate the
graph representations of all intersections and outputs a set of value estimates
with multiple output heads for each intersection. The decentralized execution
actor takes the local observation history as input and output distributions of
action as well as a so-called hyper-action to balance the multiple values
estimated from the centralized critic to further guide the updating of TSC
policies. The combination of hyper-action and multi-head values enables
multiple agents to share a single actor-critic while achieving personalized
policies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05383v2' target='_blank'>VLMs Play StarCraft II: A Benchmark and Multimodal Decision Method</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weiyu Ma, Yuqian Fu, Zecheng Zhang, Guohao Li, Bernard Ghanem</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 12:54:25</h6>
<p class='card-text'>We introduce VLM-Attention, a multimodal StarCraft II environment that aligns
artificial agent perception with the human gameplay experience. Traditional
frameworks such as SMAC rely on abstract state representations that diverge
significantly from human perception, limiting the ecological validity of agent
behavior. Our environment addresses this limitation by incorporating RGB visual
inputs and natural language observations that more closely simulate human
cognitive processes during gameplay. The VLM-Attention framework consists of
three integrated components: (1) a vision-language model enhanced with
specialized self-attention mechanisms for strategic unit targeting and
battlefield assessment, (2) a retrieval-augmented generation system that
leverages domain-specific StarCraft II knowledge to inform tactical decisions,
and (3) a dynamic role-based task distribution system that enables coordinated
multi-agent behavior. Our experimental evaluation across 21 custom scenarios
demonstrates that VLM-based agents powered by foundation models (specifically
Qwen-VL and GPT-4o) can execute complex tactical maneuvers without explicit
training, achieving comparable performance to traditional MARL methods that
require substantial training iterations. This work establishes a foundation for
developing human-aligned StarCraft II agents and advances the broader research
agenda of multimodal game AI. Our implementation is available at
https://github.com/camel-ai/VLM-Play-StarCraft2.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05092v1' target='_blank'>Multi-Robot Collaboration through Reinforcement Learning and Abstract
  Simulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Adam Labiosa, Josiah P. Hanna</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 02:23:24</h6>
<p class='card-text'>Teams of people coordinate to perform complex tasks by forming abstract
mental models of world and agent dynamics. The use of abstract models contrasts
with much recent work in robot learning that uses a high-fidelity simulator and
reinforcement learning (RL) to obtain policies for physical robots. Motivated
by this difference, we investigate the extent to which so-called abstract
simulators can be used for multi-agent reinforcement learning (MARL) and the
resulting policies successfully deployed on teams of physical robots. An
abstract simulator models the robot's target task at a high-level of
abstraction and discards many details of the world that could impact optimal
decision-making. Policies are trained in an abstract simulator then transferred
to the physical robot by making use of separately-obtained low-level perception
and motion control modules. We identify three key categories of modifications
to the abstract simulator that enable policy transfer to physical robots:
simulation fidelity enhancements, training optimizations and simulation
stochasticity. We then run an empirical study with extensive ablations to
determine the value of each modification category for enabling policy transfer
in cooperative robot soccer tasks. We also compare the performance of policies
produced by our method with a well-tuned non-learning-based behavior
architecture from the annual RoboCup competition and find that our approach
leads to a similar level of performance. Broadly we show that MARL can be use
to train cooperative physical robot behaviors using highly abstract models of
the world.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04262v1' target='_blank'>Guidelines for Applying RL and MARL in Cybersecurity Applications</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vasilios Mavroudis, Gregory Palmer, Sara Farmer, Kez Smithson Whitehead, David Foster, Adam Price, Ian Miles, Alberto Caron, Stephen Pasteris</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 09:46:16</h6>
<p class='card-text'>Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)
have emerged as promising methodologies for addressing challenges in automated
cyber defence (ACD). These techniques offer adaptive decision-making
capabilities in high-dimensional, adversarial environments. This report
provides a structured set of guidelines for cybersecurity professionals and
researchers to assess the suitability of RL and MARL for specific use cases,
considering factors such as explainability, exploration needs, and the
complexity of multi-agent coordination. It also discusses key algorithmic
approaches, implementation challenges, and real-world constraints, such as data
scarcity and adversarial interference. The report further outlines open
research questions, including policy optimality, agent cooperation levels, and
the integration of MARL systems into operational cybersecurity frameworks. By
bridging theoretical advancements and practical deployment, these guidelines
aim to enhance the effectiveness of AI-driven cyber defence strategies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03889v1' target='_blank'>Pretrained LLMs as Real-Time Controllers for Robot Operated Serial
  Production Line</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhammad Waseem, Kshitij Bhatta, Chen Li, Qing Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 20:43:49</h6>
<p class='card-text'>The manufacturing industry is undergoing a transformative shift, driven by
cutting-edge technologies like 5G, AI, and cloud computing. Despite these
advancements, effective system control, which is crucial for optimizing
production efficiency, remains a complex challenge due to the intricate,
knowledge-dependent nature of manufacturing processes and the reliance on
domain-specific expertise. Conventional control methods often demand heavy
customization, considerable computational resources, and lack transparency in
decision-making. In this work, we investigate the feasibility of using Large
Language Models (LLMs), particularly GPT-4, as a straightforward, adaptable
solution for controlling manufacturing systems, specifically, mobile robot
scheduling. We introduce an LLM-based control framework to assign mobile robots
to different machines in robot assisted serial production lines, evaluating its
performance in terms of system throughput. Our proposed framework outperforms
traditional scheduling approaches such as First-Come-First-Served (FCFS),
Shortest Processing Time (SPT), and Longest Processing Time (LPT). While it
achieves performance that is on par with state-of-the-art methods like
Multi-Agent Reinforcement Learning (MARL), it offers a distinct advantage by
delivering comparable throughput without the need for extensive retraining.
These results suggest that the proposed LLM-based solution is well-suited for
scenarios where technical expertise, computational resources, and financial
investment are limited, while decision transparency and system scalability are
critical concerns.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03796v2' target='_blank'>Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent
  Reinforcement Learning in USV Swarm</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 14:33:18</h6>
<p class='card-text'>Multi-Agent Reinforcement Learning (MARL) has shown promise in solving
complex problems involving cooperation and competition among agents, such as an
Unmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance,
and vessel protection. However, aligning system behavior with user preferences
is challenging due to the difficulty of encoding expert intuition into reward
functions. To address the issue, we propose a Reinforcement Learning with Human
Feedback (RLHF) approach for MARL that resolves credit-assignment challenges
through an Agent-Level Feedback system categorizing feedback into intra-agent,
inter-agent, and intra-team types. To overcome the challenges of direct human
feedback, we employ a Large Language Model (LLM) evaluator to validate our
approach using feedback scenarios such as region constraints, collision
avoidance, and task allocation. Our method effectively refines USV swarm
policies, addressing key challenges in multi-agent systems while maintaining
fairness and performance consistency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02913v1' target='_blank'>Towards Robust Multi-UAV Collaboration: MARL with Noise-Resilient
  Communication and Attention Mechanisms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zilin Zhao, Chishui Chen, Haotian Shi, Jiale Chen, Xuanlin Yue, Zhejian Yang, Yang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 08:05:14</h6>
<p class='card-text'>Efficient path planning for unmanned aerial vehicles (UAVs) is crucial in
remote sensing and information collection. As task scales expand, the
cooperative deployment of multiple UAVs significantly improves information
collection efficiency. However, collaborative communication and decision-making
for multiple UAVs remain major challenges in path planning, especially in noisy
environments. To efficiently accomplish complex information collection tasks in
3D space and address robust communication issues, we propose a multi-agent
reinforcement learning (MARL) framework for UAV path planning based on the
Counterfactual Multi-Agent Policy Gradients (COMA) algorithm. The framework
incorporates attention mechanism-based UAV communication protocol and
training-deployment system, significantly improving communication robustness
and individual decision-making capabilities in noisy conditions. Experiments
conducted on both synthetic and real-world datasets demonstrate that our method
outperforms existing algorithms in terms of path planning efficiency and
robustness, especially in noisy environments, achieving a 78\% improvement in
entropy reduction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02077v2' target='_blank'>M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback
  of Mixed Quality</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyan Wang, Zhicheng Zhang, Fei Fang, Yali Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 21:58:10</h6>
<p class='card-text'>Designing effective reward functions in multi-agent reinforcement learning
(MARL) is a significant challenge, often leading to suboptimal or misaligned
behaviors in complex, coordinated environments. We introduce Multi-agent
Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality (M3HF),
a novel framework that integrates multi-phase human feedback of mixed quality
into the MARL training process. By involving humans with diverse expertise
levels to provide iterative guidance, M3HF leverages both expert and non-expert
feedback to continuously refine agents' policies. During training, we
strategically pause agent learning for human evaluation, parse feedback using
large language models to assign it appropriately and update reward functions
through predefined templates and adaptive weight by using weight decay and
performance-based adjustments. Our approach enables the integration of nuanced
human insights across various levels of quality, enhancing the interpretability
and robustness of multi-agent cooperation. Empirical results in challenging
environments demonstrate that M3HF significantly outperforms state-of-the-art
methods, effectively addressing the complexities of reward design in MARL and
enabling broader human participation in the training process.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01458v1' target='_blank'>SrSv: Integrating Sequential Rollouts with Sequential Value Estimation
  for Multi-agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xu Wan, Chao Yang, Cheng Yang, Jie Song, Mingyang Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 12:17:18</h6>
<p class='card-text'>Although multi-agent reinforcement learning (MARL) has shown its success
across diverse domains, extending its application to large-scale real-world
systems still faces significant challenges. Primarily, the high complexity of
real-world environments exacerbates the credit assignment problem,
substantially reducing training efficiency. Moreover, the variability of agent
populations in large-scale scenarios necessitates scalable decision-making
mechanisms. To address these challenges, we propose a novel framework:
Sequential rollout with Sequential value estimation (SrSv). This framework aims
to capture agent interdependence and provide a scalable solution for
cooperative MARL. Specifically, SrSv leverages the autoregressive property of
the Transformer model to handle varying populations through sequential action
rollout. Furthermore, to capture the interdependence of policy distributions
and value functions among multiple agents, we introduce an innovative
sequential value estimation methodology and integrates the value approximation
into an attention-based sequential model. We evaluate SrSv on three benchmarks:
Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, and DubinsCars.
Experimental results demonstrate that SrSv significantly outperforms baseline
methods in terms of training efficiency without compromising convergence
performance. Moreover, when implemented in a large-scale DubinsCar system with
1,024 agents, our framework surpasses existing benchmarks, highlighting the
excellent scalability of SrSv.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01168v1' target='_blank'>Relativistic BGK model of Marle for polyatomic gases near equilibrium</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Byung-Hoon Hwang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 04:32:06</h6>
<p class='card-text'>In this paper, we consider the direct application of the relativistic
extended thermodynamics theory of polyatomic gases developed in [Ann. Phys. 377
(2017) 414--445] to the relativistic BGK model proposed by Marle. We present
the perturbed Marle model around the generalized J\"{u}ttner distribution and
investigate the properties of the linear operator. Then we prove the global
existence and large-time behavior of classical solutions when the initial data
is sufficiently close to a global equilibrium.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01017v1' target='_blank'>Real-World Deployment and Assessment of a Multi-Agent Reinforcement
  Learning-Based Variable Speed Limit Control System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhang Zhang, Zhiyao Zhang, Junyi Ji, Marcos Quiñones-Grueiro, William Barbour, Derek Gloudemans, Gergely Zachár, Clay Weston, Gautam Biswas, Daniel B. Work</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 21:09:16</h6>
<p class='card-text'>This article presents the first field deployment of a multi-agent
reinforcement learning (MARL) based variable speed limit (VSL) control system
on Interstate 24 (I-24) near Nashville, Tennessee. We design and demonstrate a
full pipeline from training MARL agents in a traffic simulator to a field
deployment on a 17-mile segment of I-24 encompassing 67 VSL controllers. The
system was launched on March 8th, 2024, and has made approximately 35 million
decisions on 28 million trips in six months of operation. We apply an invalid
action masking mechanism and several safety guards to ensure real-world
constraints. The MARL-based implementation operates up to 98% of the time, with
the safety guards overriding the MARL decisions for the remaining time. We
evaluate the performance of the MARL-based algorithm in comparison to a
previously deployed non-RL VSL benchmark algorithm on I-24. Results show that
the MARL-based VSL control system achieves a superior performance. The accuracy
of correctly warning drivers about slowing traffic ahead is improved by 14% and
the response delay to non-recurrent congestion is reduced by 75%. The
preliminary data shows that the VSL control system has reduced the crash rate
by 26% and the secondary crash rate by 50%. We open-sourced the deployed
MARL-based VSL algorithm at https://github.com/Lab-Work/marl-vsl-controller.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00684v1' target='_blank'>Factorized Deep Q-Network for Cooperative Multi-Agent Reinforcement
  Learning in Victim Tagging</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maria Ana Cardei, Afsaneh Doryab</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 01:32:09</h6>
<p class='card-text'>Mass casualty incidents (MCIs) are a growing concern, characterized by
complexity and uncertainty that demand adaptive decision-making strategies. The
victim tagging step in the emergency medical response must be completed quickly
and is crucial for providing information to guide subsequent time-constrained
response actions. In this paper, we present a mathematical formulation of
multi-agent victim tagging to minimize the time it takes for responders to tag
all victims. Five distributed heuristics are formulated and evaluated with
simulation experiments. The heuristics considered are on-the go, practical
solutions that represent varying levels of situational uncertainty in the form
of global or local communication capabilities, showcasing practical
constraints. We further investigate the performance of a multi-agent
reinforcement learning (MARL) strategy, factorized deep Q-network (FDQN), to
minimize victim tagging time as compared to baseline heuristics. Extensive
simulations demonstrate that between the heuristics, methods with local
communication are more efficient for adaptive victim tagging, specifically
choosing the nearest victim with the option to replan. Analyzing all
experiments, we find that our FDQN approach outperforms heuristics in
smaller-scale scenarios, while heuristics excel in more complex scenarios. Our
experiments contain diverse complexities that explore the upper limits of MARL
capabilities for real-world applications and reveal key insights.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00372v1' target='_blank'>Nucleolus Credit Assignment for Effective Coalitions in Multi-agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yugu Li, Zehong Cao, Jianglin Qiao, Siyi Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-01 07:01:58</h6>
<p class='card-text'>In cooperative multi-agent reinforcement learning (MARL), agents typically
form a single grand coalition based on credit assignment to tackle a composite
task, often resulting in suboptimal performance. This paper proposed a
nucleolus-based credit assignment grounded in cooperative game theory, enabling
the autonomous partitioning of agents into multiple small coalitions that can
effectively identify and complete subtasks within a larger composite task.
Specifically, our designed nucleolus Q-learning could assign fair credits to
each agent, and the nucleolus Q-operator provides theoretical guarantees with
interpretability for both learning convergence and the stability of the formed
small coalitions. Through experiments on Predator-Prey and StarCraft scenarios
across varying difficulty levels, our approach demonstrated the emergence of
multiple effective coalitions during MARL training, leading to faster learning
and superior performance in terms of win rate and cumulative rewards especially
in hard and super-hard environments, compared to four baseline methods. Our
nucleolus-based credit assignment showed the promise for complex composite
tasks requiring effective subteams of agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20217v1' target='_blank'>MARVEL: Multi-Agent Reinforcement Learning for constrained field-of-View
  multi-robot Exploration in Large-scale environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jimmy Chiun, Shizhe Zhang, Yizhuo Wang, Yuhong Cao, Guillaume Sartoretti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 15:58:42</h6>
<p class='card-text'>In multi-robot exploration, a team of mobile robot is tasked with efficiently
mapping an unknown environments. While most exploration planners assume
omnidirectional sensors like LiDAR, this is impractical for small robots such
as drones, where lightweight, directional sensors like cameras may be the only
option due to payload constraints. These sensors have a constrained
field-of-view (FoV), which adds complexity to the exploration problem,
requiring not only optimal robot positioning but also sensor orientation during
movement. In this work, we propose MARVEL, a neural framework that leverages
graph attention networks, together with novel frontiers and orientation
features fusion technique, to develop a collaborative, decentralized policy
using multi-agent reinforcement learning (MARL) for robots with constrained
FoV. To handle the large action space of viewpoints planning, we further
introduce a novel information-driven action pruning strategy. MARVEL improves
multi-robot coordination and decision-making in challenging large-scale indoor
environments, while adapting to various team sizes and sensor configurations
(i.e., FoV and sensor range) without additional training. Our extensive
evaluation shows that MARVEL's learned policies exhibit effective coordinated
behaviors, outperforming state-of-the-art exploration planners across multiple
metrics. We experimentally demonstrate MARVEL's generalizability in large-scale
environments, of up to 90m by 90m, and validate its practical applicability
through successful deployment on a team of real drone hardware.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20065v1' target='_blank'>RouteRL: Multi-agent reinforcement learning framework for urban route
  choice with autonomous vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ahmet Onur Akman, Anastasia Psarou, Łukasz Gorczyca, Zoltán György Varga, Grzegorz Jamróz, Rafał Kucharski</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 13:13:09</h6>
<p class='card-text'>RouteRL is a novel framework that integrates multi-agent reinforcement
learning (MARL) with a microscopic traffic simulation, facilitating the testing
and development of efficient route choice strategies for autonomous vehicles
(AVs). The proposed framework simulates the daily route choices of driver
agents in a city, including two types: human drivers, emulated using behavioral
route choice models, and AVs, modeled as MARL agents optimizing their policies
for a predefined objective. RouteRL aims to advance research in MARL, transport
modeling, and human-AI interaction for transportation applications. This study
presents a technical report on RouteRL, outlines its potential research
contributions, and showcases its impact via illustrative examples.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19717v1' target='_blank'>Exponential Topology-enabled Scalable Communication in Multi-agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinran Li, Xiaolu Wang, Chenjia Bai, Jun Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 03:15:31</h6>
<p class='card-text'>In cooperative multi-agent reinforcement learning (MARL), well-designed
communication protocols can effectively facilitate consensus among agents,
thereby enhancing task performance. Moreover, in large-scale multi-agent
systems commonly found in real-world applications, effective communication
plays an even more critical role due to the escalated challenge of partial
observability compared to smaller-scale setups. In this work, we endeavor to
develop a scalable communication protocol for MARL. Unlike previous methods
that focus on selecting optimal pairwise communication links-a task that
becomes increasingly complex as the number of agents grows-we adopt a global
perspective on communication topology design. Specifically, we propose
utilizing the exponential topology to enable rapid information dissemination
among agents by leveraging its small-diameter and small-size properties. This
approach leads to a scalable communication protocol, named ExpoComm. To fully
unlock the potential of exponential graphs as communication topologies, we
employ memory-based message processors and auxiliary tasks to ground messages,
ensuring that they reflect global information and benefit decision-making.
Extensive experiments on large-scale cooperative benchmarks, including MAgent
and Infrastructure Management Planning, demonstrate the superior performance
and robust zero-shot transferability of ExpoComm compared to existing
communication strategies. The code is publicly available at
https://github.com/LXXXXR/ExpoComm.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>