<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>HRL - 2025-06-08</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>HRL - 2025-06-08</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.04549v1' target='_blank'>Discounting and Drug Seeking in Biological Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vardhan Palod, Pranav Mahajan, Veeky Baths, Boris S. Gutkin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 01:54:09</h6>
<p class='card-text'>Despite a strong desire to quit, individuals with long-term substance use
disorder (SUD) often struggle to resist drug use, even when aware of its
harmful consequences. This disconnect between knowledge and compulsive behavior
reflects a fundamental cognitive-behavioral conflict in addiction.
Neurobiologically, differential cue-induced activity within striatal
subregions, along with dopamine-mediated connectivity from the ventral to the
dorsal striatum, contributes to compulsive drug-seeking. However, the
functional mechanism linking these findings to behavioral conflict remains
unclear. Another hallmark of addiction is temporal discounting: individuals
with drug dependence exhibit steeper discount rates than non-users. Assuming
the ventral-dorsal striatal organization reflects a gradient from cognitive to
motor representations, addiction can be modeled within a hierarchical
reinforcement learning (HRL) framework. However, integrating discounting into
biologically grounded HRL remains an open challenge. In this work, we build on
a model showing how action choices reinforced with drug rewards become
insensitive to the negative consequences that follow. We address the
integration of discounting by ensuring natural reward values converge across
all levels in the HRL hierarchy, while drug rewards diverge due to their
dopaminergic effects. Our results show that high discounting amplifies
drug-seeking across the hierarchy, linking faster discounting with increased
addiction severity and impulsivity. We demonstrate alignment with empirical
findings on temporal discounting and propose testable predictions, establishing
addiction as a disorder of hierarchical decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.02050v1' target='_blank'>Decoupled Hierarchical Reinforcement Learning with State Abstraction for
  Discrete Grids</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qingyu Xiao, Yuanlin Chang, Youtian Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-01 06:36:19</h6>
<p class='card-text'>Effective agent exploration remains a core challenge in reinforcement
learning (RL) for complex discrete state-space environments, particularly under
partial observability. This paper presents a decoupled hierarchical RL
framework integrating state abstraction (DcHRL-SA) to address this issue. The
proposed method employs a dual-level architecture, consisting of a high level
RL-based actor and a low-level rule-based policy, to promote effective
exploration. Additionally, state abstraction method is incorporated to cluster
discrete states, effectively lowering state dimensionality. Experiments
conducted in two discrete customized grid environments demonstrate that the
proposed approach consistently outperforms PPO in terms of exploration
efficiency, convergence speed, cumulative reward, and policy stability. These
results demonstrate a practical approach for integrating decoupled hierarchical
policies and state abstraction in discrete grids with large-scale exploration
space. Code will be available at https://github.com/XQY169/DcHRL-SA.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.21750v1' target='_blank'>Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional
  Subgoals</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vivienne Huiling Wang, Tinghuai Wang, Joni Pajarinen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-27 20:38:44</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) learns to make decisions on
multiple levels of temporal abstraction. A key challenge in HRL is that the
low-level policy changes over time, making it difficult for the high-level
policy to generate effective subgoals. To address this issue, the high-level
policy must capture a complex subgoal distribution while also accounting for
uncertainty in its estimates. We propose an approach that trains a conditional
diffusion model regularized by a Gaussian Process (GP) prior to generate a
complex variety of subgoals while leveraging principled GP uncertainty
quantification. Building on this framework, we develop a strategy that selects
subgoals from both the diffusion policy and GP's predictive mean. Our approach
outperforms prior HRL methods in both sample efficiency and performance on
challenging continuous control benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.21410v1' target='_blank'>MRSD: Multi-Resolution Skill Discovery for HRL Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shashank Sharma, Janina Hoffmann, Vinay Namboodiri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-27 16:38:55</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) relies on abstract skills to solve
long-horizon tasks efficiently. While existing skill discovery methods learns
these skills automatically, they are limited to a single skill per task. In
contrast, humans learn and use both fine-grained and coarse motor skills
simultaneously. Inspired by human motor control, we propose Multi-Resolution
Skill Discovery (MRSD), an HRL framework that learns multiple skill encoders at
different temporal resolutions in parallel. A high-level manager dynamically
selects among these skills, enabling adaptive control strategies over time. We
evaluate MRSD on tasks from the DeepMind Control Suite and show that it
outperforms prior state-of-the-art skill discovery and HRL methods, achieving
faster convergence and higher final performance. Our findings highlight the
benefits of integrating multi-resolution skills in HRL, paving the way for more
versatile and efficient agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19761v1' target='_blank'>Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents
  via Offline Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zican Hu, Wei Liu, Xiaoye Qu, Xiangyu Yue, Chunlin Chen, Zhi Wang, Yu Cheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 09:43:40</h6>
<p class='card-text'>While showing sophisticated reasoning abilities, large language models (LLMs)
still struggle with long-horizon decision-making tasks due to deficient
exploration and long-term credit assignment, especially in sparse-reward
scenarios. Inspired by the divide-and-conquer principle, we propose an
innovative framework **GLIDER** (**G**rounding **L**anguage Models as
Eff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical
**R**einforcement Learning) that introduces a parameter-efficient and generally
applicable hierarchy to LLM policies. We develop a scheme where the low-level
controller is supervised with abstract, step-by-step plans that are learned and
instructed by the high-level policy. This design decomposes complicated
problems into a series of coherent chain-of-thought reasoning sub-tasks,
providing flexible temporal abstraction to significantly enhance exploration
and learning for long-horizon tasks. Furthermore, GLIDER facilitates fast
online adaptation to non-stationary environments owing to the strong
transferability of its task-agnostic low-level skills. Experiments on
ScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent
performance gains, along with enhanced generalization capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.06218v1' target='_blank'>Let Humanoids Hike! Integrative Skill Development on Complex Trails</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kwan-Yee Lin, Stella X. Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-09 17:53:02</h6>
<p class='card-text'>Hiking on complex trails demands balance, agility, and adaptive
decision-making over unpredictable terrain. Current humanoid research remains
fragmented and inadequate for hiking: locomotion focuses on motor skills
without long-term goals or situational awareness, while semantic navigation
overlooks real-world embodiment and local terrain variability. We propose
training humanoids to hike on complex trails, driving integrative skill
development across visual perception, decision making, and motor execution. We
develop a learning framework, LEGO-H, that enables a vision-equipped humanoid
robot to hike complex trails autonomously. We introduce two technical
innovations: 1) A temporal vision transformer variant - tailored into
Hierarchical Reinforcement Learning framework - anticipates future local goals
to guide movement, seamlessly integrating locomotion with goal-directed
navigation. 2) Latent representations of joint movement patterns, combined with
hierarchical metric learning - enhance Privileged Learning scheme - enable
smooth policy transfer from privileged training to onboard execution. These
components allow LEGO-H to handle diverse physical and environmental challenges
without relying on predefined motion patterns. Experiments across varied
simulated trails and robot morphologies highlight LEGO-H's versatility and
robustness, positioning hiking as a compelling testbed for embodied autonomy
and LEGO-H as a baseline for future humanoid development.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.04579v1' target='_blank'>Implicitly Aligning Humans and Autonomous Agents through Shared Task
  Abstractions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Stéphane Aroca-Ouellette, Miguel Aroca-Ouellette, Katharina von der Wense, Alessandro Roncone</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-07 17:19:17</h6>
<p class='card-text'>In collaborative tasks, autonomous agents fall short of humans in their
capability to quickly adapt to new and unfamiliar teammates. We posit that a
limiting factor for zero-shot coordination is the lack of shared task
abstractions, a mechanism humans rely on to implicitly align with teammates. To
address this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework
leveraging hierarchical reinforcement learning to mimic the structured approach
humans use in collaboration. We evaluate HA$^2$ in the Overcooked environment,
demonstrating statistically significant improvement over existing baselines
when paired with both unseen agents and humans, providing better resilience to
environmental shifts, and outperforming all state-of-the-art methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.04317v1' target='_blank'>Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruize Zhang, Sirui Xiang, Zelai Xu, Feng Gao, Shilong Ji, Wenhao Tang, Wenbo Ding, Chao Yu, Yu Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-07 11:04:36</h6>
<p class='card-text'>In this paper, we tackle the problem of learning to play 3v3 multi-drone
volleyball, a new embodied competitive task that requires both high-level
strategic coordination and low-level agile control. The task is turn-based,
multi-agent, and physically grounded, posing significant challenges due to its
long-horizon dependencies, tight inter-agent coupling, and the underactuated
dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play
(HCSP), a hierarchical reinforcement learning framework that separates
centralized high-level strategic decision-making from decentralized low-level
motion control. We design a three-stage population-based training pipeline to
enable both strategy and skill to emerge from scratch without expert
demonstrations: (I) training diverse low-level skills, (II) learning high-level
strategy via self-play with fixed low-level controllers, and (III) joint
fine-tuning through co-self-play. Experiments show that HCSP achieves superior
performance, outperforming non-hierarchical self-play and rule-based
hierarchical baselines with an average 82.9\% win rate and a 71.5\% win rate
against the two-stage variant. Moreover, co-self-play leads to emergent team
behaviors such as role switching and coordinated formations, demonstrating the
effectiveness of our hierarchical design and training scheme.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.02439v1' target='_blank'>ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control
  via Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yang Deng, Yaohui Liu, Rui Liang, Dafang Zhao, Donghua Xie, Ittetsu Taniguchi, Dan Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-05 08:09:36</h6>
<p class='card-text'>The building thermodynamics model, which predicts real-time indoor
temperature changes under potential HVAC (Heating, Ventilation, and Air
Conditioning) control operations, is crucial for optimizing HVAC control in
buildings. While pioneering studies have attempted to develop such models for
various building environments, these models often require extensive data
collection periods and rely heavily on expert knowledge, making the modeling
process inefficient and limiting the reusability of the models. This paper
explores a model ensemble perspective that utilizes existing developed models
as base models to serve a target building environment, thereby providing
accurate predictions while reducing the associated efforts. Given that building
data streams are non-stationary and the number of base models may increase, we
propose a Hierarchical Reinforcement Learning (HRL) approach to dynamically
select and weight the base models. Our approach employs a two-tiered
decision-making process: the high-level focuses on model selection, while the
low-level determines the weights of the selected models. We thoroughly evaluate
the proposed approach through offline experiments and an on-site case study,
and the experimental results demonstrate the effectiveness of our method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.01979v1' target='_blank'>D3HRL: A Distributed Hierarchical Reinforcement Learning Approach Based
  on Causal Discovery and Spurious Correlation Detection</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenran Zhao, Dianxi Shi, Mengzhu Wang, Jianqiang Xia, Huanhuan Yang, Songchang Jin, Shaowu Yang, Chunping Qiu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-04 03:59:01</h6>
<p class='card-text'>Current Hierarchical Reinforcement Learning (HRL) algorithms excel in
long-horizon sequential decision-making tasks but still face two challenges:
delay effects and spurious correlations. To address them, we propose a causal
HRL approach called D3HRL. First, D3HRL models delayed effects as causal
relationships across different time spans and employs distributed causal
discovery to learn these relationships. Second, it employs conditional
independence testing to eliminate spurious correlations. Finally, D3HRL
constructs and trains hierarchical policies based on the identified true causal
relationships. These three steps are iteratively executed, gradually exploring
the complete causal chain of the task. Experiments conducted in 2D-MineCraft
and MiniGrid show that D3HRL demonstrates superior sensitivity to delay effects
and accurately identifies causal relationships, leading to reliable
decision-making in complex environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.18794v2' target='_blank'>Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation
  with Autonomous Mobile Robots</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Brendon Johnson, Alfredo Weitzenfeld</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-26 04:30:10</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) is hypothesized to be able to take
advantage of the inherent hierarchy in robot learning tasks with sparse reward
schemes, in contrast to more traditional reinforcement learning algorithms. In
this research, hierarchical reinforcement learning is evaluated and contrasted
with standard reinforcement learning in complex navigation tasks. We evaluate
unique characteristics of HRL, including their ability to create sub-goals and
the termination function. We constructed experiments to test the differences
between PPO and HRL, different ways of creating sub-goals, manual vs automatic
sub-goal creation, and the effects of the frequency of termination on
performance. These experiments highlight the advantages of HRL and how it
achieves these advantages.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.17356v1' target='_blank'>Comprehend, Divide, and Conquer: Feature Subspace Exploration via
  Multi-Agent Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weiliang Zhang, Xiaohan Huang, Yi Du, Ziyue Qiao, Qingqing Long, Zhen Meng, Yuanchun Zhou, Meng Xiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-24 08:16:36</h6>
<p class='card-text'>Feature selection aims to preprocess the target dataset, find an optimal and
most streamlined feature subset, and enhance the downstream machine learning
task. Among filter, wrapper, and embedded-based approaches, the reinforcement
learning (RL)-based subspace exploration strategy provides a novel objective
optimization-directed perspective and promising performance. Nevertheless, even
with improved performance, current reinforcement learning approaches face
challenges similar to conventional methods when dealing with complex datasets.
These challenges stem from the inefficient paradigm of using one agent per
feature and the inherent complexities present in the datasets. This observation
motivates us to investigate and address the above issue and propose a novel
approach, namely HRLFS. Our methodology initially employs a Large Language
Model (LLM)-based hybrid state extractor to capture each feature's mathematical
and semantic characteristics. Based on this information, features are
clustered, facilitating the construction of hierarchical agents for each
cluster and sub-cluster. Extensive experiments demonstrate the efficiency,
scalability, and robustness of our approach. Compared to contemporary or the
one-feature-one-agent RL-based approaches, HRLFS improves the downstream ML
performance with iterative feature subspace exploration while accelerating
total run time by reducing the number of agents involved.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.15876v2' target='_blank'>Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement
  Learning for Strategic Confrontation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qizhen Wu, Lei Chen, Kexin Liu, Jinhu Lü</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-22 13:22:58</h6>
<p class='card-text'>In swarm robotics, confrontation scenarios, including strategic
confrontations, require efficient decision-making that integrates discrete
commands and continuous actions. Traditional task and motion planning methods
separate decision-making into two layers, but their unidirectional structure
fails to capture the interdependence between these layers, limiting
adaptability in dynamic environments. Here, we propose a novel bidirectional
approach based on hierarchical reinforcement learning, enabling dynamic
interaction between the layers. This method effectively maps commands to task
allocation and actions to path planning, while leveraging cross-training
techniques to enhance learning across the hierarchical framework. Furthermore,
we introduce a trajectory prediction model that bridges abstract task
representations with actionable planning goals. In our experiments, it achieves
over 80% in confrontation win rate and under 0.01 seconds in decision time,
outperforming existing approaches. Demonstrations through large-scale tests and
real-world robot experiments further emphasize the generalization capabilities
and practical applicability of our method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.14989v1' target='_blank'>Dynamic Legged Ball Manipulation on Rugged Terrains with Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongjie Zhu, Zhuo Yang, Tianhang Wu, Luzhou Ge, Xuesong Li, Qi Liu, Xiang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-21 09:38:38</h6>
<p class='card-text'>Advancing the dynamic loco-manipulation capabilities of quadruped robots in
complex terrains is crucial for performing diverse tasks. Specifically, dynamic
ball manipulation in rugged environments presents two key challenges. The first
is coordinating distinct motion modalities to integrate terrain traversal and
ball control seamlessly. The second is overcoming sparse rewards in end-to-end
deep reinforcement learning, which impedes efficient policy convergence. To
address these challenges, we propose a hierarchical reinforcement learning
framework. A high-level policy, informed by proprioceptive data and ball
position, adaptively switches between pre-trained low-level skills such as ball
dribbling and rough terrain navigation. We further propose Dynamic
Skill-Focused Policy Optimization to suppress gradients from inactive skills
and enhance critical skill learning. Both simulation and real-world experiments
validate that our methods outperform baseline approaches in dynamic ball
manipulation across rugged terrains, highlighting its effectiveness in
challenging environments. Videos are on our website: dribble-hrl.github.io.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.05553v1' target='_blank'>Federated Hierarchical Reinforcement Learning for Adaptive Traffic
  Signal Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongjie Fu, Lingyun Zhong, Zifan Li, Xuan Di</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-07 23:02:59</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) has shown promise for adaptive
traffic signal control (ATSC), enabling multiple intersections to coordinate
signal timings in real time. However, in large-scale settings, MARL faces
constraints due to extensive data sharing and communication requirements.
Federated learning (FL) mitigates these challenges by training shared models
without directly exchanging raw data, yet traditional FL methods such as FedAvg
struggle with highly heterogeneous intersections. Different intersections
exhibit varying traffic patterns, demands, and road structures, so performing
FedAvg across all agents is inefficient. To address this gap, we propose
Hierarchical Federated Reinforcement Learning (HFRL) for ATSC. HFRL employs
clustering-based or optimization-based techniques to dynamically group
intersections and perform FedAvg independently within groups of intersections
with similar characteristics, enabling more effective coordination and
scalability than standard FedAvg. Our experiments on synthetic and real-world
traffic networks demonstrate that HFRL not only outperforms both decentralized
and standard federated RL approaches but also identifies suitable grouping
patterns based on network structure or traffic demand, resulting in a more
robust framework for distributed, heterogeneous systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.04366v1' target='_blank'>Solving Sokoban using Hierarchical Reinforcement Learning with Landmarks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sergey Pastukhov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-06 05:30:21</h6>
<p class='card-text'>We introduce a novel hierarchical reinforcement learning (HRL) framework that
performs top-down recursive planning via learned subgoals, successfully applied
to the complex combinatorial puzzle game Sokoban. Our approach constructs a
six-level policy hierarchy, where each higher-level policy generates subgoals
for the level below. All subgoals and policies are learned end-to-end from
scratch, without any domain knowledge. Our results show that the agent can
generate long action sequences from a single high-level call. While prior work
has explored 2-3 level hierarchies and subgoal-based planning heuristics, we
demonstrate that deep recursive goal decomposition can emerge purely from
learning, and that such hierarchies can scale effectively to hard puzzle
domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.21677v1' target='_blank'>A tale of two goals: leveraging sequentiality in multi-goal scenarios</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Olivier Serris, Stéphane Doncieux, Olivier Sigaud</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-27 16:47:46</h6>
<p class='card-text'>Several hierarchical reinforcement learning methods leverage planning to
create a graph or sequences of intermediate goals, guiding a lower-level
goal-conditioned (GC) policy to reach some final goals. The low-level policy is
typically conditioned on the current goal, with the aim of reaching it as
quickly as possible. However, this approach can fail when an intermediate goal
can be reached in multiple ways, some of which may make it impossible to
continue toward subsequent goals. To address this issue, we introduce two
instances of Markov Decision Process (MDP) where the optimization objective
favors policies that not only reach the current goal but also subsequent ones.
In the first, the agent is conditioned on both the current and final goals,
while in the second, it is conditioned on the next two goals in the sequence.
We conduct a series of experiments on navigation and pole-balancing tasks in
which sequences of intermediate goals are given. By evaluating policies trained
with TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that,
in most cases, conditioning on the next two goals improves stability and sample
efficiency over other approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19007v1' target='_blank'>Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chak Lam Shek, Pratap Tokekar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 15:49:56</h6>
<p class='card-text'>Large Language Models (LLMs) have shown remarkable promise in reasoning and
decision-making, yet their integration with Reinforcement Learning (RL) for
complex robotic tasks remains underexplored. In this paper, we propose an
LLM-guided hierarchical RL framework, termed LDSC, that leverages LLM-driven
subgoal selection and option reuse to enhance sample efficiency,
generalization, and multi-task adaptability. Traditional RL methods often
suffer from inefficient exploration and high computational cost. Hierarchical
RL helps with these challenges, but existing methods often fail to reuse
options effectively when faced with new tasks. To address these limitations, we
introduce a three-stage framework that uses LLMs for subgoal generation given
natural language description of the task, a reusable option learning and
selection method, and an action-level policy, enabling more effective
decision-making across diverse tasks. By incorporating LLMs for subgoal
prediction and policy guidance, our approach improves exploration efficiency
and enhances learning performance. On average, LDSC outperforms the baseline by
55.9\% in average reward, demonstrating its effectiveness in complex RL
settings. More details and experiment videos could be found in
\href{https://raaslab.org/projects/LDSC/}{this
link\footnote{https://raaslab.org/projects/LDSC}}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.14809v1' target='_blank'>Learning with Expert Abstractions for Efficient Multi-Task Continuous
  Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jeff Jewett, Sandhya Saisubramanian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-19 00:44:23</h6>
<p class='card-text'>Decision-making in complex, continuous multi-task environments is often
hindered by the difficulty of obtaining accurate models for planning and the
inefficiency of learning purely from trial and error. While precise environment
dynamics may be hard to specify, human experts can often provide high-fidelity
abstractions that capture the essential high-level structure of a task and user
preferences in the target environment. Existing hierarchical approaches often
target discrete settings and do not generalize across tasks. We propose a
hierarchical reinforcement learning approach that addresses these limitations
by dynamically planning over the expert-specified abstraction to generate
subgoals to learn a goal-conditioned policy. To overcome the challenges of
learning under sparse rewards, we shape the reward based on the optimal state
value in the abstract model. This structured decision-making process enhances
sample efficiency and facilitates zero-shot generalization. Our empirical
evaluation on a suite of procedurally generated continuous control environments
demonstrates that our approach outperforms existing hierarchical reinforcement
learning methods in terms of sample efficiency, task completion rate,
scalability to complex tasks, and generalization to novel scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.12036v1' target='_blank'>Hierarchical Reinforcement Learning for Safe Mapless Navigation with
  Congestion Estimation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianqi Gao, Xizheng Pang, Qi Liu, Yanjie Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-15 08:03:50</h6>
<p class='card-text'>Reinforcement learning-based mapless navigation holds significant potential.
However, it faces challenges in indoor environments with local minima area.
This paper introduces a safe mapless navigation framework utilizing
hierarchical reinforcement learning (HRL) to enhance navigation through such
areas. The high-level policy creates a sub-goal to direct the navigation
process. Notably, we have developed a sub-goal update mechanism that considers
environment congestion, efficiently avoiding the entrapment of the robot in
local minimum areas. The low-level motion planning policy, trained through safe
reinforcement learning, outputs real-time control instructions based on
acquired sub-goal. Specifically, to enhance the robot's environmental
perception, we introduce a new obstacle encoding method that evaluates the
impact of obstacles on the robot's motion planning. To validate the performance
of our HRL-based navigation framework, we conduct simulations in office, home,
and restaurant environments. The findings demonstrate that our HRL-based
navigation framework excels in both static and dynamic scenarios. Finally, we
implement the HRL-based navigation framework on a TurtleBot3 robot for physical
validation experiments, which exhibits its strong generalization capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06309v1' target='_blank'>On the Fly Adaptation of Behavior Tree-Based Policies through
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marco Iannotta, Johannes A. Stork, Erik Schaffernicht, Todor Stoyanov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 18:56:22</h6>
<p class='card-text'>With the rising demand for flexible manufacturing, robots are increasingly
expected to operate in dynamic environments where local -- such as slight
offsets or size differences in workpieces -- are common. We propose to address
the problem of adapting robot behaviors to these task variations with a
sample-efficient hierarchical reinforcement learning approach adapting Behavior
Tree (BT)-based policies. We maintain the core BT properties as an
interpretable, modular framework for structuring reactive behaviors, but extend
their use beyond static tasks by inherently accommodating local task
variations. To show the efficiency and effectiveness of our approach, we
conduct experiments both in simulation and on a Franka Emika Panda 7-DoF, with
the manipulator adapting to different obstacle avoidance and pivoting tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20380v1' target='_blank'>Multi-Turn Code Generation Through Single-Step Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arnav Kumar Jain, Gonzalo Gonzalez-Pumariega, Wayne Chen, Alexander M Rush, Wenting Zhao, Sanjiban Choudhury</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:55:05</h6>
<p class='card-text'>We address the problem of code generation from multi-turn execution feedback.
Existing methods either generate code without feedback or use complex,
hierarchical reinforcement learning to optimize multi-turn rewards. We propose
a simple yet scalable approach, $\mu$Code, that solves multi-turn code
generation using only single-step rewards. Our key insight is that code
generation is a one-step recoverable MDP, where the correct code can be
recovered from any intermediate code state in a single turn. $\mu$Code
iteratively trains both a generator to provide code solutions conditioned on
multi-turn execution feedback and a verifier to score the newly generated code.
Experimental evaluations show that our approach achieves significant
improvements over the state-of-the-art baselines. We provide analysis of the
design choices of the reward models and policy, and show the efficacy of
$\mu$Code at utilizing the execution feedback. Our code is available at
https://github.com/portal-cornell/muCode.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.15425v4' target='_blank'>TAG: A Decentralized Framework for Multi-Agent Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Giuseppe Paolo, Abdelhakim Benechehab, Hamza Cherkaoui, Albert Thomas, Balázs Kégl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-21 12:52:16</h6>
<p class='card-text'>Hierarchical organization is fundamental to biological systems and human
societies, yet artificial intelligence systems often rely on monolithic
architectures that limit adaptability and scalability. Current hierarchical
reinforcement learning (HRL) approaches typically restrict hierarchies to two
levels or require centralized training, which limits their practical
applicability. We introduce TAME Agent Framework (TAG), a framework for
constructing fully decentralized hierarchical multi-agent systems. TAG enables
hierarchies of arbitrary depth through a novel LevelEnv concept, which
abstracts each hierarchy level as the environment for the agents above it. This
approach standardizes information flow between levels while preserving loose
coupling, allowing for seamless integration of diverse agent types. We
demonstrate the effectiveness of TAG by implementing hierarchical architectures
that combine different RL agents across multiple levels, achieving improved
performance over classical multi-agent RL baselines on standard benchmarks. Our
results show that decentralized hierarchical organization enhances both
learning speed and final performance, positioning TAG as a promising direction
for scalable multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06772v2' target='_blank'>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 18:51:47</h6>
<p class='card-text'>We present that hierarchical LLM reasoning via scaling thought templates can
effectively optimize the reasoning search space and outperform the mathematical
reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.
We train our ReasonFlux-32B model with only 8 GPUs and introduces three
innovations: (i) a structured and generic thought template library, containing
around 500 high-level thought templates capable of generalizing to similar or
relevant reasoning problems; (ii) performing hierarchical reinforcement
learning on a sequence of thought templates instead of long CoTs, optimizing a
base LLM to plan out an optimal template trajectory for gradually handling
complex problems; (iii) a brand new inference scaling system that enables
hierarchical LLM reasoning by adaptively scaling thought templates at inference
time. With a template trajectory containing more explainable reasoning
structures than DeepSeek-R1 and o3-mini, our ReasonFlux-32B significantly
advances math reasoning capabilities to state-of-the-art levels. Notably, on
the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview
by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an
average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and
45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05537v1' target='_blank'>Sequential Stochastic Combinatorial Optimization Using Hierarchal
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinsong Feng, Zihan Yu, Yanhai Xiong, Haipeng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 12:00:30</h6>
<p class='card-text'>Reinforcement learning (RL) has emerged as a promising tool for combinatorial
optimization (CO) problems due to its ability to learn fast, effective, and
generalizable solutions. Nonetheless, existing works mostly focus on one-shot
deterministic CO, while sequential stochastic CO (SSCO) has rarely been studied
despite its broad applications such as adaptive influence maximization (IM) and
infectious disease intervention. In this paper, we study the SSCO problem where
we first decide the budget (e.g., number of seed nodes in adaptive IM)
allocation for all time steps, and then select a set of nodes for each time
step. The few existing studies on SSCO simplify the problems by assuming a
uniformly distributed budget allocation over the time horizon, yielding
suboptimal solutions. We propose a generic hierarchical RL (HRL) framework
called wake-sleep option (WS-option), a two-layer option-based framework that
simultaneously decides adaptive budget allocation on the higher layer and node
selection on the lower layer. WS-option starts with a coherent formulation of
the two-layer Markov decision processes (MDPs), capturing the interdependencies
between the two layers of decisions. Building on this, WS-option employs
several innovative designs to balance the model's training stability and
computational efficiency, preventing the vicious cyclic interference issue
between the two layers. Empirical results show that WS-option exhibits
significantly improved effectiveness and generalizability compared to
traditional methods. Moreover, the learned model can be generalized to larger
graphs, which significantly reduces the overhead of computational resources.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03960v1' target='_blank'>Bilevel Multi-Armed Bandit-Based Hierarchical Reinforcement Learning for
  Interaction-Aware Self-Driving at Unsignalized Intersections</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zengqi Peng, Yubin Wang, Lei Zheng, Jun Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 10:50:59</h6>
<p class='card-text'>In this work, we present BiM-ACPPO, a bilevel multi-armed bandit-based
hierarchical reinforcement learning framework for interaction-aware
decision-making and planning at unsignalized intersections. Essentially, it
proactively takes the uncertainties associated with surrounding vehicles (SVs)
into consideration, which encompass those stemming from the driver's intention,
interactive behaviors, and the varying number of SVs. Intermediate decision
variables are introduced to enable the high-level RL policy to provide an
interaction-aware reference, for guiding low-level model predictive control
(MPC) and further enhancing the generalization ability of the proposed
framework. By leveraging the structured nature of self-driving at unsignalized
intersections, the training problem of the RL policy is modeled as a bilevel
curriculum learning task, which is addressed by the proposed Exp3.S-based BiMAB
algorithm. It is noteworthy that the training curricula are dynamically
adjusted, thereby facilitating the sample efficiency of the RL training
process. Comparative experiments are conducted in the high-fidelity CARLA
simulator, and the results indicate that our approach achieves superior
performance compared to all baseline methods. Furthermore, experimental results
in two new urban driving scenarios clearly demonstrate the commendable
generalization performance of the proposed method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01956v2' target='_blank'>DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement
  Learning Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shashank Sharma, Janina Hoffmann, Vinay Namboodiri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-04 03:05:55</h6>
<p class='card-text'>Hierarchical Reinforcement Learning (HRL) agents often struggle with
long-horizon visual planning due to their reliance on error-prone distance
metrics. We propose Discrete Hierarchical Planning (DHP), a method that
replaces continuous distance estimates with discrete reachability checks to
evaluate subgoal feasibility. DHP recursively constructs tree-structured plans
by decomposing long-term goals into sequences of simpler subtasks, using a
novel advantage estimation strategy that inherently rewards shorter plans and
generalizes beyond training depths. In addition, to address the data efficiency
challenge, we introduce an exploration strategy that generates targeted
training examples for the planning modules without needing expert data.
Experiments in 25-room navigation environments demonstrate $100\%$ success rate
(vs $82\%$ baseline) and $73$-step average episode length (vs $158$-step
baseline). The method also generalizes to momentum-based control tasks and
requires only $\log N$ steps for replanning. Theoretical analysis and ablations
validate our design choices.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.17424v1' target='_blank'>Certificated Actor-Critic: Hierarchical Reinforcement Learning with
  Control Barrier Functions for Safe Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junjun Xie, Shuhao Zhao, Liang Hu, Huijun Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-29 05:37:47</h6>
<p class='card-text'>Control Barrier Functions (CBFs) have emerged as a prominent approach to
designing safe navigation systems of robots. Despite their popularity, current
CBF-based methods exhibit some limitations: optimization-based safe control
techniques tend to be either myopic or computationally intensive, and they rely
on simplified system models; conversely, the learning-based methods suffer from
the lack of quantitative indication in terms of navigation performance and
safety. In this paper, we present a new model-free reinforcement learning
algorithm called Certificated Actor-Critic (CAC), which introduces a
hierarchical reinforcement learning framework and well-defined reward functions
derived from CBFs. We carry out theoretical analysis and proof of our
algorithm, and propose several improvements in algorithm implementation. Our
analysis is validated by two simulation experiments, showing the effectiveness
of our proposed CAC algorithm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.14992v1' target='_blank'>Extensive Exploration in Complex Traffic Scenarios using Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhihao Zhang, Ekim Yurtsever, Keith A. Redmill</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-25 00:00:11</h6>
<p class='card-text'>Developing an automated driving system capable of navigating complex traffic
environments remains a formidable challenge. Unlike rule-based or supervised
learning-based methods, Deep Reinforcement Learning (DRL) based controllers
eliminate the need for domain-specific knowledge and datasets, thus providing
adaptability to various scenarios. Nonetheless, a common limitation of existing
studies on DRL-based controllers is their focus on driving scenarios with
simple traffic patterns, which hinders their capability to effectively handle
complex driving environments with delayed, long-term rewards, thus compromising
the generalizability of their findings. In response to these limitations, our
research introduces a pioneering hierarchical framework that efficiently
decomposes intricate decision-making problems into manageable and interpretable
subtasks. We adopt a two step training process that trains the high-level
controller and low-level controller separately. The high-level controller
exhibits an enhanced exploration potential with long-term delayed rewards, and
the low-level controller provides longitudinal and lateral control ability
using short-term instantaneous rewards. Through simulation experiments, we
demonstrate the superiority of our hierarchical controller in managing complex
highway driving situations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13084v1' target='_blank'>Attention-Driven Hierarchical Reinforcement Learning with Particle
  Filtering for Source Localization in Dynamic Fields</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiwei Shi, Mengyue Yang, Qi Zhang, Weinan Zhang, Cunjia Liu, Weiru Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 18:45:29</h6>
<p class='card-text'>In many real-world scenarios, such as gas leak detection or environmental
pollutant tracking, solving the Inverse Source Localization and
Characterization problem involves navigating complex, dynamic fields with
sparse and noisy observations. Traditional methods face significant challenges,
including partial observability, temporal and spatial dynamics,
out-of-distribution generalization, and reward sparsity. To address these
issues, we propose a hierarchical framework that integrates Bayesian inference
and reinforcement learning. The framework leverages an attention-enhanced
particle filtering mechanism for efficient and accurate belief updates, and
incorporates two complementary execution strategies: Attention Particle
Filtering Planning and Attention Particle Filtering Reinforcement Learning.
These approaches optimize exploration and adaptation under uncertainty.
Theoretical analysis proves the convergence of the attention-enhanced particle
filter, while extensive experiments across diverse scenarios validate the
framework's superior accuracy, adaptability, and computational efficiency. Our
results highlight the framework's potential for broad applications in dynamic
field estimation tasks.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>