<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-05-25</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-05-25</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17012v1' target='_blank'>SpatialScore: Towards Unified Evaluation for Multimodal Spatial
  Understanding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoning Wu, Xiao Huang, Yaohui Chen, Ya Zhang, Yanfeng Wang, Weidi Xie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 17:59:03</h6>
<p class='card-text'>Multimodal large language models (MLLMs) have achieved impressive success in
question-answering tasks, yet their capabilities for spatial understanding are
less explored. This work investigates a critical question: do existing MLLMs
possess 3D spatial perception and understanding abilities? Concretely, we make
the following contributions in this paper: (i) we introduce VGBench, a
benchmark specifically designed to assess MLLMs for visual geometry perception,
e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most
comprehensive and diverse multimodal spatial understanding benchmark to date,
integrating VGBench with relevant data from the other 11 existing datasets.
This benchmark comprises 28K samples across various spatial understanding
tasks, modalities, and QA formats, along with a carefully curated challenging
subset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent
system incorporating 9 specialized tools for spatial understanding, supporting
both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive
evaluations to reveal persistent challenges in spatial reasoning while
demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will
offer valuable insights and serve as a rigorous benchmark for the next
evolution of MLLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16833v1' target='_blank'>Strategically Linked Decisions in Long-Term Planning and Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alihan Hüyük, Finale Doshi-Velez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 16:04:17</h6>
<p class='card-text'>Long-term planning, as in reinforcement learning (RL), involves finding
strategies: actions that collectively work toward a goal rather than
individually optimizing their immediate outcomes. As part of a strategy, some
actions are taken at the expense of short-term benefit to enable future actions
with even greater returns. These actions are only advantageous if followed up
by the actions they facilitate, consequently, they would not have been taken if
those follow-ups were not available. In this paper, we quantify such
dependencies between planned actions with strategic link scores: the drop in
the likelihood of one decision under the constraint that a follow-up decision
is no longer available. We demonstrate the utility of strategic link scores
through three practical applications: (i) explaining black-box RL agents by
identifying strategically linked pairs among decisions they make, (ii)
improving the worst-case performance of decision support systems by
distinguishing whether recommended actions can be adopted as standalone
improvements or whether they are strategically linked hence requiring a
commitment to a broader strategy to be effective, and (iii) characterizing the
planning processes of non-RL agents purely through interventions aimed at
measuring strategic link scores - as an example, we consider a realistic
traffic simulator and analyze through road closures the effective planning
horizon of the emergent routing behavior of many drivers.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16832v1' target='_blank'>From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework
  for Pedagogical Visualization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haonian Ji, Shi Qiu, Siyang Xin, Siwei Han, Zhaorun Chen, Hongyi Wang, Dake Zhang, Huaxiu Yao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 16:02:18</h6>
<p class='card-text'>While foundation models (FMs), such as diffusion models and large
vision-language models (LVLMs), have been widely applied in educational
contexts, their ability to generate pedagogically effective visual explanations
remains limited. Most existing approaches focus primarily on textual reasoning,
overlooking the critical role of structured and interpretable visualizations in
supporting conceptual understanding. To better assess the visual reasoning
capabilities of FMs in educational settings, we introduce EduVisBench, a
multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem
sets requiring visually grounded solutions, along with a fine-grained
evaluation rubric informed by pedagogical theory. Our empirical analysis
reveals that existing models frequently struggle with the inherent challenge of
decomposing complex reasoning and translating it into visual representations
aligned with human cognitive processes. To address these limitations, we
propose EduVisAgent, a multi-agent collaborative framework that coordinates
specialized agents for instructional planning, reasoning decomposition,
metacognitive prompting, and visualization design. Experimental results show
that EduVisAgent substantially outperforms all baselines, achieving a 40.2%
improvement and delivering more educationally aligned visualizations.
EduVisBench and EduVisAgent are available at
https://github.com/aiming-lab/EduVisBench and
https://github.com/aiming-lab/EduVisAgent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16805v1' target='_blank'>SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous
  Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuesong Chen, Linjiang Huang, Tao Ma, Rongyao Fang, Shaoshuai Shi, Hongsheng Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 15:44:30</h6>
<p class='card-text'>The integration of Vision-Language Models (VLMs) into autonomous driving
systems has shown promise in addressing key challenges such as learning
complexity, interpretability, and common-sense reasoning. However, existing
approaches often struggle with efficient integration and realtime
decision-making due to computational demands. In this paper, we introduce
SOLVE, an innovative framework that synergizes VLMs with end-to-end (E2E)
models to enhance autonomous vehicle planning. Our approach emphasizes
knowledge sharing at the feature level through a shared visual encoder,
enabling comprehensive interaction between VLM and E2E components. We propose a
Trajectory Chain-of-Thought (T-CoT) paradigm, which progressively refines
trajectory predictions, reducing uncertainty and improving accuracy. By
employing a temporal decoupling strategy, SOLVE achieves efficient cooperation
by aligning high-quality VLM outputs with E2E real-time performance. Evaluated
on the nuScenes dataset, our method demonstrates significant improvements in
trajectory prediction accuracy, paving the way for more robust and reliable
autonomous driving systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16793v1' target='_blank'>REOBench: Benchmarking Robustness of Earth Observation Foundation Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiang Li, Yong Tao, Siyuan Zhang, Siwei Liu, Zhitong Xiong, Chunbo Luo, Lu Liu, Mykola Pechenizkiy, Xiao Xiang Zhu, Tianjin Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 15:34:50</h6>
<p class='card-text'>Earth observation foundation models have shown strong generalization across
multiple Earth observation tasks, but their robustness under real-world
perturbations remains underexplored. To bridge this gap, we introduce REOBench,
the first comprehensive benchmark for evaluating the robustness of Earth
observation foundation models across six tasks and twelve types of image
corruptions, including both appearance-based and geometric perturbations. To
ensure realistic and fine-grained evaluation, our benchmark focuses on
high-resolution optical remote sensing images, which are widely used in
critical applications such as urban planning and disaster response. We conduct
a systematic evaluation of a broad range of models trained using masked image
modeling, contrastive learning, and vision-language pre-training paradigms. Our
results reveal that (1) existing Earth observation foundation models experience
significant performance degradation when exposed to input corruptions. (2) The
severity of degradation varies across tasks, model architectures, backbone
sizes, and types of corruption, with performance drop varying from less than 1%
to over 20%. (3) Vision-language models show enhanced robustness, particularly
in multimodal tasks. REOBench underscores the vulnerability of current Earth
observation foundation models to real-world corruptions and provides actionable
insights for developing more robust and reliable models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16787v1' target='_blank'>Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ashish Sundar, Chunbo Luo, Xiaoyang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 15:28:50</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) offers an intuitive way to increase
the sample efficiency of model-free RL methods by simultaneously training a
world model that learns to predict the future. MBRL methods have progressed by
largely prioritising the actor; optimising the world model learning has been
neglected meanwhile. Improving the fidelity of the world model and reducing its
time to convergence can yield significant downstream benefits, one of which is
improving the ensuing performance of any actor it may train. We propose a novel
approach that anticipates and actively seeks out high-entropy states using
short-horizon latent predictions generated by the world model, offering a
principled alternative to traditional curiosity-driven methods that chase
once-novel states well after they were stumbled into. While many model
predictive control (MPC) based methods offer similar alternatives, they
typically lack commitment, synthesising multi step plans after every step. To
mitigate this, we present a hierarchical planner that dynamically decides when
to replan, planning horizon length, and the weighting between reward and
entropy. While our method can theoretically be applied to any model that trains
its own actors with solely model generated data, we have applied it to just
Dreamer as a proof of concept. Our method finishes the Miniworld procedurally
generated mazes 50% faster than base Dreamer at convergence and the policy
trained in imagination converges in only 60% of the environment steps that base
Dreamer needs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16756v1' target='_blank'>Representation Discrepancy Bridging Method for Remote Sensing Image-Text
  Retrieval</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hailong Ning, Siying Wang, Tao Lei, Xiaopeng Cao, Huanmin Dou, Bin Zhao, Asoke K. Nandi, Petia Radeva</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 14:59:30</h6>
<p class='card-text'>Remote Sensing Image-Text Retrieval (RSITR) plays a critical role in
geographic information interpretation, disaster monitoring, and urban planning
by establishing semantic associations between image and textual descriptions.
Existing Parameter-Efficient Fine-Tuning (PEFT) methods for Vision-and-Language
Pre-training (VLP) models typically adopt symmetric adapter structures for
exploring cross-modal correlations. However, the strong discriminative nature
of text modality may dominate the optimization process and inhibits image
representation learning. The nonnegligible imbalanced cross-modal optimization
remains a bottleneck to enhancing the model performance. To address this issue,
this study proposes a Representation Discrepancy Bridging (RDB) method for the
RSITR task. On the one hand, a Cross-Modal Asymmetric Adapter (CMAA) is
designed to enable modality-specific optimization and improve feature
alignment. The CMAA comprises a Visual Enhancement Adapter (VEA) and a Text
Semantic Adapter (TSA). VEA mines fine-grained image features by Differential
Attention (DA) mechanism, while TSA identifies key textual semantics through
Hierarchical Attention (HA) mechanism. On the other hand, this study extends
the traditional single-task retrieval framework to a dual-task optimization
framework and develops a Dual-Task Consistency Loss (DTCL). The DTCL improves
cross-modal alignment robustness through an adaptive weighted combination of
cross-modal, classification, and exponential moving average consistency
constraints. Experiments on RSICD and RSITMD datasets show that the proposed
RDB method achieves a 6%-11% improvement in mR metrics compared to
state-of-the-art PEFT methods and a 1.15%-2% improvement over the full
fine-tuned GeoRSCLIP model.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16726v1' target='_blank'>D-LIO: 6DoF Direct LiDAR-Inertial Odometry based on Simultaneous
  Truncated Distance Field Mapping</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lucia Coto-Elena, J. E. Maese, L. Merino, F. Caballero</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 14:34:32</h6>
<p class='card-text'>This paper presents a new approach for 6DoF Direct LiDAR-Inertial Odometry
(D-LIO) based on the simultaneous mapping of truncated distance fields on CPU.
Such continuous representation (in the vicinity of the points) enables working
with raw 3D LiDAR data online, avoiding the need of LiDAR feature selection and
tracking, simplifying the odometry pipeline and easily generalizing to many
scenarios. The method is based on the proposed Fast Truncated Distance Field
(Fast-TDF) method as a convenient tool to represent the environment. Such
representation enables i) solving the LiDAR point-cloud registration as a
nonlinear optimization process without the need of selecting/tracking LiDAR
features in the input data, ii) simultaneously producing an accurate truncated
distance field map of the environment, and iii) updating such map at constant
time independently of its size. The approach is tested using open datasets,
aerial and ground. It is also benchmarked against other state-of-the-art
odometry approaches, demonstrating the same or better level of accuracy with
the added value of an online-generated TDF representation of the environment,
that can be used for other robotics tasks as planning or collision avoidance.
The source code is publicly available at
https://anonymous.4open.science/r/D-LIO</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16686v1' target='_blank'>SPaRC: A Spatial Pathfinding Reasoning Challenge</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lars Benedikt Kaesberg, Jan Philip Wahle, Terry Ruas, Bela Gipp</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 13:53:50</h6>
<p class='card-text'>Existing reasoning datasets saturate and fail to test abstract, multi-step
problems, especially pathfinding and complex rule constraint satisfaction. We
introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000
2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,
requiring step-by-step planning with arithmetic and geometric rules. Humans
achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best
reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).
Models often generate invalid paths (>50% of puzzles for o4-mini), and
reasoning tokens reveal they make errors in navigation and spatial logic.
Unlike humans, who take longer on hard puzzles, models fail to scale test-time
compute with difficulty. Allowing models to make multiple solution attempts
improves accuracy, suggesting potential for better spatial reasoning with
improved training and efficient test-time scaling methods. SPaRC can be used as
a window into models' spatial reasoning limitations and drive research toward
new methods that excel in abstract, multi-step problem-solving.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16547v1' target='_blank'>Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for
  Occlusion Aware Plant Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nitesh Subedi, Hsin-Jung Yang, Devesh K. Jha, Soumik Sarkar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 11:37:39</h6>
<p class='card-text'>This paper presents an end-to-end deep reinforcement learning (RL) framework
for occlusion-aware robotic manipulation in cluttered plant environments. Our
approach enables a robot to interact with a deformable plant to reveal hidden
objects of interest, such as fruits, using multimodal observations. We decouple
the kinematic planning problem from robot control to simplify zero-shot
sim2real transfer for the trained policy. Our results demonstrate that the
trained policy, deployed using our framework, achieves up to 86.7% success in
real-world trials across diverse initial conditions. Our findings pave the way
toward autonomous, perception-driven agricultural robots that intelligently
interact with complex foliage plants to "find the fruit" in challenging
occluded scenarios, without the need for explicitly designed geometric and
dynamic models of every plant scenario.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16524v1' target='_blank'>CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation
  in Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huitong Yang, Zhuoxiao Chen, Fengyi Zhang, Zi Huang, Yadan Luo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 11:09:15</h6>
<p class='card-text'>Maintaining robust 3D perception under dynamic and unpredictable test-time
conditions remains a critical challenge for autonomous driving systems.
Existing test-time adaptation (TTA) methods often fail in high-variance tasks
like 3D object detection due to unstable optimization and sharp minima. While
recent model merging strategies based on linear mode connectivity (LMC) offer
improved stability by interpolating between fine-tuned checkpoints, they are
computationally expensive, requiring repeated checkpoint access and multiple
forward passes. In this paper, we introduce CodeMerge, a lightweight and
scalable model merging framework that bypasses these limitations by operating
in a compact latent space. Instead of loading full models, CodeMerge represents
each checkpoint with a low-dimensional fingerprint derived from the source
model's penultimate features and constructs a key-value codebook. We compute
merging coefficients using ridge leverage scores on these fingerprints,
enabling efficient model composition without compromising adaptation quality.
Our method achieves strong performance across challenging benchmarks, improving
end-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by
over 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as
online mapping, motion prediction and planning even without training. Code and
pretrained models are released in the supplementary material.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16422v1' target='_blank'>Unlocking Smarter Device Control: Foresighted Planning with a World
  Model-Driven Code Execution Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoran Yin, Xu Luo, Hao Wu, Lianli Gao, Jingkuan Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 09:08:47</h6>
<p class='card-text'>The automatic control of mobile devices is essential for efficiently
performing complex tasks that involve multiple sequential steps. However, these
tasks pose significant challenges due to the limited environmental information
available at each step, primarily through visual observations. As a result,
current approaches, which typically rely on reactive policies, focus solely on
immediate observations and often lead to suboptimal decision-making. To address
this problem, we propose \textbf{Foresighted Planning with World Model-Driven
Code Execution (FPWC)},a framework that prioritizes natural language
understanding and structured reasoning to enhance the agent's global
understanding of the environment by developing a task-oriented, refinable
\emph{world model} at the outset of the task. Foresighted actions are
subsequently generated through iterative planning within this world model,
executed in the form of executable code. Extensive experiments conducted in
simulated environments and on real mobile devices demonstrate that our method
outperforms previous approaches, particularly achieving a 44.4\% relative
improvement in task success rate compared to the state-of-the-art in the
simulated environment. Code and demo are provided in the supplementary
material.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16394v1' target='_blank'>Raw2Drive: Reinforcement Learning with Aligned World Models for
  End-to-End Autonomous Driving (in CARLA v2)</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenjie Yang, Xiaosong Jia, Qifeng Li, Xue Yang, Maoqing Yao, Junchi Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:46:53</h6>
<p class='card-text'>Reinforcement Learning (RL) can mitigate the causal confusion and
distribution shift inherent to imitation learning (IL). However, applying RL to
end-to-end autonomous driving (E2E-AD) remains an open problem for its training
difficulty, and IL is still the mainstream paradigm in both academia and
industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated
promising results in neural planning; however, these methods typically require
privileged information as input rather than raw sensor data. We fill this gap
by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently
train an auxiliary privileged world model paired with a neural planner that
uses privileged information as input. Subsequently, we introduce a raw sensor
world model trained via our proposed Guidance Mechanism, which ensures
consistency between the raw sensor world model and the privileged world model
during rollouts. Finally, the raw sensor world model combines the prior
knowledge embedded in the heads of the privileged world model to effectively
guide the training of the raw sensor policy. Raw2Drive is so far the only RL
based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it
achieves state-of-the-art performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16391v1' target='_blank'>Quantum-Driven Multihead Inland Waterbody Detection With
  Transformer-Encoded CYGNSS Delay-Doppler Map Data</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chia-Hsiang Lin, Jhao-Ting Lin, Po-Ying Chiu, Shih-Ping Chen, Charles C. H. Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:44:48</h6>
<p class='card-text'>Inland waterbody detection (IWD) is critical for water resources management
and agricultural planning. However, the development of high-fidelity IWD
mapping technology remains unresolved. We aim to propose a practical solution
based on the easily accessible data, i.e., the delay-Doppler map (DDM) provided
by NASA's Cyclone Global Navigation Satellite System (CYGNSS), which
facilitates effective estimation of physical parameters on the Earth's surface
with high temporal resolution and wide spatial coverage. Specifically, as
quantum deep network (QUEEN) has revealed its strong proficiency in addressing
classification-like tasks, we encode the DDM using a customized transformer,
followed by feeding the transformer-encoded DDM (tDDM) into a highly entangled
QUEEN to distinguish whether the tDDM corresponds to a hydrological region. In
recent literature, QUEEN has achieved outstanding performances in numerous
challenging remote sensing tasks (e.g., hyperspectral restoration, change
detection, and mixed noise removal, etc.), and its high effectiveness stems
from the fundamentally different way it adopts to extract features (the
so-called quantum unitary-computing features). The meticulously designed
IWD-QUEEN retrieves high-precision river textures, such as those in Amazon
River Basin in South America, demonstrating its superiority over traditional
classification methods and existing global hydrography maps. IWD-QUEEN,
together with its parallel quantum multihead scheme, works in a near-real-time
manner (i.e., millisecond-level computing per DDM). To broaden accessibility
for users of traditional computers, we also provide the non-quantum counterpart
of our method, called IWD-Transformer, thereby increasing the impact of this
work.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16389v1' target='_blank'>Coverage Path Planning For Multi-view SAR-UAV Observation System Under
  Energy Constraint</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Deyu Song, Xiangyin Zhang, Zipei Yu, Kaiyu Qin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:42:33</h6>
<p class='card-text'>Multi-view Synthetic Aperture Radar (SAR) imaging can effectively enhance the
performance of tasks such as automatic target recognition and image information
fusion. Unmanned aerial vehicles (UAVs) have the advantages of flexible
deployment and cost reduction. A swarm of UAVs equipped with synthetic aperture
radar imaging equipment is well suited to meet the functional requirements of
multi-view synthetic aperture radar imaging missions. However, to provide
optimal paths for SAR-UAVs from the base station to cover target viewpoints in
the mission area is of NP-hard computational complexity. In this work, the
coverage path planning problem for multi-view SAR-UAV observation systems is
studied. First, the coordinate of observation viewpoints is calculated based on
the location of targets and base station under a brief geometric model. Then,
the exact problem formulation is modeled in order to fully describe the
solution space and search for optimal paths that provide maximum coverage rate
for SAR-UAVs. Finally, an Adaptive Density Peak Clustering (ADPC) method is
proposed to overcome the additional energy consumption due to the viewpoints
being far away from the base station. The Particle Swarm Optimization (PSO)
algorithm is introduced for optimal path generation. Experimental results
demonstrate the effectiveness and computational efficiency of the proposed
approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16377v1' target='_blank'>VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with
  World Models for Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yansong Qu, Zilin Huang, Zihao Sheng, Jiancong Chen, Sikai Chen, Samuel Labi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:29:59</h6>
<p class='card-text'>Reinforcement learning (RL)-based autonomous driving policy learning faces
critical limitations such as low sample efficiency and poor generalization; its
reliance on online interactions and trial-and-error learning is especially
unacceptable in safety-critical scenarios. Existing methods including safe RL
often fail to capture the true semantic meaning of "safety" in complex driving
contexts, leading to either overly conservative driving behavior or constraint
violations. To address these challenges, we propose VL-SAFE, a world
model-based safe RL framework with Vision-Language model
(VLM)-as-safety-guidance paradigm, designed for offline safe policy learning.
Specifically, we construct offline datasets containing data collected by expert
agents and labeled with safety scores derived from VLMs. A world model is
trained to generate imagined rollouts together with safety estimations,
allowing the agent to perform safe planning without interacting with the real
environment. Based on these imagined trajectories and safety evaluations,
actor-critic learning is conducted under VLM-based safety guidance to optimize
the driving policy more safely and efficiently. Extensive evaluations
demonstrate that VL-SAFE achieves superior sample efficiency, generalization,
safety, and overall performance compared to existing baselines. To the best of
our knowledge, this is the first work that introduces a VLM-guided world
model-based approach for safe autonomous driving. The demo video and code can
be accessed at: https://ys-qu.github.io/vlsafe-website/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16351v1' target='_blank'>Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency
  Transcription and Detection</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenxu Guo, Jiachen Lian, Xuanru Zhou, Jinming Zhang, Shuhe Li, Zongli Ye, Hwi Joo Park, Anaisha Das, Zoe Ezzes, Jet Vonk, Brittany Morin, Rian Bogley, Lisa Wauters, Zachary Miller, Maria Gorno-Tempini, Gopala Anumanchipalli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:02:50</h6>
<p class='card-text'>Automatic detection of speech dysfluency aids speech-language pathologists in
efficient transcription of disordered speech, enhancing diagnostics and
treatment planning. Traditional methods, often limited to classification,
provide insufficient clinical insight, and text-independent models misclassify
dysfluency, especially in context-dependent cases. This work introduces
Dysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes
and detects dysfluency. Unlike previous models, Dysfluent-WFST operates with
upstream encoders like WavLM and requires no additional training. It achieves
state-of-the-art performance in both phonetic error rate and dysfluency
detection on simulated and real speech data. Our approach is lightweight,
interpretable, and effective, demonstrating that explicit modeling of
pronunciation behavior in decoding, rather than complex architectures, is key
to improving dysfluency processing systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16249v1' target='_blank'>Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based
  Predictive Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhen Zhang, Xiangyu Chu, Yunxi Tang, Lulu Zhao, Jing Huang, Zhongliang Jiang, K. W. Samuel Au</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 05:36:00</h6>
<p class='card-text'>Manipulating elasto-plastic objects remains a significant challenge due to
severe self-occlusion, difficulties of representation, and complicated
dynamics. This work proposes a novel framework for elasto-plastic object
manipulation with a quasi-static assumption for motions, leveraging 3D
occupancy to represent such objects, a learned dynamics model trained with 3D
occupancy, and a learning-based predictive control algorithm to address these
challenges effectively. We build a novel data collection platform to collect
full spatial information and propose a pipeline for generating a 3D occupancy
dataset. To infer the 3D occupancy during manipulation, an occupancy prediction
network is trained with multiple RGB images supervised by the generated
dataset. We design a deep neural network empowered by a 3D convolution neural
network (CNN) and a graph neural network (GNN) to predict the complex
deformation with the inferred 3D occupancy results. A learning-based predictive
control algorithm is introduced to plan the robot actions, incorporating a
novel shape-based action initialization module specifically designed to improve
the planner efficiency. The proposed framework in this paper can successfully
shape the elasto-plastic objects into a given goal shape and has been verified
in various experiments both in simulation and the real world.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16100v1' target='_blank'>BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zifeng Wang, Benjamin Danek, Jimeng Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 01:02:21</h6>
<p class='card-text'>Validating scientific hypotheses is a central challenge in biomedical
research, and remains difficult for artificial intelligence (AI) agents due to
the complexity of real-world data analysis and evidence interpretation. In this
work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on
realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K
consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,
curated from over 300 published biomedical studies to reflect the structure and
reasoning found in authentic research workflows. Each task includes a
structured hypothesis derived from the original study's conclusions, expressed
in the affirmative to reflect the language of scientific reporting, and one or
more pieces of supporting evidence grounded in empirical data tables. While
these hypotheses mirror published claims, they remain testable using standard
statistical or machine learning methods. The benchmark enables evaluation along
four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and
conclusion, (3) correctness of the reasoning process, and (4) executability of
the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable
hypotheses: cases where the available data are insufficient to support or
refute a claim, reflecting a common yet underexplored scenario in real-world
science. We propose BioDSA-1K as a foundation for building and evaluating
generalizable, trustworthy AI agents for biomedical discovery.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16080v1' target='_blank'>SynEVO: A neuro-inspired spatiotemporal evolutional framework for
  cross-domain adaptation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiayue Liu, Zhongchao Yi, Zhengyang Zhou, Qihe Huang, Kuo Yang, Xu Wang, Yang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 23:45:51</h6>
<p class='card-text'>Discovering regularities from spatiotemporal systems can benefit various
scientific and social planning. Current spatiotemporal learners usually train
an independent model from a specific source data that leads to limited
transferability among sources, where even correlated tasks requires new design
and training. The key towards increasing cross-domain knowledge is to enable
collective intelligence and model evolution. In this paper, inspired by
neuroscience theories, we theoretically derive the increased information
boundary via learning cross-domain collective intelligence and propose a
Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the
model independence and enables cross-domain knowledge to be shared and
aggregated. Specifically, we first re-order the sample groups to imitate the
human curriculum learning, and devise two complementary learners, elastic
common container and task-independent extractor to allow model growth and
task-wise commonality and personality disentanglement. Then an adaptive dynamic
coupler with a new difference metric determines whether the new sample group
should be incorporated into common container to achieve model evolution under
various domains. Experiments show that SynEVO improves the generalization
capacity by at most 42% under cross-domain scenarios and SynEVO provides a
paradigm of NeuroAI for knowledge transfer and adaptation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15972v1' target='_blank'>Extremum Seeking for PDE Systems using Physics-Informed Neural Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haojin Guo, Zongyi Guo, Jianguo Guo, Tiago Roux Oliveira</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 19:44:27</h6>
<p class='card-text'>Extremum Seeking (ES) is an effective real-time optimization method for PDE
systems in cascade with nonlinear quadratic maps. To address PDEs in the
feedback loop, a boundary control law and a re-design of the additive probing
signal are mandatory. The latter, commonly called "trajectory generation" or
"motion planning," involves designing perturbation signals that anticipate
their propagation through PDEs. Specifically, this requires solving motion
planning problems for systems governed by parabolic and hyperbolic PDEs.
Physics-Informed Neural Networks (PINN) is a powerful tool for solving PDEs by
embedding physical laws as constraints in the neural network's loss function,
enabling efficient solutions for high-dimensional, nonlinear, and complex
problems. This paper proposes a novel construction integrating PINN and ES,
automating the motion planning process for specific PDE systems and eliminating
the need for case-by-case analytical derivations. The proposed strategy
efficiently extracts perturbation signals, optimizing the PDE system.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15925v1' target='_blank'>VERDI: VLM-Embedded Reasoning for Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bowen Feng, Zhiting Mei, Baiang Li, Julian Ost, Roger Girgis, Anirudha Majumdar, Felix Heide</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 18:24:36</h6>
<p class='card-text'>While autonomous driving (AD) stacks struggle with decision making under
partial observability and real-world complexity, human drivers are capable of
commonsense reasoning to make near-optimal decisions with limited information.
Recent work has attempted to leverage finetuned Vision-Language Models (VLMs)
for trajectory planning at inference time to emulate human behavior. Despite
their success in benchmark evaluations, these methods are often impractical to
deploy (a 70B parameter VLM inference at merely 8 tokens per second requires
more than 160G of memory), and their monolithic network structure prohibits
safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for
autonomous Driving (VERDI), a training-time framework that distills the
reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI
augments modular differentiable end-to-end (e2e) AD models by aligning
intermediate module outputs at the perception, prediction, and planning stages
with text features explaining the driving reasoning process produced by VLMs.
By encouraging alignment in latent space, \textsc{VERDI} enables the modular AD
stack to internalize structured reasoning, without incurring the inference-time
costs of large VLMs. We demonstrate the effectiveness of our method on the
NuScenes dataset and find that VERDI outperforms existing e2e methods that do
not embed reasoning by 10% in $\ell_{2}$ distance, while maintaining high
inference speed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15917v1' target='_blank'>How to factor 2048 bit RSA integers with less than a million noisy
  qubits</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Craig Gidney</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 18:11:44</h6>
<p class='card-text'>Planning the transition to quantum-safe cryptosystems requires understanding
the cost of quantum attacks on vulnerable cryptosystems. In Gidney+Eker{\aa}
2019, I co-published an estimate stating that 2048 bit RSA integers could be
factored in eight hours by a quantum computer with 20 million noisy qubits. In
this paper, I substantially reduce the number of qubits required. I estimate
that a 2048 bit RSA integer could be factored in less than a week by a quantum
computer with less than a million noisy qubits. I make the same assumptions as
in 2019: a square grid of qubits with nearest neighbor connections, a uniform
gate error rate of $0.1\%$, a surface code cycle time of 1 microsecond, and a
control system reaction time of $10$ microseconds.
  The qubit count reduction comes mainly from using approximate residue
arithmetic (Chevignard+Fouque+Schrottenloher 2024), from storing idle logical
qubits with yoked surface codes (Gidney+Newman+Brooks+Jones 2023), and from
allocating less space to magic state distillation by using magic state
cultivation (Gidney+Shutty+Jones 2024). The longer runtime is mainly due to
performing more Toffoli gates and using fewer magic state factories compared to
Gidney+Eker{\aa} 2019. That said, I reduce the Toffoli count by over 100x
compared to Chevignard+Fouque+Schrottenloher 2024.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15782v1' target='_blank'>Solving General-Utility Markov Decision Processes in the Single-Trial
  Regime with Online Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pedro P. Santos, Alberto Sardinha, Francisco S. Melo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 17:32:23</h6>
<p class='card-text'>In this work, we contribute the first approach to solve infinite-horizon
discounted general-utility Markov decision processes (GUMDPs) in the
single-trial regime, i.e., when the agent's performance is evaluated based on a
single trajectory. First, we provide some fundamental results regarding policy
optimization in the single-trial regime, investigating which class of policies
suffices for optimality, casting our problem as a particular MDP that is
equivalent to our original problem, as well as studying the computational
hardness of policy optimization in the single-trial regime. Second, we show how
we can leverage online planning techniques, in particular a Monte-Carlo tree
search algorithm, to solve GUMDPs in the single-trial regime. Third, we provide
experimental results showcasing the superior performance of our approach in
comparison to relevant baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15754v1' target='_blank'>Improving planning and MBRL with temporally-extended actions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Palash Chatterjee, Roni Khardon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 16:59:32</h6>
<p class='card-text'>Continuous time systems are often modeled using discrete time dynamics but
this requires a small simulation step to maintain accuracy. In turn, this
requires a large planning horizon which leads to computationally demanding
planning problems and reduced performance. Previous work in model free
reinforcement learning has partially addressed this issue using action repeats
where a policy is learned to determine a discrete action duration. Instead we
propose to control the continuous decision timescale directly by using
temporally-extended actions and letting the planner treat the duration of the
action as an additional optimization variable along with the standard action
variables. This additional structure has multiple advantages. It speeds up
simulation time of trajectories and, importantly, it allows for deep horizon
search in terms of primitive actions while using a shallow search depth in the
planner. In addition, in the model based reinforcement learning (MBRL) setting,
it reduces compounding errors from model learning and improves training time
for models. We show that this idea is effective and that the range for action
durations can be automatically selected using a multi-armed bandit formulation
and integrated into the MBRL framework. An extensive experimental evaluation
both in planning and in MBRL, shows that our approach yields faster planning,
better solutions, and that it enables solutions to problems that are not solved
in the standard formulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15733v1' target='_blank'>Distributionally Robust Planning of Hydrogen-Electrical Microgrids for
  Sea Islands</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuchen Dong, Zhengsong Lu, Xiaoyu Cao, Zhengwen He, Tanveer Hossain Bhuiyan, Bo Zeng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 16:39:36</h6>
<p class='card-text'>This paper presents a distributionally robust planning method for
hydrogen-electrical microgrids over islands, where the cross-island energy
exchange is supported by a maritime hydrogen transport network. This planning
problem is complicated due to heterogeneous off-shore wind-driven uncertainties
(i.e., renewable power, transport availability, demand fluctuations, and grid
faulting), a subset of which exhibit endogenous uncertainty, as they can be
affected by proactive measures (e.g., grid hardening) or infrastructure
investment. To capture these features, a two-stage distributionally robust
optimization (DRO) model is developed considering decision-dependent
uncertainty (DDU), which encompasses variation of the underlying distributional
ambiguity due to the change of the first stage decisions. Notably, the complete
recourse property is missing, which is often neglected in existing DRO studies.
Nevertheless, different from the case for land-based microgrids, this issue is
critical and fundamental for sea island systems due to their particular
physical and logistical requirements. To address these issues, we develop a
C&CG algorithm that is customized with strong cutting planes to handle DRO with
a varying DDU ambiguity set and feasibility requirements. Numerical results
demonstrate the cost-effectiveness and resilience of the proposed planning
framework, along with the nontrivial improvements of the algorithm in both
solution accuracy and computational efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15725v1' target='_blank'>UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV
  Imitation Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiangyu Wang, Donglin Yang, Yue Liao, Wenhao Zheng, wenjun wu, Bin Dai, Hongsheng Li, Si Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 16:31:28</h6>
<p class='card-text'>Unmanned Aerial Vehicles (UAVs) are evolving into language-interactive
platforms, enabling more intuitive forms of human-drone interaction. While
prior works have primarily focused on high-level planning and long-horizon
navigation, we shift attention to language-guided fine-grained trajectory
control, where UAVs execute short-range, reactive flight behaviors in response
to language instructions. We formalize this problem as the Flying-on-a-Word
(Flow) task and introduce UAV imitation learning as an effective approach. In
this framework, UAVs learn fine-grained control policies by mimicking expert
pilot trajectories paired with atomic language instructions. To support this
paradigm, we present UAV-Flow, the first real-world benchmark for
language-conditioned, fine-grained UAV control. It includes a task formulation,
a large-scale dataset collected in diverse environments, a deployable control
framework, and a simulation suite for systematic evaluation. Our design enables
UAVs to closely imitate the precise, expert-level flight trajectories of human
pilots and supports direct deployment without sim-to-real gap. We conduct
extensive experiments on UAV-Flow, benchmarking VLN and VLA paradigms. Results
show that VLA models are superior to VLN baselines and highlight the critical
role of spatial grounding in the fine-grained Flow setting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15686v1' target='_blank'>Path Planning Algorithm Comparison Analysis for Wireless AUVs Energy
  Sharing System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhengji Feng, Hengxiang Chen, Liqun Chen, Heyan Li, Xiaolin Mou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 16:02:13</h6>
<p class='card-text'>Autonomous underwater vehicles (AUVs) are increasingly used in marine
research, military applications, and undersea exploration. However, their
operational range is significantly affected by battery performance. In this
paper, a framework for a wireless energy sharing system among AUVs is proposed,
enabling rapid energy replenishment. Path planning plays a crucial role in the
energy-sharing process and autonomous navigation, as it must generate feasible
trajectories toward designated goals. This article focuses on efficient
obstacle avoidance in complex underwater environments, including irregularly
shaped obstacles and narrow passages. The proposed method combines
Rapidly-exploring Random Trees Star (RRT*) with Particle Swarm Optimization
(PSO) to improve path planning efficiency. Comparative analysis of the two
algorithms is presented through simulation results in both random and irregular
obstacle environments. Index Terms: Wireless charging, autonomous underwater
vehicles (AUVs), path planning, irregular obstacles, narrow passages, RRT*,
particle swarm optimization (PSO).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15679v1' target='_blank'>SwarmDiff: Swarm Robotic Trajectory Planning in Cluttered Environments
  via Diffusion Transformer</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kang Ding, Chunxuan Jiao, Yunze Hu, Kangjie Zhou, Pengying Wu, Yao Mu, Chang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 15:56:55</h6>
<p class='card-text'>Swarm robotic trajectory planning faces challenges in computational
efficiency, scalability, and safety, particularly in complex, obstacle-dense
environments. To address these issues, we propose SwarmDiff, a hierarchical and
scalable generative framework for swarm robots. We model the swarm's
macroscopic state using Probability Density Functions (PDFs) and leverage
conditional diffusion models to generate risk-aware macroscopic trajectory
distributions, which then guide the generation of individual robot trajectories
at the microscopic level. To ensure a balance between the swarm's optimal
transportation and risk awareness, we integrate Wasserstein metrics and
Conditional Value at Risk (CVaR). Additionally, we introduce a Diffusion
Transformer (DiT) to improve sampling efficiency and generation quality by
capturing long-range dependencies. Extensive simulations and real-world
experiments demonstrate that SwarmDiff outperforms existing methods in
computational efficiency, trajectory validity, and scalability, making it a
reliable solution for swarm robotic trajectory planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15517v1' target='_blank'>Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot
  Manipulation Datasets</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaiyuan Chen, Shuangyu Xie, Zehan Ma, Ken Goldberg</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 13:42:52</h6>
<p class='card-text'>Vision-Language Models (VLMs) acquire real-world knowledge and general
reasoning ability through Internet-scale image-text corpora. They can augment
robotic systems with scene understanding and task planning, and assist
visuomotor policies that are trained on robot trajectory data. We explore the
reverse paradigm - using rich, real, multi-modal robot trajectory data to
enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual
Question Answering (VQA) dataset generation framework for VLMs. Given a human
tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual
and non-descriptive sensory modalities, such as end-effector pose, gripper
aperture, and force sensing. Based on these modalities, it segments the robot
trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses
scene and interaction understanding to identify 3D properties of the robot,
task goal, and the target object. The properties are used to generate
representative VQA queries - images with textural multiple-choice questions -
based on spatial, goal-conditioned, and interaction reasoning question
templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710
questions covering 463 distinct scenes and 3,396 robotic manipulation tasks
from 176k real robot trajectories. Results suggest that Robo2VLM-1 can
benchmark and improve VLM capabilities in spatial and interaction reasoning.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>