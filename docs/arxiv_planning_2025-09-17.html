<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-09-17</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-09-17</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13312v1' target='_blank'>WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for
  Open-Ended Deep Research</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, Jun Zhang, Jingren Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 17:57:21</h6>
<p class='card-text'>This paper tackles open-ended deep research (OEDR), a complex challenge where
AI agents must synthesize vast web-scale information into insightful reports.
Current approaches are plagued by dual-fold limitations: static research
pipelines that decouple planning from evidence acquisition and one-shot
generation paradigms that easily suffer from long-context failure issues like
"loss in the middle" and hallucinations. To address these challenges, we
introduce WebWeaver, a novel dual-agent framework that emulates the human
research process. The planner operates in a dynamic cycle, iteratively
interleaving evidence acquisition with outline optimization to produce a
comprehensive, source-grounded outline linking to a memory bank of evidence.
The writer then executes a hierarchical retrieval and writing process,
composing the report section by section. By performing targeted retrieval of
only the necessary evidence from the memory bank for each part, it effectively
mitigates long-context issues. Our framework establishes a new state-of-the-art
across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and
DeepResearchGym. These results validate our human-centric, iterative
methodology, demonstrating that adaptive planning and focused synthesis are
crucial for producing high-quality, reliable, and well-structured reports.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13209v1' target='_blank'>Cardinality-Constrained Bilevel Capacity Expansion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lei Guo, Jiayang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 16:15:59</h6>
<p class='card-text'>As a fundamental problem in transportation and operations research, the
bilevel capacity expansion problem (BCEP) has been extensively studied for
decades. In practice, BCEPs are commonly addressed in two stages: first,
pre-select a small set of links for expansion; then, optimize their capacities.
However, this sequential and separable approach can lead to suboptimal
solutions as it neglects the critical interdependence between link selection
and capacity allocation. In this paper, we propose to introduce a cardinality
constraint into the BCEP to limit the number of expansion locations rather than
fixing such locations beforehand. This allows us to search over all possible
link combinations within the prescribed limit, thereby enabling the joint
optimization of both expansion locations and capacity levels. The resulting
cardinality-constrained BCEP (CCBCEP) is computationally challenging due to the
combination of a nonconvex equilibrium constraint and a nonconvex and
discontinuous cardinality constraint. To address this challenge, we develop a
penalized difference-of-convex (DC) approach that transforms the original
problem into a sequence of tractable subproblems by exploiting its inherent DC
structure and the special properties of the cardinality constraint. We prove
that the method converges to approximate Karush-Kuhn-Tucker (KKT) solutions
with arbitrarily prescribed accuracy. Numerical experiments further show that
the proposed approach consistently outperforms alternative methods for
identifying practically feasible expansion plans investing only a few links,
both in solution quality and computational efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13171v1' target='_blank'>Hybrid Active-Passive Galactic Cosmic Ray Simulator: in-silico design
  and optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Luca Lunati, Enrico Pierobon, Uli Weber, Tim Wagner, Tabea Pfuhl, Marco Durante, Christoph Schuy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 15:22:28</h6>
<p class='card-text'>High-energy heavy-ion particle accelerators have long served as a proxy for
the harsh space radiation environment, enabling both fundamental life-science
research and applied testing of flight components. Typically, monoenergetic
high-energy heavy-ion beams are used to mimic the complex mixed radiation field
encountered in low Earth orbit and beyond. However, synergistic effects arising
from the spatial or temporal proximity of interactions of different radiation
qualities in a mixed field cannot be fully assessed with such beams. Therefore,
spearheaded by developments at the NASA Space Radiation Laboratory, the GSI
Helmholtzzentrum fuer Schwerionenforschung, supported by ESA, has developed
advanced space radiation simulation capabilities to support space radiation
studies in Europe. Here, we report the design, optimization, and in-silico
benchmarking of GSI's hybrid active-passive GCR simulator. Additionally, a
computationally optimized phase-space particle source for Geant4 is presented,
which will be made available to external users to support their own in-silico
studies and experimental planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13104v1' target='_blank'>Solar Flare Hard X-ray Polarimetry with the CUbesat Solar Polarimeter
  (CUSP) mission</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nicolas De Angelis, Andrea Alimenti, Davide Albanesi, Ilaria Baffo, Daniele Brienza, Riccardo Campana, Valerio Campamaggiore, Mauro Centrone, Enrico Costa, Giovanni Cucinella, Andrea Curatolo, Giovanni De Cesare, Giulia de Iulis, Ettore Del Monte, Andrea Del Re, Sergio Di Cosimo, Simone Di Filippo, Giuseppe Di Persio, Immacolata Donnarumma, Sergio Fabiani, Pierluigi Fanelli, Nicolas Gagliardi, Abhay Kumar, Alessandro Lacerenza, Paolo Leonetti, Pasqualino Loffredo, Giovanni Lombardi, Matteo Mergè, Gabriele Minervini, Dario Modenini, Fabio Muleri, Andrea Negri, Daniele Pecorella, Massimo Perelli, Alice Ponti, Paolo Romano, Alda Rubini, Emanuele Scalise, Enrico Silva, Paolo Soffitta, Paolo Tortora, Alessandro Turchi, Valerio Vagelli, Emanuele Zaccagnino, Alessandro Zambardi, Costantino Zazza</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 14:04:08</h6>
<p class='card-text'>The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission planned for
a launch in low-Earth orbit and aimed to measure the linear polarization of
solar flares in the hard X-ray band by means of a Compton scattering
polarimeter. CUSP will allow us to study the magnetic reconnection and particle
acceleration in the flaring magnetic structures of our star. CUSP is a project
in the framework of the Alcor Program of the Italian Space Agency aimed at
developing new CubeSat missions. It is undergoing a 12-month Phase B that
started in December 2024.
  The Compton polarimeter on board CUSP is composed of two acquisition chains
based on plastic scintillators read out by Multi-Anode PhotoMultiplier Tubes
for the scatterer part and GAGG crystals coupled to Avalanche PhotoDiodes for
the absorbers. An event coincident between the two readout schemes will lead to
a measurement of the incoming X-ray's azimuthal scattering angle, linked to the
polarization of the solar flare in a statistical manner. The current status of
the CUSP mission design, mission analysis, and payload scientific performance
will be reported. The latter will be discussed based on preliminary laboratory
results obtained in parallel with Geant4 simulations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13095v1' target='_blank'>Empowering Multi-Robot Cooperation via Sequential World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijie Zhao, Honglei Guo, Shengqian Chen, Kaixuan Xu, Bo Jiang, Yuanheng Zhu, Dongbin Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 13:52:30</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) has shown significant potential in
robotics due to its high sample efficiency and planning capability. However,
extending MBRL to multi-robot cooperation remains challenging due to the
complexity of joint dynamics. To address this, we propose the Sequential World
Model (SeqWM), a novel framework that integrates the sequential paradigm into
model-based multi-agent reinforcement learning. SeqWM employs independent,
sequentially structured agent-wise world models to decompose complex joint
dynamics. Latent rollouts and decision-making are performed through sequential
communication, where each agent generates its future trajectory and plans its
actions based on the predictions of its predecessors. This design enables
explicit intention sharing, enhancing cooperative performance, and reduces
communication overhead to linear complexity. Results in challenging simulated
environments (Bi-DexHands and Multi-Quad) show that SeqWM outperforms existing
state-of-the-art model-free and model-based baselines in both overall
performance and sample efficiency, while exhibiting advanced cooperative
behaviors such as predictive adaptation and role division. Furthermore, SeqWM
has been success fully deployed on physical quadruped robots, demonstrating its
effectiveness in real-world multi-robot systems. Demos and code are available
at: https://github.com/zhaozijie2022/seqwm-marl</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13065v1' target='_blank'>A Dantzig-Wolfe Reformulation for Automated Aircraft Arrival Routing and
  Scheduling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Roghayeh Hajizadeh, Tatiana Polishchuk, Elina Rönnberg, Christiane Schmidt</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 13:21:14</h6>
<p class='card-text'>We consider the problem of computing aircraft arrival routes in a terminal
maneuvering area (TMA) together with an automated scheduling of all the
arrivals within a given time interval. The arrival routes are modeled as
energy-efficient continuous-descent operations, such that separation based on
wake-turbulence categories is guaranteed within the TMA. We propose a new model
based on a Dantzig-Wolfe reformulation of a previous model for this problem. As
in the previous model, we include tree consistency across consecutive planning
intervals. However, the reformulation enables us to further improve the model
and also consider aircraft that remain in the TMA from the previous period, a
feature critical for operational safety. In computational experiments for
Stockholm Arlanda airport, the new model consistently outperforms the previous
one: we obtain solutions within 5 seconds to 12.65 minutes compared to 40.9
hours with the old model for instances of half hours with high traffic. In
addition, we are able to solve instances of a full hour of arriving aircraft
with high traffic (33 aircraft) within 22.22 to 58.57 minutes, whereas the old
model could not solve these instances at all. While we schedule all aircraft as
continuous-descent arrivals, our model can be applied to any type of speed
profiles for the arriving aircraft.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12926v1' target='_blank'>Population Estimation using Deep Learning over Gandhinagar Urban Area</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jai Singla, Peal Jotania, Keivalya Pandya</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 10:25:46</h6>
<p class='card-text'>Population estimation is crucial for various applications, from resource
allocation to urban planning. Traditional methods such as surveys and censuses
are expensive, time-consuming and also heavily dependent on human resources,
requiring significant manpower for data collection and processing. In this
study a deep learning solution is proposed to estimate population using high
resolution (0.3 m) satellite imagery, Digital Elevation Models (DEM) of 0.5m
resolution and vector boundaries. Proposed method combines Convolution Neural
Network (CNN) architecture for classification task to classify buildings as
residential and non-residential and Artificial Neural Network (ANN)
architecture to estimate the population. Approx. 48k building footprints over
Gandhinagar urban area are utilized containing both residential and
non-residential, with residential categories further used for building-level
population estimation. Experimental results on a large-scale dataset
demonstrate the effectiveness of our model, achieving an impressive overall
F1-score of 0.9936. The proposed system employs advanced geospatial analysis
with high spatial resolution to estimate Gandhinagar population at 278,954. By
integrating real-time data updates, standardized metrics, and infrastructure
planning capabilities, this automated approach addresses critical limitations
of conventional census-based methodologies. The framework provides
municipalities with a scalable and replicable tool for optimized resource
management in rapidly urbanizing cities, showcasing the efficiency of AI-driven
geospatial analytics in enhancing data-driven urban governance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12916v1' target='_blank'>Quasi-static shape control of soft, morphing structures</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Eszter Fehér, András Árpád Sipos, Péter Várkonyi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 10:09:20</h6>
<p class='card-text'>Inspired by biological systems, we introduce a general framework for
quasi-static shape control of human-scale structures under slowly varying
external actions or requirements. In this setting, shape control aims to
traverse the stable sub-manifolds of the equilibrium set to meet some
predefined requirements or optimization criteria. As finite deformations are
allowed, the equilibrium set may have a non-trivial topology. This paper
explores the implications of large shape changes and high compliance, such as
the emergence of unstable equilibria and equilibrium sets with non-trivial
topology. We identify various adaptivity scenarios, ranging from inverse
kinematics to optimization and path planning problems, and discuss the role of
time-dependent loads and requirements. The applicability of the proposed
concepts is demonstrated through the example of a curved Kirchhoff rod that is
susceptible to snap-through behavior.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12912v1' target='_blank'>Spotting the Unfriendly Robot - Towards better Metrics for Interactions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Raphael Wenzel, Malte Probst</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 10:05:52</h6>
<p class='card-text'>Establishing standardized metrics for Social Robot Navigation (SRN)
algorithms for assessing the quality and social compliance of robot behavior
around humans is essential for SRN research. Currently, commonly used
evaluation metrics lack the ability to quantify how cooperative an agent
behaves in interaction with humans. Concretely, in a simple frontal approach
scenario, no metric specifically captures if both agents cooperate or if one
agent stays on collision course and the other agent is forced to evade. To
address this limitation, we propose two new metrics, a conflict intensity
metric and the responsibility metric. Together, these metrics are capable of
evaluating the quality of human-robot interactions by showing how much a given
algorithm has contributed to reducing a conflict and which agent actually took
responsibility of the resolution. This work aims to contribute to the
development of a comprehensive and standardized evaluation methodology for SRN,
ultimately enhancing the safety, efficiency, and social acceptance of robots in
human-centric environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12910v1' target='_blank'>Investigating the capability of the Cherenkov Telescope Array
  Observatory to detect gamma-ray emission from simulated stationary neutrino
  sources identified by KM3NeT</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gloria Maria Cicciari, Manuela Mallamaci, Giovanni Marsella, Alberto Rosales de León, Olga Sergijenko, Giovanna Ferrara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 10:04:48</h6>
<p class='card-text'>The simultaneous observation of gamma rays and neutrinos from the same
astrophysical source offers a unique opportunity to probe particle acceleration
and interaction mechanisms in ultra-high-energy environments. The Cherenkov
Telescope Array Observatory (CTAO) is a next-generation ground-based gamma-ray
facility, sensitive to energies from 20~GeV to 300~TeV. In this work, we
present for the first time a performance study of CTAO based on joint
simulations of steady-state sources emitting both neutrinos and gamma rays,
under the assumption that neutrino events are detected by the KM3NeT telescope
in the Northern Hemisphere. To identify potentially observable sources, we
apply a neutrino-based selection filter according to KM3NeT's discovery
potential. We then simulate gamma-ray detectability with CTAO, taking into
account visibility, sensitivity, and extragalactic background light absorption.
The analysis is specifically focused on exploring the detectability of sources
at low neutrino luminosities, limited to values below
$10^{52}\,\mathrm{erg\,yr^{-1}}$, in order to assess the performance of CTAO
and KM3NeT in identifying faint extragalactic emitters. Particular attention is
given to the strategic role of KM3NeT's geographic location, which provides
access to Southern-sky sources, and to the impact of the planned CTA+ upgrade,
which will enhance CTAO-South with Large-Sized Telescopes (LSTs). Our results
highlight the importance of coordinated multi-messenger strategies between
KM3NeT and CTAO to maximize the discovery potential of astrophysical neutrino
sources.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12813v1' target='_blank'>Bridging Perception and Planning: Towards End-to-End Planning for Signal
  Temporal Logic Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bowen Ye, Junyue Huang, Yang Liu, Xiaozhen Qiao, Xiang Yin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 08:31:22</h6>
<p class='card-text'>We investigate the task and motion planning problem for Signal Temporal Logic
(STL) specifications in robotics. Existing STL methods rely on pre-defined maps
or mobility representations, which are ineffective in unstructured real-world
environments. We propose the \emph{Structured-MoE STL Planner}
(\textbf{S-MSP}), a differentiable framework that maps synchronized multi-view
camera observations and an STL specification directly to a feasible trajectory.
S-MSP integrates STL constraints within a unified pipeline, trained with a
composite loss that combines trajectory reconstruction and STL robustness. A
\emph{structure-aware} Mixture-of-Experts (MoE) model enables horizon-aware
specialization by projecting sub-tasks into temporally anchored embeddings. We
evaluate S-MSP using a high-fidelity simulation of factory-logistics scenarios
with temporally constrained tasks. Experiments show that S-MSP outperforms
single-expert baselines in STL satisfaction and trajectory feasibility. A
rule-based \emph{safety filter} at inference improves physical executability
without compromising logical correctness, showcasing the practicality of the
approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12759v1' target='_blank'>A-TDOM: Active TDOM via On-the-Fly 3DGS</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiwei Xu, Xiang Wang, Yifei Yu, Wentian Gan, Luca Morelli, Giulio Perda, Xiongwu Xiao, Zongqian Zhan, Xin Wang, Fabio Remondino</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 07:18:59</h6>
<p class='card-text'>True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product in
various fields such as urban management, city planning, land surveying, etc.
However, traditional TDOM generation methods generally rely on a complex
offline photogrammetric pipeline, resulting in delays that hinder real-time
applications. Moreover, the quality of TDOM may degrade due to various
challenges, such as inaccurate camera poses or Digital Surface Model (DSM) and
scene occlusions. To address these challenges, this work introduces A-TDOM, a
near real-time TDOM generation method based on On-the-Fly 3DGS optimization. As
each image is acquired, its pose and sparse point cloud are computed via
On-the-Fly SfM. Then new Gaussians are integrated and optimized into previously
unseen or coarsely reconstructed regions. By integrating with orthogonal
splatting, A-TDOM can render just after each update of a new 3DGS field.
Initial experiments on multiple benchmarks show that the proposed A-TDOM is
capable of actively rendering TDOM in near real-time, with 3DGS optimization
for each new image in seconds while maintaining acceptable rendering quality
and TDOM geometric accuracy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12718v1' target='_blank'>EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pukun Zhao, Longxiang Wang, Miaowei Wang, Chen Chen, Fanqing Zhou, Haojian Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 06:21:38</h6>
<p class='card-text'>Most existing spatial reasoning benchmarks focus on static or globally
observable environments, failing to capture the challenges of long-horizon
reasoning and memory utilization under partial observability and dynamic
changes. We introduce two dynamic spatial benchmarks, locally observable maze
navigation and match-2 elimination that systematically evaluate models'
abilities in spatial understanding and adaptive planning when local perception,
environment feedback, and global objectives are tightly coupled. Each action
triggers structural changes in the environment, requiring continuous update of
cognition and strategy. We further propose a subjective experience-based memory
mechanism for cross-task experience transfer and validation. Experiments show
that our benchmarks reveal key limitations of mainstream models in dynamic
spatial reasoning and long-term memory, providing a comprehensive platform for
future methodological advances. Our code and data are available at
https://anonymous.4open.science/r/EvoEmpirBench-143C/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12644v1' target='_blank'>AI-Driven Adaptive Air Transit Network with Modular Aerial Pods</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amir Shafiee, Alireza Yazdiani, Hanieh Rastegar, Rui Li, Rayan Karim, Aolei Cao, Ziyang Li, Xieqing Yu, Charlle Sy, Zhaoyao Bao, Xi Cheng, H. Oliver Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 04:00:58</h6>
<p class='card-text'>This paper presents an adaptive air transit network leveraging modular aerial
pods and artificial intelligence (AI) to address urban mobility challenges.
Passenger demand, forecasted from AI models, serves as input parameters for a
Mixed-Integer Nonlinear Programming (MINLP) optimization model that dynamically
adjusts pod dispatch schedules and train lengths in response to demand
variations. The results reveal a complex interplay of factors, including demand
levels, headway bounds, train configurations, and fleet sizes, which
collectively influence network performance and service quality. The proposed
system demonstrates the importance of dynamic adjustments, where modularity
mitigates capacity bottlenecks and improves operational efficiency.
Additionally, the framework enhances energy efficiency and optimizes resource
utilization through flexible and adaptive scheduling. This framework provides a
foundation for a responsive and sustainable urban air mobility solution,
supporting the shift from static planning to agile, data-driven operations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12626v1' target='_blank'>DoubleAgents: Exploring Mechanisms of Building Trust with Proactive AI</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tao Long, Xuanming Zhang, Sitong Wang, Zhou Yu, Lydia B Chilton</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 03:43:13</h6>
<p class='card-text'>Agentic workflows promise efficiency, but adoption hinges on whether people
actually trust systems that act on their behalf. We present DoubleAgents, an
agentic planning tool that embeds transparency and control through user
intervention, value-reflecting policies, rich state visualizations, and
uncertainty flagging for human coordination tasks. A built-in respondent
simulation generates realistic scenarios, allowing users to rehearse, refine
policies, and calibrate their reliance before live use. We evaluate
DoubleAgents in a two-day lab study (n=10), two deployments (n=2), and a
technical evaluation. Results show that participants initially hesitated to
delegate but grew more reliant as they experienced transparency, control, and
adaptive learning during simulated cases. Deployment results demonstrate
DoubleAgents' real-world relevance and usefulness, showing that the effort
required scaled appropriately with task complexity and contextual data. We
contribute trust-by-design patterns and mechanisms for proactive AI --
consistency, controllability, and explainability -- along with simulation as a
safe path to build and calibrate trust over time.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12516v1' target='_blank'>Zero to Autonomy in Real-Time: Online Adaptation of Dynamics in
  Unstructured Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:William Ward, Sarah Etter, Jesse Quattrociocchi, Christian Ellis, Adam J. Thorpe, Ufuk Topcu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 23:39:55</h6>
<p class='card-text'>Autonomous robots must go from zero prior knowledge to safe control within
seconds to operate in unstructured environments. Abrupt terrain changes, such
as a sudden transition to ice, create dynamics shifts that can destabilize
planners unless the model adapts in real-time. We present a method for online
adaptation that combines function encoders with recursive least squares,
treating the function encoder coefficients as latent states updated from
streaming odometry. This yields constant-time coefficient estimation without
gradient-based inner-loop updates, enabling adaptation from only a few seconds
of data. We evaluate our approach on a Van der Pol system to highlight
algorithmic behavior, in a Unity simulator for high-fidelity off-road
navigation, and on a Clearpath Jackal robot, including on a challenging terrain
at a local ice rink. Across these settings, our method improves model accuracy
and downstream planning, reducing collisions compared to static and
meta-learning baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12499v1' target='_blank'>Digital Twin-Assisted Resilient Planning for mmWave IAB Networks via
  Graph Attention Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jie Zhang, Mostafa Rahmani Ghourtani, Swarna Bindu Chetty, Paul Daniel Mitchell, Hamed Ahmadi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 22:53:29</h6>
<p class='card-text'>Digital Twin (DT) technology enables real-time monitoring and optimization of
complex network infrastructures by creating accurate virtual replicas of
physical systems. In millimeter-wave (mmWave) 5G/6G networks, the deployment of
Integrated Access and Backhaul (IAB) nodes faces highly dynamic urban
environments, necessitating intelligent DT-enabled optimization frameworks.
Traditional IAB deployment optimization approaches struggle with the
combinatorial complexity of jointly optimizing coverage, connectivity, and
resilience, often leading to suboptimal solutions that are vulnerable to
network disruptions. With this consideration, we propose a novel Graph
Attention Network v2 (GATv2)-based reinforcement learning approach for
resilient IAB deployment in urban mmWave networks. Specifically, we formulate
the deployment problem as a Markov Decision Process (MDP) with explicit
resilience constraints and employ edge-conditioned GATv2 to capture complex
spatial dependencies between heterogeneous node types and dynamic connectivity
patterns. The attention mechanism enables the model to focus on critical
deployment locations to maximize coverage and ensure fault tolerance through
redundant backhaul connections. To address the inherent vulnerability of mmWave
links, we train the GATv2 policy using Proximal Policy Optimization (PPO) with
a carefully designed balance between coverage, cost, and resilience.
Comprehensive simulations across three urban scenarios demonstrate that our
method achieves 98.5-98.7 percent coverage with 14.3-26.7 percent fewer nodes
than baseline approaches, while maintaining 87.1 percent coverage retention
under 30 percent link failures, representing 11.3-15.4 percent improvement in
fault tolerance compared to state-of-the-art methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12441v1' target='_blank'>Automatic Network Planning with Digital Radio Twin</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaomeng Li, Yuru Zhang, Qiang Liu, Mehmet Can Vuran, Nathan Huynh, Li Zhao, Mizan Rahman, Eren Erman Ozguven</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 20:48:50</h6>
<p class='card-text'>Network planning seeks to determine base station parameters that maximize
coverage and capacity in cellular networks. However, achieving optimal planning
remains challenging due to the diversity of deployment scenarios and the
significant simulation-to-reality discrepancy. In this paper, we propose
\emph{AutoPlan}, a new automatic network planning framework by leveraging
digital radio twin (DRT) techniques. We derive the DRT by finetuning the
parameters of building materials to reduce the sim-to-real discrepancy based on
crowdsource real-world user data. Leveraging the DRT, we design a Bayesian
optimization based algorithm to optimize the deployment parameters of base
stations efficiently. Using the field measurement from Husker-Net, we
extensively evaluate \emph{AutoPlan} under various deployment scenarios, in
terms of both coverage and capacity. The evaluation results show that
\emph{AutoPlan} flexibly adapts to different scenarios and achieves performance
comparable to exhaustive search, while requiring less than 2\% of its
computation time.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12420v1' target='_blank'>System Reliability Estimation via Shrinkage</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Beidi Qiang, Edsel Pena</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 20:16:09</h6>
<p class='card-text'>In a coherent reliability system composed of multiple components configured
according to a specific structure function, the distribution of system time to
failure, or system lifetime, is often of primary interest. Accurate estimation
of system reliability is critical in a wide range of engineering and industrial
applications, forming decisions in system design, maintenance planning, and
risk assessment. The system lifetime distribution can be estimated directly
using the observed system failure times. However, when component-level lifetime
data is available, it can yield improved estimates of system reliability. In
this work, we demonstrate that under nonparametric assumptions about the
component time-to-failure distributions, traditional estimators such as the
Product-Limit Estimator (PLE) can be further improved under specific loss
functions. We propose a novel methodology that enhances the nonparametric
system reliability estimation through a shrinkage transformation applied to
component-level estimators. This shrinkage approach leads to improved
efficiency in estimating system reliability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12362v1' target='_blank'>Catastrophic disruption of asteroid 2023 CX1 and implications for
  planetary defense</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Auriane Egal, Denis Vida, François Colas, Brigitte Zanda, Sylvain Bouley, Asma Steinhausser, Pierre Vernazza, Ludovic Ferrière, Jérôme Gattacceca, Mirel Birlan, Jérémie Vaubaillon, Karl Antier, Simon Anghel, Josselin Desmars, Kévin Baillié, Lucie Maquet, Sébastien Bouquillon, Adrien Malgoyre, Simon Jeanne, Jiři Borovička, Pavel Spurný, Hadrien A. R. Devillepoix, Marco Micheli, Davide Farnocchia, Shantanu Naidu, Peter Brown, Paul Wiegert, Krisztián Sárneczky, András Pál, Nick Moskovitz, Theodore Kareta, Toni Santana-Ros, Alexis Le Pichon, Gilles Mazet-Roux, Julien Vergoz, Luke McFadden, Jelle Assink, Läslo Evers, Daniela Krietsch, Henner Busemann, Colin Maden, Lisa Maria Eckart, Jean-Alix Barrat, Pavel Povinec, Ivan Sykora, Ivan Kontul', Oscar Marchhart, Martin Martschini, Silke Merchel, Alexander Wieser, Matthieu Gounelle, Sylvain Pont, Pierre Sans-Jofre, Sebastiaan de Vet, Ioannis Baziotis, Miroslav Brož, Michaël Marsset, Jérôme Vergne, Josef Hanuš, Maxime Devogèle, Luca Conversi, Francisco Ocaña, Luca Buzzi, Dan Alin Nedelcu, Adrian Sonka, Florent Losse, Philippe Dupouy, Korado Korlević, Dieter Husar, Jost Jahn, Damir Šegon, Mark McIntyre, Ralf Neubert, Pierre Beck, Patrick Shober, Anthony Lagain, Josep Maria Trigo-Rodriguez, Enrique Herrero, Jim Rowe, Andrew R. D. Smedley, Ashley King, Salma Sylla, Daniele Gardiol, Dario Barghini, Hervé Lamy, Emmanuel Jehin, Detlef Koschny, Bjorn Poppe, Andrés Jordán, Rene A. Mendez, Katherine Vieira, Hebe Cremades, Hasnaa Chennaoui Aoudjehane, Zouhair Benkhaldoun, Olivier Hernandez, Darrel Robertson, Peter Jenniskens</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 18:56:54</h6>
<p class='card-text'>Mitigation of the threat from airbursting asteroids requires an understanding
of the potential risk they pose for the ground. How asteroids release their
kinetic energy in the atmosphere is not well understood due to the rarity of
significant impacts. Ordinary chondrites, in particular L chondrites, represent
a frequent type of Earth-impacting asteroids. Here, we present the first
comprehensive, space-to-lab characterization of an L chondrite impact. Small
asteroid 2023 CX1 was detected in space and predicted to impact over Normandy,
France, on 13 February 2023. Observations from multiple independent sensors and
reduction techniques revealed an unusual but potentially high-risk
fragmentation behavior. The nearly spherical 650 $\pm$ 160 kg (72 $\pm$ 6 cm
diameter) asteroid catastrophically fragmented around 28 km altitude, releasing
98% of its total energy in a concentrated region of the atmosphere. The
resulting shockwave was spherical, not cylindrical, and released more energy
closer to the ground. This type of fragmentation increases the risk of
significant damage at ground level. These results warrant consideration for a
planetary defense strategy for cases where a >3-4 MPa dynamic pressure is
expected, including planning for evacuation of areas beneath anticipated
disruption locations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12183v1' target='_blank'>JD.com Improves Fulfillment Efficiency with Data-driven Integrated
  Assortment Planning and Inventory Allocation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zuo-Jun Max Shen, Shuo Sun, Yongzhi Qi, Hao Hu, Ningxuan Kang, Jianshen Zhang, Xin Wang, Xiaoming Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 17:45:31</h6>
<p class='card-text'>This paper presents data-driven approaches for integrated assortment planning
and inventory allocation that significantly improve fulfillment efficiency at
JD.com, a leading E-commerce company. JD.com uses a two-level distribution
network that includes regional distribution centers (RDCs) and front
distribution centers (FDCs). Selecting products to stock at FDCs and then
optimizing daily inventory allocation from RDCs to FDCs is critical to
improving fulfillment efficiency, which is crucial for enhancing customer
experiences. For assortment planning, we propose efficient algorithms to
maximize the number of orders that can be fulfilled by FDCs (local
fulfillment). For inventory allocation, we develop a novel end-to-end algorithm
that integrates forecasting, optimization, and simulation to minimize lost
sales and inventory transfer costs. Numerical experiments demonstrate that our
methods outperform existing approaches, increasing local order fulfillment
rates by 0.54% and our inventory allocation algorithm increases FDC demand
satisfaction rates by 1.05%. Considering the high-volume operations of JD.com,
with millions of weekly orders per region, these improvements yield substantial
benefits beyond the company's established supply chain system. Implementation
across JD.com's network has reduced costs, improved stock availability, and
increased local order fulfillment rates for millions of orders annually.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12091v1' target='_blank'>Bridging Engineering and AI Planning through Model-Based Knowledge
  Transformation for the Validation of Automated Production System Variants</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hamied Nabizada, Lasse Beers, Alain Chahine, Felix Gehlhoff, Oliver Niggemann, Alexander Fay</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 16:18:08</h6>
<p class='card-text'>Engineering models created in Model-Based Systems Engineering (MBSE)
environments contain detailed information about system structure and behavior.
However, they typically lack symbolic planning semantics such as preconditions,
effects, and constraints related to resource availability and timing. This
limits their ability to evaluate whether a given system variant can fulfill
specific tasks and how efficiently it performs compared to alternatives.
  To address this gap, this paper presents a model-driven method that enables
the specification and automated generation of symbolic planning artifacts
within SysML-based engineering models. A dedicated SysML profile introduces
reusable stereotypes for core planning constructs. These are integrated into
existing model structures and processed by an algorithm that generates a valid
domain file and a corresponding problem file in Planning Domain Definition
Language (PDDL). In contrast to previous approaches that rely on manual
transformations or external capability models, the method supports native
integration and maintains consistency between engineering and planning
artifacts.
  The applicability of the method is demonstrated through a case study from
aircraft assembly. The example illustrates how existing engineering models are
enriched with planning semantics and how the proposed workflow is applied to
generate consistent planning artifacts from these models. The generated
planning artifacts enable the validation of system variants through AI
planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12069v1' target='_blank'>U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in
  CBCT</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhi Qin Tan, Xiatian Zhu, Owen Addison, Yunpeng Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 15:52:43</h6>
<p class='card-text'>Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in
dentistry, providing volumetric information about the anatomical structures of
jaws and teeth. Accurate segmentation of these anatomies is critical for
clinical applications such as diagnosis and surgical planning, but remains
time-consuming and challenging. In this paper, we present U-Mamba2, a new
neural network architecture designed for multi-anatomy CBCT segmentation in the
context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state
space models into the U-Net architecture, enforcing stronger structural
constraints for higher efficiency without compromising performance. In
addition, we integrate interactive click prompts with cross-attention blocks,
pre-train U-Mamba2 using self-supervised learning, and incorporate dental
domain knowledge into the model design to address key challenges of dental
anatomy segmentation in CBCT. Extensive experiments, including independent
tests, demonstrate that U-Mamba2 is both effective and efficient, securing top
3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2
achieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with
an average inference time of XX (TBC during the ODIN workshop). In Task 2,
U-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out
test data. The code is publicly available at
https://github.com/zhiqin1998/UMamba2.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12068v1' target='_blank'>End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical
  Imaging Data</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Farahdiba Zarin, Nicolas Padoy, Jérémy Dana, Vinkle Srivastav</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 15:52:20</h6>
<p class='card-text'>The fine-grained surface reconstruction of different organs from 3D medical
imaging can provide advanced diagnostic support and improved surgical planning.
However, the representation of the organs is often limited by the resolution,
with a detailed higher resolution requiring more memory and computing
footprint. Implicit representations of objects have been proposed to alleviate
this problem in general computer vision by providing compact and differentiable
functions to represent the 3D object shapes. However, architectural and
data-related differences prevent the direct application of these methods to
medical images. This work introduces ImplMORe, an end-to-end deep learning
method using implicit surface representations for multi-organ reconstruction
from 3D medical images. ImplMORe incorporates local features using a 3D CNN
encoder and performs multi-scale interpolation to learn the features in the
continuous domain using occupancy functions. We apply our method for single and
multiple organ reconstructions using the totalsegmentator dataset. By
leveraging the continuous nature of occupancy functions, our approach
outperforms the discrete explicit representation based surface reconstruction
approaches, providing fine-grained surface details of the organ at a resolution
higher than the given input image. The source code will be made publicly
available at: https://github.com/CAMMA-public/ImplMORe</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.12010v1' target='_blank'>Generalizing Behavior via Inverse Reinforcement Learning with
  Closed-Form Reward Centroids</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Filippo Lazzati, Alberto Maria Metelli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 14:53:54</h6>
<p class='card-text'>We study the problem of generalizing an expert agent's behavior, provided
through demonstrations, to new environments and/or additional constraints.
Inverse Reinforcement Learning (IRL) offers a promising solution by seeking to
recover the expert's underlying reward function, which, if used for planning in
the new settings, would reproduce the desired behavior. However, IRL is
inherently ill-posed: multiple reward functions, forming the so-called feasible
set, can explain the same observed behavior. Since these rewards may induce
different policies in the new setting, in the absence of additional
information, a decision criterion is needed to select which policy to deploy.
In this paper, we propose a novel, principled criterion that selects the
"average" policy among those induced by the rewards in a certain bounded subset
of the feasible set. Remarkably, we show that this policy can be obtained by
planning with the reward centroid of that subset, for which we derive a
closed-form expression. We then present a provably efficient algorithm for
estimating this centroid using an offline dataset of expert demonstrations
only. Finally, we conduct numerical simulations that illustrate the
relationship between the expert's behavior and the behavior produced by our
method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.11930v1' target='_blank'>VH-Diffuser: Variable Horizon Diffusion Planner for Time-Aware
  Goal-Conditioned Trajectory Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruijia Liu, Ancheng Hou, Shaoyuan Li, Xiang Yin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 13:46:27</h6>
<p class='card-text'>Diffusion-based planners have gained significant recent attention for their
robustness and performance in long-horizon tasks. However, most existing
planners rely on a fixed, pre-specified horizon during both training and
inference. This rigidity often produces length-mismatch (trajectories that are
too short or too long) and brittle performance across instances with varying
geometric or dynamical difficulty. In this paper, we introduce the Variable
Horizon Diffuser (VHD) framework, which treats the horizon as a learned
variable rather than a fixed hyperparameter. Given a start-goal pair, we first
predict an instance-specific horizon using a learned Length Predictor model,
which guides a Diffusion Planner to generate a trajectory of the desired
length. Our design maintains compatibility with existing diffusion planners by
controlling trajectory length through initial noise shaping and training on
randomly cropped sub-trajectories, without requiring architectural changes.
Empirically, VHD improves success rates and path efficiency in maze-navigation
and robot-arm control benchmarks, showing greater robustness to horizon
mismatch and unseen lengths, while keeping training simple and offline-only.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.11903v1' target='_blank'>Wavelet-SARIMA-Transformer: A Hybrid Model for Rainfall Forecasting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junmoni Saikia, Kuldeep Goswami, Sarat C. Kakaty</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 13:27:19</h6>
<p class='card-text'>This study develops and evaluates a novel hybridWavelet SARIMA Transformer,
WST framework to forecast using monthly rainfall across five meteorological
subdivisions of Northeast India over the 1971 to 2023 period. The approach
employs the Maximal Overlap Discrete Wavelet Transform, MODWT with four wavelet
families such as, Haar, Daubechies, Symlet, Coiflet etc. to achieve shift
invariant, multiresolution decomposition of the rainfall series. Linear and
seasonal components are modeled using Seasonal ARIMA, SARIMA, while nonlinear
components are modeled by a Transformer network, and forecasts are
reconstructed via inverse MODWT. Comprehensive validation using an 80 is to 20
train test split and multiple performance indices such as, RMSE, MAE, SMAPE,
Willmotts d, Skill Score, Percent Bias, Explained Variance, and Legates McCabes
E1 demonstrates the superiority of the Haar-based hybrid model, WHST. Across
all subdivisions, WHST consistently achieved lower forecast errors, stronger
agreement with observed rainfall, and unbiased predictions compared with stand
alone SARIMA, stand-alone Transformer, and two-stage wavelet hybrids. Residual
adequacy was confirmed through the Ljung Box test, while Taylor diagrams
provided an integrated assessment of correlation, variance fidelity, and RMSE,
further reinforcing the robustness of the proposed approach. The results
highlight the effectiveness of integrating multiresolution signal decomposition
with complementary linear and deep learning models for hydroclimatic
forecasting. Beyond rainfall, the proposed WST framework offers a scalable
methodology for forecasting complex environmental time series, with direct
implications for flood risk management, water resources planning, and climate
adaptation strategies in data-sparse and climate-sensitive regions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.11867v2' target='_blank'>Letter of Intent: AICE - 100m Atom Interferometer Experiment at CERN</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Charles Baynham, Andrea Bertoldi, Diego Blas, Oliver Buchmueller, Sergio Calatroni, Vassilis Charmandaris, Maria Luisa Chiofalo, Pierre Cladé, Jonathon Coleman, Fabio Di Pumpo, John Ellis, Naceur Gaaloul, Saïda Guellati-Khelifa, Tiffany Harte, Richard Hobson, Michael Holynski, Samuel Lellouch, Lucas Lombriser, Elias Lopez Asamar, Michele Maggiore, Christopher McCabe, Jeremiah Mitchell, Ernst M. Rasel, Federico Sanchez Nieto, Wolfgang Schleich, Dennis Schlippert, Ulrich Schneider, Steven Schramm, Marcelle Soares-Santos, Guglielmo M. Tino, Jonathan N. Tinsley, Tristan Valenzuela, Maurits van der Grinten, Wolf von Klitzing</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 12:39:51</h6>
<p class='card-text'>We propose an O(100)m Atom Interferometer (AI) experiment - AICE - to be
installed against a wall of the PX46 access shaft to the LHC. This experiment
would probe unexplored ranges of the possible couplings of bosonic ultralight
dark matter (ULDM) to atomic constituents and undertake a pioneering search for
gravitational waves (GWs) at frequencies intermediate between those to which
existing and planned experiments are sensitive, among other fundamental physics
studies. A conceptual feasibility study showed that this AI experiment could be
isolated from the LHC by installing a shielding wall in the TX46 gallery, and
surveyed issues related to the proximity of the LHC machine, finding no
technical obstacles. A detailed technical implementation study has shown that
the preparatory civil-engineering work, installation of bespoke radiation
shielding, deployment of access-control systems and safety alarms, and
installation of an elevator platform could be carried out during LS3, allowing
installation and operation of the AICE detector to proceed during Run 4 without
impacting HL-LHC operation. These studies have established that PX46 is a
uniquely promising location for an AI experiment. We foresee that, if the CERN
management encourages this Letter of Intent, a significant fraction of the
Terrestrial Very Long Baseline Atom Interferometer (TVLBAI) Proto-Collaboration
may wish to contribute to AICE.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.11793v1' target='_blank'>UniPilot: Enabling GPS-Denied Autonomy Across Embodiments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mihir Kulkarni, Mihir Dharmadhikari, Nikhil Khedekar, Morten Nissov, Mohit Singh, Philipp Weiss, Kostas Alexis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 11:23:58</h6>
<p class='card-text'>This paper presents UniPilot, a compact hardware-software autonomy payload
that can be integrated across diverse robot embodiments to enable autonomous
operation in GPS-denied environments. The system integrates a multi-modal
sensing suite including LiDAR, radar, vision, and inertial sensing for robust
operation in conditions where uni-modal approaches may fail. UniPilot runs a
complete autonomy software comprising multi-modal perception, exploration and
inspection path planning, and learning-based navigation policies. The payload
provides robust localization, mapping, planning, and safety and control
capabilities in a single unit that can be deployed across a wide range of
platforms. A large number of experiments are conducted across diverse
environments and on a variety of robot platforms to validate the mapping,
planning, and safe navigation capabilities enabled by the payload.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.11740v1' target='_blank'>From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile
  Manipulator for Supermarket Stocking and Fronting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Davide Peron, Victor Nan Fernandez-Ayala, Lukas Segelmark</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-15 09:42:13</h6>
<p class='card-text'>Autonomous stocking in retail environments, particularly supermarkets,
presents challenges due to dynamic human interactions, constrained spaces, and
diverse product geometries. This paper introduces an efficient end-to-end
robotic system for autonomous shelf stocking and fronting, integrating
commercially available hardware with a scalable algorithmic architecture. A
major contribution of this work is the system integration of off-the-shelf
hardware and ROS2-based perception, planning, and control into a single
deployable platform for retail environments. Our solution leverages Behavior
Trees (BTs) for task planning, fine-tuned vision models for object detection,
and a two-step Model Predictive Control (MPC) framework for precise shelf
navigation using ArUco markers. Laboratory experiments replicating realistic
supermarket conditions demonstrate reliable performance, achieving over 98%
success in pick-and-place operations across a total of more than 700 stocking
events. However, our comparative benchmarks indicate that the performance and
cost-effectiveness of current autonomous systems remain inferior to that of
human workers, which we use to highlight key improvement areas and quantify the
progress still required before widespread commercial deployment can
realistically be achieved.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>