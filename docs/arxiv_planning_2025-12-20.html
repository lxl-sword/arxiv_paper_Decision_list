<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-12-20</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-12-20</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16909v1' target='_blank'>MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 18:59:03</h6>
<p class='card-text'>Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16887v1' target='_blank'>An evacuation simulator for pedestrian dynamics based on the Social Force Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Julián López, Virginia Mazzone, M. Leticia Rubio Puzzo, Juan Cruz Moreno</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 18:51:08</h6>
<p class='card-text'>The evacuation of pedestrians from enclosed spaces represents a key problem in safety engineering and infrastructure design. Analyzing the collective dynamics that emerge during evacuation processes requires simulation tools capable of capturing individual interactions and spatial constraints realistically.
  In this work, we present \textit{SiCoBioNa}, an open-source evacuation simulator based on the Social Force Model (SFM). The software provides an intuitive graphical interface that allows users to configure pedestrian properties, spatial geometries, and initial conditions without requiring prior expertise in numerical modeling techniques. The SFM framework enables the representation of goal-oriented motion, interpersonal interactions, and interactions with fixed obstacles.
  The simulator generates both quantitative data and visual outputs, facilitating the analysis of evacuation dynamics and the evaluation of different spatial configurations. Due to its modular and extensible design, \textit{SiCoBioNa} serves as a reproducible research tool for studies on pedestrian dynamics providing practical support for evacuation planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16864v1' target='_blank'>RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianyuan Qu, Lei Ke, Xiaohang Zhan, Longxiang Tang, Yuqi Liu, Bohao Peng, Bei Yu, Dong Yu, Jiaya Jia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 18:34:23</h6>
<p class='card-text'>Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16861v1' target='_blank'>ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zihan Zhou, Animesh Garg, Ajay Mandlekar, Caelan Garrett</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 18:32:39</h6>
<p class='card-text'>Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16798v1' target='_blank'>Clinical beam test of inter- and intra-fraction relative range monitoring in carbon ion radiotherapy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Devin Hymers, Sebastian Schroeder, Olga Bertini, Stephan Brons, Johann Heuser, Joerg Lehnert, Christian Joachim Schmidt, Dennis Mücher</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 17:29:28</h6>
<p class='card-text'>Interaction Vertex Imaging (IVI) is used for range monitoring (RM) in carbon ion radiotherapy. The purpose of RM is to measure the Bragg peak (BP) position for each contributing beam, and detect any changes. Currently, there is no consensus on a clinical RM method, the use of which would improve the safety and consistency of treatment. The prototype filtered IVI (fIVI) Range Monitoring System is the first system to apply large-area and high-rate-capable silicon detectors to IVI. Two layers of these detectors track prompt secondary fragments for use in RM. This device monitored 16 cm and 32 cm diameter cylindrical plastic phantoms irradiated by clinical carbon ion beams at the Heidelberg Ion Beam Therapy Center. Approximately 20 different BP depths were delivered to each phantom, with a minimum depth difference of 0.8 mm and a maximum depth difference of 51.9 mm and 82.5 mm respectively. For large BP range differences, the relationship between the true depth difference and that measured by fIVI is quadratic, although for small differences, the deviation from a linear relationship with a slope of 1 is negligible. RM performance is strongly dependent on the number of tracked particles, particularly in the clinically-relevant regime. Significant performance differences exist between the two phantoms, with millimetric precision at clinical doses being achieved only for the 16 cm phantom. The performance achieved by the prototype fIVI Range Monitoring System is consistent with previous investigations of IVI, despite measuring at more challenging shallow BP positions. Further significant improvements are possible through increasing the sensitive area of the tracking system beyond the prototype, which will both allow an improvement in precision for the most intense points of a scanned treatment plan and expand the number of points for which millimetric precision may be achieved.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16793v1' target='_blank'>PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaopeng Lin, Shijie Lian, Bin Yu, Ruoqi Yang, Changti Wu, Yuzhuo Miao, Yurun Jin, Yukun Shi, Cong Huang, Bojun Cheng, Kai Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 17:27:03</h6>
<p class='card-text'>Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16792v1' target='_blank'>Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Endar Suprih Wihidayat, Sieteng Soh, Kwan-Wu Chin, Duc-son Pham</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 17:25:55</h6>
<p class='card-text'>In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16733v1' target='_blank'>Discovering and Learning Probabilistic Models of Black-Box AI Capabilities</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel Bramblett, Rushang Karia, Adrian Ciotinga, Ruthvick Suresh, Pulkit Verma, YooJung Choi, Siddharth Srivastava</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 16:32:06</h6>
<p class='card-text'>Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16724v1' target='_blank'>VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yixiang Chen, Yan Huang, Keji He, Peiyan Li, Liang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 16:26:17</h6>
<p class='card-text'>When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16611v1' target='_blank'>Hypervelocity Impact Debris Cloud Trajectory-Planning based on Additive Manufactured Lattice Structures</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bilin Zheng, Xiao Kang, Xiaoyu Zhang, Hao Zhou, Mengchuan Xu, Chang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 14:52:26</h6>
<p class='card-text'>Space debris and micrometeoroid (MMOD) impacts pose a serious threat to the safe operation of spacecraft. However, traditional protective structures typically suffer from limitations such as excessive thickness and inadequate load-bearing capacity. Guided by the design concepts of debris-cloud deflection and hierarchical energy dissipation, this study proposes a trajectory-planning lattice protective structure. First, the lattice parameters and geometry were designed according to the functional relationship between the incident angle and the transmitted/ricochet trajectory angles. Subsequently, multi-angle hypervelocity impact experiments were carried out to evaluate the proposed lattice protection structure. In combination with post-impact CT three-dimensional reconstruction and smoothed particle hydrodynamics (SPH) numerical simulations, the protective mechanisms of the lattice structure were systematically characterized and clarified. The results demonstrate that, for three oblique incidence conditions, the lattice structure remained intact and significantly deflected the debris-cloud momentum direction while effectively dissipating its kinetic energy. The angled plates with gradient designs enabled continuous changes in the momentum direction and stepwise kinetic energy dissipation through multiple cycles of debrisation, dispersion, and trajectory deflection. This research presents a novel, engineering-ready approach for spacecraft MMOD protection and validates the potential of trajectory-planning lattice structures for hypervelocity impact defense.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16577v1' target='_blank'>CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nico Albert Disch, Saikat Roy, Constantin Ulrich, Yannick Kirchhoff, Maximilian Rokuss, Robin Peretzke, David Zimmerer, Klaus Maier-Hein</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 14:16:46</h6>
<p class='card-text'>Forecasting how 3D medical scans evolve over time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16514v1' target='_blank'>Algorithmic Monetary Policies for Blockchain Participation Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Diodato Ferraioli, Paolo Penna, Manvir Schneider, Carmine Ventre</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 13:28:00</h6>
<p class='card-text'>A central challenge in blockchain tokenomics is aligning short-term performance incentives with long-term decentralization goals. We propose a framework for algorithmic monetary policies that navigates this tradeoff in repeated participation games. Agents, characterized by type (capability) and stake, choose to participate or abstain at each round; the policy (probabilistically) selects high-type agents for task execution (maximizing throughput) while distributing rewards to sustain decentralization. We analyze equilibria under two agent behaviors: myopic (short-term utility maximization) and foresighted (multi-round planning). For myopic agents, performance-centric policies risk centralization, but foresight enables stable decentralization with some volatility to the token value. We further discuss virtual stake--a hybrid of type and stake--as an alternative approach. We show that the initial virtual stake distribution critically impacts long-term outcomes, suggesting that policies must indirectly manage decentralization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16449v1' target='_blank'>Single-View Shape Completion for Robotic Grasping in Clutter</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abhishek Kashyap, Yuxuan Yang, Henrik Andreasson, Todor Stoyanov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 12:11:05</h6>
<p class='card-text'>In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16355v1' target='_blank'>Back with Weight: Revisiting Very Heavy Ions for Precision Radiotherapy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jeannette Jansen, Olga Sokol, Yolanda Prezado, Marco Durante</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 09:48:01</h6>
<p class='card-text'>Accelerated charged particles offer significant physical advantages over X-rays in radiotherapy. In addition to their superior depth-dose distribution, heavy ions provide notable biological benefits compared to protons. Specifically, at therapeutic energies, very heavy ions are expected to exhibit high relative biological effectiveness and a low oxygen enhancement ratio, making them potentially ideal for treating radioresistant tumors. Over fifty years ago, the Lawrence Berkeley Laboratory started a clinical trial to treat cancer patients with particles ranging from helium to argon. However, treatments with ions of atomic number greater than 10 proved highly toxic to normal tissue. Most patients were ultimately treated with helium ions, whose biological effects closely resemble those of protons. In recent years, novel strategies have emerged that can reduce normal tissue toxicity in radiotherapy while preserving tumor control. These advancements open the possibility of revisiting the clinical use of heavier ions. In this paper, we propose that neon ions may serve as an effective modality for treating resistant, hypoxic tumors. Their associated toxicity may be mitigated by employing approaches such as ultra-high dose rate (FLASH) or spatially fractionated radiotherapy. Current plans and prospects for the clinical application of neon ions will be discussed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16301v1' target='_blank'>Adaptation of Agentic AI</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pengcheng Jiang, Jiacheng Lin, Zhiyi Shi, Zifeng Wang, Luxi He, Yichen Wu, Ming Zhong, Peiyang Song, Qizheng Zhang, Heng Wang, Xueqiang Xu, Hanwen Xu, Pengrui Han, Dylan Zhang, Jiashuo Sun, Chaoqi Yang, Kun Qian, Tian Wang, Changran Hu, Manling Li, Quanzheng Li, Hao Peng, Sheng Wang, Jingbo Shang, Chao Zhang, Jiaxuan You, Liyuan Liu, Pan Lu, Yu Zhang, Heng Ji, Yejin Choi, Dawn Song, Jimeng Sun, Jiawei Han</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 08:38:51</h6>
<p class='card-text'>Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16279v1' target='_blank'>QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiliu Yang, Yilei Jiang, Qunzhong Wang, Yingshui Tan, Xiaoyong Zhu, Sherman S. M. Chow, Bo Zheng, Xiangyu Yue</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 07:58:40</h6>
<p class='card-text'>Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16275v1' target='_blank'>GFLAN: Generative Functional Layouts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohamed Abouagour, Eleftherios Garyfallidis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 07:52:47</h6>
<p class='card-text'>Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16235v1' target='_blank'>AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Satya Narayana Panda, Vaishnavi Kukkala, Spandana Iyer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 06:28:51</h6>
<p class='card-text'>Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation?
  We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment.
  In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16228v1' target='_blank'>Quantifying Functional Criticality of Lifelines Through Mobility-Derived Population-Facility Dependence for Human-Centered Resilience</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junwei Ma, Bo Li, Xiangpeng Li, Chenyue Liu, Ali Mostafavi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 06:21:52</h6>
<p class='card-text'>Lifeline infrastructure underpins the continuity of daily life, yet conventional criticality assessments remain largely asset-centric, inferring importance from physical capacity or network topology rather than actual behavioral reliance. This disconnect frequently obscures the true societal cost of disruption, particularly in underserved communities where residents lack service alternatives. This study bridges the gap between engineering risk analysis and human mobility analysis by introducing functional criticality, a human-centered metric that quantifies the behavioral indispensability of specific facilities based on large-scale visitation patterns. Leveraging 1.02 million anonymized mobility records for Harris County, Texas, we operationalize lifeline criticality as a function of visitation intensity, catchment breadth, and origin-specific substitutability. Results reveal that dependence is highly concentrated: a small subset of super-critical facilities (2.8% of grocery stores and 14.8% of hospitals) supports a disproportionate share of routine access. By coupling these behavioral scores with probabilistic flood hazard models for 2020 and 2060, we demonstrate a pronounced risk-multiplier effect. While physical flood depths increase only moderately under future climate scenarios, the population-weighted vulnerability of access networks surges by 67.6%. This sharp divergence establishes that future hazards will disproportionately intersect with the specific nodes communities rely on most. The proposed framework advances resilience assessment by embedding human behavior directly into the definition of infrastructure importance, providing a scalable foundation for equitable, adaptive, and human-centered resilience planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16153v1' target='_blank'>A Square Kilometre Array Pulsar Census</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:E. F. Keane, V. Graber, L. Levin, C. M. Tan, O. A. Johnson, C. Ng, C. Pardo-Araujo, M. Ronchi, D. Vohl, M. Xue, The SKA Pulsar Science Working Group</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 04:16:36</h6>
<p class='card-text'>Most of the pulsar science case with the Square Kilometre Array (SKA) depends on long-term precision pulsar timing of a large number of pulsars, as well as astrometric measurements of these using very long baseline interferometry (VLBI). But before we can time them, or VLBI them, we must first find them. Here, we describe the considerations and strategies one needs to account for when planning an all-sky blind pulsar survey using the SKA. Based on our understanding of the pulsar population, the performance of the now-under-construction SKA elements, and practical constraints such as evading radio frequency interference, we project pulsar survey yields using two complementary methods for a number of illustrative survey designs, combining SKA1-Low and SKA1-Mid Bands 1 and 2 in a variety of ways. A composite survey using both Mid and Low is optimal, with Mid Band 2 focused in the plane. We find that, given its much higher effective area and survey speed, the best strategy is to use SKA1-Low to cover as much sky as possible, ideally also overlapping with the areas covered by Mid. In our most realistic scenario, we find that an all-sky blind survey with Phase 1 of the SKA with the AA* array assembly will detect $\sim10,000$ slow pulsars and $\sim 800$ millisecond pulsars (MSPs) if SKA1-Mid covers the region within $5°$ of the plane, while higher latitudes will be covered with SKA1-Low. The yield with AA4 is $\sim 20\%$ higher. One could increase these numbers by increasing the range covered by SKA1-Mid Bands 1 and 2, at the cost of a considerably longer survey. The pulsar census will enable us to set new constraints on the uncertain physical properties of the entire neutron star population. This will be crucial for addressing major SKA science questions including the dense-matter equation of state, strong-field gravity tests, and gravitational wave astronomy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16069v1' target='_blank'>A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maolin Lei, Edoardo Romiti, Arturo Laurenzi, Rui Dai, Matteo Dalle Vedove, Jiatao Ding, Daniele Fontanelli, Nikos Tsagarakis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 01:27:34</h6>
<p class='card-text'>Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16067v1' target='_blank'>WING: An Adaptive and Gamified Mobile Learning Platform for Neurodivergent Literacy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mirella Emily Bezerra Santana, Cauã Otaviano Jordão, Victor Barbosa dos Santos, Leonardo José Oliveira Ibiapina, Gabriel Moraes da Silva, Marina Robalinho Cavalcanti, Lucas Rodolfo Celestino Farias</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-18 01:21:43</h6>
<p class='card-text'>This paper presents WING, an adaptive and gamified mobile learning platform designed to support literacy development for neurodivergent children. Motivated by the limitations of traditional literacy approaches in addressing diverse cognitive profiles, the platform integrates inclusive Human-Computer Interaction principles, multisensory design, and adaptive learning paths. WING digitally transposes the Alfabetização Adaptada (AFA) method into an interactive mobile environment, combining usability guidelines for neurodivergent users with gamification strategies to enhance engagement and autonomy. The study follows an applied research methodology, encompassing requirements elicitation, inclusive interface design, high-fidelity prototyping, and qualitative and quantitative evaluation planning. Preliminary results include a functional minimum viable product validated through expert feedback and public exhibitions, indicating the feasibility and potential pedagogical impact of the proposed approach. The platform aims to act as a complementary educational tool, promoting accessibility, personalization, and inclusive digital literacy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.16011v1' target='_blank'>dLITE: Differentiable Lighting-Informed Trajectory Evaluation for On-Orbit Inspection</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jack Naylor, Raghav Mishra, Nicholas H. Barbara, Donald G. Dansereau</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-17 22:40:05</h6>
<p class='card-text'>Visual inspection of space-borne assets is of increasing interest to spacecraft operators looking to plan maintenance, characterise damage, and extend the life of high-value satellites in orbit. The environment of Low Earth Orbit (LEO) presents unique challenges when planning inspection operations that maximise visibility, information, and data quality. Specular reflection of sunlight from spacecraft bodies, self-shadowing, and dynamic lighting in LEO significantly impact the quality of data captured throughout an orbit. This is exacerbated by the relative motion between spacecraft, which introduces variable imaging distances and attitudes during inspection. Planning inspection trajectories with the aide of simulation is a common approach. However, the ability to design and optimise an inspection trajectory specifically to improve the resulting image quality in proximity operations remains largely unexplored. In this work, we present $\partial$LITE, an end-to-end differentiable simulation pipeline for on-orbit inspection operations. We leverage state-of-the-art differentiable rendering tools and a custom orbit propagator to enable end-to-end optimisation of orbital parameters based on visual sensor data. $\partial$LITE enables us to automatically design non-obvious trajectories, vastly improving the quality and usefulness of attained data. To our knowledge, our differentiable inspection-planning pipeline is the first of its kind and provides new insights into modern computational approaches to spacecraft mission planning. Project page: https://appearance-aware.github.io/dlite/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.15933v1' target='_blank'>City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dwip Dalal, Utkarsh Mishra, Narendra Ahuja, Nebojsa Jojic</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-17 19:59:31</h6>
<p class='card-text'>Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.15692v1' target='_blank'>mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-17 18:47:31</h6>
<p class='card-text'>Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.15840v1' target='_blank'>Large Video Planner Enables Generalizable Robot Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Boyuan Chen, Tianyuan Zhang, Haoran Geng, Kiwhan Song, Caiyi Zhang, Peihao Li, William T. Freeman, Jitendra Malik, Pieter Abbeel, Russ Tedrake, Vincent Sitzmann, Yilun Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-17 18:35:54</h6>
<p class='card-text'>General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.15681v1' target='_blank'>Radiomics and Clinical Features in Predictive Modelling of Brain Metastases Recurrence</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ines Faria, Matheus Silva, Crystian Saraiva, Jose Soares, Victor Alves</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-17 18:32:56</h6>
<p class='card-text'>Brain metastases affect approximately between 20% and 40% of cancer patients and are commonly treated with radiotherapy or radiosurgery. Early prediction of recurrence following treatment could enable timely clinical intervention and improve patient outcomes. This study proposes an artificial intelligence based approach for predicting brain metastasis recurrence using multimodal imaging and clinical data. A retrospective cohort of 97 patients was collected, including Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) acquired before treatment and at first follow-up, together with relevant clinical variables. Image preprocessing included CT windowing and artifact reduction, MRI enhancement, and multimodal CT MRI registration. After applying inclusion criteria, 53 patients were retained for analysis. Radiomics features were extracted from the imaging data, and delta radiomics was employed to characterize temporal changes between pre-treatment and follow-up scans. Multiple machine learning classifiers were trained and evaluated, including an analysis of discrepancies between treatment planning target volumes and delivered isodose volumes. Despite limitations related to sample size and class imbalance, the results demonstrate the feasibility of radiomics based models, namely ensemble models, for recurrence prediction and suggest a potential association between radiation dose discrepancies and recurrence risk. This work supports further investigation of AI-driven tools to assist clinical decision-making in brain metastasis management.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.15833v1' target='_blank'>The Simons Observatory: forecasted constraints on primordial gravitational waves with the expanded array of Small Aperture Telescopes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:The Simons Observatory Collaboration, I. Abril-Cabezas, S. Adachi, P. Ade, A. E. Adler, P. Agrawal, J. Aguirre, S. Aiola, T. Alford, A. Ali, D. Alonso, M. A. Alvarez, R. An, M. Aravena, K. Arnold, P. Ashton, F. Astori, Z. Atkins, J. Austermann, S. Azzoni, C. Baccigalupi, D. Baker, R. Balafendiev, A. Baleato Lizancos, D. Barron, P. Barry, J. Bartlett, A. Basyrov, N. Battaglia, E. S. Battistelli, R. Battye, A. Bayer, A. Bazarko, J. A. Beall, R. Bean, D. Beck, S. Beckman, J. Begin, A. Beheshti, B. Beringue, T. Bhandarkar, S. Bhimani, F. Bianchini, E. Biermann, M. Billi, S. Biquard, B. Bixler, L. Bizzarri, S. Boada, D. Boettger, B. Bolliet, J. R. Bond, J. Borrill, J. Borrow, C. Braithwaite, T. L. R. Brien, M. L. Brown, S. M. Bruno, S. Bryan, R. Bustos, H. Cai, E. Calabrese, V. Calafut, F. M. Carl, A. Carones, J. Carron, A. Challinor, E. Chamberlain, P. Chanial, N. Chen, K. Cheung, B. Chiang, Y. Chinone, J. Chluba, H. S. Cho, S. K. Choi, M. Chu, J. Clancy, S. E. Clark, P. Clarke, J. Cleary, D. L. Clements, J. Connors, C. Contaldi, G. Coppi, L. Corbett, N. F. Cothard, W. Coulton, D. Crichton, K. D. Crowley, K. T. Crowley, A. Cukierman, J. M. D'Ewart, K. Dachlythra, O. Darwish, R. Datta, S. Day-Weiss, T. de Haan, S. Desai, M. Devlin, L. Di Mascolo, S. Dicker, K. Ding, C. Doux, P. Dow, S. Doyle, C. J. Duell, S. M. Duff, A. J. Duivenvoorden, J. Dunkley, M. Duparc, D. Dutcher, R. Dünner, M. Edenton, H. El Bouhargani, C. Embil Villagra, J. Errard, G. Fabbian, V. Fanfani, F. Farhadi Khouzani, G. S. Farren, J. Fergusson, S. Ferraro, R. Flauger, M. Forconi, A. Foster, K. Freese, J. C. Frisch, A. Frolov, G. Fuller, N. Galitzki, P. A. Gallardo, G. Galloni, J. T. Galvez Ghersi, K. Ganga, X. Garrido, E. Gawiser, M. Gerbino, R. Gerras, S. Giardiello, A. Gill, V. Gilles, U. Giri, E. Gleave, V. Gluscevic, N. Goeckner-Wald, S. Goldstein, J. E. Golec, S. Gordon, M. Gralla, S. Gratton, D. Green, J. C. Groh, C. Groppi, S. Grubb, Y. Guan, N. Gupta, J. E. Guðmundsson, B. Hadzhiyska, S. Hagstotz, P. Hargrave, S. Haridas, K. Harrington, I. Harrison, M. Hasegawa, M. Hasselfield, V. Haynes, M. Hazumi, A. He, E. Healy, S. W. Henderson, B. S. Hensley, E. Hertig, C. Hervías-Caimapo, M. Higuchi, C. A. Hill, J. C. Hill, M. Hilton, A. D. Hincks, G. Hinshaw, R. Hložek, A. Y. Q. Ho, S. Ho, S. P. Ho, T. D. Hoang, J. Hoh, J. Holder, J. Hood, E. Hornecker, A. L. Hornsby, S. C. Hotinli, Z. Huang, Z. B. Huber, J. Hubmayr, K. Huffenberger, A. Hughes, J. P. Hughes, A. Idicherian Lonappan, M. Ikape, K. Inaba, K. Irwin, J. Iuliano, A. H. Jaffe, B. Jain, D. Jain, H. T. Jense, O. Jeong, A. Johnson, B. R. Johnson, M. Johnson, M. E. Jones, N. Joshi, B. Jost, W. Kabalan, V. Kabra, D. Kaneko, J. Kania, E. D. Karpel, Y. Kasai, N. Katayama, B. Keating, B. Keller, R. Keskitalo, A. A. Khatua, J. Kim, T. Kisner, K. Kiuchi, K. Knowles, A. M. Kofman, Y. Koizumi, B. J. Koopman, A. Kosowsky, R. Kou, N. Krachmalnicoff, D. Kramer, A. Krishak, A. Krolewski, A. Kusaka, A. Kusiak, Y. Kvasiuk, P. La Plante, A. La Posta, A. Laguë, A. Lai, J. Lashner, M. Lattanzi, A. Lee, E. Lee, J. Leech, L. Legrand, C. Lessler, J. S. Leung, A. Lewis, Y. Li, Z. Li, M. Limon, L. Lin, E. Linder, M. Link, J. Liu, Y. Liu, J. Lloyd, J. Lonergan, T. Louis, T. Lucas, M. Ludlam, M. Lungu, M. Lyons, N. MacCrann, A. MacInnis, M. Madhavacheril, D. Mak, F. Maldonado, M. Mallaby-Kay, A. Manduca, A. Mangu, H. Mani, A. S. Maniyar, G. A. Marques, P. Masson, J. Mates, J. Mathewson, T. Matsumura, P. Mauskopf, A. May, N. McCallum, H. McCarrick, F. McCarthy, M. McCulloch, J. McMahon, P. D. Meerburg, Y. Mehta, J. Melin, E. Meulbroek, J. Meyers, A. Middleton, Y. Miki, A. Miller, M. Mirmelstein, Y. Mizozoe, B. Mohammadian, G. Montefalcone, K. Moodley, J. Moore, T. Morris, M. Morshed, T. Morton, E. Moser, T. Mroczkowski, M. Murata, J. Myers, M. Münchmeyer, S. Naess, H. Nakata, T. Namikawa, M. Nashimoto, F. Nati, P. Natoli, M. Negrello, S. K. Nerval, L. Newburgh, D. V. Nguyen, A. Nicola, M. D. Niemack, H. Nishino, Y. Nishinomiya, A. Novelli, S. O'Neill, N. Okumoto, A. Orlando, J. Orlowski-Scherer, L. Pagano, L. A. Page, S. Pandey, A. Papageorgiou, I. Paraskevakos, B. Partridge, D. Patel, R. Patki, S. Paulino Korte, M. Peel, K. Perez Sarmiento, F. Perrotta, P. Phakathi, L. Piccirillo, E. Pierpaoli, T. Pinsonneault-Marotte, G. Pisano, J. Pitocco, D. Poletti, C. Popik, B. Prasad, R. Puddu, G. Puglisi, F. J. Qu, F. Rahman, M. J. Randall, C. Ranucci, C. Raum, R. Reeves, C. L. Reichardt, M. Remazeilles, X. Ren, Y. Rephaeli, D. Riechers, B. Reid Guachalla, A. Rizzieri, J. Robe, M. F. Robertson, N. Robertson, K. Rogers, F. Rojas, A. Romero, E. Rosenberg, A. Rotti, S. Rowe, A. Roy, S. Sadeh, N. Sailer, K. Sakaguri, T. Sakuma, Y. Sakurai, M. Salatino, G. H. Sanders, D. Sasaki, M. Sathyanarayana Rao, T. P. Satterthwaite, L. Saunders, L. Scalcinati, E. Schaan, B. Schmitt, M. Schmittfull, N. Sehgal, J. Seibert, Y. Seino, U. Seljak, S. Shaikh, E. Shaw, P. Shellard, B. Sherwin, M. Shimon, J. E. Shroyer, C. Sierra, J. Sievers, C. Sifón, P. Sikhosana, M. Silva-Feaver, S. M. Simon, A. Sinclair, K. Smith, W. Sohn, X. Song, R. F. Sonka, T. Souverin, J. Spisak, S. T. Staggs, G. Stein, J. R. Stevens, R. Stompor, E. Storer, R. Sudiwala, Y. Sueno, J. Sugiyama, P. Suman, K. M. Surrao, S. Sutariya, A. Suzuki, J. Suzuki, O. Tajima, R. Takaku, S. Takakura, A. Takeuchi, I. Tansieri, A. C. Taylor, G. Teply, T. Terasaki, A. Thomas, D. B. Thomas, R. Thornton, P. Timbie, H. Trac, T. Tsan, E. Tsang King Sang, C. Tucker, J. Ullom, L. Vacher, L. Vale, A. van Engelen, J. Van Lanen, J. van Marrewijk, D. D. Van Winkle, C. Vargas, E. M. Vavagiakis, I. Veenendaal, C. Vergès, A. Villarrubia Aguilar, M. Vissers, M. Viña, K. Wagoner, S. Walker, L. Walters, Y. Wang, B. Westbrook, J. Williams, P. Williams, J. Wilson, H. Winch, E. J. Wollack, K. Wolz, J. Wong, Z. Xu, K. Yamada, E. Young, B. Yu, C. Yu, G. Zagatti, M. Zannoni, W. Zhang, K. Zheng, N. Zhu, A. Zonca, I. Zubeldia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-17 18:08:20</h6>
<p class='card-text'>We present updated forecasts for the scientific performance of the degree-scale (0.5 deg FWHM at 93 GHz), deep-field survey to be conducted by the Simons Observatory (SO). By 2027, the SO Small Aperture Telescope (SAT) complement will be doubled from three to six telescopes, including a doubling of the detector count in the 93 GHz and 145 GHz channels to 48,160 detectors. Combined with a planned extension of the survey duration to 2035, this expansion will significantly enhance SO's search for a $B$-mode signal in the polarisation of the cosmic microwave background, a potential signature of gravitational waves produced in the very early Universe. Assuming a $1/f$ noise model with knee multipole $\ell_{\rm knee} = 50$ and a moderately complex model for Galactic foregrounds, we forecast a $1σ$ (or 68% confidence level) constraint on the tensor-to-scalar ratio $r$ of $σ_r = 1.2\times10^{-3}$, assuming no primordial $B$-modes are present. This forecast assumes that 70% of the $B$-mode lensing signal can ultimately be removed using high resolution observations from the SO Large Aperture Telescope (LAT) and overlapping large-scale structure surveys. For more optimistic assumptions regarding foregrounds and noise, and assuming the same level of delensing, this forecast constraint improves to $σ_r = 7\times10^{-4}$. These forecasts represent a major improvement in SO's constraining power, being a factor of around 2.5 times better than what could be achieved with the originally planned campaign, which assumed the existing three SATs would conduct a five-year survey.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.15652v1' target='_blank'>Towards ALMA2040: An update from the European community and invitation to contribute</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Stefano Facchini, Jacqueline Hodge, Jes Jørgensen, Eva Schinnerer, Gie Han Tan, Tom Bakx, Andrey Baryshev, Maite Beltran, Leindert Boogaard, Roberto Decarli, María Díaz Trigo, Jan Forbrich, Peter Huggard, Elizabeth Humphreys, Violette Impellizeri, Karri Koljonen, Kuo Liu, Luca Matrà, Miguel Pereira Santella, Arianna Piccialli, Gergö Popping, Miguel Querejeta, Miriam Rengel, Francesca Rizzo, Lucie Rowland, Hannah Stacey, Wouter Vlemmings, Catherine Walsh, Sven Wedemeyer, Martina Wiedner</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-17 18:03:55</h6>
<p class='card-text'>Over the last 15 years, the Atacama Large Millimeter/submillimeter Array (ALMA) has revolutionized astrophysics by providing unprecedented resolution and sensitivity in observing the cold universe, including the formation of stars, planets, and galaxies. With groundbreaking discoveries ranging from the first detailed images of protoplanetary disks to the kinematics of galaxies in the Epoch of Reionization, ALMA has showcased the vast discovery potential of the (sub-)mm wavelength regime. However, in another 15 years from now--in the 2040s--the science landscape will have changed dramatically as new major observational facilities will have started their operations or have come towards advanced maturity in their scientific outcome (e.g., JWST, Rubin Observatory, ELT, Euclid, Gaia, Plato, Ariel, Roman Space Telescope, SPHEREx, LiteBIRD, LISA, SKA and others). At the same time, ALMA's current Wideband Sensitivity Upgrade will have been in place for ~10 years, and ALMA itself will have been operational for 30 years. To fully exploit this era, the community needs a next-generation facility operating at (sub-)mm wavelengths with capabilities far beyond those possible within ALMA's current infrastructure.
  To this end, ALMA2040 is a community-driven initiative to define the key scientific questions of the 2040s and translate them into a technical vision for a next-generation transformational (sub-)millimeter facility. Our goal with this document is to summarize the current status of the effort, synthesize outcomes from the 2025 workshops, outline next steps toward a reference design concept, and invite broad participation from the global mm/sub-mm community to help shape this future facility.
  In the following we provide details on the process and scope. We invite everyone who wishes to join the effort and/or contribute to the dedicated White Papers planned for 2026.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2512.15824v1' target='_blank'>State-Augmented Graphs for Circular Economy Triage</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Richard Fox, Rui Li, Gustav Jonsson, Farzaneh Goli, Miying Yang, Emel Aktas, Yongjing Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-12-17 16:23:47</h6>
<p class='card-text'>Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>