<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-06-09</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-06-09</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06261v1' target='_blank'>Reflect-then-Plan: Offline Model-Based Planning through a Doubly
  Bayesian Lens</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jihwan Jeong, Xiaoyu Wang, Jingmin Wang, Scott Sanner, Pascal Poupart</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 17:40:12</h6>
<p class='card-text'>Offline reinforcement learning (RL) is crucial when online exploration is
costly or unsafe but often struggles with high epistemic uncertainty due to
limited data. Existing methods rely on fixed conservative policies, restricting
adaptivity and generalization. To address this, we propose Reflect-then-Plan
(RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach.
RefPlan unifies uncertainty modeling and MB planning by recasting planning as
Bayesian posterior estimation. At deployment, it updates a belief over
environment dynamics using real-time observations, incorporating uncertainty
into MB planning via marginalization. Empirical results on standard benchmarks
show that RefPlan significantly improves the performance of conservative
offline RL policies. In particular, RefPlan maintains robust performance under
high epistemic uncertainty and limited data, while demonstrating resilience to
changing environment dynamics, improving the flexibility, generalizability, and
robustness of offline-learned policies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06199v1' target='_blank'>3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World
  Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongyan Zhi, Peihao Chen, Siyuan Zhou, Yubo Dong, Quanxi Wu, Lei Han, Mingkui Tan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 16:00:31</h6>
<p class='card-text'>Manipulation has long been a challenging task for robots, while humans can
effortlessly perform complex interactions with objects, such as hanging a cup
on the mug rack. A key reason is the lack of a large and uniform dataset for
teaching robots manipulation skills. Current robot datasets often record robot
action in different action spaces within a simple scene. This hinders the robot
to learn a unified and robust action representation for different robots within
diverse scenes. Observing how humans understand a manipulation task, we find
that understanding how the objects should move in the 3D space is a critical
clue for guiding actions. This clue is embodiment-agnostic and suitable for
both humans and different robots. Motivated by this, we aim to learn a 3D flow
world model from both human and robot manipulation data. This model predicts
the future movement of the interacting objects in 3D space, guiding action
planning for manipulation. Specifically, we synthesize a large-scale 3D optical
flow dataset, named ManiFlow-110k, through a moving object auto-detect
pipeline. A video diffusion-based world model then learns manipulation physics
from these data, generating 3D optical flow trajectories conditioned on
language instructions. With the generated 3D object optical flow, we propose a
flow-guided rendering mechanism, which renders the predicted final state and
leverages GPT-4o to assess whether the predicted flow aligns with the task
description. This equips the robot with a closed-loop planning ability.
Finally, we consider the predicted 3D optical flow as constraints for an
optimization policy to determine a chunk of robot actions for manipulation.
Extensive experiments demonstrate strong generalization across diverse robotic
manipulation tasks and reliable cross-embodiment adaptation without
hardware-specific training.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06121v1' target='_blank'>Decomposability-Guaranteed Cooperative Coevolution for Large-Scale
  Itinerary Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyu Zhang, Peilan Xu, Yuetong Sun, Yuhui Shi, Wenjian Luo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 14:31:57</h6>
<p class='card-text'>Large-scale itinerary planning is a variant of the traveling salesman
problem, aiming to determine an optimal path that maximizes the collected
points of interest (POIs) scores while minimizing travel time and cost, subject
to travel duration constraints. This paper analyzes the decomposability of
large-scale itinerary planning, proving that strict decomposability is
difficult to satisfy, and introduces a weak decomposability definition based on
a necessary condition, deriving the corresponding graph structures that fulfill
this property. With decomposability guaranteed, we propose a novel
multi-objective cooperative coevolutionary algorithm for large-scale itinerary
planning, addressing the challenges of component imbalance and interactions.
Specifically, we design a dynamic decomposition strategy based on the
normalized fitness within each component, define optimization potential
considering component scale and contribution, and develop a computational
resource allocation strategy. Finally, we evaluate the proposed algorithm on a
set of real-world datasets. Comparative experiments with state-of-the-art
multi-objective itinerary planning algorithms demonstrate the superiority of
our approach, with performance advantages increasing as the problem scale
grows.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06094v1' target='_blank'>On-board Mission Replanning for Adaptive Cooperative Multi-Robot Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Elim Kwan, Rehman Qureshi, Liam Fletcher, Colin Laganier, Victoria Nockles, Richard Walters</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 13:54:19</h6>
<p class='card-text'>Cooperative autonomous robotic systems have significant potential for
executing complex multi-task missions across space, air, ground, and maritime
domains. But they commonly operate in remote, dynamic and hazardous
environments, requiring rapid in-mission adaptation without reliance on fragile
or slow communication links to centralised compute. Fast, on-board replanning
algorithms are therefore needed to enhance resilience. Reinforcement Learning
shows strong promise for efficiently solving mission planning tasks when
formulated as Travelling Salesperson Problems (TSPs), but existing methods: 1)
are unsuitable for replanning, where agents do not start at a single location;
2) do not allow cooperation between agents; 3) are unable to model tasks with
variable durations; or 4) lack practical considerations for on-board
deployment. Here we define the Cooperative Mission Replanning Problem as a
novel variant of multiple TSP with adaptations to overcome these issues, and
develop a new encoder/decoder-based model using Graph Attention Networks and
Attention Models to solve it effectively and efficiently. Using a simple
example of cooperative drones, we show our replanner consistently (90% of the
time) maintains performance within 10% of the state-of-the-art LKH3 heuristic
solver, whilst running 85-370 times faster on a Raspberry Pi. This work paves
the way for increased resilience in autonomous multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06084v1' target='_blank'>WisWheat: A Three-Tiered Vision-Language Dataset for Wheat Management</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bowen Yuan, Selena Song, Javier Fernandez, Yadan Luo, Mahsa Baktashmotlagh, Zijian Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 13:45:34</h6>
<p class='card-text'>Wheat management strategies play a critical role in determining yield.
Traditional management decisions often rely on labour-intensive expert
inspections, which are expensive, subjective and difficult to scale. Recently,
Vision-Language Models (VLMs) have emerged as a promising solution to enable
scalable, data-driven management support. However, due to a lack of
domain-specific knowledge, directly applying VLMs to wheat management tasks
results in poor quantification and reasoning capabilities, ultimately producing
vague or even misleading management recommendations. In response, we propose
WisWheat, a wheat-specific dataset with a three-layered design to enhance VLM
performance on wheat management tasks: (1) a foundational pretraining dataset
of 47,871 image-caption pairs for coarsely adapting VLMs to wheat morphology;
(2) a quantitative dataset comprising 7,263 VQA-style image-question-answer
triplets for quantitative trait measuring tasks; and (3) an Instruction
Fine-tuning dataset with 4,888 samples targeting biotic and abiotic stress
diagnosis and management plan for different phenological stages. Extensive
experimental results demonstrate that fine-tuning open-source VLMs (e.g.,
Qwen2.5 7B) on our dataset leads to significant performance improvements.
Specifically, the Qwen2.5 VL 7B fine-tuned on our wheat instruction dataset
achieves accuracy scores of 79.2% and 84.6% on wheat stress and growth stage
conversation tasks respectively, surpassing even general-purpose commercial
models such as GPT-4o by a margin of 11.9% and 34.6%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06038v1' target='_blank'>Trajectory Optimization for UAV-Based Medical Delivery with Temporal
  Logic Constraints and Convex Feasible Set Collision Avoidance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaiyuan Chen, Yuhan Suo, Shaowei Cui, Yuanqing Xia, Wannian Liang, Shuo Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 12:39:02</h6>
<p class='card-text'>This paper addresses the problem of trajectory optimization for unmanned
aerial vehicles (UAVs) performing time-sensitive medical deliveries in urban
environments. Specifically, we consider a single UAV with 3 degree-of-freedom
dynamics tasked with delivering blood packages to multiple hospitals, each with
a predefined time window and priority. Mission objectives are encoded using
Signal Temporal Logic (STL), enabling the formal specification of
spatial-temporal constraints. To ensure safety, city buildings are modeled as
3D convex obstacles, and obstacle avoidance is handled through a Convex
Feasible Set (CFS) method. The entire planning problem-combining UAV dynamics,
STL satisfaction, and collision avoidance-is formulated as a convex
optimization problem that ensures tractability and can be solved efficiently
using standard convex programming techniques. Simulation results demonstrate
that the proposed method generates dynamically feasible, collision-free
trajectories that satisfy temporal mission goals, providing a scalable and
reliable approach for autonomous UAV-based medical logistics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06028v1' target='_blank'>End-to-End Framework for Robot Lawnmower Coverage Path Planning using
  Cellular Decomposition</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nikunj Shah, Utsav Dey, Kenji Nishimiya</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 12:20:45</h6>
<p class='card-text'>Efficient Coverage Path Planning (CPP) is necessary for autonomous robotic
lawnmowers to effectively navigate and maintain lawns with diverse and
irregular shapes. This paper introduces a comprehensive end-to-end pipeline for
CPP, designed to convert user-defined boundaries on an aerial map into
optimized coverage paths seamlessly. The pipeline includes user input
extraction, coordinate transformation, area decomposition and path generation
using our novel AdaptiveDecompositionCPP algorithm, preview and customization
through an interactive coverage path visualizer, and conversion to actionable
GPS waypoints. The AdaptiveDecompositionCPP algorithm combines cellular
decomposition with an adaptive merging strategy to reduce non-mowing travel
thereby enhancing operational efficiency. Experimental evaluations,
encompassing both simulations and real-world lawnmower tests, demonstrate the
effectiveness of the framework in coverage completeness and mowing efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06024v1' target='_blank'>On Inverse Problems, Parameter Estimation, and Domain Generalization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Deborah Pereg</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 12:15:02</h6>
<p class='card-text'>Signal restoration and inverse problems are key elements in most real-world
data science applications. In the past decades, with the emergence of machine
learning methods, inversion of measurements has become a popular step in almost
all physical applications, which is normally executed prior to downstream tasks
that often involve parameter estimation. In this work, we analyze the general
problem of parameter estimation in an inverse problem setting. First, we
address the domain-shift problem by re-formulating it in direct relation with
the discrete parameter estimation analysis. We analyze a significant
vulnerability in current attempts to enforce domain generalization, which we
dubbed the Double Meaning Theorem. Our theoretical findings are experimentally
illustrated for domain shift examples in image deblurring and speckle
suppression in medical imaging. We then proceed to a theoretical analysis of
parameter estimation given observed measurements before and after data
processing involving an inversion of the observations. We compare this setting
for invertible and non-invertible (degradation) processes. We distinguish
between continuous and discrete parameter estimation, corresponding with
regression and classification problems, respectively. Our theoretical findings
align with the well-known information-theoretic data processing inequality, and
to a certain degree question the common misconception that data-processing for
inversion, based on modern generative models that may often produce outstanding
perceptual quality, will necessarily improve the following parameter estimation
objective. It is our hope that this paper will provide practitioners with
deeper insights that may be leveraged in the future for the development of more
efficient and informed strategic system planning, critical in safety-sensitive
applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06022v1' target='_blank'>Millikelvin-precision temperature sensing for advanced cryogenic
  detectors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maria Antonova, Jordi Capó, Anselmo Cervera, Pablo Fernández, Miguel Á. García-Peris, Xavier Pons</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 12:14:12</h6>
<p class='card-text'>Precise temperature monitoring -- to the level of a few milli-Kelvin -- is
essential for the operation of large-scale cryostats requiring a recirculation
system. In particular, the performance of Liquid Argon Time Projection Chambers
-- such as those planned for the DUNE experiment -- strongly relies on proper
argon purification and mixing, which can be characterized by a sufficiently
dense grid of high-precision temperature probes. In this article, we present a
novel technique for the cross-calibration of Resistance Temperature Detectors
in cryogenic liquids, developed as part of the temperature monitoring system
for a DUNE prototype. This calibration has enabled the validation and
optimization of the system's components, achieving an unprecedented precision
of 2.5 mK.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06012v1' target='_blank'>Enhanced Trust Region Sequential Convex Optimization for Multi-Drone
  Thermal Screening Trajectory Planning in Urban Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaiyuan Chen, Zhengjie Hu, Shaolin Zhang, Yuanqing Xia, Wannian Liang, Shuo Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 11:57:30</h6>
<p class='card-text'>The rapid detection of abnormal body temperatures in urban populations is
essential for managing public health risks, especially during outbreaks of
infectious diseases. Multi-drone thermal screening systems offer promising
solutions for fast, large-scale, and non-intrusive human temperature
monitoring. However, trajectory planning for multiple drones in complex urban
environments poses significant challenges, including collision avoidance,
coverage efficiency, and constrained flight environments. In this study, we
propose an enhanced trust region sequential convex optimization (TR-SCO)
algorithm for optimal trajectory planning of multiple drones performing thermal
screening tasks. Our improved algorithm integrates a refined convex
optimization formulation within a trust region framework, effectively balancing
trajectory smoothness, obstacle avoidance, altitude constraints, and maximum
screening coverage. Simulation results demonstrate that our approach
significantly improves trajectory optimality and computational efficiency
compared to conventional convex optimization methods. This research provides
critical insights and practical contributions toward deploying efficient
multi-drone systems for real-time thermal screening in urban areas. For reader
who are interested in our research, we release our source code at
https://github.com/Cherry0302/Enhanced-TR-SCO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05997v1' target='_blank'>Improving Long-Range Navigation with Spatially-Enhanced Recurrent Memory
  via End-to-End Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fan Yang, Per Frivik, David Hoeller, Chen Wang, Cesar Cadena, Marco Hutter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 11:35:48</h6>
<p class='card-text'>Recent advancements in robot navigation, especially with end-to-end learning
approaches like reinforcement learning (RL), have shown remarkable efficiency
and effectiveness. Yet, successful navigation still relies on two key
capabilities: mapping and planning, whether explicit or implicit. Classical
approaches use explicit mapping pipelines to register ego-centric observations
into a coherent map frame for the planner. In contrast, end-to-end learning
achieves this implicitly, often through recurrent neural networks (RNNs) that
fuse current and past observations into a latent space for planning. While
architectures such as LSTM and GRU capture temporal dependencies, our findings
reveal a key limitation: their inability to perform effective spatial
memorization. This skill is essential for transforming and integrating
sequential observations from varying perspectives to build spatial
representations that support downstream planning. To address this, we propose
Spatially-Enhanced Recurrent Units (SRUs), a simple yet effective modification
to existing RNNs, designed to enhance spatial memorization capabilities. We
introduce an attention-based architecture with SRUs, enabling long-range
navigation using a single forward-facing stereo camera. Regularization
techniques are employed to ensure robust end-to-end recurrent training via RL.
Experimental results show our approach improves long-range navigation by 23.5%
compared to existing RNNs. Furthermore, with SRU memory, our method outperforms
the RL baseline with explicit mapping and memory modules, achieving a 29.6%
improvement in diverse environments requiring long-horizon mapping and
memorization. Finally, we address the sim-to-real gap by leveraging large-scale
pretraining on synthetic depth data, enabling zero-shot transfer to diverse and
complex real-world environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05941v1' target='_blank'>Comparative Analysis of Modern Machine Learning Models for Retail Sales
  Forecasting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Luka Hobor, Mario Brcic, Lidija Polutnik, Ante Kapetanovic</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 10:08:17</h6>
<p class='card-text'>Accurate forecasting is key for all business planning. When estimated sales
are too high, brick-and-mortar retailers may incur higher costs due to unsold
inventories, higher labor and storage space costs, etc. On the other hand, when
forecasts underestimate the level of sales, firms experience lost sales,
shortages, and impact on the reputation of the retailer in their relevant
market. Accurate forecasting presents a competitive advantage for companies. It
facilitates the achievement of revenue and profit goals and execution of
pricing strategy and tactics. In this study, we provide an exhaustive
assessment of the forecasting models applied to a high-resolution
brick-and-mortar retail dataset. Our forecasting framework addresses the
problems found in retail environments, including intermittent demand, missing
values, and frequent product turnover. We compare tree-based ensembles (such as
XGBoost and LightGBM) and state-of-the-art neural network architectures
(including N-BEATS, NHITS, and the Temporal Fusion Transformer) across various
experimental settings. Our results show that localized modeling strategies
especially those using tree-based models on individual groups with non-imputed
data, consistently deliver superior forecasting accuracy and computational
efficiency. In contrast, neural models benefit from advanced imputation
methods, yet still fall short in handling the irregularities typical of
physical retail data. These results further practical understanding for model
selection in retail environment and highlight the significance of data
preprocessing to improve forecast performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05933v1' target='_blank'>Machine Learning Predictions for Traffic Equilibria in Road Renovation
  Scheduling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Robbert Bosch, Wouter van Heeswijk, Patricia Rogetzer, Martijn Mes</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 09:59:05</h6>
<p class='card-text'>Accurately estimating the impact of road maintenance schedules on traffic
conditions is important because maintenance operations can substantially worsen
congestion if not carefully planned. Reliable estimates allow planners to avoid
excessive delays during periods of roadwork. Since the exact increase in
congestion is difficult to predict analytically, traffic simulations are
commonly used to assess the redistribution of the flow of traffic. However,
when applied to long-term maintenance planning involving many overlapping
projects and scheduling alternatives, these simulations must be run thousands
of times, resulting in a significant computational burden. This paper
investigates the use of machine learning-based surrogate models to predict
network-wide congestion caused by simultaneous road renovations. We frame the
problem as a supervised learning task, using one-hot encodings, engineered
traffic features, and heuristic approximations. A range of linear,
ensemble-based, probabilistic, and neural regression models is evaluated under
an online learning framework in which data progressively becomes available. The
experimental results show that the Costliest Subset Heuristic provides a
reasonable approximation when limited training data is available, and that most
regression models fail to outperform it, with the exception of XGBoost, which
achieves substantially better accuracy. In overall performance, XGBoost
significantly outperforms alternatives in a range of metrics, most strikingly
Mean Absolute Percentage Error (MAPE) and Pinball loss, where it achieves a
MAPE of 11% and outperforms the next-best model by 20% and 38% respectively.
This modeling approach has the potential to reduce the computational burden of
large-scale traffic assignment problems in maintenance planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05911v1' target='_blank'>Simulation-based inference with neural posterior estimation applied to
  X-ray spectral fitting II -- High-resolution spectroscopy with the X-ray
  Integral Field Unit</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Simon Dupourqué, Didier Barret</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 09:32:08</h6>
<p class='card-text'>X-ray spectral fitting in high-energy astrophysics can be reliably
accelerated using Machine Learning. In particular, Simulation-based Inference
(SBI) produces accurate posterior distributions in the Gaussian and Poisson
regime for low-resolution spectra, much faster than other exact approaches such
as Monte Carlo Markov Chains or Nested Sampling. We now aim to highlight the
capabilities of SBI for high-resolution spectra, as what will be provided by
the newAthena X-ray Integral Field Unit (X-IFU). The large number of channels
encourages us to use compressed representations of the spectra, taking
advantage of the likelihood-free inference aspect of SBI. Two compression
schemes are explored, using either simple summary statistics, such as the
counts in arbitrary bins or ratios between these bins. We benchmark the
efficiency of these approaches using simulated X-IFU spectra with various
spectral models, including smooth comptonised spectra, relativistic reflexion
models and plasma emission models. We find that using simple and meaningful
summary statistics is much more efficient than working directly with the full
spectrum, and can derive posterior distributions comparable to those from exact
computation using nested sampling. Multi-round inference converges quickly to
the good solution. Amortized single round inference requires more simulations,
hence longer training time, but can be used to infer model parameters from many
observations afterwards. Information from the emission lines must be accounted
for using dedicated summary statistics. SBI for X-ray spectral fitting is a
robust technique that delivers well calibrated posteriors. This approach shows
great promises for high-resolution spectra, offering its potential for the
scientific exploitation of the X-IFU. We now plan to apply it to the current
era of high-resolution telescopes, and further challenge this approach with
real data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05810v1' target='_blank'>Trajectory Entropy: Modeling Game State Stability from Multimodality
  Trajectory Prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yesheng Zhang, Wenjian Sun, Yuheng Chen, Qingwei Liu, Qi Lin, Rui Zhang, Xu Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 07:17:55</h6>
<p class='card-text'>Complex interactions among agents present a significant challenge for
autonomous driving in real-world scenarios. Recently, a promising approach has
emerged, which formulates the interactions of agents as a level-k game
framework. It effectively decouples agent policies by hierarchical game levels.
However, this framework ignores both the varying driving complexities among
agents and the dynamic changes in agent states across game levels, instead
treating them uniformly. Consequently, redundant and error-prone computations
are introduced into this framework. To tackle the issue, this paper proposes a
metric, termed as Trajectory Entropy, to reveal the game status of agents
within the level-k game framework. The key insight stems from recognizing the
inherit relationship between agent policy uncertainty and the associated
driving complexity. Specifically, Trajectory Entropy extracts statistical
signals representing uncertainty from the multimodality trajectory prediction
results of agents in the game. Then, the signal-to-noise ratio of this signal
is utilized to quantify the game status of agents. Based on the proposed
Trajectory Entropy, we refine the current level-k game framework through a
simple gating mechanism, significantly improving overall accuracy while
reducing computational costs. Our method is evaluated on the Waymo and nuPlan
datasets, in terms of trajectory prediction, open-loop and closed-loop planning
tasks. The results demonstrate the state-of-the-art performance of our method,
with precision improved by up to 19.89% for prediction and up to 16.48% for
planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05776v1' target='_blank'>On the stability of global forecasting models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marco Zanotti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 06:13:29</h6>
<p class='card-text'>Forecast stability, that is the consistency of predictions over time, is
essential in business settings where sudden shifts in forecasts can disrupt
planning and erode trust in predictive systems. Despite its importance,
stability is often overlooked in favor of accuracy, particularly in global
forecasting models. In this study, we evaluate the stability of point and
probabilistic forecasts across different retraining frequencies and ensemble
strategies using two large retail datasets (M5 and VN1). To do this, we
introduce a new metric for probabilistic stability (MQC) and analyze ten
different global models and four ensemble configurations. The results show that
less frequent retraining not only preserves but often improves forecast
stability, while ensembles, especially those combining diverse pool of models,
further enhance consistency without sacrificing accuracy. These findings
challenge the need for continuous retraining and highlight ensemble diversity
as a key factor in reducing forecast stability. The study promotes a shift
toward stability-aware forecasting practices, offering practical guidelines for
building more robust and sustainable prediction systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05752v1' target='_blank'>Integrating Spatiotemporal Features in LSTM for Spatially Informed
  COVID-19 Hospitalization Forecasting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhongying Wang, Thoai D. Ngo, Hamidreza Zoraghein, Benjamin Lucas, Morteza Karimzadeh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 05:22:11</h6>
<p class='card-text'>The COVID-19 pandemic's severe impact highlighted the need for accurate,
timely hospitalization forecasting to support effective healthcare planning.
However, most forecasting models struggled, especially during variant surges,
when they were needed most. This study introduces a novel Long Short-Term
Memory (LSTM) framework for forecasting daily state-level incident
hospitalizations in the United States. We present a spatiotemporal feature,
Social Proximity to Hospitalizations (SPH), derived from Facebook's Social
Connectedness Index to improve forecasts. SPH serves as a proxy for interstate
population interaction, capturing transmission dynamics across space and time.
Our parallel LSTM architecture captures both short- and long-term temporal
dependencies, and our multi-horizon ensembling strategy balances consistency
and forecasting error. Evaluation against COVID-19 Forecast Hub ensemble models
during the Delta and Omicron surges reveals superiority of our model. On
average, our model surpasses the ensemble by 27, 42, 54, and 69
hospitalizations per state on the $7^{th}$, $14^{th}$, $21^{st}$, and $28^{th}$
forecast days, respectively, during the Omicron surge. Data-ablation
experiments confirm SPH's predictive power, highlighting its effectiveness in
enhancing forecasting models. This research not only advances hospitalization
forecasting but also underscores the significance of spatiotemporal features,
such as SPH, in refining predictive performance in modeling the complex
dynamics of infectious disease spread.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05745v1' target='_blank'>SPRINT: Enabling Interleaved Planning and Parallelized Execution in
  Reasoning Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Emil Biju, Shayan Talaei, Zhemin Huang, Mohammadreza Pourreza, Azalia Mirhoseini, Amin Saberi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 05:10:31</h6>
<p class='card-text'>Large reasoning models (LRMs) excel at complex reasoning tasks but typically
generate lengthy sequential chains-of-thought, resulting in long inference
times before arriving at the final answer. To address this challenge, we
introduce SPRINT, a novel post-training and inference-time framework designed
to enable LRMs to dynamically identify and exploit opportunities for
parallelization during their reasoning process. SPRINT incorporates an
innovative data curation pipeline that reorganizes natural language reasoning
trajectories into structured rounds of long-horizon planning and parallel
execution. By fine-tuning LRMs on a small amount of such curated data, the
models learn to dynamically identify independent subtasks within extended
reasoning processes and effectively execute them in parallel. Through extensive
evaluations, we show that the models fine-tuned with the SPRINT framework match
the performance of reasoning models on complex domains such as mathematics
while generating up to ~39% fewer sequential tokens on problems requiring more
than 8000 output tokens. Finally, we observe consistent results transferred to
two out-of-distribution tasks of GPQA and Countdown with up to 45% and 65%
reduction in average sequential tokens for longer reasoning trajectories, while
achieving the performance of the fine-tuned reasoning model.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05701v1' target='_blank'>Statistically Valid Post-Deployment Monitoring Should Be Standard for
  AI-Based Digital Health</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pavel Dolin, Weizhi Li, Gautam Dasarathy, Visar Berisha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 03:04:44</h6>
<p class='card-text'>This position paper argues that post-deployment monitoring in clinical AI is
underdeveloped and proposes statistically valid and label-efficient testing
frameworks as a principled foundation for ensuring reliability and safety in
real-world deployment. A recent review found that only 9% of FDA-registered
AI-based healthcare tools include a post-deployment surveillance plan. Existing
monitoring approaches are often manual, sporadic, and reactive, making them
ill-suited for the dynamic environments in which clinical models operate. We
contend that post-deployment monitoring should be grounded in label-efficient
and statistically valid testing frameworks, offering a principled alternative
to current practices. We use the term "statistically valid" to refer to methods
that provide explicit guarantees on error rates (e.g., Type I/II error), enable
formal inference under pre-defined assumptions, and support
reproducibility--features that align with regulatory requirements.
Specifically, we propose that the detection of changes in the data and model
performance degradation should be framed as distinct statistical hypothesis
testing problems. Grounding monitoring in statistical rigor ensures a
reproducible and scientifically sound basis for maintaining the reliability of
clinical AI systems. Importantly, it also opens new research directions for the
technical community--spanning theory, methods, and tools for statistically
principled detection, attribution, and mitigation of post-deployment model
failures in real-world settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05567v1' target='_blank'>Partially-Supervised Neural Network Model For Quadratic Multiparametric
  Programming</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fuat Can Beylunioglu, Mehrdad Pirnia, P. Robert Duimering</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 20:26:18</h6>
<p class='card-text'>Neural Networks (NN) with ReLU activation functions are used to model
multiparametric quadratic optimization problems (mp-QP) in diverse engineering
applications. Researchers have suggested leveraging the piecewise affine
property of deep NN models to solve mp-QP with linear constraints, which also
exhibit piecewise affine behaviour. However, traditional deep NN applications
to mp-QP fall short of providing optimal and feasible predictions, even when
trained on large datasets. This study proposes a partially-supervised NN (PSNN)
architecture that directly represents the mathematical structure of the global
solution function. In contrast to generic NN training approaches, the proposed
PSNN method derives a large proportion of model weights directly from the
mathematical properties of the optimization problem, producing more accurate
solutions despite significantly smaller training data sets. Many energy
management problems are formulated as QP, so we apply the proposed approach to
energy systems (specifically DC optimal power flow) to demonstrate proof of
concept. Model performance in terms of solution accuracy and speed of
predictions was compared against a commercial solver and a generic Deep NN
model based on classical training. Results show KKT sufficient conditions for
PSNN consistently outperform generic NN architectures with classical training
using far less data, including when tested on extreme, out-of-training
distribution test data. Given its speed advantages over traditional solvers,
the PSNN model can quickly produce optimal and feasible solutions within a
second for millions of input parameters sampled from a distribution of
stochastic demands and renewable generator dispatches, which can be used for
simulations and long term planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05523v1' target='_blank'>MORSE-500: A Programmatically Controllable Video Benchmark to
  Stress-Test Multimodal Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zikui Cai, Andrew Wang, Anirudh Satheesh, Ankit Nakhawa, Hyunwoo Jae, Keenan Powell, Minghui Liu, Neel Jay, Sungbin Oh, Xiyao Wang, Yongyuan Liang, Tom Goldstein, Furong Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 19:12:45</h6>
<p class='card-text'>Despite rapid advances in vision-language models (VLMs), current benchmarks
for multimodal reasoning fall short in three key dimensions. First, they
overwhelmingly rely on static images, failing to capture the temporal
complexity of real-world environments. Second, they narrowly focus on
mathematical problem-solving, neglecting the broader spectrum of reasoning
skills -- including abstract, physical, planning, spatial, and temporal
capabilities -- required for robust multimodal intelligence. Third, many
benchmarks quickly saturate, offering limited headroom for diagnosing failure
modes or measuring continued progress. We introduce MORSE-500 (Multimodal
Reasoning Stress-test Environment), a video benchmark composed of 500 fully
scripted clips with embedded questions spanning six complementary reasoning
categories. Each instance is programmatically generated using deterministic
Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and
curated real footage. This script-driven design allows fine-grained control
over visual complexity, distractor density, and temporal dynamics -- enabling
difficulty to be scaled systematically as models improve. Unlike static
benchmarks that become obsolete once saturated, MORSE-500 is built to evolve:
its controllable generation pipeline supports the creation of arbitrarily
challenging new instances, making it ideally suited for stress-testing
next-generation models. Initial experiments with state-of-the-art systems --
including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest
available at the time, alongside strong open-source models -- reveal
substantial performance gaps across all categories, with particularly large
deficits in abstract and planning tasks. We release the full dataset,
generation scripts, and evaluation harness to support transparent,
reproducible, and forward-looking multimodal reasoning research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05294v1' target='_blank'>A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$: Robust Imitation
  via Learning to Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arnav Kumar Jain, Vibhakar Mohta, Subin Kim, Atiksh Bhardwaj, Juntao Ren, Yunhai Feng, Sanjiban Choudhury, Gokul Swamy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 17:47:40</h6>
<p class='card-text'>The fundamental limitation of the behavioral cloning (BC) approach to
imitation learning is that it only teaches an agent what the expert did at
states the expert visited. This means that when a BC agent makes a mistake
which takes them out of the support of the demonstrations, they often don't
know how to recover from it. In this sense, BC is akin to giving the agent the
fish -- giving them dense supervision across a narrow set of states -- rather
than teaching them to fish: to be able to reason independently about achieving
the expert's outcome even when faced with unseen situations at test-time. In
response, we explore learning to search (L2S) from expert demonstrations, i.e.
learning the components required to, at test time, plan to match expert
outcomes, even after making a mistake. These include (1) a world model and (2)
a reward model. We carefully ablate the set of algorithmic and design decisions
required to combine these and other components for stable and
sample/interaction-efficient learning of recovery behavior without additional
human corrections. Across a dozen visual manipulation tasks from three
benchmarks, our approach $\texttt{SAILOR}$ consistently out-performs
state-of-the-art Diffusion Policies trained via BC on the same data.
Furthermore, scaling up the amount of demonstrations used for BC by
5-10$\times$ still leaves a performance gap. We find that $\texttt{SAILOR}$ can
identify nuanced failures and is robust to reward hacking. Our code is
available at https://github.com/arnavkj1995/SAILOR .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05251v1' target='_blank'>Cooperation and the Design of Public Goods</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:J. Carlos Martínez Mori, Alejandro Toriello</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 17:11:53</h6>
<p class='card-text'>We consider the cooperative elements that arise in the design of public
goods, such as transportation policies and infrastructure. These involve a
variety of stakeholders: governments, businesses, advocates, and users. Their
eventual deployment depends on the decision maker's ability to garner
sufficient support from each of these groups; we formalize these strategic
requirements from the perspective of cooperative game theory. Specifically, we
introduce non-transferable utility, linear production (NTU LP) games, which
combine the game-theoretic tensions inherent in public decision-making with the
modeling flexibility of linear programming. We derive structural properties
regarding the non-emptiness, representability and complexity of the core, a
solution concept that models the viability of cooperation. In particular, we
provide fairly general sufficient conditions under which the core of an NTU LP
game is guaranteed to be non-empty, prove that determining membership in the
core is co-NP-complete, and develop a cutting plane algorithm to optimize
various social welfare objectives subject to core membership. Lastly, we apply
these results in a data-driven case study on service plan optimization for the
Chicago bus system. As our study illustrates, cooperation is necessary for the
successful deployment of transportation service plans and similar public goods,
but it may also have adverse or counterintuitive distributive implications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05168v1' target='_blank'>Fabrica: Dual-Arm Assembly of General Multi-Part Objects via Integrated
  Planning and Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yunsheng Tian, Joshua Jacob, Yijiang Huang, Jialiang Zhao, Edward Gu, Pingchuan Ma, Annan Zhang, Farhad Javid, Branden Romero, Sachin Chitta, Shinjiro Sueda, Hui Li, Wojciech Matusik</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 15:43:52</h6>
<p class='card-text'>Multi-part assembly poses significant challenges for robots to execute
long-horizon, contact-rich manipulation with generalization across complex
geometries. We present Fabrica, a dual-arm robotic system capable of end-to-end
planning and control for autonomous assembly of general multi-part objects. For
planning over long horizons, we develop hierarchies of precedence, sequence,
grasp, and motion planning with automated fixture generation, enabling general
multi-step assembly on any dual-arm robots. The planner is made efficient
through a parallelizable design and is optimized for downstream control
stability. For contact-rich assembly steps, we propose a lightweight
reinforcement learning framework that trains generalist policies across object
geometries, assembly directions, and grasp poses, guided by equivariance and
residual actions obtained from the plan. These policies transfer zero-shot to
the real world and achieve 80% successful steps. For systematic evaluation, we
propose a benchmark suite of multi-part assemblies resembling industrial and
daily objects across diverse categories and geometries. By integrating
efficient global planning and robust local control, we showcase the first
system to achieve complete and generalizable real-world multi-part assembly
without domain knowledge or human demonstrations. Project website:
http://fabrica.csail.mit.edu/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05148v2' target='_blank'>Polarized Neutrons at ISIS: Recent Developments And Highlights</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Polly Mitchell, Holly I. Barnfield, Mark Devonport, Kirill Nemkovski, Gøran J. Nilsen, Peter Galsworthy, Gavin B. G. Stenning</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 15:28:11</h6>
<p class='card-text'>We present two recent projects which aim to improve the performance of
polarized neutron scattering experiments using hyperpolarized $^{3}He$ spin
filters at ISIS. The first is the optimization of a new compact magnetostatic
cavity ("Magic Box") to house the $^{3}He$ spin filters based on an existing
design. With a length of only 380 mm, it provides a field gradient relaxation
time for the $^{3}He$ cell of 421 h in ambient conditions. It also contains a
radiofrequency coil for adiabatic fast passage flipping. The second project is
dedicated to the improvement of the $^{3}He$ relaxation time inside the spin
filter cell. We have developed a chamber which allows for the deposition of
alkali metal coatings on the surface of substrates. This emulates the spin
filter cell walls, as well as subsequent heat treatment, thus mimicking the
preparation of a new spin filter cell. The chamber is air-tight and has
transparent windows, so that the structure resulting from the deposition of
alkali metal on the surface of the wafer can be studied by X-ray or neutron
reflectometry. We plan to continue this work by performing a systematic study
at various conditions, which should help to shed light on the long-standing
mystery of how alkali metal coatings help to improve relaxation time of
$^{3}He$ cells. The first results are discussed in the text.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05109v1' target='_blank'>Truly Self-Improving Agents Require Intrinsic Metacognitive Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tennison Liu, Mihaela van der Schaar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 14:53:35</h6>
<p class='card-text'>Self-improving agents aim to continuously acquire new capabilities with
minimal supervision. However, current approaches face two key limitations:
their self-improvement processes are often rigid, fail to generalize across
tasks domains, and struggle to scale with increasing agent capabilities. We
argue that effective self-improvement requires intrinsic metacognitive
learning, defined as an agent's intrinsic ability to actively evaluate, reflect
on, and adapt its own learning processes. Drawing inspiration from human
metacognition, we introduce a formal framework comprising three components:
metacognitive knowledge (self-assessment of capabilities, tasks, and learning
strategies), metacognitive planning (deciding what and how to learn), and
metacognitive evaluation (reflecting on learning experiences to improve future
learning). Analyzing existing self-improving agents, we find they rely
predominantly on extrinsic metacognitive mechanisms, which are fixed,
human-designed loops that limit scalability and adaptability. Examining each
component, we contend that many ingredients for intrinsic metacognition are
already present. Finally, we explore how to optimally distribute metacognitive
responsibilities between humans and agents, and robustly evaluate and improve
intrinsic metacognitive learning, key challenges that must be addressed to
enable truly sustained, generalized, and aligned self-improvement.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05106v1' target='_blank'>EDEN: Efficient Dual-Layer Exploration Planning for Fast UAV Autonomous
  Exploration in Large 3-D Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qianli Dong, Xuebo Zhang, Shiyong Zhang, Ziyu Wang, Zhe Ma, Haobo Xi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 14:49:17</h6>
<p class='card-text'>Efficient autonomous exploration in large-scale environments remains
challenging due to the high planning computational cost and low-speed
maneuvers. In this paper, we propose a fast and computationally efficient
dual-layer exploration planning method. The insight of our dual-layer method is
efficiently finding an acceptable long-term region routing and greedily
exploring the target in the region of the first routing area with high speed.
Specifically, the proposed method finds the long-term area routing through an
approximate algorithm to ensure real-time planning in large-scale environments.
Then, the viewpoint in the first routing region with the lowest
curvature-penalized cost, which can effectively reduce decelerations caused by
sharp turn motions, will be chosen as the next exploration target. To further
speed up the exploration, we adopt an aggressive and safe exploration-oriented
trajectory to enhance exploration continuity. The proposed method is compared
to state-of-the-art methods in challenging simulation environments. The results
show that the proposed method outperforms other methods in terms of exploration
efficiency, computational cost, and trajectory speed. We also conduct
real-world experiments to validate the effectiveness of the proposed method.
The code will be open-sourced.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05087v1' target='_blank'>Interpretable Multimodal Framework for Human-Centered Street Assessment:
  Integrating Visual-Language Models for Perceptual Urban Diagnostics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:HaoTian Lan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 14:34:04</h6>
<p class='card-text'>While objective street metrics derived from imagery or GIS have become
standard in urban analytics, they remain insufficient to capture subjective
perceptions essential to inclusive urban design. This study introduces a novel
Multimodal Street Evaluation Framework (MSEF) that fuses a vision transformer
(VisualGLM-6B) with a large language model (GPT-4), enabling interpretable
dual-output assessment of streetscapes. Leveraging over 15,000 annotated
street-view images from Harbin, China, we fine-tune the framework using LoRA
and P-Tuning v2 for parameter-efficient adaptation. The model achieves an F1
score of 0.84 on objective features and 89.3 percent agreement with aggregated
resident perceptions, validated across stratified socioeconomic geographies.
Beyond classification accuracy, MSEF captures context-dependent contradictions:
for instance, informal commerce boosts perceived vibrancy while simultaneously
reducing pedestrian comfort. It also identifies nonlinear and semantically
contingent patterns -- such as the divergent perceptual effects of
architectural transparency across residential and commercial zones -- revealing
the limits of universal spatial heuristics. By generating natural-language
rationales grounded in attention mechanisms, the framework bridges sensory data
with socio-affective inference, enabling transparent diagnostics aligned with
SDG 11. This work offers both methodological innovation in urban perception
modeling and practical utility for planning systems seeking to reconcile
infrastructural precision with lived experience.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05080v1' target='_blank'>Parking, Perception, and Retail: Street-Level Determinants of Community
  Vitality in Harbin</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:HaoTian Lan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 14:28:48</h6>
<p class='card-text'>The commercial vitality of community-scale streets in Chinese cities is
shaped by complex interactions between vehicular accessibility, environmental
quality, and pedestrian perception. This study proposes an interpretable,
image-based framework to examine how street-level features -- including parked
vehicle density, greenery, cleanliness, and street width -- impact retail
performance and user satisfaction in Harbin, China. Leveraging street view
imagery and a multimodal large language model (VisualGLM-6B), we construct a
Community Commercial Vitality Index (CCVI) from Meituan and Dianping data and
analyze its relationship with spatial attributes extracted via GPT-4-based
perception modeling. Our findings reveal that while moderate vehicle presence
may enhance commercial access, excessive on-street parking -- especially in
narrow streets -- erodes walkability and reduces both satisfaction and
shop-level pricing. In contrast, streets with higher perceived greenery and
cleanliness show significantly greater satisfaction scores but only weak
associations with pricing. Street width moderates the effects of vehicle
presence, underscoring the importance of spatial configuration. These results
demonstrate the value of integrating AI-assisted perception with urban
morphological analysis to capture non-linear and context-sensitive drivers of
commercial success. This study advances both theoretical and methodological
frontiers by highlighting the conditional role of vehicle activity in
neighborhood commerce and demonstrating the feasibility of multimodal AI for
perceptual urban diagnostics. The implications extend to urban design, parking
management, and scalable planning tools for community revitalization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.04970v1' target='_blank'>Bringing SAM to new heights: Leveraging elevation data for tree crown
  segmentation from drone imagery</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mélisande Teng, Arthur Ouaknine, Etienne Laliberté, Yoshua Bengio, David Rolnick, Hugo Larochelle</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 12:43:11</h6>
<p class='card-text'>Information on trees at the individual level is crucial for monitoring forest
ecosystems and planning forest management. Current monitoring methods involve
ground measurements, requiring extensive cost, time and labor. Advances in
drone remote sensing and computer vision offer great potential for mapping
individual trees from aerial imagery at broad-scale. Large pre-trained vision
models, such as the Segment Anything Model (SAM), represent a particularly
compelling choice given limited labeled data. In this work, we compare methods
leveraging SAM for the task of automatic tree crown instance segmentation in
high resolution drone imagery in three use cases: 1) boreal plantations, 2)
temperate forests and 3) tropical forests. We also study the integration of
elevation data into models, in the form of Digital Surface Model (DSM)
information, which can readily be obtained at no additional cost from RGB drone
imagery. We present BalSAM, a model leveraging SAM and DSM information, which
shows potential over other methods, particularly in the context of plantations.
We find that methods using SAM out-of-the-box do not outperform a custom Mask
R-CNN, even with well-designed prompts. However, efficiently tuning SAM
end-to-end and integrating DSM information are both promising avenues for tree
crown instance segmentation models.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>