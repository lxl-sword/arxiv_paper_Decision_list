<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>HRL - 2025-03-15</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>HRL - 2025-03-15</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06309v1' target='_blank'>On the Fly Adaptation of Behavior Tree-Based Policies through
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marco Iannotta, Johannes A. Stork, Erik Schaffernicht, Todor Stoyanov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 18:56:22</h6>
<p class='card-text'>With the rising demand for flexible manufacturing, robots are increasingly
expected to operate in dynamic environments where local -- such as slight
offsets or size differences in workpieces -- are common. We propose to address
the problem of adapting robot behaviors to these task variations with a
sample-efficient hierarchical reinforcement learning approach adapting Behavior
Tree (BT)-based policies. We maintain the core BT properties as an
interpretable, modular framework for structuring reactive behaviors, but extend
their use beyond static tasks by inherently accommodating local task
variations. To show the efficiency and effectiveness of our approach, we
conduct experiments both in simulation and on a Franka Emika Panda 7-DoF, with
the manipulator adapting to different obstacle avoidance and pivoting tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20380v1' target='_blank'>Multi-Turn Code Generation Through Single-Step Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arnav Kumar Jain, Gonzalo Gonzalez-Pumariega, Wayne Chen, Alexander M Rush, Wenting Zhao, Sanjiban Choudhury</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:55:05</h6>
<p class='card-text'>We address the problem of code generation from multi-turn execution feedback.
Existing methods either generate code without feedback or use complex,
hierarchical reinforcement learning to optimize multi-turn rewards. We propose
a simple yet scalable approach, $\mu$Code, that solves multi-turn code
generation using only single-step rewards. Our key insight is that code
generation is a one-step recoverable MDP, where the correct code can be
recovered from any intermediate code state in a single turn. $\mu$Code
iteratively trains both a generator to provide code solutions conditioned on
multi-turn execution feedback and a verifier to score the newly generated code.
Experimental evaluations show that our approach achieves significant
improvements over the state-of-the-art baselines. We provide analysis of the
design choices of the reward models and policy, and show the efficacy of
$\mu$Code at utilizing the execution feedback. Our code is available at
https://github.com/portal-cornell/muCode.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.15425v4' target='_blank'>TAG: A Decentralized Framework for Multi-Agent Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Giuseppe Paolo, Abdelhakim Benechehab, Hamza Cherkaoui, Albert Thomas, Balázs Kégl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-21 12:52:16</h6>
<p class='card-text'>Hierarchical organization is fundamental to biological systems and human
societies, yet artificial intelligence systems often rely on monolithic
architectures that limit adaptability and scalability. Current hierarchical
reinforcement learning (HRL) approaches typically restrict hierarchies to two
levels or require centralized training, which limits their practical
applicability. We introduce TAME Agent Framework (TAG), a framework for
constructing fully decentralized hierarchical multi-agent systems. TAG enables
hierarchies of arbitrary depth through a novel LevelEnv concept, which
abstracts each hierarchy level as the environment for the agents above it. This
approach standardizes information flow between levels while preserving loose
coupling, allowing for seamless integration of diverse agent types. We
demonstrate the effectiveness of TAG by implementing hierarchical architectures
that combine different RL agents across multiple levels, achieving improved
performance over classical multi-agent RL baselines on standard benchmarks. Our
results show that decentralized hierarchical organization enhances both
learning speed and final performance, positioning TAG as a promising direction
for scalable multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06772v2' target='_blank'>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 18:51:47</h6>
<p class='card-text'>We present that hierarchical LLM reasoning via scaling thought templates can
effectively optimize the reasoning search space and outperform the mathematical
reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.
We train our ReasonFlux-32B model with only 8 GPUs and introduces three
innovations: (i) a structured and generic thought template library, containing
around 500 high-level thought templates capable of generalizing to similar or
relevant reasoning problems; (ii) performing hierarchical reinforcement
learning on a sequence of thought templates instead of long CoTs, optimizing a
base LLM to plan out an optimal template trajectory for gradually handling
complex problems; (iii) a brand new inference scaling system that enables
hierarchical LLM reasoning by adaptively scaling thought templates at inference
time. With a template trajectory containing more explainable reasoning
structures than DeepSeek-R1 and o3-mini, our ReasonFlux-32B significantly
advances math reasoning capabilities to state-of-the-art levels. Notably, on
the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview
by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an
average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and
45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05537v1' target='_blank'>Sequential Stochastic Combinatorial Optimization Using Hierarchal
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinsong Feng, Zihan Yu, Yanhai Xiong, Haipeng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 12:00:30</h6>
<p class='card-text'>Reinforcement learning (RL) has emerged as a promising tool for combinatorial
optimization (CO) problems due to its ability to learn fast, effective, and
generalizable solutions. Nonetheless, existing works mostly focus on one-shot
deterministic CO, while sequential stochastic CO (SSCO) has rarely been studied
despite its broad applications such as adaptive influence maximization (IM) and
infectious disease intervention. In this paper, we study the SSCO problem where
we first decide the budget (e.g., number of seed nodes in adaptive IM)
allocation for all time steps, and then select a set of nodes for each time
step. The few existing studies on SSCO simplify the problems by assuming a
uniformly distributed budget allocation over the time horizon, yielding
suboptimal solutions. We propose a generic hierarchical RL (HRL) framework
called wake-sleep option (WS-option), a two-layer option-based framework that
simultaneously decides adaptive budget allocation on the higher layer and node
selection on the lower layer. WS-option starts with a coherent formulation of
the two-layer Markov decision processes (MDPs), capturing the interdependencies
between the two layers of decisions. Building on this, WS-option employs
several innovative designs to balance the model's training stability and
computational efficiency, preventing the vicious cyclic interference issue
between the two layers. Empirical results show that WS-option exhibits
significantly improved effectiveness and generalizability compared to
traditional methods. Moreover, the learned model can be generalized to larger
graphs, which significantly reduces the overhead of computational resources.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03960v1' target='_blank'>Bilevel Multi-Armed Bandit-Based Hierarchical Reinforcement Learning for
  Interaction-Aware Self-Driving at Unsignalized Intersections</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zengqi Peng, Yubin Wang, Lei Zheng, Jun Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 10:50:59</h6>
<p class='card-text'>In this work, we present BiM-ACPPO, a bilevel multi-armed bandit-based
hierarchical reinforcement learning framework for interaction-aware
decision-making and planning at unsignalized intersections. Essentially, it
proactively takes the uncertainties associated with surrounding vehicles (SVs)
into consideration, which encompass those stemming from the driver's intention,
interactive behaviors, and the varying number of SVs. Intermediate decision
variables are introduced to enable the high-level RL policy to provide an
interaction-aware reference, for guiding low-level model predictive control
(MPC) and further enhancing the generalization ability of the proposed
framework. By leveraging the structured nature of self-driving at unsignalized
intersections, the training problem of the RL policy is modeled as a bilevel
curriculum learning task, which is addressed by the proposed Exp3.S-based BiMAB
algorithm. It is noteworthy that the training curricula are dynamically
adjusted, thereby facilitating the sample efficiency of the RL training
process. Comparative experiments are conducted in the high-fidelity CARLA
simulator, and the results indicate that our approach achieves superior
performance compared to all baseline methods. Furthermore, experimental results
in two new urban driving scenarios clearly demonstrate the commendable
generalization performance of the proposed method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01956v1' target='_blank'>DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement
  Learning Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shashank Sharma, Janina Hoffmann, Vinay Namboodiri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-04 03:05:55</h6>
<p class='card-text'>In this paper, we address the challenge of long-horizon visual planning tasks
using Hierarchical Reinforcement Learning (HRL). Our key contribution is a
Discrete Hierarchical Planning (DHP) method, an alternative to traditional
distance-based approaches. We provide theoretical foundations for the method
and demonstrate its effectiveness through extensive empirical evaluations.
  Our agent recursively predicts subgoals in the context of a long-term goal
and receives discrete rewards for constructing plans as compositions of
abstract actions. The method introduces a novel advantage estimation strategy
for tree trajectories, which inherently encourages shorter plans and enables
generalization beyond the maximum tree depth. The learned policy function
allows the agent to plan efficiently, requiring only $\log N$ computational
steps, making re-planning highly efficient. The agent, based on a soft-actor
critic (SAC) framework, is trained using on-policy imagination data.
Additionally, we propose a novel exploration strategy that enables the agent to
generate relevant training examples for the planning modules. We evaluate our
method on long-horizon visual planning tasks in a 25-room environment, where it
significantly outperforms previous benchmarks at success rate and average
episode length. Furthermore, an ablation study highlights the individual
contributions of key modules to the overall performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.17424v1' target='_blank'>Certificated Actor-Critic: Hierarchical Reinforcement Learning with
  Control Barrier Functions for Safe Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junjun Xie, Shuhao Zhao, Liang Hu, Huijun Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-29 05:37:47</h6>
<p class='card-text'>Control Barrier Functions (CBFs) have emerged as a prominent approach to
designing safe navigation systems of robots. Despite their popularity, current
CBF-based methods exhibit some limitations: optimization-based safe control
techniques tend to be either myopic or computationally intensive, and they rely
on simplified system models; conversely, the learning-based methods suffer from
the lack of quantitative indication in terms of navigation performance and
safety. In this paper, we present a new model-free reinforcement learning
algorithm called Certificated Actor-Critic (CAC), which introduces a
hierarchical reinforcement learning framework and well-defined reward functions
derived from CBFs. We carry out theoretical analysis and proof of our
algorithm, and propose several improvements in algorithm implementation. Our
analysis is validated by two simulation experiments, showing the effectiveness
of our proposed CAC algorithm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.14992v1' target='_blank'>Extensive Exploration in Complex Traffic Scenarios using Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhihao Zhang, Ekim Yurtsever, Keith A. Redmill</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-25 00:00:11</h6>
<p class='card-text'>Developing an automated driving system capable of navigating complex traffic
environments remains a formidable challenge. Unlike rule-based or supervised
learning-based methods, Deep Reinforcement Learning (DRL) based controllers
eliminate the need for domain-specific knowledge and datasets, thus providing
adaptability to various scenarios. Nonetheless, a common limitation of existing
studies on DRL-based controllers is their focus on driving scenarios with
simple traffic patterns, which hinders their capability to effectively handle
complex driving environments with delayed, long-term rewards, thus compromising
the generalizability of their findings. In response to these limitations, our
research introduces a pioneering hierarchical framework that efficiently
decomposes intricate decision-making problems into manageable and interpretable
subtasks. We adopt a two step training process that trains the high-level
controller and low-level controller separately. The high-level controller
exhibits an enhanced exploration potential with long-term delayed rewards, and
the low-level controller provides longitudinal and lateral control ability
using short-term instantaneous rewards. Through simulation experiments, we
demonstrate the superiority of our hierarchical controller in managing complex
highway driving situations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13084v1' target='_blank'>Attention-Driven Hierarchical Reinforcement Learning with Particle
  Filtering for Source Localization in Dynamic Fields</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiwei Shi, Mengyue Yang, Qi Zhang, Weinan Zhang, Cunjia Liu, Weiru Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 18:45:29</h6>
<p class='card-text'>In many real-world scenarios, such as gas leak detection or environmental
pollutant tracking, solving the Inverse Source Localization and
Characterization problem involves navigating complex, dynamic fields with
sparse and noisy observations. Traditional methods face significant challenges,
including partial observability, temporal and spatial dynamics,
out-of-distribution generalization, and reward sparsity. To address these
issues, we propose a hierarchical framework that integrates Bayesian inference
and reinforcement learning. The framework leverages an attention-enhanced
particle filtering mechanism for efficient and accurate belief updates, and
incorporates two complementary execution strategies: Attention Particle
Filtering Planning and Attention Particle Filtering Reinforcement Learning.
These approaches optimize exploration and adaptation under uncertainty.
Theoretical analysis proves the convergence of the attention-enhanced particle
filter, while extensive experiments across diverse scenarios validate the
framework's superior accuracy, adaptability, and computational efficiency. Our
results highlight the framework's potential for broad applications in dynamic
field estimation tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13132v1' target='_blank'>A Hierarchical Reinforcement Learning Framework for Multi-UAV Combat
  Using Leader-Follower Strategy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinhui Pang, Jinglin He, Noureldin Mohamed Abdelaal Ahmed Mohamed, Changqing Lin, Zhihui Zhang, Xiaoshuai Hao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 02:41:36</h6>
<p class='card-text'>Multi-UAV air combat is a complex task involving multiple autonomous UAVs, an
evolving field in both aerospace and artificial intelligence. This paper aims
to enhance adversarial performance through collaborative strategies. Previous
approaches predominantly discretize the action space into predefined actions,
limiting UAV maneuverability and complex strategy implementation. Others
simplify the problem to 1v1 combat, neglecting the cooperative dynamics among
multiple UAVs. To address the high-dimensional challenges inherent in
six-degree-of-freedom space and improve cooperation, we propose a hierarchical
framework utilizing the Leader-Follower Multi-Agent Proximal Policy
Optimization (LFMAPPO) strategy. Specifically, the framework is structured into
three levels. The top level conducts a macro-level assessment of the
environment and guides execution policy. The middle level determines the angle
of the desired action. The bottom level generates precise action commands for
the high-dimensional action space. Moreover, we optimize the state-value
functions by assigning distinct roles with the leader-follower strategy to
train the top-level policy, followers estimate the leader's utility, promoting
effective cooperation among agents. Additionally, the incorporation of a target
selector, aligned with the UAVs' posture, assesses the threat level of targets.
Finally, simulation experiments validate the effectiveness of our proposed
method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.07274v2' target='_blank'>Mining Intraday Risk Factor Collections via Hierarchical Reinforcement
  Learning based on Transferred Options</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenyan Xu, Jiayu Chen, Chen Li, Yonghong Hu, Zhonghua Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-13 12:38:05</h6>
<p class='card-text'>Traditional risk factors like beta, size/value, and momentum often lag behind
market dynamics in measuring and predicting stock return volatility.
Statistical models like PCA and factor analysis fail to capture hidden
nonlinear relationships. Genetic programming (GP) can identify nonlinear
factors but often lacks mechanisms for evaluating factor quality, and the
resulting formulas are complex. To address these challenges, we propose a
Hierarchical Proximal Policy Optimization (HPPO) framework for automated factor
generation and evaluation. HPPO uses two PPO models: a high-level policy
assigns weights to stock features, and a low-level policy identifies latent
nonlinear relationships. The Pearson correlation between generated factors and
return volatility serves as the reward signal. Transfer learning pre-trains the
high-level policy on large-scale historical data, fine-tuning it with the
latest data to adapt to new features and shifts. Experiments show the HPPO-TO
algorithm achieves a 25\% excess return in HFT markets across China (CSI
300/800), India (Nifty 100), and the US (S\&P 500). Code and data are available
at https://github.com/wencyxu/HRL-HF_risk_factor_set.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.06554v1' target='_blank'>Hierarchical Reinforcement Learning for Optimal Agent Grouping in
  Cooperative Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Liyuan Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-11 14:22:10</h6>
<p class='card-text'>This paper presents a hierarchical reinforcement learning (RL) approach to
address the agent grouping or pairing problem in cooperative multi-agent
systems. The goal is to simultaneously learn the optimal grouping and agent
policy. By employing a hierarchical RL framework, we distinguish between
high-level decisions of grouping and low-level agents' actions. Our approach
utilizes the CTDE (Centralized Training with Decentralized Execution) paradigm,
ensuring efficient learning and scalable execution. We incorporate
permutation-invariant neural networks to handle the homogeneity and cooperation
among agents, enabling effective coordination. The option-critic algorithm is
adapted to manage the hierarchical decision-making process, allowing for
dynamic and optimal policy adjustments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.02368v1' target='_blank'>Enhancing Workplace Productivity and Well-being Using AI Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ravirajan K, Arvind Sundarajan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-04 20:11:00</h6>
<p class='card-text'>This paper discusses the use of Artificial Intelligence (AI) to enhance
workplace productivity and employee well-being. By integrating machine learning
(ML) techniques with neurobiological data, the proposed approaches ensure
alignment with human ethical standards through value alignment models and
Hierarchical Reinforcement Learning (HRL) for autonomous task management. The
system utilizes biometric feedback from employees to generate personalized
health prompts, fostering a supportive work environment that encourages
physical activity. Additionally, we explore decentralized multi-agent systems
for improved collaboration and decision-making frameworks that enhance
transparency. Various approaches using ML techniques in conjunction with AI
implementations are discussed. Together, these innovations aim to create a more
productive and health-conscious workplace. These outcomes assist HR management
and organizations in launching more rational career progression streams for
employees and facilitating organizational transformation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.19538v1' target='_blank'>Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot
  Task Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-27 09:07:11</h6>
<p class='card-text'>To improve the efficiency of warehousing system and meet huge customer
orders, we aim to solve the challenges of dimension disaster and dynamic
properties in hyper scale multi-robot task planning (MRTP) for robotic mobile
fulfillment system (RMFS). Existing research indicates that hierarchical
reinforcement learning (HRL) is an effective method to reduce these challenges.
Based on that, we construct an efficient multi-stage HRL-based multi-robot task
planner for hyper scale MRTP in RMFS, and the planning process is represented
with a special temporal graph topology. To ensure optimality, the planner is
designed with a centralized architecture, but it also brings the challenges of
scaling up and generalization that require policies to maintain performance for
various unlearned scales and maps. To tackle these difficulties, we first
construct a hierarchical temporal attention network (HTAN) to ensure basic
ability of handling inputs with unfixed lengths, and then design multi-stage
curricula for hierarchical policy learning to further improve the scaling up
and generalization ability while avoiding catastrophic forgetting.
Additionally, we notice that policies with hierarchical structure suffer from
unfair credit assignment that is similar to that in multi-agent reinforcement
learning, inspired of which, we propose a hierarchical reinforcement learning
algorithm with counterfactual rollout baseline to improve learning performance.
Experimental results demonstrate that our planner outperform other
state-of-the-art methods on various MRTP instances in both simulated and
real-world RMFS. Also, our planner can successfully scale up to hyper scale
MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on
unlearned maps while keeping superior performance over other methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.16395v1' target='_blank'>Autonomous Option Invention for Continual Hierarchical Reinforcement
  Learning and Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rashmeet Kaur Nayyar, Siddharth Srivastava</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-20 23:04:52</h6>
<p class='card-text'>Abstraction is key to scaling up reinforcement learning (RL). However,
autonomously learning abstract state and action representations to enable
transfer and generalization remains a challenging open problem. This paper
presents a novel approach for inventing, representing, and utilizing options,
which represent temporally extended behaviors, in continual RL settings. Our
approach addresses streams of stochastic problems characterized by long
horizons, sparse rewards, and unknown transition and reward functions.
  Our approach continually learns and maintains an interpretable state
abstraction, and uses it to invent high-level options with abstract symbolic
representations. These options meet three key desiderata: (1) composability for
solving tasks effectively with lookahead planning, (2) reusability across
problem instances for minimizing the need for relearning, and (3) mutual
independence for reducing interference among options. Our main contributions
are approaches for continually learning transferable, generalizable options
with symbolic representations, and for integrating search techniques with RL to
efficiently plan over these learned options to solve new problems. Empirical
results demonstrate that the resulting approach effectively learns and
transfers abstract knowledge across problem instances, achieving superior
sample efficiency compared to state-of-the-art methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.17854v1' target='_blank'>Active Geospatial Search for Efficient Tenant Eviction Outreach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anindya Sarkar, Alex DiChristofano, Sanmay Das, Patrick J. Fowler, Nathan Jacobs, Yevgeniy Vorobeychik</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-19 23:40:36</h6>
<p class='card-text'>Tenant evictions threaten housing stability and are a major concern for many
cities. An open question concerns whether data-driven methods enhance outreach
programs that target at-risk tenants to mitigate their risk of eviction. We
propose a novel active geospatial search (AGS) modeling framework for this
problem. AGS integrates property-level information in a search policy that
identifies a sequence of rental units to canvas to both determine their
eviction risk and provide support if needed. We propose a hierarchical
reinforcement learning approach to learn a search policy for AGS that scales to
large urban areas containing thousands of parcels, balancing exploration and
exploitation and accounting for travel costs and a budget constraint.
Crucially, the search policy adapts online to newly discovered information
about evictions. Evaluation using eviction data for a large urban area
demonstrates that the proposed framework and algorithmic approach are
considerably more effective at sequentially identifying eviction cases than
baseline methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.14584v1' target='_blank'>Simulation-Free Hierarchical Latent Policy Planning for Proactive
  Dialogues</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Yiheng Sun, Zerui Chen, Ming Liu, Bing Qin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-19 07:06:01</h6>
<p class='card-text'>Recent advancements in proactive dialogues have garnered significant
attention, particularly for more complex objectives (e.g. emotion support and
persuasion). Unlike traditional task-oriented dialogues, proactive dialogues
demand advanced policy planning and adaptability, requiring rich scenarios and
comprehensive policy repositories to develop such systems. However, existing
approaches tend to rely on Large Language Models (LLMs) for user simulation and
online learning, leading to biases that diverge from realistic scenarios and
result in suboptimal efficiency. Moreover, these methods depend on manually
defined, context-independent, coarse-grained policies, which not only incur
high expert costs but also raise concerns regarding their completeness. In our
work, we highlight the potential for automatically discovering policies
directly from raw, real-world dialogue records. To this end, we introduce a
novel dialogue policy planning framework, LDPP. It fully automates the process
from mining policies in dialogue records to learning policy planning.
Specifically, we employ a variant of the Variational Autoencoder to discover
fine-grained policies represented as latent vectors. After automatically
annotating the data with these latent policy labels, we propose an Offline
Hierarchical Reinforcement Learning (RL) algorithm in the latent space to
develop effective policy planning capabilities. Our experiments demonstrate
that LDPP outperforms existing methods on two proactive scenarios, even
surpassing ChatGPT with only a 1.8-billion-parameter LLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.02998v2' target='_blank'>Accelerating Task Generalisation with Multi-Level Hierarchical Options</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas P Cannon, Özgür Simsek</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-05 11:00:09</h6>
<p class='card-text'>Creating reinforcement learning agents that generalise effectively to new
tasks is a key challenge in AI research. This paper introduces Fracture Cluster
Options (FraCOs), a multi-level hierarchical reinforcement learning method that
achieves state-of-the-art performance on difficult generalisation tasks. FraCOs
identifies patterns in agent behaviour and forms options based on the expected
future usefulness of those patterns, enabling rapid adaptation to new tasks. In
tabular settings, FraCOs demonstrates effective transfer and improves
performance as it grows in hierarchical depth. We evaluate FraCOs against
state-of-the-art deep reinforcement learning algorithms in several complex
procedurally generated environments. Our results show that FraCOs achieves
higher in-distribution and out-of-distribution performance than competitors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.01184v1' target='_blank'>Guiding Multi-agent Multi-task Reinforcement Learning by a Hierarchical
  Framework with Logical Reward Shaping</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chanjuan Liu, Jinmiao Cong, Bingcai Chen, Yaochu Jin, Enqiang Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-02 09:03:23</h6>
<p class='card-text'>Multi-agent hierarchical reinforcement learning (MAHRL) has been studied as
an effective means to solve intelligent decision problems in complex and
large-scale environments. However, most current MAHRL algorithms follow the
traditional way of using reward functions in reinforcement learning, which
limits their use to a single task. This study aims to design a multi-agent
cooperative algorithm with logic reward shaping (LRS), which uses a more
flexible way of setting the rewards, allowing for the effective completion of
multi-tasks. LRS uses Linear Temporal Logic (LTL) to express the internal logic
relation of subtasks within a complex task. Then, it evaluates whether the
subformulae of the LTL expressions are satisfied based on a designed reward
structure. This helps agents to learn to effectively complete tasks by adhering
to the LTL expressions, thus enhancing the interpretability and credibility of
their decisions. To enhance coordination and cooperation among multiple agents,
a value iteration technique is designed to evaluate the actions taken by each
agent. Based on this evaluation, a reward function is shaped for coordination,
which enables each agent to evaluate its status and complete the remaining
subtasks through experiential learning. Experiments have been conducted on
various types of tasks in the Minecraft-like environment. The results
demonstrate that the proposed algorithm can improve the performance of
multi-agents when learning to complete multi-tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.00361v1' target='_blank'>Hierarchical Preference Optimization: Learning to achieve goals via
  feasible subgoals prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Utsav Singh, Souradip Chakraborty, Wesley A. Suttle, Brian M. Sadler, Anit Kumar Sahu, Mubarak Shah, Vinay P. Namboodiri, Amrit Singh Bedi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-01 04:58:40</h6>
<p class='card-text'>This work introduces Hierarchical Preference Optimization (HPO), a novel
approach to hierarchical reinforcement learning (HRL) that addresses
non-stationarity and infeasible subgoal generation issues when solving complex
robotic control tasks. HPO leverages maximum entropy reinforcement learning
combined with token-level Direct Preference Optimization (DPO), eliminating the
need for pre-trained reference policies that are typically unavailable in
challenging robotic scenarios. Mathematically, we formulate HRL as a bi-level
optimization problem and transform it into a primitive-regularized DPO
formulation, ensuring feasible subgoal generation and avoiding degenerate
solutions. Extensive experiments on challenging robotic navigation and
manipulation tasks demonstrate impressive performance of HPO, where it shows an
improvement of up to 35% over the baselines. Furthermore, ablation studies
validate our design choices, and quantitative analyses confirm the ability of
HPO to mitigate non-stationarity and infeasible subgoal generation issues in
HRL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.24089v1' target='_blank'>Demystifying Linear MDPs and Novel Dynamics Aggregation Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joongkyu Lee, Min-hwan Oh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-31 16:21:41</h6>
<p class='card-text'>In this work, we prove that, in linear MDPs, the feature dimension $d$ is
lower bounded by $S/U$ in order to aptly represent transition probabilities,
where $S$ is the size of the state space and $U$ is the maximum size of
directly reachable states. Hence, $d$ can still scale with $S$ depending on the
direct reachability of the environment. To address this limitation of linear
MDPs, we propose a novel structural aggregation framework based on dynamics,
named as the "dynamics aggregation". For this newly proposed framework, we
design a provably efficient hierarchical reinforcement learning algorithm in
linear function approximation that leverages aggregated sub-structures. Our
proposed algorithm exhibits statistical efficiency, achieving a regret of $
\tilde{O} ( d_{\psi}^{3/2} H^{3/2}\sqrt{ N T} )$, where $d_{\psi}$ represents
the feature dimension of aggregated subMDPs and $N$ signifies the number of
aggregated subMDPs. We establish that the condition $d_{\psi}^3 N \ll d^{3}$ is
readily met in most real-world environments with hierarchical structures,
enabling a substantial improvement in the regret bound compared to LSVI-UCB,
which enjoys a regret of $ \tilde{O} (d^{3/2} H^{3/2} \sqrt{ T})$. To the best
of our knowledge, this work presents the first HRL algorithm with linear
function approximation that offers provable guarantees.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.23156v2' target='_blank'>VisualPredicator: Learning Abstract World Models with Neuro-Symbolic
  Predicates for Robot Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B. Tenenbaum, Tom Silver, João F. Henriques, Kevin Ellis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-30 16:11:05</h6>
<p class='card-text'>Broadly intelligent agents should form task-specific abstractions that
selectively expose the essential elements of a task, while abstracting away the
complexity of the raw sensorimotor space. In this work, we present
Neuro-Symbolic Predicates, a first-order abstraction language that combines the
strengths of symbolic and neural knowledge representations. We outline an
online algorithm for inventing such predicates and learning abstract world
models. We compare our approach to hierarchical reinforcement learning,
vision-language model planning, and symbolic predicate invention approaches, on
both in- and out-of-distribution tasks across five simulated robotic domains.
Results show that our approach offers better sample complexity, stronger
out-of-distribution generalization, and improved interpretability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.20180v2' target='_blank'>Copyright-Aware Incentive Scheme for Generative Art Models Using
  Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuan Shi, Yifei Song, Xiaoli Tang, Lingjuan Lyu, Boi Faltings</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-26 13:29:43</h6>
<p class='card-text'>Generative art using Diffusion models has achieved remarkable performance in
image generation and text-to-image tasks. However, the increasing demand for
training data in generative art raises significant concerns about copyright
infringement, as models can produce images highly similar to copyrighted works.
Existing solutions attempt to mitigate this by perturbing Diffusion models to
reduce the likelihood of generating such images, but this often compromises
model performance. Another approach focuses on economically compensating data
holders for their contributions, yet it fails to address copyright loss
adequately. Our approach begin with the introduction of a novel copyright
metric grounded in copyright law and court precedents on infringement. We then
employ the TRAK method to estimate the contribution of data holders. To
accommodate the continuous data collection process, we divide the training into
multiple rounds. Finally, We designed a hierarchical budget allocation method
based on reinforcement learning to determine the budget for each round and the
remuneration of the data holder based on the data holder's contribution and
copyright loss in each round. Extensive experiments across three datasets show
that our method outperforms all eight benchmarks, demonstrating its
effectiveness in optimizing budget distribution in a copyright-aware manner. To
the best of our knowledge, this is the first technical work that introduces to
incentive contributors and protect their copyrights by compensating them.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.14927v1' target='_blank'>Hierarchical Reinforced Trader (HRT): A Bi-Level Approach for Optimizing
  Stock Selection and Execution</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijie Zhao, Roy E. Welsch</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-19 01:29:38</h6>
<p class='card-text'>Leveraging Deep Reinforcement Learning (DRL) in automated stock trading has
shown promising results, yet its application faces significant challenges,
including the curse of dimensionality, inertia in trading actions, and
insufficient portfolio diversification. Addressing these challenges, we
introduce the Hierarchical Reinforced Trader (HRT), a novel trading strategy
employing a bi-level Hierarchical Reinforcement Learning framework. The HRT
integrates a Proximal Policy Optimization (PPO)-based High-Level Controller
(HLC) for strategic stock selection with a Deep Deterministic Policy Gradient
(DDPG)-based Low-Level Controller (LLC) tasked with optimizing trade executions
to enhance portfolio value. In our empirical analysis, comparing the HRT agent
with standalone DRL models and the S&P 500 benchmark during both bullish and
bearish market conditions, we achieve a positive and higher Sharpe ratio. This
advancement not only underscores the efficacy of incorporating hierarchical
structures into DRL strategies but also mitigates the aforementioned
challenges, paving the way for designing more profitable and robust trading
algorithms in complex markets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.13979v2' target='_blank'>RecoveryChaining: Learning Local Recovery Policies for Robust
  Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shivam Vats, Devesh K. Jha, Maxim Likhachev, Oliver Kroemer, Diego Romeres</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-17 19:14:43</h6>
<p class='card-text'>Model-based planners and controllers are commonly used to solve complex
manipulation problems as they can efficiently optimize diverse objectives and
generalize to long horizon tasks. However, they often fail during deployment
due to noisy actuation, partial observability and imperfect models. To enable a
robot to recover from such failures, we propose to use hierarchical
reinforcement learning to learn a recovery policy. The recovery policy is
triggered when a failure is detected based on sensory observations and seeks to
take the robot to a state from which it can complete the task using the nominal
model-based controllers. Our approach, called RecoveryChaining, uses a hybrid
action space, where the model-based controllers are provided as additional
\emph{nominal} options which allows the recovery policy to decide how to
recover, when to switch to a nominal controller and which controller to switch
to even with \emph{sparse rewards}. We evaluate our approach in three
multi-step manipulation tasks with sparse rewards, where it learns
significantly more robust recovery policies than those learned by baselines. We
successfully transfer recovery policies learned in simulation to a physical
robot to demonstrate the feasibility of sim-to-real transfer with our method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.11251v1' target='_blank'>Disentangled Unsupervised Skill Discovery for Efficient Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiaheng Hu, Zizhao Wang, Peter Stone, Roberto Martín-Martín</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-15 04:13:20</h6>
<p class='card-text'>A hallmark of intelligent agents is the ability to learn reusable skills
purely from unsupervised interaction with the environment. However, existing
unsupervised skill discovery methods often learn entangled skills where one
skill variable simultaneously influences many entities in the environment,
making downstream skill chaining extremely challenging. We propose Disentangled
Unsupervised Skill Discovery (DUSDi), a method for learning disentangled skills
that can be efficiently reused to solve downstream tasks. DUSDi decomposes
skills into disentangled components, where each skill component only affects
one factor of the state space. Importantly, these skill components can be
concurrently composed to generate low-level actions, and efficiently chained to
tackle downstream tasks through hierarchical Reinforcement Learning. DUSDi
defines a novel mutual-information-based objective to enforce disentanglement
between the influences of different skill components, and utilizes value
factorization to optimize this objective efficiently. Evaluated in a set of
challenging environments, DUSDi successfully learns disentangled skills, and
significantly outperforms previous skill discovery methods when it comes to
applying the learned skills to solve downstream tasks. Code and skills
visualization at jiahenghu.github.io/DUSDi-site/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09505v1' target='_blank'>HG2P: Hippocampus-inspired High-reward Graph and Model-Free Q-Gradient
  Penalty for Path Planning and Motion Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoran Wang, Yaoru Sun, Zeshen Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-12 11:46:31</h6>
<p class='card-text'>Goal-conditioned hierarchical reinforcement learning (HRL) decomposes complex
reaching tasks into a sequence of simple subgoal-conditioned tasks, showing
significant promise for addressing long-horizon planning in large-scale
environments. This paper bridges the goal-conditioned HRL based on graph-based
planning to brain mechanisms, proposing a hippocampus-striatum-like
dual-controller hypothesis. Inspired by the brain mechanisms of organisms
(i.e., the high-reward preferences observed in hippocampal replay) and
instance-based theory, we propose a high-return sampling strategy for
constructing memory graphs, improving sample efficiency. Additionally, we
derive a model-free lower-level Q-function gradient penalty to resolve the
model dependency issues present in prior work, improving the generalization of
Lipschitz constraints in applications. Finally, we integrate these two
extensions, High-reward Graph and model-free Gradient Penalty (HG2P), into the
state-of-the-art framework ACLG, proposing a novel goal-conditioned HRL
framework, HG2P+ACLG. Experimentally, the results demonstrate that our method
outperforms state-of-the-art goal-conditioned HRL algorithms on a variety of
long-horizon navigation tasks and robotic manipulation tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.08997v2' target='_blank'>Hierarchical Universal Value Function Approximators</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rushiv Arora</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-11 17:09:26</h6>
<p class='card-text'>There have been key advancements to building universal approximators for
multi-goal collections of reinforcement learning value functions -- key
elements in estimating long-term returns of states in a parameterized manner.
We extend this to hierarchical reinforcement learning, using the options
framework, by introducing hierarchical universal value function approximators
(H-UVFAs). This allows us to leverage the added benefits of scaling, planning,
and generalization expected in temporal abstraction settings. We develop
supervised and reinforcement learning methods for learning embeddings of the
states, goals, options, and actions in the two hierarchical value functions:
$Q(s, g, o; \theta)$ and $Q(s, g, o, a; \theta)$. Finally we demonstrate
generalization of the HUVFAs and show they outperform corresponding UVFAs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.07933v1' target='_blank'>Offline Hierarchical Reinforcement Learning via Inverse Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Carolin Schmidt, Daniele Gammelli, James Harrison, Marco Pavone, Filipe Rodrigues</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-10 14:00:21</h6>
<p class='card-text'>Hierarchical policies enable strong performance in many sequential
decision-making problems, such as those with high-dimensional action spaces,
those requiring long-horizon planning, and settings with sparse rewards.
However, learning hierarchical policies from static offline datasets presents a
significant challenge. Crucially, actions taken by higher-level policies may
not be directly observable within hierarchical controllers, and the offline
dataset might have been generated using a different policy structure, hindering
the use of standard offline learning algorithms. In this work, we propose OHIO:
a framework for offline reinforcement learning (RL) of hierarchical policies.
Our framework leverages knowledge of the policy structure to solve the inverse
problem, recovering the unobservable high-level actions that likely generated
the observed data under our hierarchical policy. This approach constructs a
dataset suitable for off-the-shelf offline training. We demonstrate our
framework on robotic and network optimization problems and show that it
substantially outperforms end-to-end RL methods and improves robustness. We
investigate a variety of instantiations of our framework, both in direct
deployment of policies trained offline and when online fine-tuning is
performed.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>