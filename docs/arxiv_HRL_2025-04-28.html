<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>HRL - 2025-04-28</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>HRL - 2025-04-28</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.17356v1' target='_blank'>Comprehend, Divide, and Conquer: Feature Subspace Exploration via
  Multi-Agent Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weiliang Zhang, Xiaohan Huang, Yi Du, Ziyue Qiao, Qingqing Long, Zhen Meng, Yuanchun Zhou, Meng Xiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-24 08:16:36</h6>
<p class='card-text'>Feature selection aims to preprocess the target dataset, find an optimal and
most streamlined feature subset, and enhance the downstream machine learning
task. Among filter, wrapper, and embedded-based approaches, the reinforcement
learning (RL)-based subspace exploration strategy provides a novel objective
optimization-directed perspective and promising performance. Nevertheless, even
with improved performance, current reinforcement learning approaches face
challenges similar to conventional methods when dealing with complex datasets.
These challenges stem from the inefficient paradigm of using one agent per
feature and the inherent complexities present in the datasets. This observation
motivates us to investigate and address the above issue and propose a novel
approach, namely HRLFS. Our methodology initially employs a Large Language
Model (LLM)-based hybrid state extractor to capture each feature's mathematical
and semantic characteristics. Based on this information, features are
clustered, facilitating the construction of hierarchical agents for each
cluster and sub-cluster. Extensive experiments demonstrate the efficiency,
scalability, and robustness of our approach. Compared to contemporary or the
one-feature-one-agent RL-based approaches, HRLFS improves the downstream ML
performance with iterative feature subspace exploration while accelerating
total run time by reducing the number of agents involved.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.15876v2' target='_blank'>Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement
  Learning for Strategic Confrontation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qizhen Wu, Lei Chen, Kexin Liu, Jinhu Lü</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-22 13:22:58</h6>
<p class='card-text'>In swarm robotics, confrontation scenarios, including strategic
confrontations, require efficient decision-making that integrates discrete
commands and continuous actions. Traditional task and motion planning methods
separate decision-making into two layers, but their unidirectional structure
fails to capture the interdependence between these layers, limiting
adaptability in dynamic environments. Here, we propose a novel bidirectional
approach based on hierarchical reinforcement learning, enabling dynamic
interaction between the layers. This method effectively maps commands to task
allocation and actions to path planning, while leveraging cross-training
techniques to enhance learning across the hierarchical framework. Furthermore,
we introduce a trajectory prediction model that bridges abstract task
representations with actionable planning goals. In our experiments, it achieves
over 80% in confrontation win rate and under 0.01 seconds in decision time,
outperforming existing approaches. Demonstrations through large-scale tests and
real-world robot experiments further emphasize the generalization capabilities
and practical applicability of our method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.14989v1' target='_blank'>Dynamic Legged Ball Manipulation on Rugged Terrains with Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongjie Zhu, Zhuo Yang, Tianhang Wu, Luzhou Ge, Xuesong Li, Qi Liu, Xiang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-21 09:38:38</h6>
<p class='card-text'>Advancing the dynamic loco-manipulation capabilities of quadruped robots in
complex terrains is crucial for performing diverse tasks. Specifically, dynamic
ball manipulation in rugged environments presents two key challenges. The first
is coordinating distinct motion modalities to integrate terrain traversal and
ball control seamlessly. The second is overcoming sparse rewards in end-to-end
deep reinforcement learning, which impedes efficient policy convergence. To
address these challenges, we propose a hierarchical reinforcement learning
framework. A high-level policy, informed by proprioceptive data and ball
position, adaptively switches between pre-trained low-level skills such as ball
dribbling and rough terrain navigation. We further propose Dynamic
Skill-Focused Policy Optimization to suppress gradients from inactive skills
and enhance critical skill learning. Both simulation and real-world experiments
validate that our methods outperform baseline approaches in dynamic ball
manipulation across rugged terrains, highlighting its effectiveness in
challenging environments. Videos are on our website: dribble-hrl.github.io.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.05553v1' target='_blank'>Federated Hierarchical Reinforcement Learning for Adaptive Traffic
  Signal Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongjie Fu, Lingyun Zhong, Zifan Li, Xuan Di</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-07 23:02:59</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) has shown promise for adaptive
traffic signal control (ATSC), enabling multiple intersections to coordinate
signal timings in real time. However, in large-scale settings, MARL faces
constraints due to extensive data sharing and communication requirements.
Federated learning (FL) mitigates these challenges by training shared models
without directly exchanging raw data, yet traditional FL methods such as FedAvg
struggle with highly heterogeneous intersections. Different intersections
exhibit varying traffic patterns, demands, and road structures, so performing
FedAvg across all agents is inefficient. To address this gap, we propose
Hierarchical Federated Reinforcement Learning (HFRL) for ATSC. HFRL employs
clustering-based or optimization-based techniques to dynamically group
intersections and perform FedAvg independently within groups of intersections
with similar characteristics, enabling more effective coordination and
scalability than standard FedAvg. Our experiments on synthetic and real-world
traffic networks demonstrate that HFRL not only outperforms both decentralized
and standard federated RL approaches but also identifies suitable grouping
patterns based on network structure or traffic demand, resulting in a more
robust framework for distributed, heterogeneous systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.04366v1' target='_blank'>Solving Sokoban using Hierarchical Reinforcement Learning with Landmarks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sergey Pastukhov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-06 05:30:21</h6>
<p class='card-text'>We introduce a novel hierarchical reinforcement learning (HRL) framework that
performs top-down recursive planning via learned subgoals, successfully applied
to the complex combinatorial puzzle game Sokoban. Our approach constructs a
six-level policy hierarchy, where each higher-level policy generates subgoals
for the level below. All subgoals and policies are learned end-to-end from
scratch, without any domain knowledge. Our results show that the agent can
generate long action sequences from a single high-level call. While prior work
has explored 2-3 level hierarchies and subgoal-based planning heuristics, we
demonstrate that deep recursive goal decomposition can emerge purely from
learning, and that such hierarchies can scale effectively to hard puzzle
domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.21677v1' target='_blank'>A tale of two goals: leveraging sequentiality in multi-goal scenarios</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Olivier Serris, Stéphane Doncieux, Olivier Sigaud</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-27 16:47:46</h6>
<p class='card-text'>Several hierarchical reinforcement learning methods leverage planning to
create a graph or sequences of intermediate goals, guiding a lower-level
goal-conditioned (GC) policy to reach some final goals. The low-level policy is
typically conditioned on the current goal, with the aim of reaching it as
quickly as possible. However, this approach can fail when an intermediate goal
can be reached in multiple ways, some of which may make it impossible to
continue toward subsequent goals. To address this issue, we introduce two
instances of Markov Decision Process (MDP) where the optimization objective
favors policies that not only reach the current goal but also subsequent ones.
In the first, the agent is conditioned on both the current and final goals,
while in the second, it is conditioned on the next two goals in the sequence.
We conduct a series of experiments on navigation and pole-balancing tasks in
which sequences of intermediate goals are given. By evaluating policies trained
with TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that,
in most cases, conditioning on the next two goals improves stability and sample
efficiency over other approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19007v1' target='_blank'>Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chak Lam Shek, Pratap Tokekar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 15:49:56</h6>
<p class='card-text'>Large Language Models (LLMs) have shown remarkable promise in reasoning and
decision-making, yet their integration with Reinforcement Learning (RL) for
complex robotic tasks remains underexplored. In this paper, we propose an
LLM-guided hierarchical RL framework, termed LDSC, that leverages LLM-driven
subgoal selection and option reuse to enhance sample efficiency,
generalization, and multi-task adaptability. Traditional RL methods often
suffer from inefficient exploration and high computational cost. Hierarchical
RL helps with these challenges, but existing methods often fail to reuse
options effectively when faced with new tasks. To address these limitations, we
introduce a three-stage framework that uses LLMs for subgoal generation given
natural language description of the task, a reusable option learning and
selection method, and an action-level policy, enabling more effective
decision-making across diverse tasks. By incorporating LLMs for subgoal
prediction and policy guidance, our approach improves exploration efficiency
and enhances learning performance. On average, LDSC outperforms the baseline by
55.9\% in average reward, demonstrating its effectiveness in complex RL
settings. More details and experiment videos could be found in
\href{https://raaslab.org/projects/LDSC/}{this
link\footnote{https://raaslab.org/projects/LDSC}}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.14809v1' target='_blank'>Learning with Expert Abstractions for Efficient Multi-Task Continuous
  Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jeff Jewett, Sandhya Saisubramanian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-19 00:44:23</h6>
<p class='card-text'>Decision-making in complex, continuous multi-task environments is often
hindered by the difficulty of obtaining accurate models for planning and the
inefficiency of learning purely from trial and error. While precise environment
dynamics may be hard to specify, human experts can often provide high-fidelity
abstractions that capture the essential high-level structure of a task and user
preferences in the target environment. Existing hierarchical approaches often
target discrete settings and do not generalize across tasks. We propose a
hierarchical reinforcement learning approach that addresses these limitations
by dynamically planning over the expert-specified abstraction to generate
subgoals to learn a goal-conditioned policy. To overcome the challenges of
learning under sparse rewards, we shape the reward based on the optimal state
value in the abstract model. This structured decision-making process enhances
sample efficiency and facilitates zero-shot generalization. Our empirical
evaluation on a suite of procedurally generated continuous control environments
demonstrates that our approach outperforms existing hierarchical reinforcement
learning methods in terms of sample efficiency, task completion rate,
scalability to complex tasks, and generalization to novel scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.12036v1' target='_blank'>Hierarchical Reinforcement Learning for Safe Mapless Navigation with
  Congestion Estimation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianqi Gao, Xizheng Pang, Qi Liu, Yanjie Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-15 08:03:50</h6>
<p class='card-text'>Reinforcement learning-based mapless navigation holds significant potential.
However, it faces challenges in indoor environments with local minima area.
This paper introduces a safe mapless navigation framework utilizing
hierarchical reinforcement learning (HRL) to enhance navigation through such
areas. The high-level policy creates a sub-goal to direct the navigation
process. Notably, we have developed a sub-goal update mechanism that considers
environment congestion, efficiently avoiding the entrapment of the robot in
local minimum areas. The low-level motion planning policy, trained through safe
reinforcement learning, outputs real-time control instructions based on
acquired sub-goal. Specifically, to enhance the robot's environmental
perception, we introduce a new obstacle encoding method that evaluates the
impact of obstacles on the robot's motion planning. To validate the performance
of our HRL-based navigation framework, we conduct simulations in office, home,
and restaurant environments. The findings demonstrate that our HRL-based
navigation framework excels in both static and dynamic scenarios. Finally, we
implement the HRL-based navigation framework on a TurtleBot3 robot for physical
validation experiments, which exhibits its strong generalization capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06309v1' target='_blank'>On the Fly Adaptation of Behavior Tree-Based Policies through
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marco Iannotta, Johannes A. Stork, Erik Schaffernicht, Todor Stoyanov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 18:56:22</h6>
<p class='card-text'>With the rising demand for flexible manufacturing, robots are increasingly
expected to operate in dynamic environments where local -- such as slight
offsets or size differences in workpieces -- are common. We propose to address
the problem of adapting robot behaviors to these task variations with a
sample-efficient hierarchical reinforcement learning approach adapting Behavior
Tree (BT)-based policies. We maintain the core BT properties as an
interpretable, modular framework for structuring reactive behaviors, but extend
their use beyond static tasks by inherently accommodating local task
variations. To show the efficiency and effectiveness of our approach, we
conduct experiments both in simulation and on a Franka Emika Panda 7-DoF, with
the manipulator adapting to different obstacle avoidance and pivoting tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20380v1' target='_blank'>Multi-Turn Code Generation Through Single-Step Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arnav Kumar Jain, Gonzalo Gonzalez-Pumariega, Wayne Chen, Alexander M Rush, Wenting Zhao, Sanjiban Choudhury</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:55:05</h6>
<p class='card-text'>We address the problem of code generation from multi-turn execution feedback.
Existing methods either generate code without feedback or use complex,
hierarchical reinforcement learning to optimize multi-turn rewards. We propose
a simple yet scalable approach, $\mu$Code, that solves multi-turn code
generation using only single-step rewards. Our key insight is that code
generation is a one-step recoverable MDP, where the correct code can be
recovered from any intermediate code state in a single turn. $\mu$Code
iteratively trains both a generator to provide code solutions conditioned on
multi-turn execution feedback and a verifier to score the newly generated code.
Experimental evaluations show that our approach achieves significant
improvements over the state-of-the-art baselines. We provide analysis of the
design choices of the reward models and policy, and show the efficacy of
$\mu$Code at utilizing the execution feedback. Our code is available at
https://github.com/portal-cornell/muCode.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.15425v4' target='_blank'>TAG: A Decentralized Framework for Multi-Agent Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Giuseppe Paolo, Abdelhakim Benechehab, Hamza Cherkaoui, Albert Thomas, Balázs Kégl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-21 12:52:16</h6>
<p class='card-text'>Hierarchical organization is fundamental to biological systems and human
societies, yet artificial intelligence systems often rely on monolithic
architectures that limit adaptability and scalability. Current hierarchical
reinforcement learning (HRL) approaches typically restrict hierarchies to two
levels or require centralized training, which limits their practical
applicability. We introduce TAME Agent Framework (TAG), a framework for
constructing fully decentralized hierarchical multi-agent systems. TAG enables
hierarchies of arbitrary depth through a novel LevelEnv concept, which
abstracts each hierarchy level as the environment for the agents above it. This
approach standardizes information flow between levels while preserving loose
coupling, allowing for seamless integration of diverse agent types. We
demonstrate the effectiveness of TAG by implementing hierarchical architectures
that combine different RL agents across multiple levels, achieving improved
performance over classical multi-agent RL baselines on standard benchmarks. Our
results show that decentralized hierarchical organization enhances both
learning speed and final performance, positioning TAG as a promising direction
for scalable multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06772v2' target='_blank'>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 18:51:47</h6>
<p class='card-text'>We present that hierarchical LLM reasoning via scaling thought templates can
effectively optimize the reasoning search space and outperform the mathematical
reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.
We train our ReasonFlux-32B model with only 8 GPUs and introduces three
innovations: (i) a structured and generic thought template library, containing
around 500 high-level thought templates capable of generalizing to similar or
relevant reasoning problems; (ii) performing hierarchical reinforcement
learning on a sequence of thought templates instead of long CoTs, optimizing a
base LLM to plan out an optimal template trajectory for gradually handling
complex problems; (iii) a brand new inference scaling system that enables
hierarchical LLM reasoning by adaptively scaling thought templates at inference
time. With a template trajectory containing more explainable reasoning
structures than DeepSeek-R1 and o3-mini, our ReasonFlux-32B significantly
advances math reasoning capabilities to state-of-the-art levels. Notably, on
the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview
by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an
average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and
45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05537v1' target='_blank'>Sequential Stochastic Combinatorial Optimization Using Hierarchal
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinsong Feng, Zihan Yu, Yanhai Xiong, Haipeng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 12:00:30</h6>
<p class='card-text'>Reinforcement learning (RL) has emerged as a promising tool for combinatorial
optimization (CO) problems due to its ability to learn fast, effective, and
generalizable solutions. Nonetheless, existing works mostly focus on one-shot
deterministic CO, while sequential stochastic CO (SSCO) has rarely been studied
despite its broad applications such as adaptive influence maximization (IM) and
infectious disease intervention. In this paper, we study the SSCO problem where
we first decide the budget (e.g., number of seed nodes in adaptive IM)
allocation for all time steps, and then select a set of nodes for each time
step. The few existing studies on SSCO simplify the problems by assuming a
uniformly distributed budget allocation over the time horizon, yielding
suboptimal solutions. We propose a generic hierarchical RL (HRL) framework
called wake-sleep option (WS-option), a two-layer option-based framework that
simultaneously decides adaptive budget allocation on the higher layer and node
selection on the lower layer. WS-option starts with a coherent formulation of
the two-layer Markov decision processes (MDPs), capturing the interdependencies
between the two layers of decisions. Building on this, WS-option employs
several innovative designs to balance the model's training stability and
computational efficiency, preventing the vicious cyclic interference issue
between the two layers. Empirical results show that WS-option exhibits
significantly improved effectiveness and generalizability compared to
traditional methods. Moreover, the learned model can be generalized to larger
graphs, which significantly reduces the overhead of computational resources.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03960v1' target='_blank'>Bilevel Multi-Armed Bandit-Based Hierarchical Reinforcement Learning for
  Interaction-Aware Self-Driving at Unsignalized Intersections</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zengqi Peng, Yubin Wang, Lei Zheng, Jun Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 10:50:59</h6>
<p class='card-text'>In this work, we present BiM-ACPPO, a bilevel multi-armed bandit-based
hierarchical reinforcement learning framework for interaction-aware
decision-making and planning at unsignalized intersections. Essentially, it
proactively takes the uncertainties associated with surrounding vehicles (SVs)
into consideration, which encompass those stemming from the driver's intention,
interactive behaviors, and the varying number of SVs. Intermediate decision
variables are introduced to enable the high-level RL policy to provide an
interaction-aware reference, for guiding low-level model predictive control
(MPC) and further enhancing the generalization ability of the proposed
framework. By leveraging the structured nature of self-driving at unsignalized
intersections, the training problem of the RL policy is modeled as a bilevel
curriculum learning task, which is addressed by the proposed Exp3.S-based BiMAB
algorithm. It is noteworthy that the training curricula are dynamically
adjusted, thereby facilitating the sample efficiency of the RL training
process. Comparative experiments are conducted in the high-fidelity CARLA
simulator, and the results indicate that our approach achieves superior
performance compared to all baseline methods. Furthermore, experimental results
in two new urban driving scenarios clearly demonstrate the commendable
generalization performance of the proposed method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01956v1' target='_blank'>DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement
  Learning Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shashank Sharma, Janina Hoffmann, Vinay Namboodiri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-04 03:05:55</h6>
<p class='card-text'>In this paper, we address the challenge of long-horizon visual planning tasks
using Hierarchical Reinforcement Learning (HRL). Our key contribution is a
Discrete Hierarchical Planning (DHP) method, an alternative to traditional
distance-based approaches. We provide theoretical foundations for the method
and demonstrate its effectiveness through extensive empirical evaluations.
  Our agent recursively predicts subgoals in the context of a long-term goal
and receives discrete rewards for constructing plans as compositions of
abstract actions. The method introduces a novel advantage estimation strategy
for tree trajectories, which inherently encourages shorter plans and enables
generalization beyond the maximum tree depth. The learned policy function
allows the agent to plan efficiently, requiring only $\log N$ computational
steps, making re-planning highly efficient. The agent, based on a soft-actor
critic (SAC) framework, is trained using on-policy imagination data.
Additionally, we propose a novel exploration strategy that enables the agent to
generate relevant training examples for the planning modules. We evaluate our
method on long-horizon visual planning tasks in a 25-room environment, where it
significantly outperforms previous benchmarks at success rate and average
episode length. Furthermore, an ablation study highlights the individual
contributions of key modules to the overall performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.17424v1' target='_blank'>Certificated Actor-Critic: Hierarchical Reinforcement Learning with
  Control Barrier Functions for Safe Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junjun Xie, Shuhao Zhao, Liang Hu, Huijun Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-29 05:37:47</h6>
<p class='card-text'>Control Barrier Functions (CBFs) have emerged as a prominent approach to
designing safe navigation systems of robots. Despite their popularity, current
CBF-based methods exhibit some limitations: optimization-based safe control
techniques tend to be either myopic or computationally intensive, and they rely
on simplified system models; conversely, the learning-based methods suffer from
the lack of quantitative indication in terms of navigation performance and
safety. In this paper, we present a new model-free reinforcement learning
algorithm called Certificated Actor-Critic (CAC), which introduces a
hierarchical reinforcement learning framework and well-defined reward functions
derived from CBFs. We carry out theoretical analysis and proof of our
algorithm, and propose several improvements in algorithm implementation. Our
analysis is validated by two simulation experiments, showing the effectiveness
of our proposed CAC algorithm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.14992v1' target='_blank'>Extensive Exploration in Complex Traffic Scenarios using Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhihao Zhang, Ekim Yurtsever, Keith A. Redmill</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-25 00:00:11</h6>
<p class='card-text'>Developing an automated driving system capable of navigating complex traffic
environments remains a formidable challenge. Unlike rule-based or supervised
learning-based methods, Deep Reinforcement Learning (DRL) based controllers
eliminate the need for domain-specific knowledge and datasets, thus providing
adaptability to various scenarios. Nonetheless, a common limitation of existing
studies on DRL-based controllers is their focus on driving scenarios with
simple traffic patterns, which hinders their capability to effectively handle
complex driving environments with delayed, long-term rewards, thus compromising
the generalizability of their findings. In response to these limitations, our
research introduces a pioneering hierarchical framework that efficiently
decomposes intricate decision-making problems into manageable and interpretable
subtasks. We adopt a two step training process that trains the high-level
controller and low-level controller separately. The high-level controller
exhibits an enhanced exploration potential with long-term delayed rewards, and
the low-level controller provides longitudinal and lateral control ability
using short-term instantaneous rewards. Through simulation experiments, we
demonstrate the superiority of our hierarchical controller in managing complex
highway driving situations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13084v1' target='_blank'>Attention-Driven Hierarchical Reinforcement Learning with Particle
  Filtering for Source Localization in Dynamic Fields</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiwei Shi, Mengyue Yang, Qi Zhang, Weinan Zhang, Cunjia Liu, Weiru Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 18:45:29</h6>
<p class='card-text'>In many real-world scenarios, such as gas leak detection or environmental
pollutant tracking, solving the Inverse Source Localization and
Characterization problem involves navigating complex, dynamic fields with
sparse and noisy observations. Traditional methods face significant challenges,
including partial observability, temporal and spatial dynamics,
out-of-distribution generalization, and reward sparsity. To address these
issues, we propose a hierarchical framework that integrates Bayesian inference
and reinforcement learning. The framework leverages an attention-enhanced
particle filtering mechanism for efficient and accurate belief updates, and
incorporates two complementary execution strategies: Attention Particle
Filtering Planning and Attention Particle Filtering Reinforcement Learning.
These approaches optimize exploration and adaptation under uncertainty.
Theoretical analysis proves the convergence of the attention-enhanced particle
filter, while extensive experiments across diverse scenarios validate the
framework's superior accuracy, adaptability, and computational efficiency. Our
results highlight the framework's potential for broad applications in dynamic
field estimation tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13132v1' target='_blank'>A Hierarchical Reinforcement Learning Framework for Multi-UAV Combat
  Using Leader-Follower Strategy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinhui Pang, Jinglin He, Noureldin Mohamed Abdelaal Ahmed Mohamed, Changqing Lin, Zhihui Zhang, Xiaoshuai Hao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 02:41:36</h6>
<p class='card-text'>Multi-UAV air combat is a complex task involving multiple autonomous UAVs, an
evolving field in both aerospace and artificial intelligence. This paper aims
to enhance adversarial performance through collaborative strategies. Previous
approaches predominantly discretize the action space into predefined actions,
limiting UAV maneuverability and complex strategy implementation. Others
simplify the problem to 1v1 combat, neglecting the cooperative dynamics among
multiple UAVs. To address the high-dimensional challenges inherent in
six-degree-of-freedom space and improve cooperation, we propose a hierarchical
framework utilizing the Leader-Follower Multi-Agent Proximal Policy
Optimization (LFMAPPO) strategy. Specifically, the framework is structured into
three levels. The top level conducts a macro-level assessment of the
environment and guides execution policy. The middle level determines the angle
of the desired action. The bottom level generates precise action commands for
the high-dimensional action space. Moreover, we optimize the state-value
functions by assigning distinct roles with the leader-follower strategy to
train the top-level policy, followers estimate the leader's utility, promoting
effective cooperation among agents. Additionally, the incorporation of a target
selector, aligned with the UAVs' posture, assesses the threat level of targets.
Finally, simulation experiments validate the effectiveness of our proposed
method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.07274v2' target='_blank'>Mining Intraday Risk Factor Collections via Hierarchical Reinforcement
  Learning based on Transferred Options</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenyan Xu, Jiayu Chen, Chen Li, Yonghong Hu, Zhonghua Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-13 12:38:05</h6>
<p class='card-text'>Traditional risk factors like beta, size/value, and momentum often lag behind
market dynamics in measuring and predicting stock return volatility.
Statistical models like PCA and factor analysis fail to capture hidden
nonlinear relationships. Genetic programming (GP) can identify nonlinear
factors but often lacks mechanisms for evaluating factor quality, and the
resulting formulas are complex. To address these challenges, we propose a
Hierarchical Proximal Policy Optimization (HPPO) framework for automated factor
generation and evaluation. HPPO uses two PPO models: a high-level policy
assigns weights to stock features, and a low-level policy identifies latent
nonlinear relationships. The Pearson correlation between generated factors and
return volatility serves as the reward signal. Transfer learning pre-trains the
high-level policy on large-scale historical data, fine-tuning it with the
latest data to adapt to new features and shifts. Experiments show the HPPO-TO
algorithm achieves a 25\% excess return in HFT markets across China (CSI
300/800), India (Nifty 100), and the US (S\&P 500). Code and data are available
at https://github.com/wencyxu/HRL-HF_risk_factor_set.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.06554v1' target='_blank'>Hierarchical Reinforcement Learning for Optimal Agent Grouping in
  Cooperative Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Liyuan Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-11 14:22:10</h6>
<p class='card-text'>This paper presents a hierarchical reinforcement learning (RL) approach to
address the agent grouping or pairing problem in cooperative multi-agent
systems. The goal is to simultaneously learn the optimal grouping and agent
policy. By employing a hierarchical RL framework, we distinguish between
high-level decisions of grouping and low-level agents' actions. Our approach
utilizes the CTDE (Centralized Training with Decentralized Execution) paradigm,
ensuring efficient learning and scalable execution. We incorporate
permutation-invariant neural networks to handle the homogeneity and cooperation
among agents, enabling effective coordination. The option-critic algorithm is
adapted to manage the hierarchical decision-making process, allowing for
dynamic and optimal policy adjustments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.02368v1' target='_blank'>Enhancing Workplace Productivity and Well-being Using AI Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ravirajan K, Arvind Sundarajan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-04 20:11:00</h6>
<p class='card-text'>This paper discusses the use of Artificial Intelligence (AI) to enhance
workplace productivity and employee well-being. By integrating machine learning
(ML) techniques with neurobiological data, the proposed approaches ensure
alignment with human ethical standards through value alignment models and
Hierarchical Reinforcement Learning (HRL) for autonomous task management. The
system utilizes biometric feedback from employees to generate personalized
health prompts, fostering a supportive work environment that encourages
physical activity. Additionally, we explore decentralized multi-agent systems
for improved collaboration and decision-making frameworks that enhance
transparency. Various approaches using ML techniques in conjunction with AI
implementations are discussed. Together, these innovations aim to create a more
productive and health-conscious workplace. These outcomes assist HR management
and organizations in launching more rational career progression streams for
employees and facilitating organizational transformation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.19538v1' target='_blank'>Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot
  Task Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-27 09:07:11</h6>
<p class='card-text'>To improve the efficiency of warehousing system and meet huge customer
orders, we aim to solve the challenges of dimension disaster and dynamic
properties in hyper scale multi-robot task planning (MRTP) for robotic mobile
fulfillment system (RMFS). Existing research indicates that hierarchical
reinforcement learning (HRL) is an effective method to reduce these challenges.
Based on that, we construct an efficient multi-stage HRL-based multi-robot task
planner for hyper scale MRTP in RMFS, and the planning process is represented
with a special temporal graph topology. To ensure optimality, the planner is
designed with a centralized architecture, but it also brings the challenges of
scaling up and generalization that require policies to maintain performance for
various unlearned scales and maps. To tackle these difficulties, we first
construct a hierarchical temporal attention network (HTAN) to ensure basic
ability of handling inputs with unfixed lengths, and then design multi-stage
curricula for hierarchical policy learning to further improve the scaling up
and generalization ability while avoiding catastrophic forgetting.
Additionally, we notice that policies with hierarchical structure suffer from
unfair credit assignment that is similar to that in multi-agent reinforcement
learning, inspired of which, we propose a hierarchical reinforcement learning
algorithm with counterfactual rollout baseline to improve learning performance.
Experimental results demonstrate that our planner outperform other
state-of-the-art methods on various MRTP instances in both simulated and
real-world RMFS. Also, our planner can successfully scale up to hyper scale
MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on
unlearned maps while keeping superior performance over other methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.16395v1' target='_blank'>Autonomous Option Invention for Continual Hierarchical Reinforcement
  Learning and Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rashmeet Kaur Nayyar, Siddharth Srivastava</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-20 23:04:52</h6>
<p class='card-text'>Abstraction is key to scaling up reinforcement learning (RL). However,
autonomously learning abstract state and action representations to enable
transfer and generalization remains a challenging open problem. This paper
presents a novel approach for inventing, representing, and utilizing options,
which represent temporally extended behaviors, in continual RL settings. Our
approach addresses streams of stochastic problems characterized by long
horizons, sparse rewards, and unknown transition and reward functions.
  Our approach continually learns and maintains an interpretable state
abstraction, and uses it to invent high-level options with abstract symbolic
representations. These options meet three key desiderata: (1) composability for
solving tasks effectively with lookahead planning, (2) reusability across
problem instances for minimizing the need for relearning, and (3) mutual
independence for reducing interference among options. Our main contributions
are approaches for continually learning transferable, generalizable options
with symbolic representations, and for integrating search techniques with RL to
efficiently plan over these learned options to solve new problems. Empirical
results demonstrate that the resulting approach effectively learns and
transfers abstract knowledge across problem instances, achieving superior
sample efficiency compared to state-of-the-art methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.17854v1' target='_blank'>Active Geospatial Search for Efficient Tenant Eviction Outreach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anindya Sarkar, Alex DiChristofano, Sanmay Das, Patrick J. Fowler, Nathan Jacobs, Yevgeniy Vorobeychik</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-19 23:40:36</h6>
<p class='card-text'>Tenant evictions threaten housing stability and are a major concern for many
cities. An open question concerns whether data-driven methods enhance outreach
programs that target at-risk tenants to mitigate their risk of eviction. We
propose a novel active geospatial search (AGS) modeling framework for this
problem. AGS integrates property-level information in a search policy that
identifies a sequence of rental units to canvas to both determine their
eviction risk and provide support if needed. We propose a hierarchical
reinforcement learning approach to learn a search policy for AGS that scales to
large urban areas containing thousands of parcels, balancing exploration and
exploitation and accounting for travel costs and a budget constraint.
Crucially, the search policy adapts online to newly discovered information
about evictions. Evaluation using eviction data for a large urban area
demonstrates that the proposed framework and algorithmic approach are
considerably more effective at sequentially identifying eviction cases than
baseline methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.14584v1' target='_blank'>Simulation-Free Hierarchical Latent Policy Planning for Proactive
  Dialogues</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Yiheng Sun, Zerui Chen, Ming Liu, Bing Qin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-19 07:06:01</h6>
<p class='card-text'>Recent advancements in proactive dialogues have garnered significant
attention, particularly for more complex objectives (e.g. emotion support and
persuasion). Unlike traditional task-oriented dialogues, proactive dialogues
demand advanced policy planning and adaptability, requiring rich scenarios and
comprehensive policy repositories to develop such systems. However, existing
approaches tend to rely on Large Language Models (LLMs) for user simulation and
online learning, leading to biases that diverge from realistic scenarios and
result in suboptimal efficiency. Moreover, these methods depend on manually
defined, context-independent, coarse-grained policies, which not only incur
high expert costs but also raise concerns regarding their completeness. In our
work, we highlight the potential for automatically discovering policies
directly from raw, real-world dialogue records. To this end, we introduce a
novel dialogue policy planning framework, LDPP. It fully automates the process
from mining policies in dialogue records to learning policy planning.
Specifically, we employ a variant of the Variational Autoencoder to discover
fine-grained policies represented as latent vectors. After automatically
annotating the data with these latent policy labels, we propose an Offline
Hierarchical Reinforcement Learning (RL) algorithm in the latent space to
develop effective policy planning capabilities. Our experiments demonstrate
that LDPP outperforms existing methods on two proactive scenarios, even
surpassing ChatGPT with only a 1.8-billion-parameter LLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.02998v3' target='_blank'>Accelerating Task Generalisation with Multi-Level Skill Hierarchies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas P Cannon, Özgür Simsek</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-05 11:00:09</h6>
<p class='card-text'>Creating reinforcement learning agents that generalise effectively to new
tasks is a key challenge in AI research. This paper introduces Fracture Cluster
Options (FraCOs), a multi-level hierarchical reinforcement learning method that
achieves state-of-the-art performance on difficult generalisation tasks. FraCOs
identifies patterns in agent behaviour and forms options based on the expected
future usefulness of those patterns, enabling rapid adaptation to new tasks. In
tabular settings, FraCOs demonstrates effective transfer and improves
performance as it grows in hierarchical depth. We evaluate FraCOs against
state-of-the-art deep reinforcement learning algorithms in several complex
procedurally generated environments. Our results show that FraCOs achieves
higher in-distribution and out-of-distribution performance than competitors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.01184v1' target='_blank'>Guiding Multi-agent Multi-task Reinforcement Learning by a Hierarchical
  Framework with Logical Reward Shaping</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chanjuan Liu, Jinmiao Cong, Bingcai Chen, Yaochu Jin, Enqiang Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-02 09:03:23</h6>
<p class='card-text'>Multi-agent hierarchical reinforcement learning (MAHRL) has been studied as
an effective means to solve intelligent decision problems in complex and
large-scale environments. However, most current MAHRL algorithms follow the
traditional way of using reward functions in reinforcement learning, which
limits their use to a single task. This study aims to design a multi-agent
cooperative algorithm with logic reward shaping (LRS), which uses a more
flexible way of setting the rewards, allowing for the effective completion of
multi-tasks. LRS uses Linear Temporal Logic (LTL) to express the internal logic
relation of subtasks within a complex task. Then, it evaluates whether the
subformulae of the LTL expressions are satisfied based on a designed reward
structure. This helps agents to learn to effectively complete tasks by adhering
to the LTL expressions, thus enhancing the interpretability and credibility of
their decisions. To enhance coordination and cooperation among multiple agents,
a value iteration technique is designed to evaluate the actions taken by each
agent. Based on this evaluation, a reward function is shaped for coordination,
which enables each agent to evaluate its status and complete the remaining
subtasks through experiential learning. Experiments have been conducted on
various types of tasks in the Minecraft-like environment. The results
demonstrate that the proposed algorithm can improve the performance of
multi-agents when learning to complete multi-tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.00361v1' target='_blank'>Hierarchical Preference Optimization: Learning to achieve goals via
  feasible subgoals prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Utsav Singh, Souradip Chakraborty, Wesley A. Suttle, Brian M. Sadler, Anit Kumar Sahu, Mubarak Shah, Vinay P. Namboodiri, Amrit Singh Bedi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-01 04:58:40</h6>
<p class='card-text'>This work introduces Hierarchical Preference Optimization (HPO), a novel
approach to hierarchical reinforcement learning (HRL) that addresses
non-stationarity and infeasible subgoal generation issues when solving complex
robotic control tasks. HPO leverages maximum entropy reinforcement learning
combined with token-level Direct Preference Optimization (DPO), eliminating the
need for pre-trained reference policies that are typically unavailable in
challenging robotic scenarios. Mathematically, we formulate HRL as a bi-level
optimization problem and transform it into a primitive-regularized DPO
formulation, ensuring feasible subgoal generation and avoiding degenerate
solutions. Extensive experiments on challenging robotic navigation and
manipulation tasks demonstrate impressive performance of HPO, where it shows an
improvement of up to 35% over the baselines. Furthermore, ablation studies
validate our design choices, and quantitative analyses confirm the ability of
HPO to mitigate non-stationarity and infeasible subgoal generation issues in
HRL.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>