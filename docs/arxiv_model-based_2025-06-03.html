<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>model-based - 2025-06-03</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>model-based - 2025-06-03</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19698v2' target='_blank'>JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance
  Asymmetry in Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jing Yu Lim, Zarif Ikram, Samson Yu, Haozhe Ma, Tze-Yun Leong, Dianbo Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 08:52:45</h6>
<p class='card-text'>Recent advances in model-based reinforcement learning (MBRL) have achieved
super-human level performance on the Atari100k benchmark, driven by
reinforcement learning agents trained on powerful diffusion world models.
However, we identify that the current aggregates mask a major performance
asymmetry: MBRL agents dramatically outperform humans in some tasks despite
drastically underperforming in others, with the former inflating the aggregate
metrics. This is especially pronounced in pixel-based agents trained with
diffusion world models. In this work, we address the pronounced asymmetry
observed in pixel-based agents as an initial attempt to reverse the worrying
upward trend observed in them. We address the problematic aggregates by
delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal
importance on metrics from both sets. Next, we hypothesize this pronounced
asymmetry is due to the lack of temporally-structured latent space trained with
the World Model objective in pixel-based methods. Lastly, to address this
issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion
world model trained end-to-end with the self-consistency objective. JEDI
outperforms SOTA models in human-optimal tasks while staying competitive across
the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the
latest pixel-based diffusion baseline. Overall, our work rethinks what it truly
means to cross human-level performance in Atari100k.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16787v2' target='_blank'>Enter the Void - Planning to Seek Entropy When Reward is Scarce</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ashish Sundar, Chunbo Luo, Xiaoyang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 15:28:50</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) offers an intuitive way to increase
the sample efficiency of model-free RL methods by simultaneously training a
world model that learns to predict the future. MBRL methods have progressed by
largely prioritising the actor; optimising the world model learning has been
neglected meanwhile. Improving the fidelity of the world model and reducing its
time to convergence can yield significant downstream benefits, one of which is
improving the ensuing performance of any actor it may train. We propose a novel
approach that anticipates and actively seeks out high-entropy states using
short-horizon latent predictions generated by the world model, offering a
principled alternative to traditional curiosity-driven methods that chase
once-novel states well after they were stumbled into. While many model
predictive control (MPC) based methods offer similar alternatives, they
typically lack commitment, synthesising multi step plans after every step. To
mitigate this, we present a hierarchical planner that dynamically decides when
to replan, planning horizon length, and the weighting between reward and
entropy. While our method can theoretically be applied to any model that trains
its own actors with solely model generated data, we have applied it to just
Dreamer as a proof of concept. Our method finishes the Miniworld procedurally
generated mazes 50% faster than base Dreamer at convergence and the policy
trained in imagination converges in only 60% of the environment steps that base
Dreamer needs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16394v1' target='_blank'>Raw2Drive: Reinforcement Learning with Aligned World Models for
  End-to-End Autonomous Driving (in CARLA v2)</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenjie Yang, Xiaosong Jia, Qifeng Li, Xue Yang, Maoqing Yao, Junchi Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:46:53</h6>
<p class='card-text'>Reinforcement Learning (RL) can mitigate the causal confusion and
distribution shift inherent to imitation learning (IL). However, applying RL to
end-to-end autonomous driving (E2E-AD) remains an open problem for its training
difficulty, and IL is still the mainstream paradigm in both academia and
industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated
promising results in neural planning; however, these methods typically require
privileged information as input rather than raw sensor data. We fill this gap
by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently
train an auxiliary privileged world model paired with a neural planner that
uses privileged information as input. Subsequently, we introduce a raw sensor
world model trained via our proposed Guidance Mechanism, which ensures
consistency between the raw sensor world model and the privileged world model
during rollouts. Finally, the raw sensor world model combines the prior
knowledge embedded in the heads of the privileged world model to effectively
guide the training of the raw sensor policy. Raw2Drive is so far the only RL
based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it
achieves state-of-the-art performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15754v1' target='_blank'>Improving planning and MBRL with temporally-extended actions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Palash Chatterjee, Roni Khardon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 16:59:32</h6>
<p class='card-text'>Continuous time systems are often modeled using discrete time dynamics but
this requires a small simulation step to maintain accuracy. In turn, this
requires a large planning horizon which leads to computationally demanding
planning problems and reduced performance. Previous work in model free
reinforcement learning has partially addressed this issue using action repeats
where a policy is learned to determine a discrete action duration. Instead we
propose to control the continuous decision timescale directly by using
temporally-extended actions and letting the planner treat the duration of the
action as an additional optimization variable along with the standard action
variables. This additional structure has multiple advantages. It speeds up
simulation time of trajectories and, importantly, it allows for deep horizon
search in terms of primitive actions while using a shallow search depth in the
planner. In addition, in the model based reinforcement learning (MBRL) setting,
it reduces compounding errors from model learning and improves training time
for models. We show that this idea is effective and that the range for action
durations can be automatically selected using a multi-armed bandit formulation
and integrated into the MBRL framework. An extensive experimental evaluation
both in planning and in MBRL, shows that our approach yields faster planning,
better solutions, and that it enables solutions to problems that are not solved
in the standard formulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13709v1' target='_blank'>Policy-Driven World Model Adaptation for Robust Offline Model-based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiayu Chen, Aravind Venugopal, Jeff Schneider</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-19 20:14:33</h6>
<p class='card-text'>Offline reinforcement learning (RL) offers a powerful paradigm for
data-driven control. Compared to model-free approaches, offline model-based RL
(MBRL) explicitly learns a world model from a static dataset and uses it as a
surrogate simulator, improving data efficiency and enabling potential
generalization beyond the dataset support. However, most existing offline MBRL
methods follow a two-stage training procedure: first learning a world model by
maximizing the likelihood of the observed transitions, then optimizing a policy
to maximize its expected return under the learned model. This objective
mismatch results in a world model that is not necessarily optimized for
effective policy learning. Moreover, we observe that policies learned via
offline MBRL often lack robustness during deployment, and small adversarial
noise in the environment can lead to significant performance degradation. To
address these, we propose a framework that dynamically adapts the world model
alongside the policy under a unified learning objective aimed at improving
robustness. At the core of our method is a maximin optimization problem, which
we solve by innovatively utilizing Stackelberg learning dynamics. We provide
theoretical analysis to support our design and introduce computationally
efficient implementations. We benchmark our algorithm on twelve noisy D4RL
MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its
state-of-the-art performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13144v1' target='_blank'>Temporal Distance-aware Transition Augmentation for Offline Model-based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongsu Lee, Minhae Kwon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-19 14:11:14</h6>
<p class='card-text'>The goal of offline reinforcement learning (RL) is to extract a
high-performance policy from the fixed datasets, minimizing performance
degradation due to out-of-distribution (OOD) samples. Offline model-based RL
(MBRL) is a promising approach that ameliorates OOD issues by enriching
state-action transitions with augmentations synthesized via a learned dynamics
model. Unfortunately, seminal offline MBRL methods often struggle in
sparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL
framework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA),
that generates augmented transitions in a temporally structured latent space
rather than in raw state space. To model long-horizon behavior, TempDATA learns
a latent abstraction that captures a temporal distance from both trajectory and
transition levels of state space. Our experiments confirm that TempDATA
outperforms previous offline MBRL methods and achieves matching or surpassing
the performance of diffusion-based trajectory augmentation and goal-conditioned
RL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.01712v1' target='_blank'>World Model-Based Learning for Long-Term Age of Information Minimization
  in Vehicular Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lingyi Wang, Rashed Shelim, Walid Saad, Naren Ramakrishnan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-03 06:23:18</h6>
<p class='card-text'>Traditional reinforcement learning (RL)-based learning approaches for
wireless networks rely on expensive trial-and-error mechanisms and real-time
feedback based on extensive environment interactions, which leads to low data
efficiency and short-sighted policies. These limitations become particularly
problematic in complex, dynamic networks with high uncertainty and long-term
planning requirements. To address these limitations, in this paper, a novel
world model-based learning framework is proposed to minimize
packet-completeness-aware age of information (CAoI) in a vehicular network.
Particularly, a challenging representative scenario is considered pertaining to
a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network,
which is characterized by high mobility, frequent signal blockages, and
extremely short coherence time. Then, a world model framework is proposed to
jointly learn a dynamic model of the mmWave V2X environment and use it to
imagine trajectories for learning how to perform link scheduling. In
particular, the long-term policy is learned in differentiable imagined
trajectories instead of environment interactions. Moreover, owing to its
imagination abilities, the world model can jointly predict time-varying
wireless data and optimize link scheduling in real-world wireless and V2X
networks. Thus, during intervals without actual observations, the world model
remains capable of making efficient decisions. Extensive experiments are
performed on a realistic simulator based on Sionna that integrates
physics-based end-to-end channel modeling, ray-tracing, and scene geometries
with material properties. Simulation results show that the proposed world model
achieves a significant improvement in data efficiency, and achieves 26%
improvement and 16% improvement in CAoI, respectively, compared to the
model-based RL (MBRL) method and the model-free RL (MFRL) method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.16680v1' target='_blank'>Offline Robotic World Model: Learning Robotic Policies without a Physics
  Simulator</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenhao Li, Andreas Krause, Marco Hutter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-23 12:58:15</h6>
<p class='card-text'>Reinforcement Learning (RL) has demonstrated impressive capabilities in
robotic control but remains challenging due to high sample complexity, safety
concerns, and the sim-to-real gap. While offline RL eliminates the need for
risky real-world exploration by learning from pre-collected data, it suffers
from distributional shift, limiting policy generalization. Model-Based RL
(MBRL) addresses this by leveraging predictive models for synthetic rollouts,
yet existing approaches often lack robust uncertainty estimation, leading to
compounding errors in offline settings. We introduce Offline Robotic World
Model (RWM-O), a model-based approach that explicitly estimates epistemic
uncertainty to improve policy learning without reliance on a physics simulator.
By integrating these uncertainty estimates into policy optimization, our
approach penalizes unreliable transitions, reducing overfitting to model errors
and enhancing stability. Experimental results show that RWM-O improves
generalization and safety, enabling policy learning purely from real-world data
and advancing scalable, data-efficient RL for robotics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.16588v1' target='_blank'>Data-Assimilated Model-Based Reinforcement Learning for Partially
  Observed Chaotic Flows</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Defne E. Ozan, Andrea Nóvoa, Luca Magri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-23 10:12:53</h6>
<p class='card-text'>The goal of many applications in energy and transport sectors is to control
turbulent flows. However, because of chaotic dynamics and high dimensionality,
the control of turbulent flows is exceedingly difficult. Model-free
reinforcement learning (RL) methods can discover optimal control policies by
interacting with the environment, but they require full state information,
which is often unavailable in experimental settings. We propose a
data-assimilated model-based RL (DA-MBRL) framework for systems with partial
observability and noisy measurements. Our framework employs a control-aware
Echo State Network for data-driven prediction of the dynamics, and integrates
data assimilation with an Ensemble Kalman Filter for real-time state
estimation. An off-policy actor-critic algorithm is employed to learn optimal
control strategies from state estimates. The framework is tested on the
Kuramoto-Sivashinsky equation, demonstrating its effectiveness in stabilizing a
spatiotemporally chaotic flow from noisy and partial measurements.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.06721v1' target='_blank'>Learning global control of underactuated systems with Model-Based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Niccolò Turcato, Marco Calì, Alberto Dalla Libera, Giulio Giacomuzzo, Ruggero Carli, Diego Romeres</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-09 09:20:37</h6>
<p class='card-text'>This short paper describes our proposed solution for the third edition of the
"AI Olympics with RealAIGym" competition, held at ICRA 2025. We employed
Monte-Carlo Probabilistic Inference for Learning Control (MC-PILCO), an MBRL
algorithm recognized for its exceptional data efficiency across various
low-dimensional robotic tasks, including cart-pole, ball \& plate, and Furuta
pendulum systems. MC-PILCO optimizes a system dynamics model using interaction
data, enabling policy refinement through simulation rather than direct system
data optimization. This approach has proven highly effective in physical
systems, offering greater data efficiency than Model-Free (MF) alternatives.
Notably, MC-PILCO has previously won the first two editions of this
competition, demonstrating its robustness in both simulated and real-world
environments. Besides briefly reviewing the algorithm, we discuss the most
critical aspects of the MC-PILCO implementation in the tasks at hand: learning
a global policy for the pendubot and acrobot systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.04164v2' target='_blank'>MInCo: Mitigating Information Conflicts in Distracted Visual Model-based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shiguang Sun, Hanbo Zhang, Zeyang Liu, Xinrui Yang, Lipeng Wan, Bing Yan, Xingyu Chen, Xuguang Lan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-05 12:57:31</h6>
<p class='card-text'>Existing visual model-based reinforcement learning (MBRL) algorithms with
observation reconstruction often suffer from information conflicts, making it
difficult to learn compact representations and hence result in less robust
policies, especially in the presence of task-irrelevant visual distractions. In
this paper, we first reveal that the information conflicts in current visual
MBRL algorithms stem from visual representation learning and latent dynamics
modeling with an information-theoretic perspective. Based on this finding, we
present a new algorithm to resolve information conflicts for visual MBRL, named
MInCo, which mitigates information conflicts by leveraging negative-free
contrastive learning, aiding in learning invariant representation and robust
policies despite noisy observations. To prevent the dominance of visual
representation learning, we introduce time-varying reweighting to bias the
learning towards dynamics modeling as training proceeds. We evaluate our method
on several robotic control tasks with dynamic background distractions. Our
experiments demonstrate that MInCo learns invariant representations against
background noise and consistently outperforms current state-of-the-art visual
MBRL methods. Code is available at https://github.com/ShiguangSun/minco.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.21983v2' target='_blank'>Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams
  and Teams of LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abed Kareem Musaffar, Anand Gokhale, Sirui Zeng, Rasta Tadayon, Xifeng Yan, Ambuj Singh, Francesco Bullo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-27 21:01:02</h6>
<p class='card-text'>As artificial intelligence (AI) assistants become more widely adopted in
safety-critical domains, it becomes important to develop safeguards against
potential failures or adversarial attacks. A key prerequisite to developing
these safeguards is understanding the ability of these AI assistants to mislead
human teammates. We investigate this attack problem within the context of an
intellective strategy game where a team of three humans and one AI assistant
collaborate to answer a series of trivia questions. Unbeknownst to the humans,
the AI assistant is adversarial. Leveraging techniques from Model-Based
Reinforcement Learning (MBRL), the AI assistant learns a model of the humans'
trust evolution and uses that model to manipulate the group decision-making
process to harm the team. We evaluate two models -- one inspired by literature
and the other data-driven -- and find that both can effectively harm the human
team. Moreover, we find that in this setting our data-driven model is capable
of accurately predicting how human agents appraise their teammates given
limited information on prior interactions. Finally, we compare the performance
of state-of-the-art LLM models to human agents on our influence allocation task
to evaluate whether the LLMs allocate influence similarly to humans or if they
are more robust to our attack. These results enhance our understanding of
decision-making dynamics in small human-AI teams and lay the foundation for
defense strategies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.20462v1' target='_blank'>Multi-agent Uncertainty-Aware Pessimistic Model-Based Reinforcement
  Learning for Connected Autonomous Vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruoqi Wen, Rongpeng Li, Xing Xu, Zhifeng Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-26 11:49:02</h6>
<p class='card-text'>Deep Reinforcement Learning (DRL) holds significant promise for achieving
human-like Autonomous Vehicle (AV) capabilities, but suffers from low sample
efficiency and challenges in reward design. Model-Based Reinforcement Learning
(MBRL) offers improved sample efficiency and generalizability compared to
Model-Free Reinforcement Learning (MFRL) in various multi-agent decision-making
scenarios. Nevertheless, MBRL faces critical difficulties in estimating
uncertainty during the model learning phase, thereby limiting its scalability
and applicability in real-world scenarios. Additionally, most Connected
Autonomous Vehicle (CAV) studies focus on single-agent decision-making, while
existing multi-agent MBRL solutions lack computationally tractable algorithms
with Probably Approximately Correct (PAC) guarantees, an essential factor for
ensuring policy reliability with limited training data. To address these
challenges, we propose MA-PMBRL, a novel Multi-Agent Pessimistic Model-Based
Reinforcement Learning framework for CAVs, incorporating a max-min optimization
approach to enhance robustness and decision-making. To mitigate the inherent
subjectivity of uncertainty estimation in MBRL and avoid incurring catastrophic
failures in AV, MA-PMBRL employs a pessimistic optimization framework combined
with Projected Gradient Descent (PGD) for both model and policy learning.
MA-PMBRL also employs general function approximations under partial dataset
coverage to enhance learning efficiency and system-level performance. By
bounding the suboptimality of the resulting policy under mild theoretical
assumptions, we successfully establish PAC guarantees for MA-PMBRL,
demonstrating that the proposed framework represents a significant step toward
scalable, efficient, and reliable multi-agent decision-making for CAVs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.20139v1' target='_blank'>Look Before Leap: Look-Ahead Planning with Uncertainty in Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongshuai Liu, Xin Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-26 01:07:35</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) has demonstrated superior sample
efficiency compared to model-free reinforcement learning (MFRL). However, the
presence of inaccurate models can introduce biases during policy learning,
resulting in misleading trajectories. The challenge lies in obtaining accurate
models due to limited diverse training data, particularly in regions with
limited visits (uncertain regions). Existing approaches passively quantify
uncertainty after sample generation, failing to actively collect uncertain
samples that could enhance state coverage and improve model accuracy. Moreover,
MBRL often faces difficulties in making accurate multi-step predictions,
thereby impacting overall performance. To address these limitations, we propose
a novel framework for uncertainty-aware policy optimization with model-based
exploratory planning. In the model-based planning phase, we introduce an
uncertainty-aware k-step lookahead planning approach to guide action selection
at each step. This process involves a trade-off analysis between model
uncertainty and value function approximation error, effectively enhancing
policy performance. In the policy optimization phase, we leverage an
uncertainty-driven exploratory policy to actively collect diverse training
samples, resulting in improved model accuracy and overall performance of the RL
agent. Our approach offers flexibility and applicability to tasks with varying
state/action spaces and reward structures. We validate its effectiveness
through experiments on challenging robotic manipulation tasks and Atari games,
surpassing state-of-the-art methods with fewer interactions, thereby leading to
significant performance improvements.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.17693v1' target='_blank'>Conditional Diffusion Model with OOD Mitigation as High-Dimensional
  Offline Resource Allocation Planner in Clustered Ad Hoc Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kechen Meng, Sinuo Zhang, Rongpeng Li, Chan Wang, Ming Lei, Zhifeng Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-22 08:27:09</h6>
<p class='card-text'>Due to network delays and scalability limitations, clustered ad hoc networks
widely adopt Reinforcement Learning (RL) for on-demand resource allocation.
Albeit its demonstrated agility, traditional Model-Free RL (MFRL) solutions
struggle to tackle the huge action space, which generally explodes
exponentially along with the number of resource allocation units, enduring low
sampling efficiency and high interaction cost. In contrast to MFRL, Model-Based
RL (MBRL) offers an alternative solution to boost sample efficiency and
stabilize the training by explicitly leveraging a learned environment model.
However, establishing an accurate dynamic model for complex and noisy
environments necessitates a careful balance between model accuracy and
computational complexity $\&$ stability. To address these issues, we propose a
Conditional Diffusion Model Planner (CDMP) for high-dimensional offline
resource allocation in clustered ad hoc networks. By leveraging the astonishing
generative capability of Diffusion Models (DMs), our approach enables the
accurate modeling of high-quality environmental dynamics while leveraging an
inverse dynamics model to plan a superior policy. Beyond simply adopting DMs in
offline RL, we further incorporate the CDMP algorithm with a theoretically
guaranteed, uncertainty-aware penalty metric, which theoretically and
empirically manifests itself in mitigating the Out-of-Distribution
(OOD)-induced distribution shift issue underlying scarce training data.
Extensive experiments also show that our model outperforms MFRL in average
reward and Quality of Service (QoS) while demonstrating comparable performance
to other MBRL algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09719v1' target='_blank'>Towards Causal Model-Based Policy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alberto Caron, Vasilios Mavroudis, Chris Hicks</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 18:09:02</h6>
<p class='card-text'>Real-world decision-making problems are often marked by complex, uncertain
dynamics that can shift or break under changing conditions. Traditional
Model-Based Reinforcement Learning (MBRL) approaches learn predictive models of
environment dynamics from queried trajectories and then use these models to
simulate rollouts for policy optimization. However, such methods do not account
for the underlying causal mechanisms that govern the environment, and thus
inadvertently capture spurious correlations, making them sensitive to
distributional shifts and limiting their ability to generalize. The same
naturally holds for model-free approaches. In this work, we introduce Causal
Model-Based Policy Optimization (C-MBPO), a novel framework that integrates
causal learning into the MBRL pipeline to achieve more robust, explainable, and
generalizable policy learning algorithms.
  Our approach centers on first inferring a Causal Markov Decision Process
(C-MDP) by learning a local Structural Causal Model (SCM) of both the state and
reward transition dynamics from trajectories gathered online. C-MDPs differ
from classic MDPs in that we can decompose causal dependencies in the
environment dynamics via specifying an associated Causal Bayesian Network.
C-MDPs allow for targeted interventions and counterfactual reasoning, enabling
the agent to distinguish between mere statistical correlations and causal
relationships. The learned SCM is then used to simulate counterfactual
on-policy transitions and rewards under hypothetical actions (or
``interventions"), thereby guiding policy optimization more effectively. The
resulting policy learned by C-MBPO can be shown to be robust to a class of
distributional shifts that affect spurious, non-causal relationships in the
dynamics. We demonstrate this through some simple experiments involving near
and far OOD dynamics drifts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05573v1' target='_blank'>InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle
  Exploration through Curiosity Driven Generalized World Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Feeza Khan Khanzada, Jaerock Kwon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 16:56:00</h6>
<p class='card-text'>Model-based Reinforcement Learning (MBRL) has emerged as a promising paradigm
for autonomous driving, where data efficiency and robustness are critical. Yet,
existing solutions often rely on carefully crafted, task specific extrinsic
rewards, limiting generalization to new tasks or environments. In this paper,
we propose InDRiVE (Intrinsic Disagreement based Reinforcement for Vehicle
Exploration), a method that leverages purely intrinsic, disagreement based
rewards within a Dreamer based MBRL framework. By training an ensemble of world
models, the agent actively explores high uncertainty regions of environments
without any task specific feedback. This approach yields a task agnostic latent
representation, allowing for rapid zero shot or few shot fine tuning on
downstream driving tasks such as lane following and collision avoidance.
Experimental results in both seen and unseen environments demonstrate that
InDRiVE achieves higher success rates and fewer infractions compared to
DreamerV2 and DreamerV3 baselines despite using significantly fewer training
steps. Our findings highlight the effectiveness of purely intrinsic exploration
for learning robust vehicle control behaviors, paving the way for more scalable
and adaptable autonomous driving systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01178v1' target='_blank'>Differentiable Information Enhanced Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoyuan Zhang, Xinyan Cai, Bo Liu, Weidong Huang, Song-Chun Zhu, Siyuan Qi, Yaodong Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 04:51:40</h6>
<p class='card-text'>Differentiable environments have heralded new possibilities for learning
control policies by offering rich differentiable information that facilitates
gradient-based methods. In comparison to prevailing model-free reinforcement
learning approaches, model-based reinforcement learning (MBRL) methods exhibit
the potential to effectively harness the power of differentiable information
for recovering the underlying physical dynamics. However, this presents two
primary challenges: effectively utilizing differentiable information to 1)
construct models with more accurate dynamic prediction and 2) enhance the
stability of policy training. In this paper, we propose a Differentiable
Information Enhanced MBRL method, MB-MIX, to address both challenges. Firstly,
we adopt a Sobolev model training approach that penalizes incorrect model
gradient outputs, enhancing prediction accuracy and yielding more precise
models that faithfully capture system dynamics. Secondly, we introduce mixing
lengths of truncated learning windows to reduce the variance in policy gradient
estimation, resulting in improved stability during policy learning. To validate
the effectiveness of our approach in differentiable environments, we provide
theoretical analysis and empirical results. Notably, our approach outperforms
previous model-based and model-free methods, in multiple challenging tasks
involving controllable rigid robots such as humanoid robots' motion control and
deformable object manipulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20168v1' target='_blank'>Accelerating Model-Based Reinforcement Learning with State-Space World
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maria Krinner, Elie Aljalbout, Angel Romero, Davide Scaramuzza</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 15:05:25</h6>
<p class='card-text'>Reinforcement learning (RL) is a powerful approach for robot learning.
However, model-free RL (MFRL) requires a large number of environment
interactions to learn successful control policies. This is due to the noisy RL
training updates and the complexity of robotic systems, which typically involve
highly non-linear dynamics and noisy sensor signals. In contrast, model-based
RL (MBRL) not only trains a policy but simultaneously learns a world model that
captures the environment's dynamics and rewards. The world model can either be
used for planning, for data collection, or to provide first-order policy
gradients for training. Leveraging a world model significantly improves sample
efficiency compared to model-free RL. However, training a world model alongside
the policy increases the computational complexity, leading to longer training
times that are often intractable for complex real-world scenarios. In this
work, we propose a new method for accelerating model-based RL using state-space
world models. Our approach leverages state-space models (SSMs) to parallelize
the training of the dynamics model, which is typically the main computational
bottleneck. Additionally, we propose an architecture that provides privileged
information to the world model during training, which is particularly relevant
for partially observable environments. We evaluate our method in several
real-world agile quadrotor flight tasks, involving complex dynamics, for both
fully and partially observable environments. We demonstrate a significant
speedup, reducing the world model training time by up to 10 times, and the
overall MBRL training time by up to 4 times. This benefit comes without
compromising performance, as our method achieves similar sample efficiency and
task rewards to state-of-the-art MBRL methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11480v1' target='_blank'>Enhancing Offline Model-Based RL via Active Model Selection: A Bayesian
  Optimization Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu-Wei Yang, Yun-Ming Chan, Wei Hung, Xi Liu, Ping-Chun Hsieh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-17 06:34:58</h6>
<p class='card-text'>Offline model-based reinforcement learning (MBRL) serves as a competitive
framework that can learn well-performing policies solely from pre-collected
data with the help of learned dynamics models. To fully unleash the power of
offline MBRL, model selection plays a pivotal role in determining the dynamics
model utilized for downstream policy learning. However, offline MBRL
conventionally relies on validation or off-policy evaluation, which are rather
inaccurate due to the inherent distribution shift in offline RL. To tackle
this, we propose BOMS, an active model selection framework that enhances model
selection in offline MBRL with only a small online interaction budget, through
the lens of Bayesian optimization (BO). Specifically, we recast model selection
as BO and enable probabilistic inference in BOMS by proposing a novel
model-induced kernel, which is theoretically grounded and computationally
efficient. Through extensive experiments, we show that BOMS improves over the
baseline methods with a small amount of online interaction comparable to only
$1\%$-$2.5\%$ of offline training data on various RL tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.10077v1' target='_blank'>Towards Empowerment Gain through Causal Structure Learning in
  Model-Based RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongye Cao, Fan Feng, Meng Fang, Shaokang Dong, Tianpei Yang, Jing Huo, Yang Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-14 10:59:09</h6>
<p class='card-text'>In Model-Based Reinforcement Learning (MBRL), incorporating causal structures
into dynamics models provides agents with a structured understanding of the
environments, enabling efficient decision. Empowerment as an intrinsic
motivation enhances the ability of agents to actively control their
environments by maximizing the mutual information between future states and
actions. We posit that empowerment coupled with causal understanding can
improve controllability, while enhanced empowerment gain can further facilitate
causal reasoning in MBRL. To improve learning efficiency and controllability,
we propose a novel framework, Empowerment through Causal Learning (ECL), where
an agent with the awareness of causal dynamics models achieves
empowerment-driven exploration and optimizes its causal structure for task
learning. Specifically, ECL operates by first training a causal dynamics model
of the environment based on collected data. We then maximize empowerment under
the causal structure for exploration, simultaneously using data gathered
through exploration to update causal dynamics model to be more controllable
than dense dynamics model without causal structure. In downstream task
learning, an intrinsic curiosity reward is included to balance the causality,
mitigating overfitting. Importantly, ECL is method-agnostic and is capable of
integrating various causal discovery methods. We evaluate ECL combined with 3
causal discovery methods across 6 environments including pixel-based tasks,
demonstrating its superior performance compared to other causal MBRL methods,
in terms of causal discovery, sample efficiency, and asymptotic performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05595v1' target='_blank'>Data efficient Robotic Object Throwing with Model-Based Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Niccolò Turcato, Giulio Giacomuzzo, Matteo Terreran, Davide Allegro, Ruggero Carli, Alberto Dalla Libera</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 14:43:42</h6>
<p class='card-text'>Pick-and-place (PnP) operations, featuring object grasping and trajectory
planning, are fundamental in industrial robotics applications. Despite many
advancements in the field, PnP is limited by workspace constraints, reducing
flexibility. Pick-and-throw (PnT) is a promising alternative where the robot
throws objects to target locations, leveraging extrinsic resources like gravity
to improve efficiency and expand the workspace. However, PnT execution is
complex, requiring precise coordination of high-speed movements and object
dynamics. Solutions to the PnT problem are categorized into analytical and
learning-based approaches. Analytical methods focus on system modeling and
trajectory generation but are time-consuming and offer limited generalization.
Learning-based solutions, in particular Model-Free Reinforcement Learning
(MFRL), offer automation and adaptability but require extensive interaction
time. This paper introduces a Model-Based Reinforcement Learning (MBRL)
framework, MC-PILOT, which combines data-driven modeling with policy
optimization for efficient and accurate PnT tasks. MC-PILOT accounts for model
uncertainties and release errors, demonstrating superior performance in
simulations and real-world tests with a Franka Emika Panda manipulator. The
proposed approach generalizes rapidly to new targets, offering advantages over
analytical and Model-Free methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01591v2' target='_blank'>Improving Transformer World Models for Data-Efficient RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Antoine Dedieu, Joseph Ortiz, Xinghua Lou, Carter Wendelken, Wolfgang Lehrach, J Swaroop Guntupalli, Miguel Lazaro-Gredilla, Kevin Patrick Murphy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-03 18:25:17</h6>
<p class='card-text'>We present an approach to model-based RL that achieves a new state of the art
performance on the challenging Craftax-classic benchmark, an open-world 2D
survival game that requires agents to exhibit a wide range of general abilities
-- such as strong generalization, deep exploration, and long-term reasoning.
With a series of careful design choices aimed at improving sample efficiency,
our MBRL algorithm achieves a reward of 69.66% after only 1M environment steps,
significantly outperforming DreamerV3, which achieves 53.2%, and, for the first
time, exceeds human performance of 65.0%. Our method starts by constructing a
SOTA model-free baseline, using a novel policy architecture that combines CNNs
and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna
with warmup", which trains the policy on real and imaginary data, (b) "nearest
neighbor tokenizer" on image patches, which improves the scheme to create the
transformer world model (TWM) inputs, and (c) "block teacher forcing", which
allows the TWM to reason jointly about the future tokens of the next timestep.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.16918v2' target='_blank'>On Rollouts in Model-Based Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bernd Frauenknecht, Devdutt Subhasish, Friedrich Solowjow, Sebastian Trimpe</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-28 13:02:52</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) seeks to enhance data efficiency by
learning a model of the environment and generating synthetic rollouts from it.
However, accumulated model errors during these rollouts can distort the data
distribution, negatively impacting policy learning and hindering long-term
planning. Thus, the accumulation of model errors is a key bottleneck in current
MBRL methods. We propose Infoprop, a model-based rollout mechanism that
separates aleatoric from epistemic model uncertainty and reduces the influence
of the latter on the data distribution. Further, Infoprop keeps track of
accumulated model errors along a model rollout and provides termination
criteria to limit data corruption. We demonstrate the capabilities of Infoprop
in the Infoprop-Dyna algorithm, reporting state-of-the-art performance in
Dyna-style MBRL on common MuJoCo benchmark tasks while substantially increasing
rollout length and data quality.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.16733v1' target='_blank'>Dream to Drive with Predictive Individual World Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yinfeng Gao, Qichao Zhang, Da-wei Ding, Dongbin Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-28 06:18:29</h6>
<p class='card-text'>It is still a challenging topic to make reactive driving behaviors in complex
urban environments as road users' intentions are unknown. Model-based
reinforcement learning (MBRL) offers great potential to learn a reactive policy
by constructing a world model that can provide informative states and
imagination training. However, a critical limitation in relevant research lies
in the scene-level reconstruction representation learning, which may overlook
key interactive vehicles and hardly model the interactive features among
vehicles and their long-term intentions. Therefore, this paper presents a novel
MBRL method with a predictive individual world model (PIWM) for autonomous
driving. PIWM describes the driving environment from an individual-level
perspective and captures vehicles' interactive relations and their intentions
via trajectory prediction task. Meanwhile, a behavior policy is learned jointly
with PIWM. It is trained in PIWM's imagination and effectively navigates in the
urban driving scenes leveraging intention-aware latent states. The proposed
method is trained and evaluated on simulation environments built upon
real-world challenging interactive scenarios. Compared with popular model-free
and state-of-the-art model-based reinforcement learning methods, experimental
results show that the proposed method achieves the best performance in terms of
safety and efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.16443v1' target='_blank'>Objects matter: object-centric world models improve reinforcement
  learning in visually complex environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weipu Zhang, Adam Jelley, Trevor McInroe, Amos Storkey</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-27 19:07:06</h6>
<p class='card-text'>Deep reinforcement learning has achieved remarkable success in learning
control policies from pixels across a wide range of tasks, yet its application
remains hindered by low sample efficiency, requiring significantly more
environment interactions than humans to reach comparable performance.
Model-based reinforcement learning (MBRL) offers a solution by leveraging
learnt world models to generate simulated experience, thereby improving sample
efficiency. However, in visually complex environments, small or dynamic
elements can be critical for decision-making. Yet, traditional MBRL methods in
pixel-based environments typically rely on auto-encoding with an $L_2$ loss,
which is dominated by large areas and often fails to capture decision-relevant
details. To address these limitations, we propose an object-centric MBRL
pipeline, which integrates recent advances in computer vision to allow agents
to focus on key decision-related elements. Our approach consists of four main
steps: (1) annotating key objects related to rewards and goals with
segmentation masks, (2) extracting object features using a pre-trained, frozen
foundation vision model, (3) incorporating these object features with the raw
observations to predict environmental dynamics, and (4) training the policy
using imagined trajectories generated by this object-centric world model.
Building on the efficient MBRL algorithm STORM, we call this pipeline OC-STORM.
We demonstrate OC-STORM's practical value in overcoming the limitations of
conventional MBRL approaches on both Atari games and the visually complex game
Hollow Knight.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.11949v1' target='_blank'>GLAM: Global-Local Variation Awareness in Mamba-based World Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qian He, Wenqi Liang, Chunhui Hao, Gan Sun, Jiandong Tian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-21 07:47:03</h6>
<p class='card-text'>Mimicking the real interaction trajectory in the inference of the world model
has been shown to improve the sample efficiency of model-based reinforcement
learning (MBRL) algorithms. Many methods directly use known state sequences for
reasoning. However, this approach fails to enhance the quality of reasoning by
capturing the subtle variation between states. Much like how humans infer
trends in event development from this variation, in this work, we introduce
Global-Local variation Awareness Mamba-based world model (GLAM) that improves
reasoning quality by perceiving and predicting variation between states. GLAM
comprises two Mambabased parallel reasoning modules, GMamba and LMamba, which
focus on perceiving variation from global and local perspectives, respectively,
during the reasoning process. GMamba focuses on identifying patterns of
variation between states in the input sequence and leverages these patterns to
enhance the prediction of future state variation. LMamba emphasizes reasoning
about unknown information, such as rewards, termination signals, and visual
representations, by perceiving variation in adjacent states. By integrating the
strengths of the two modules, GLAM accounts for highervalue variation in
environmental changes, providing the agent with more efficient
imagination-based training. We demonstrate that our method outperforms existing
methods in normalized human scores on the Atari 100k benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.09611v1' target='_blank'>EVaDE : Event-Based Variational Thompson Sampling for Model-Based
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Siddharth Aravindan, Dixant Mittal, Wee Sun Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-16 15:35:48</h6>
<p class='card-text'>Posterior Sampling for Reinforcement Learning (PSRL) is a well-known
algorithm that augments model-based reinforcement learning (MBRL) algorithms
with Thompson sampling. PSRL maintains posterior distributions of the
environment transition dynamics and the reward function, which are intractable
for tasks with high-dimensional state and action spaces. Recent works show that
dropout, used in conjunction with neural networks, induces variational
distributions that can approximate these posteriors. In this paper, we propose
Event-based Variational Distributions for Exploration (EVaDE), which are
variational distributions that are useful for MBRL, especially when the
underlying domain is object-based. We leverage the general domain knowledge of
object-based domains to design three types of event-based convolutional layers
to direct exploration. These layers rely on Gaussian dropouts and are inserted
between the layers of the deep neural network model to help facilitate
variational Thompson sampling. We empirically show the effectiveness of
EVaDE-equipped Simulated Policy Learning (EVaDE-SimPLe) on the 100K Atari game
suite.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.06635v1' target='_blank'>A Reduced Order Iterative Linear Quadratic Regulator (ILQR) Technique
  for the Optimal Control of Nonlinear Partial Differential Equations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aayushman Sharma, Suman Chakravorty</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-11 20:53:33</h6>
<p class='card-text'>In this paper, we introduce a reduced order model-based reinforcement
learning (MBRL) approach, utilizing the Iterative Linear Quadratic Regulator
(ILQR) algorithm for the optimal control of nonlinear partial differential
equations (PDEs). The approach proposes a novel modification of the ILQR
technique: it uses the Method of Snapshots to identify a reduced order Linear
Time Varying (LTV) approximation of the nonlinear PDE dynamics around a current
estimate of the optimal trajectory, utilizes the identified LTV model to solve
a time-varying reduced order LQR problem to obtain an improved estimate of the
optimal trajectory along with a new reduced basis, and iterates till
convergence. The convergence behavior of the reduced order approach is analyzed
and the algorithm is shown to converge to a limit set that is dependent on the
truncation error in the reduction. The proposed approach is tested on the
viscous Burger's equation and two phase-field models for microstructure
evolution in materials, and the results show that there is a significant
reduction in the computational burden over the standard ILQR approach, without
significantly sacrificing performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.02774v1' target='_blank'>Learn A Flexible Exploration Model for Parameterized Action Markov
  Decision Processes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijian Wang, Bin Wang, Mingwen Shao, Hongbo Dou, Boxiang Tao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-06 05:33:09</h6>
<p class='card-text'>Hybrid action models are widely considered an effective approach to
reinforcement learning (RL) modeling. The current mainstream method is to train
agents under Parameterized Action Markov Decision Processes (PAMDPs), which
performs well in specific environments. Unfortunately, these models either
exhibit drastic low learning efficiency in complex PAMDPs or lose crucial
information in the conversion between raw space and latent space. To enhance
the learning efficiency and asymptotic performance of the agent, we propose a
model-based RL (MBRL) algorithm, FLEXplore. FLEXplore learns a
parameterized-action-conditioned dynamics model and employs a modified Model
Predictive Path Integral control. Unlike conventional MBRL algorithms, we
carefully design the dynamics loss function and reward smoothing process to
learn a loose yet flexible model. Additionally, we use the variational lower
bound to maximize the mutual information between the state and the hybrid
action, enhancing the exploration effectiveness of the agent. We theoretically
demonstrate that FLEXplore can reduce the regret of the rollout trajectory
through the Wasserstein Metric under given Lipschitz conditions. Our empirical
results on several standard benchmarks show that FLEXplore has outstanding
learning efficiency and asymptotic performance compared to other baselines.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>