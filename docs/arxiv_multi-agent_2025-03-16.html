<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>multi-agent - 2025-03-16</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>multi-agent - 2025-03-16</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10049v1' target='_blank'>Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based
  Planner and Graph-based Policy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziqi Jia, Junjie Li, Xiaoyang Qu, Jianzong Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 05:02:49</h6>
<p class='card-text'>Multi-agent systems (MAS) have shown great potential in executing complex
tasks, but coordination and safety remain significant challenges. Multi-Agent
Reinforcement Learning (MARL) offers a promising framework for agent
collaboration, but it faces difficulties in handling complex tasks and
designing reward functions. The introduction of Large Language Models (LLMs)
has brought stronger reasoning and cognitive abilities to MAS, but existing
LLM-based systems struggle to respond quickly and accurately in dynamic
environments. To address these challenges, we propose LLM-based Graph
Collaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and
MARL. This framework decomposes complex tasks into executable subtasks and
achieves efficient collaboration among multiple agents through graph-based
coordination. Specifically, LGC-MARL consists of two main components: an LLM
planner and a graph-based collaboration meta policy. The LLM planner transforms
complex task instructions into a series of executable subtasks, evaluates the
rationality of these subtasks using a critic model, and generates an action
dependency graph. The graph-based collaboration meta policy facilitates
communication and collaboration among agents based on the action dependency
graph, and adapts to new task environments through meta-learning. Experimental
results on the AI2-THOR simulation platform demonstrate the superior
performance and scalability of LGC-MARL in completing various complex tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09501v1' target='_blank'>ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 16:05:31</h6>
<p class='card-text'>Recent research on Reasoning of Large Language Models (LLMs) has sought to
further enhance their performance by integrating meta-thinking -- enabling
models to monitor, evaluate, and control their reasoning processes for more
adaptive and effective problem-solving. However, current single-agent work
lacks a specialized design for acquiring meta-thinking, resulting in low
efficacy. To address this challenge, we introduce Reinforced Meta-thinking
Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement
Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think
about thinking. ReMA decouples the reasoning process into two hierarchical
agents: a high-level meta-thinking agent responsible for generating strategic
oversight and plans, and a low-level reasoning agent for detailed executions.
Through iterative reinforcement learning with aligned objectives, these agents
explore and learn collaboration, leading to improved generalization and
robustness. Experimental results demonstrate that ReMA outperforms single-agent
RL baselines on complex reasoning tasks, including competitive-level
mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation
studies further illustrate the evolving dynamics of each distinct agent,
providing valuable insights into how the meta-thinking reasoning process
enhances the reasoning capabilities of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08740v1' target='_blank'>Cooperative Bearing-Only Target Pursuit via Multiagent Reinforcement
  Learning: Design and Experiment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianan Li, Zhikun Wang, Susheng Ding, Shiliang Guo, Shiyu Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 08:21:35</h6>
<p class='card-text'>This paper addresses the multi-robot pursuit problem for an unknown target,
encompassing both target state estimation and pursuit control. First, in state
estimation, we focus on using only bearing information, as it is readily
available from vision sensors and effective for small, distant targets.
Challenges such as instability due to the nonlinearity of bearing measurements
and singularities in the two-angle representation are addressed through a
proposed uniform bearing-only information filter. This filter integrates
multiple 3D bearing measurements, provides a concise formulation, and enhances
stability and resilience to target loss caused by limited field of view (FoV).
Second, in target pursuit control within complex environments, where challenges
such as heterogeneity and limited FoV arise, conventional methods like
differential games or Voronoi partitioning often prove inadequate. To address
these limitations, we propose a novel multiagent reinforcement learning (MARL)
framework, enabling multiple heterogeneous vehicles to search, localize, and
follow a target while effectively handling those challenges. Third, to bridge
the sim-to-real gap, we propose two key techniques: incorporating adjustable
low-level control gains in training to replicate the dynamics of real-world
autonomous ground vehicles (AGVs), and proposing spectral-normalized RL
algorithms to enhance policy smoothness and robustness. Finally, we demonstrate
the successful zero-shot transfer of the MARL controllers to AGVs, validating
the effectiveness and practical feasibility of our approach. The accompanying
video is available at https://youtu.be/HO7FJyZiJ3E.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08728v1' target='_blank'>Enhancing Traffic Signal Control through Model-based Reinforcement
  Learning and Policy Reuse</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yihong Li, Chengwei Zhang, Furui Zhan, Wanting Liu, Kailing Zhou, Longji Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 01:21:13</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) has shown significant potential in
traffic signal control (TSC). However, current MARL-based methods often suffer
from insufficient generalization due to the fixed traffic patterns and road
network conditions used during training. This limitation results in poor
adaptability to new traffic scenarios, leading to high retraining costs and
complex deployment. To address this challenge, we propose two algorithms:
PLight and PRLight. PLight employs a model-based reinforcement learning
approach, pretraining control policies and environment models using predefined
source-domain traffic scenarios. The environment model predicts the state
transitions, which facilitates the comparison of environmental features.
PRLight further enhances adaptability by adaptively selecting pre-trained
PLight agents based on the similarity between the source and target domains to
accelerate the learning process in the target domain. We evaluated the
algorithms through two transfer settings: (1) adaptability to different traffic
scenarios within the same road network, and (2) generalization across different
road networks. The results show that PRLight significantly reduces the
adaptation time compared to learning from scratch in new TSC scenarios,
achieving optimal performance using similarities between available and target
scenarios.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>