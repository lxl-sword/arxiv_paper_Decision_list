<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>multi-agent - 2025-07-27</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>multi-agent - 2025-07-27</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.18333v1' target='_blank'>Remembering the Markov Property in Cooperative MARL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kale-ab Abebe Tessera, Leonard Hinckeldey, Riccardo Zamboni, David Abel, Amos Storkey</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-24 11:59:42</h6>
<p class='card-text'>Cooperative multi-agent reinforcement learning (MARL) is typically formalised
as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),
where agents must reason about the environment and other agents' behaviour. In
practice, current model-free MARL algorithms use simple recurrent function
approximators to address the challenge of reasoning about others using partial
information. In this position paper, we argue that the empirical success of
these methods is not due to effective Markov signal recovery, but rather to
learning simple conventions that bypass environment observations and memory.
Through a targeted case study, we show that co-adapting agents can learn
brittle conventions, which then fail when partnered with non-adaptive agents.
Crucially, the same models can learn grounded policies when the task design
necessitates it, revealing that the issue is not a fundamental limitation of
the learning models but a failure of the benchmark design. Our analysis also
suggests that modern MARL environments may not adequately test the core
assumptions of Dec-POMDPs. We therefore advocate for new cooperative
environments built upon two core principles: (1) behaviours grounded in
observations and (2) memory-based reasoning about other agents, ensuring
success requires genuine skill rather than fragile, co-adapted agreements.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.18059v1' target='_blank'>Multi-Agent Guided Policy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yueheng Li, Guangming Xie, Zongqing Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-24 03:22:21</h6>
<p class='card-text'>Due to practical constraints such as partial observability and limited
communication, Centralized Training with Decentralized Execution (CTDE) has
become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning
(MARL). However, existing CTDE methods often underutilize centralized training
or lack theoretical guarantees. We propose Multi-Agent Guided Policy
Optimization (MAGPO), a novel framework that better leverages centralized
training by integrating centralized guidance with decentralized execution.
MAGPO uses an auto-regressive joint policy for scalable, coordinated
exploration and explicitly aligns it with decentralized policies to ensure
deployability under partial observability. We provide theoretical guarantees of
monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across
6 diverse environments. Results show that MAGPO consistently outperforms strong
CTDE baselines and matches or surpasses fully centralized approaches, offering
a principled and practical solution for decentralized multi-agent learning. Our
code and experimental data can be found in https://github.com/liyheng/MAGPO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.16796v1' target='_blank'>Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading
  with Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mian Ibad Ali Shah, Enda Barrett, Karl Mason</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-22 17:46:28</h6>
<p class='card-text'>This paper presents a novel framework for Peer-to-Peer (P2P) energy trading
that integrates uncertainty-aware prediction with multi-agent reinforcement
learning (MARL), addressing a critical gap in current literature. In contrast
to previous works relying on deterministic forecasts, the proposed approach
employs a heteroscedastic probabilistic transformer-based prediction model
called Knowledge Transformer with Uncertainty (KTU) to explicitly quantify
prediction uncertainty, which is essential for robust decision-making in the
stochastic environment of P2P energy trading. The KTU model leverages
domain-specific features and is trained with a custom loss function that
ensures reliable probabilistic forecasts and confidence intervals for each
prediction. Integrating these uncertainty-aware forecasts into the MARL
framework enables agents to optimize trading strategies with a clear
understanding of risk and variability. Experimental results show that the
uncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to
5.7% without P2P trading and 3.2% with P2P trading, while increasing
electricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak
hour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These
improvements are even more pronounced when P2P trading is enabled, highlighting
the synergy between advanced forecasting and market mechanisms for resilient,
economically efficient energy communities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.16479v1' target='_blank'>Arbitrage Tactics in the Local Markets via Hierarchical Multi-agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoyang Zhang, Mina Montazeri, Philipp Heer, Koen Kok, Nikolaos G. Paterakis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-22 11:35:37</h6>
<p class='card-text'>Strategic bidding tactics employed by prosumers in local markets, including
the Local Electricity Market (LEM) and Local Flexibility Market (LFM), have
attracted significant attention due to their potential to enhance economic
benefits for market participants through optimized energy management and
bidding. While existing research has explored strategic bidding in a single
market with multi-agent reinforcement learning (MARL) algorithms, arbitrage
opportunities across local markets remain unexplored. This paper introduces a
hierarchical MARL (HMARL) algorithm designed to enable aggregator arbitrage
across multiple local markets. The strategic behavior of these aggregators in
local markets is modeled as a two-stage Markov game: the first stage involves
the LEM, while the second stage encompasses both the LFM and the balancing
market. To solve this two-stage Markov game, the HMARL framework assigns two
sub-agents to each aggregator, a primary sub-agent and a secondary sub-agent.
Without the arbitrage strategy, these sub-agents operate in silos, with the
primary sub-agent focusing on first-stage profits and the secondary sub-agent
on second-stage profits, each employing independent MARLs. On the contrary,
when implementing the arbitrage strategy with the proposed HMARL, the
sub-agents communicate and coordinate to perform arbitrage across multiple
local markets, enhancing overall efficiency. The case study, conducted under a
scenario where all aggregators employ the arbitrage strategy, shows that
despite higher initial costs in the LEM, this strategy generates substantial
savings in the LFM and the balancing market, resulting in a total profit
increase of $40.6\%$ on average. This highlights the capability of the proposed
HMARL to address the two-stage Markov game and facilitate arbitrage across
local markets, thereby enhancing profitability for participants.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.16382v1' target='_blank'>Application of LLM Guided Reinforcement Learning in Formation Control
  with Collision Avoidance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenhao Yao, Zike Yuan, Xiaoxu Liu, Chi Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-22 09:26:00</h6>
<p class='card-text'>Multi-Agent Systems (MAS) excel at accomplishing complex objectives through
the collaborative efforts of individual agents. Among the methodologies
employed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of
the most efficacious algorithms. However, when confronted with the complex
objective of Formation Control with Collision Avoidance (FCCA): designing an
effective reward function that facilitates swift convergence of the policy
network to an optimal solution. In this paper, we introduce a novel framework
that aims to overcome this challenge. By giving large language models (LLMs) on
the prioritization of tasks and the observable information available to each
agent, our framework generates reward functions that can be dynamically
adjusted online based on evaluation outcomes by employing more advanced
evaluation metrics rather than the rewards themselves. This mechanism enables
the MAS to simultaneously achieve formation control and obstacle avoidance in
dynamic environments with enhanced efficiency, requiring fewer iterations to
reach superior performance levels. Our empirical studies, conducted in both
simulation and real-world settings, validate the practicality and effectiveness
of our proposed approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.16306v1' target='_blank'>COMPASS: Cooperative Multi-Agent Persistent Monitoring using
  Spatio-Temporal Attention Network</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xingjian Zhang, Yizhuo Wang, Guillaume Sartoretti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-22 07:44:08</h6>
<p class='card-text'>Persistent monitoring of dynamic targets is essential in real-world
applications such as disaster response, environmental sensing, and wildlife
conservation, where mobile agents must continuously gather information under
uncertainty. We propose COMPASS, a multi-agent reinforcement learning (MARL)
framework that enables decentralized agents to persistently monitor multiple
moving targets efficiently. We model the environment as a graph, where nodes
represent spatial locations and edges capture topological proximity, allowing
agents to reason over structured layouts and revisit informative regions as
needed. Each agent independently selects actions based on a shared
spatio-temporal attention network that we design to integrate historical
observations and spatial context. We model target dynamics using Gaussian
Processes (GPs), which support principled belief updates and enable
uncertainty-aware planning. We train COMPASS using centralized value estimation
and decentralized policy execution under an adaptive reward setting. Our
extensive experiments demonstrate that COMPASS consistently outperforms strong
baselines in uncertainty reduction, target coverage, and coordination
efficiency across dynamic multi-target scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.16249v1' target='_blank'>Multi-Agent Reinforcement Learning for Sample-Efficient Deep Neural
  Network Mapping</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Srivatsan Krishnan, Jason Jabbour, Dan Zhang, Natasha Jaques, Aleksandra Faust, Shayegan Omidshafiei, Vijay Janapa Reddi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-22 05:51:07</h6>
<p class='card-text'>Mapping deep neural networks (DNNs) to hardware is critical for optimizing
latency, energy consumption, and resource utilization, making it a cornerstone
of high-performance accelerator design. Due to the vast and complex mapping
space, reinforcement learning (RL) has emerged as a promising approach-but its
effectiveness is often limited by sample inefficiency. We present a
decentralized multi-agent reinforcement learning (MARL) framework designed to
overcome this challenge. By distributing the search across multiple agents, our
framework accelerates exploration. To avoid inefficiencies from training
multiple agents in parallel, we introduce an agent clustering algorithm that
assigns similar mapping parameters to the same agents based on correlation
analysis. This enables a decentralized, parallelized learning process that
significantly improves sample efficiency. Experimental results show our MARL
approach improves sample efficiency by 30-300x over standard single-agent RL,
achieving up to 32.61x latency reduction and 16.45x energy-delay product (EDP)
reduction under iso-sample conditions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.15351v1' target='_blank'>One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step
  Policy Optimization for Order Dispatch on Ride-Sharing Platforms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijian Zhao, Sen Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-21 08:04:31</h6>
<p class='card-text'>On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.15174v1' target='_blank'>Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in
  Multi-Agent Traffic Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Justin Turnau, Longchao Da, Khoa Vo, Ferdous Al Rafi, Shreyas Bachiraju, Tiejin Chen, Hua Wei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-21 01:33:59</h6>
<p class='card-text'>Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.14995v1' target='_blank'>LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for
  Real-Time P2P Energy Trading</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chengwei Lou, Zekai Jin, Wei Tang, Guangfei Geng, Jin Yang, Lu Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-20 14:59:18</h6>
<p class='card-text'>Real-time peer-to-peer (P2P) electricity markets dynamically adapt to
fluctuations in renewable energy and variations in demand, maximizing economic
benefits through instantaneous price responses while enhancing grid
flexibility. However, scaling expert guidance for massive personalized
prosumers poses critical challenges, including diverse decision-making demands
and lack of customized modeling frameworks. This paper proposed an integrated
large language model-multi-agent reinforcement learning (LLM-MARL) framework
for real-time P2P energy trading to address challenges such as the limited
technical capability of prosumers, the lack of expert experience, and security
issues of distribution networks. LLMs are introduced as experts to generate
personalized strategy, guiding MARL under the centralized training with
decentralized execution (CTDE) paradigm through imitation learning. A
differential attention-based critic network is designed to enhance convergence
performance. Experimental results demonstrate that LLM generated strategies
effectively substitute human experts. The proposed multi-agent imitation
learning algorithms achieve significantly lower economic costs and voltage
violation rates on test sets compared to baselines algorithms, while
maintaining robust stability. This work provides an effective solution for
real-time P2P electricity market decision-making by bridging expert knowledge
with agent learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.13846v1' target='_blank'>Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in
  Dynamic Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kathrin Korte, Christian Medeiros Adriano, Sona Ghahremani, Holger Giese</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-18 11:59:55</h6>
<p class='card-text'>[Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.12110v1' target='_blank'>Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of
  CAVs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ye Han, Lijun Zhang, Dejian Meng, Zhuang Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-16 10:27:36</h6>
<p class='card-text'>The exploration-exploitation trade-off constitutes one of the fundamental
challenges in reinforcement learning (RL), which is exacerbated in multi-agent
reinforcement learning (MARL) due to the exponential growth of joint
state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)
method for optimizing cooperative decision-making of connected and autonomous
vehicles (CAVs) in mixed traffic. This work presents two primary contributions:
First, we construct a game topology tensor for dynamic traffic flow,
effectively compressing high-dimensional traffic state information and decrease
the search space for MARL algorithms. Second, building upon the designed game
topology tensor and using QMIX as the backbone RL algorithm, we establish a
topology-enhanced MARL framework incorporating visit counts and agent mutual
information. Extensive simulations across varying traffic densities and CAV
penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations
encompassing training dynamics, exploration patterns, macroscopic traffic
performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL
successfully balances exploration and exploitation. Consequently, it exhibits
superior performance in terms of traffic efficiency, safety, decision
smoothness, and task completion. Furthermore, the algorithm demonstrates
decision-making rationality comparable to or exceeding that of human drivers in
both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is
available at
\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.10913v1' target='_blank'>A Learning Framework For Cooperative Collision Avoidance of UAV Swarms
  Leveraging Domain Knowledge</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuangyao Huang, Haibo Zhang, Zhiyi Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-15 02:09:53</h6>
<p class='card-text'>This paper presents a multi-agent reinforcement learning (MARL) framework for
cooperative collision avoidance of UAV swarms leveraging domain
knowledge-driven reward. The reward is derived from knowledge in the domain of
image processing, approximating contours on a two-dimensional field. By
modeling obstacles as maxima on the field, collisions are inherently avoided as
contours never go through peaks or intersect. Additionally, counters are smooth
and energy-efficient. Our framework enables training with large swarm sizes as
the agent interaction is minimized and the need for complex credit assignment
schemes or observation sharing mechanisms in state-of-the-art MARL approaches
are eliminated. Moreover, UAVs obtain the ability to adapt to complex
environments where contours may be non-viable or non-existent through intensive
training. Extensive experiments are conducted to evaluate the performances of
our framework against state-of-the-art MARL algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.10251v1' target='_blank'>ToMacVF : Temporal Macro-action Value Factorization for Asynchronous
  Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenjing Zhang, Wei Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-14 13:18:13</h6>
<p class='card-text'>Existing asynchronous MARL methods based on MacDec-POMDP typically construct
training trajectory buffers by simply sampling limited and biased data at the
endpoints of macro-actions, and directly apply conventional MARL methods on the
buffers. As a result, these methods lead to an incomplete and inaccurate
representation of the macro-action execution process, along with unsuitable
credit assignments. To solve these problems, the Temporal Macro-action Value
Factorization (ToMacVF) is proposed to achieve fine-grained temporal credit
assignment for macro-action contributions. A centralized training buffer,
called Macro-action Segmented Joint Experience Replay Trajectory (Mac-SJERT),
is designed to incorporate with ToMacVF to collect accurate and complete
macro-action execution information, supporting a more comprehensive and precise
representation of the macro-action process. To ensure principled and
fine-grained asynchronous value factorization, the consistency requirement
between joint and individual macro-action selection called Temporal
Macro-action based IGM (To-Mac-IGM) is formalized, proving that it generalizes
the synchronous cases. Based on To-Mac-IGM, a modularized ToMacVF architecture,
which satisfies CTDE principle, is designed to conveniently integrate previous
value factorization methods. Next, the ToMacVF algorithm is devised as an
implementation of the ToMacVF architecture. Experimental results demonstrate
that, compared to asynchronous baselines, our ToMacVF algorithm not only
achieves optimal performance but also exhibits strong adaptability and
robustness across various asynchronous multi-agent experimental scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.10142v1' target='_blank'>Adaptability in Multi-Agent Reinforcement Learning: A Framework and
  Unified Review</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Siyi Hu, Mohamad A Hady, Jianglin Qiao, Jimmy Cao, Mahardhika Pratama, Ryszard Kowalczyk</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-14 10:39:17</h6>
<p class='card-text'>Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.09989v1' target='_blank'>Improving monotonic optimization in heterogeneous multi-agent
  reinforcement learning with optimal marginal deterministic policy gradient</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoyang Yu, Youfang Lin, Shuo Wang, Sheng Han</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-14 07:16:01</h6>
<p class='card-text'>In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.09179v1' target='_blank'>Hide-and-Shill: A Reinforcement Learning Framework for Market
  Manipulation Detection in Symphony-a Decentralized Multi-Agent System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ronghua Shi, Yiou Liu, Xinyu Ying, Yang Tan, Yuchun Feng, Lynn Ai, Bill Shi, Xuhui Wang, Zhuang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-12 07:55:40</h6>
<p class='card-text'>Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.09094v1' target='_blank'>Transformer based Collaborative Reinforcement Learning for Fluid Antenna
  System (FAS)-enabled 3D UAV Positioning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoren Xu, Hao Xu, Dongyu Wei, Walid Saad, Mehdi Bennis, Mingzhe Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-12 00:31:15</h6>
<p class='card-text'>In this paper, a novel Three dimensional (3D) positioning framework of fluid
antenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In
the proposed framework, a set of controlled UAVs cooperatively estimate the
real-time 3D position of a target UAV. Here, the active UAV transmits a
measurement signal to the passive UAVs via the reflection from the target UAV.
Each passive UAV estimates the distance of the active-target-passive UAV link
and selects an antenna port to share the distance information with the base
station (BS) that calculates the real-time position of the target UAV. As the
target UAV is moving due to its task operation, the controlled UAVs must
optimize their trajectories and select optimal antenna port, aiming to estimate
the real-time position of the target UAV. We formulate this problem as an
optimization problem to minimize the target UAV positioning error via
optimizing the trajectories of all controlled UAVs and antenna port selection
of passive UAVs. Here, an attention-based recurrent multi-agent reinforcement
learning (AR-MARL) scheme is proposed, which enables each controlled UAV to use
the local Q function to determine its trajectory and antenna port while
optimizing the target UAV positioning performance without knowing the
trajectories and antenna port selections of other controlled UAVs. Different
from current MARL methods, the proposed method uses a recurrent neural network
(RNN) that incorporates historical state-action pairs of each controlled UAV,
and an attention mechanism to analyze the importance of these historical
state-action pairs, thus improving the global Q function approximation accuracy
and the target UAV positioning accuracy. Simulation results show that the
proposed AR-MARL scheme can reduce the average positioning error by up to 17.5%
and 58.5% compared to the VD-MARL scheme and the proposed method without FAS.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.13370v1' target='_blank'>H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion
  Guidance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shijun Guo, Haoran Xu, Yaming Yang, Ziyu Guan, Wei Zhao, Xinyi Zhang, Yishan Song, Jiwei Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-11 09:56:33</h6>
<p class='card-text'>The openness of social media enables the free exchange of opinions, but it
also presents challenges in guiding opinion evolution towards global consensus.
Existing methods often directly modify user views or enforce cross-group
connections. These intrusive interventions undermine user autonomy, provoke
psychological resistance, and reduce the efficiency of global consensus.
Additionally, due to the lack of a long-term perspective, promoting local
consensus often exacerbates divisions at the macro level. To address these
issues, we propose the hierarchical, non-intrusive opinion guidance framework,
H-NeiFi. It first establishes a two-layer dynamic model based on social roles,
considering the behavioral characteristics of both experts and non-experts.
Additionally, we introduce a non-intrusive neighbor filtering method that
adaptively controls user communication channels. Using multi-agent
reinforcement learning (MARL), we optimize information propagation paths
through a long-term reward function, avoiding direct interference with user
interactions. Experiments show that H-NeiFi increases consensus speed by 22.0%
to 30.7% and maintains global convergence even in the absence of experts. This
approach enables natural and efficient consensus guidance by protecting user
interaction autonomy, offering a new paradigm for social network governance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.07320v1' target='_blank'>Optimizing Communication and Device Clustering for Clustered Federated
  Learning with Differential Privacy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongyu Wei, Xiaoren Xu, Shiwen Mao, Mingzhe Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-09 22:44:26</h6>
<p class='card-text'>In this paper, a secure and communication-efficient clustered federated
learning (CFL) design is proposed. In our model, several base stations (BSs)
with heterogeneous task-handling capabilities and multiple users with
non-independent and identically distributed (non-IID) data jointly perform CFL
training incorporating differential privacy (DP) techniques. Since each BS can
process only a subset of the learning tasks and has limited wireless resource
blocks (RBs) to allocate to users for federated learning (FL) model parameter
transmission, it is necessary to jointly optimize RB allocation and user
scheduling for CFL performance optimization. Meanwhile, our considered CFL
method requires devices to use their limited data and FL model information to
determine their task identities, which may introduce additional communication
overhead. We formulate an optimization problem whose goal is to minimize the
training loss of all learning tasks while considering device clustering, RB
allocation, DP noise, and FL model transmission delay. To solve the problem, we
propose a novel dynamic penalty function assisted value decomposed multi-agent
reinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to
independently determine their connected users, RBs, and DP noise of the
connected users but jointly minimize the training loss of all learning tasks
across all BSs. Different from the existing MARL methods that assign a large
penalty for invalid actions, we propose a novel penalty assignment scheme that
assigns penalty depending on the number of devices that cannot meet
communication constraints (e.g., delay), which can guide the MARL scheme to
quickly find valid actions, thus improving the convergence speed. Simulation
results show that the DPVD-MARL can improve the convergence rate by up to 20%
and the ultimate accumulated rewards by 15% compared to independent Q-learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.07074v1' target='_blank'>Graph-Based Complexity Metrics for Multi-Agent Curriculum Learning: A
  Validated Approach to Task Ordering in Cooperative Coordination Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Farhaan Ebadulla, Dharini Hindlatti, Srinivaasan NS, Apoorva VH, Ayman Aftab</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-09 17:31:35</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) faces significant challenges in
task sequencing and curriculum design, particularly for cooperative
coordination scenarios. While curriculum learning has demonstrated success in
single-agent domains, principled approaches for multi-agent coordination remain
limited due to the absence of validated task complexity metrics. This approach
presents a graph-based coordination complexity metric that integrates agent
dependency entropy, spatial interference patterns, and goal overlap analysis to
predict task difficulty in multi-agent environments. The complexity metric
achieves strong empirical validation with rho = 0.952 correlation (p < 0.001)
between predicted complexity and empirical difficulty determined by random
agent performance evaluation. This approach evaluates the curriculum learning
framework using MADDPG across two distinct coordination environments: achieving
56x performance improvement in tight coordination tasks (MultiWalker) and
demonstrating systematic task progression in cooperative navigation (Simple
Spread). Through systematic analysis, coordination tightness emerges as a
predictor of curriculum learning effectiveness, where environments requiring
strict agent interdependence benefit substantially from structured progression.
This approach provides a validated complexity metric for multi-agent curriculum
design and establishes empirical guidelines for multi-robot coordination
applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.06997v1' target='_blank'>Federated Learning-based MARL for Strengthening Physical-Layer Security
  in B5G Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Deemah H. Tashman, Soumaya Cherkaoui, Walaa Hamouda</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-09 16:24:15</h6>
<p class='card-text'>This paper explores the application of a federated learning-based multi-agent
reinforcement learning (MARL) strategy to enhance physical-layer security (PLS)
in a multi-cellular network within the context of beyond 5G networks. At each
cell, a base station (BS) operates as a deep reinforcement learning (DRL) agent
that interacts with the surrounding environment to maximize the secrecy rate of
legitimate users in the presence of an eavesdropper. This eavesdropper attempts
to intercept the confidential information shared between the BS and its
authorized users. The DRL agents are deemed to be federated since they only
share their network parameters with a central server and not the private data
of their legitimate users. Two DRL approaches, deep Q-network (DQN) and
Reinforce deep policy gradient (RDPG), are explored and compared. The results
demonstrate that RDPG converges more rapidly than DQN. In addition, we
demonstrate that the proposed method outperforms the distributed DRL approach.
Furthermore, the outcomes illustrate the trade-off between security and
complexity.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.06690v1' target='_blank'>Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guobin Zhu, Rui Zhou, Wenkang Ji, Hongyin Zhang, Donglin Wang, Shiyu Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-09 09:34:41</h6>
<p class='card-text'>Multi-task multi-agent reinforcement learning (MT-MARL) has recently gained
attention for its potential to enhance MARL's adaptability across multiple
tasks. However, it is challenging for existing multi-task learning methods to
handle complex problems, as they are unable to handle unrelated tasks and
possess limited knowledge transfer capabilities. In this paper, we propose a
hierarchical approach that efficiently addresses these challenges. The
high-level module utilizes a skill graph, while the low-level module employs a
standard MARL algorithm. Our approach offers two contributions. First, we
consider the MT-MARL problem in the context of unrelated tasks, expanding the
scope of MTRL. Second, the skill graph is used as the upper layer of the
standard hierarchical approach, with training independent of the lower layer,
effectively handling unrelated tasks and enhancing knowledge transfer
capabilities. Extensive experiments are conducted to validate these advantages
and demonstrate that the proposed method outperforms the latest hierarchical
MAPPO algorithms. Videos and code are available at
https://github.com/WindyLab/MT-MARL-SG</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.06004v1' target='_blank'>From General Relation Patterns to Task-Specific Decision-Making in
  Continual Multi-Agent Coordination</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chang Yao, Youfang Lin, Shoucheng Song, Hao Wu, Yuqing Ma, Shang Han, Kai Lv</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-08 14:07:53</h6>
<p class='card-text'>Continual Multi-Agent Reinforcement Learning (Co-MARL) requires agents to
address catastrophic forgetting issues while learning new coordination policies
with the dynamics team. In this paper, we delve into the core of Co-MARL,
namely Relation Patterns, which refer to agents' general understanding of
interactions. In addition to generality, relation patterns exhibit
task-specificity when mapped to different action spaces. To this end, we
propose a novel method called General Relation Patterns-Guided Task-Specific
Decision-Maker (RPG). In RPG, agents extract relation patterns from dynamic
observation spaces using a relation capturer. These task-agnostic relation
patterns are then mapped to different action spaces via a task-specific
decision-maker generated by a conditional hypernetwork. To combat forgetting,
we further introduce regularization items on both the relation capturer and the
conditional hypernetwork. Results on SMAC and LBF demonstrate that RPG
effectively prevents catastrophic forgetting when learning new tasks and
achieves zero-shot generalization to unseen tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.10566v1' target='_blank'>AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous
  Symbol Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hung Ming Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-07 09:52:49</h6>
<p class='card-text'>In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.02698v1' target='_blank'>Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains:
  Benchmarking Strategic Agent Behaviours under Realistically Simulated Market
  Conditions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas Hazenberg, Yao Ma, Seyed Sahand Mohammadi Ziabari, Marijn van Rijswijk</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-03 15:07:37</h6>
<p class='card-text'>This study investigates how Multi-Agent Reinforcement Learning (MARL) can
improve dynamic pricing strategies in supply chains, particularly in contexts
where traditional ERP systems rely on static, rule-based approaches that
overlook strategic interactions among market actors. While recent research has
applied reinforcement learning to pricing, most implementations remain
single-agent and fail to model the interdependent nature of real-world supply
chains. This study addresses that gap by evaluating the performance of three
MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,
within a simulated environment informed by real e-commerce transaction data and
a LightGBM demand prediction model. Results show that rule-based agents achieve
near-perfect fairness (Jain's Index: 0.9896) and the highest price stability
(volatility: 0.024), but they fully lack competitive dynamics. Among MARL
agents, MADQN exhibits the most aggressive pricing behaviour, with the highest
volatility and the lowest fairness (0.5844). MADDPG provides a more balanced
approach, supporting market competition (share volatility: 9.5 pp) while
maintaining relatively high fairness (0.8819) and stable pricing. These
findings suggest that MARL introduces emergent strategic behaviour not captured
by static pricing rules and may inform future developments in dynamic pricing.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2507.01378v1' target='_blank'>RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyao Wang, Rongpeng Li, Sizhao Li, Yuming Xiang, Haiping Wang, Zhifeng Zhao, Honggang Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-07-02 05:44:17</h6>
<p class='card-text'>Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as
a critical research focus, and it typically requires the swarm to navigate
effectively while avoiding obstacles and achieving continuous coverage over
multiple mission targets. Although traditional Multi-Agent Reinforcement
Learning (MARL) approaches offer dynamic adaptability, they are hindered by the
semantic gap in numerical communication and the rigidity of homogeneous role
structures, resulting in poor generalization and limited task scalability.
Recent advances in Large Language Model (LLM)-based control frameworks
demonstrate strong semantic reasoning capabilities by leveraging extensive
prior knowledge. However, due to the lack of online learning and over-reliance
on static priors, these works often struggle with effective exploration,
leading to reduced individual potential and overall system performance. To
address these limitations, we propose a Role-Adaptive LLM-Driven Yoked
navigation algorithm RALLY. Specifically, we first develop an LLM-driven
semantic decision framework that uses structured natural language for efficient
semantic communication and collaborative reasoning. Afterward, we introduce a
dynamic role-heterogeneity mechanism for adaptive role switching and
personalized decision-making. Furthermore, we propose a Role-value Mixing
Network (RMIX)-based assignment strategy that integrates LLM offline priors
with MARL online policies to enable semi-offline training of role selection
strategies. Experiments in the Multi-Agent Particle Environment (MPE)
environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY
outperforms conventional approaches in terms of task coverage, convergence
speed, and generalization, highlighting its strong potential for collaborative
navigation in agentic multi-UAV systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.20039v1' target='_blank'>Learning Bilateral Team Formation in Cooperative Multi-Agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Koorosh Moslemi, Chi-Guhn Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-24 22:40:05</h6>
<p class='card-text'>Team formation and the dynamics of team-based learning have drawn significant
interest in the context of Multi-Agent Reinforcement Learning (MARL). However,
existing studies primarily focus on unilateral groupings, predefined teams, or
fixed-population settings, leaving the effects of algorithmic bilateral
grouping choices in dynamic populations underexplored. To address this gap, we
introduce a framework for learning two-sided team formation in dynamic
multi-agent systems. Through this study, we gain insight into what algorithmic
properties in bilateral team formation influence policy performance and
generalization. We validate our approach using widely adopted multi-agent
scenarios, demonstrating competitive performance and improved generalization in
most scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.19846v1' target='_blank'>JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents
  with Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ai Han, Junxing Hu, Pu Wei, Zhiqian Zhang, Yuhang Guo, Jiawei Lu, Zicheng Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-24 17:59:31</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm
for increasingly complex tasks. However, joint evolution across heterogeneous
agents remains challenging due to cooperative inefficiency and training
instability. In this paper, we propose the joint evolution dynamics for MARL
called JoyAgents-R1, which first applies Group Relative Policy Optimization
(GRPO) to the joint training of heterogeneous multi-agents. By iteratively
refining agents' large language models (LLMs) and memories, the method achieves
holistic equilibrium with optimal decision-making and memory capabilities.
Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on
the behavior of each agent across entire reasoning trajectories to enhance GRPO
sampling efficiency while maintaining policy diversity. Then, our marginal
benefit-driven selection strategy identifies top-$K$ sampling groups with
maximal reward fluctuations, enabling targeted agent model updates that improve
training stability and maximize joint benefits through cost-effective parameter
adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution
mechanism that repurposes GRPO rewards as cost-free supervisory signals to
eliminate repetitive reasoning and accelerate convergence. Experiments across
general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves
performance comparable to that of larger LLMs while built on smaller
open-source models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.19417v1' target='_blank'>Center of Gravity-Guided Focusing Influence Mechanism for Multi-Agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yisak Park, Sunwoo Lee, Seungyul Han</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-24 08:35:15</h6>
<p class='card-text'>Cooperative multi-agent reinforcement learning (MARL) under sparse rewards
presents a fundamental challenge due to limited exploration and insufficient
coordinated attention among agents. In this work, we propose the Focusing
Influence Mechanism (FIM), a novel framework that enhances cooperation by
directing agent influence toward task-critical elements, referred to as Center
of Gravity (CoG) state dimensions, inspired by Clausewitz's military theory.
FIM consists of three core components: (1) identifying CoG state dimensions
based on their stability under agent behavior, (2) designing counterfactual
intrinsic rewards to promote meaningful influence on these dimensions, and (3)
encouraging persistent and synchronized focus through eligibility-trace-based
credit accumulation. These mechanisms enable agents to induce more targeted and
effective state transitions, facilitating robust cooperation even in extremely
sparse reward settings. Empirical evaluations across diverse MARL benchmarks
demonstrate that the proposed FIM significantly improves cooperative
performance compared to baselines.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>