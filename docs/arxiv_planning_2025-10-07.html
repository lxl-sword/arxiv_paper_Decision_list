<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-10-07</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-10-07</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.05091v1' target='_blank'>Factuality Matters: When Image Generation and Editing Meet Structured
  Visuals</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Le Zhuo, Songhao Han, Yuandong Pu, Boxiang Qiu, Sayak Paul, Yue Liao, Yihao Liu, Jie Shao, Xi Chen, Si Liu, Hongsheng Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 17:56:55</h6>
<p class='card-text'>While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.05080v1' target='_blank'>MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yangyang Wang, Tayo Fabusuyi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 17:50:56</h6>
<p class='card-text'>This study presents a novel small-area estimation framework to enhance urban
transportation planning through detailed characterization of travel behavior.
Our approach improves on the four-step travel model by employing publicly
available microdata files and machine learning methods to predict travel
behavior for a representative, synthetic population at small geographic areas.
This approach enables high-resolution estimation of trip generation, trip
distribution, mode choice, and route assignment. Validation using ACS/PUMS
work-commute datasets demonstrates that our framework achieves higher accuracy
compared to conventional approaches. The resulting granular insights enable the
tailoring of interventions to address localized situations and support a range
of policy applications and targeted interventions, including the optimal
placement of micro-fulfillment centers, effective curb-space management, and
the design of more inclusive transportation solutions particularly for
vulnerable communities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04991v1' target='_blank'>Efficient Navigation in Unknown Indoor Environments with Vision-Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:D. Schwartz, K. Kondo, J. P. How</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 16:26:16</h6>
<p class='card-text'>We present a novel high-level planning framework that leverages
vision-language models (VLMs) to improve autonomous navigation in unknown
indoor environments with many dead ends. Traditional exploration methods often
take inefficient routes due to limited global reasoning and reliance on local
heuristics. In contrast, our approach enables a VLM to reason directly about an
occupancy map in a zero-shot manner, selecting subgoals that are likely to lead
to more efficient paths. At each planning step, we convert a 3D occupancy grid
into a partial 2D map of the environment, and generate candidate subgoals. Each
subgoal is then evaluated and ranked against other candidates by the model. We
integrate this planning scheme into DYNUS \cite{kondo2025dynus}, a
state-of-the-art trajectory planner, and demonstrate improved navigation
efficiency in simulation. The VLM infers structural patterns (e.g., rooms,
corridors) from incomplete maps and balances the need to make progress toward a
goal against the risk of entering unknown space. This reduces common greedy
failures (e.g., detouring into small rooms) and achieves about 10\% shorter
paths on average.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04829v1' target='_blank'>Selection Bias in Hybrid Randomized Controlled Trials using External
  Controls: A Simulation Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Han Chang Chiam, Franz KÃ¶nig, Martin Posch</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 14:13:30</h6>
<p class='card-text'>Hybrid randomized controlled trials (hybrid RCTs) integrate external control
data, such as historical or concurrent data, with data from randomized trials.
While numerous frequentist and Bayesian methods, such as the test-then-pool and
Meta-Analytic-Predictive prior, have been developed to account for potential
disagreement between the external control and randomized data, they cannot
ensure strict type I error rate control. However, these methods can reduce
biases stemming from systematic differences between external controls and trial
data. A critical yet underexplored issue in hybrid RCTs is the prespecification
of external data to be used in analysis.
  The validity of statistical conclusions in hybrid RCTs depends on the
assumption that external control selection is independent of historical trials
outcomes. In practice, historical data may be accessible during the planning
stage, potentially influencing important decisions, such as which historical
datasets to include or the sample size of the prospective part of the hybrid
trial, thus introducing bias. Such data-driven design choices can be an
additional source of bias, which can occur even when historical and prospective
controls are exchangeable.
  Through a simulation study, we quantify the biases introduced by
outcome-dependent selection of historical controls in hybrid RCTs using both
Bayesian and frequentist approaches, and discuss potential strategies to
mitigate this bias. Our scenarios consider variability and time trends in the
historical studies, distributional shifts between historical and prospective
control groups, sample sizes and allocation ratios, as well as the number of
studies included. The impact of different rules for selecting external controls
is demonstrated using a clinical trial example.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04807v1' target='_blank'>Efficient Probabilistic Planning with Maximum-Coverage Distributionally
  Robust Backward Reachable Trees</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alex Rose, Naman Aggarwal, Christopher Jewison, Jonathan P. How</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 13:46:55</h6>
<p class='card-text'>This paper presents a new multi-query motion planning algorithm for linear
Gaussian systems with the goal of reaching a Euclidean ball with high
probability. We develop a new formulation for ball-shaped ambiguity sets of
Gaussian distributions and leverage it to develop a distributionally robust
belief roadmap construction algorithm. This algorithm synthe- sizes robust
controllers which are certified to be safe for maximal size ball-shaped
ambiguity sets of Gaussian distributions. Our algorithm achieves better
coverage than the maximal coverage algorithm for planning over Gaussian
distributions [1], and we identify mild conditions under which our algorithm
achieves strictly better coverage. For the special case of no process noise or
state constraints, we formally prove that our algorithm achieves maximal
coverage. In addition, we present a second multi-query motion planning
algorithm for linear Gaussian systems with the goal of reaching a region
parameterized by the Minkowski sum of an ellipsoid and a Euclidean ball with
high probability. This algorithm plans over ellipsoidal sets of maximal size
ball-shaped ambiguity sets of Gaussian distributions, and provably achieves
equal or better coverage than the best-known algorithm for planning over
ellipsoidal ambiguity sets of Gaussian distributions [2]. We demonstrate the
efficacy of both methods in a wide range of conditions via extensive simulation
experiments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04781v1' target='_blank'>Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage
  Digitization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Javed Ahmad, Federico DassiÃ¨, Selene Frascella, Gabriele Marchello, Ferdinando Cannella, Arianna Traviglia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 12:58:41</h6>
<p class='card-text'>High-fidelity 3D scanning is essential for preserving cultural heritage
artefacts, supporting documentation, analysis, and long-term conservation.
However, conventional methods typically require specialized expertise and
manual intervention to maintain optimal scanning conditions and coverage. We
present an automated two-robot scanning system that eliminates the need for
handheld or semi-automatic workflows by combining coordinated robotic
manipulation with high-resolution 3D scanning. Our system parameterizes the
scanning space into distinct regions, enabling coordinated motion planning
between a scanner-equipped robot and a tray-handling robot. Optimized
trajectory planning and waypoint distribution ensure comprehensive surface
coverage, minimize occlusions, and balance reconstruction accuracy with system
efficiency. Experimental results show that our approach achieves significantly
lower Chamfer Distance and higher F-score compared to baseline methods,
offering superior geometric accuracy, improved digitization efficiency, and
reduced reliance on expert operators.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04696v1' target='_blank'>Building Gradient by Gradient: Decentralised Energy Functions for
  Bimanual Robot Assembly</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexander L. Mitchell, Joe Watson, Ingmar Posner</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 11:10:11</h6>
<p class='card-text'>There are many challenges in bimanual assembly, including high-level
sequencing, multi-robot coordination, and low-level, contact-rich operations
such as component mating. Task and motion planning (TAMP) methods, while
effective in this domain, may be prohibitively slow to converge when adapting
to disturbances that require new task sequencing and optimisation. These events
are common during tight-tolerance assembly, where difficult-to-model dynamics
such as friction or deformation require rapid replanning and reattempts.
Moreover, defining explicit task sequences for assembly can be cumbersome,
limiting flexibility when task replanning is required. To simplify this
planning, we introduce a decentralised gradient-based framework that uses a
piecewise continuous energy function through the automatic composition of
adaptive potential functions. This approach generates sub-goals using only
myopic optimisation, rather than long-horizon planning. It demonstrates
effectiveness at solving long-horizon tasks due to the structure and adaptivity
of the energy function. We show that our approach scales to physical bimanual
assembly tasks for constructing tight-tolerance assemblies. In these
experiments, we discover that our gradient-based rapid replanning framework
generates automatic retries, coordinated motions and autonomous handovers in an
emergent fashion.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04673v1' target='_blank'>Watch and Learn: Learning to Use Computers from Online Videos</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chan Hee Song, Yiwen Song, Palash Goyal, Yu Su, Oriana Riva, Hamid Palangi, Tomas Pfister</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 10:29:00</h6>
<p class='card-text'>Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04615v1' target='_blank'>Design Process of a Self Adaptive Smart Serious Games Ecosystem</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:X. Tao, P. Chen, M. Tsami, F. Khayati, M. Eckert</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 09:28:31</h6>
<p class='card-text'>This paper outlines the design vision and planned evolution of Blexer v3, a
modular and AI-driven rehabilitation ecosystem based on serious games. Building
on insights from previous versions of the system, we propose a new architecture
that aims to integrate multimodal sensing, real-time reasoning, and intelligent
control. The envisioned system will include distinct modules for data
collection, user state inference, and gameplay adaptation. Key features such as
dynamic difficulty adjustment (DDA) and procedural content generation (PCG) are
also considered to support personalized interventions. We present the complete
conceptual framework of Blexer v3, which defines the modular structure and data
flow of the system. This serves as the foundation for the next phase: the
development of a functional prototype and its integration into clinical
rehabilitation scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04592v1' target='_blank'>MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile
  Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yilin Mei, Peng Qiu, Wei Zhang, WenChao Zhang, Wenjie Song</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 08:46:56</h6>
<p class='card-text'>Recent advances in robotics have been largely driven by imitation learning,
which depends critically on large-scale, high-quality demonstration data.
However, collecting such data remains a significant challenge-particularly for
mobile manipulators, which must coordinate base locomotion and arm manipulation
in high-dimensional, dynamic, and partially observable environments.
Consequently, most existing research remains focused on simpler tabletop
scenarios, leaving mobile manipulation relatively underexplored. To bridge this
gap, we present \textit{MobRT}, a digital twin-based framework designed to
simulate two primary categories of complex, whole-body tasks: interaction with
articulated objects (e.g., opening doors and drawers) and mobile-base
pick-and-place operations. \textit{MobRT} autonomously generates diverse and
realistic demonstrations through the integration of virtual kinematic control
and whole-body motion planning, enabling coherent and physically consistent
execution. We evaluate the quality of \textit{MobRT}-generated data across
multiple baseline algorithms, establishing a comprehensive benchmark and
demonstrating a strong correlation between task success and the number of
generated trajectories. Experiments integrating both simulated and real-world
demonstrations confirm that our approach markedly improves policy
generalization and performance, achieving robust results in both simulated and
real-world environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04560v1' target='_blank'>ContextNav: Towards Agentic Multimodal In-Context Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Honghao Fu, Yuan Ouyang, Kai-Wei Chang, Yiwei Wang, Zi Huang, Yujun Cai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 07:49:52</h6>
<p class='card-text'>Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04532v1' target='_blank'>More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in
  Training Vision-Language Driving Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xurui Song, Shuo Huai, JingJing Jiang, Jiayi Kong, Jun Luo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 06:50:16</h6>
<p class='card-text'>Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04510v1' target='_blank'>Real-time Prediction of Urban Sound Propagation with Conditioned
  Normalizing Flows</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Achim Eckerle, Martin Spitznagel, Janis Keuper</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 06:00:08</h6>
<p class='card-text'>Accurate and fast urban noise prediction is pivotal for public health and for
regulatory workflows in cities, where the Environmental Noise Directive
mandates regular strategic noise maps and action plans, often needed in
permission workflows, right-of-way allocation, and construction scheduling.
Physics-based solvers are too slow for such time-critical, iterative "what-if"
studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating
for generating standards-compliant urban sound-pressure maps from 2D urban
layouts in real time per 256x256 map on a single RTX 4090), enabling
interactive exploration directly on commodity hardware. On datasets covering
Baseline, Diffraction, and Reflection regimes, our model accelerates map
generation by >2000 times over a reference solver while improving NLoS accuracy
by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE
with high structural fidelity. The model reproduces diffraction and
interference patterns and supports instant recomputation under source or
geometry changes, making it a practical engine for urban planning, compliance
mapping, and operations (e.g., temporary road closures, night-work variance
assessments).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04494v1' target='_blank'>NaturalEdit: Code Modification through Direct Interaction with Adaptive
  Natural Language Representation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ningzhi Tang, David Meininger, Gelei Xu, Yiyu Shi, Yu Huang, Collin McMillan, Toby Jia-Jun Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-06 05:07:34</h6>
<p class='card-text'>Code modification requires developers to comprehend code, plan changes,
articulate intentions, and validate outcomes, making it a cognitively demanding
process. Generated natural language code summaries aid comprehension but remain
static and limited in supporting the full workflow. We present NaturalEdit, a
system that makes code summaries interactive and adaptive representations
directly linked to source code. Grounded in the Cognitive Dimensions of
Notations, NaturalEdit implements a paradigm of code modification through
interaction with natural language representations through three key features:
(1) adaptive multi-faceted representation of code summaries with flexible
Abstraction Gradient; (2) interactive mapping mechanisms between summaries and
codes, ensuring a tight Closeness of Mapping; and (3) intent-driven,
bidirectional synchronization that reduces Viscosity in editing and validation.
A technical evaluation confirms the performance of NaturalEdit, and a user
study with 12 developers shows that it enhances comprehension, intent
articulation, and validation, giving developers greater confidence and control.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04386v1' target='_blank'>SSM-CGM: Interpretable State-Space Forecasting Model of Continuous
  Glucose Monitoring for Personalized Diabetes Management</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shakson Isaac, Yentl Collin, Chirag Patel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 22:37:28</h6>
<p class='card-text'>Continuous glucose monitoring (CGM) generates dense data streams critical for
diabetes management, but most used forecasting models lack interpretability for
clinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting
model that integrates CGM and wearable activity signals from the AI-READI
cohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer
baseline, adds interpretability through variable selection and temporal
attribution, and enables counterfactual forecasts simulating how planned
changes in physiological signals (e.g., heart rate, respiration) affect
near-term glucose. Together, these features make SSM-CGM an interpretable,
physiologically grounded framework for personalized diabetes management.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04358v1' target='_blank'>Score-based generative emulation of impact-relevant Earth system model
  outputs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shahine Bouabid, Andre Nogueira Souza, Raffaele Ferrari</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 20:54:19</h6>
<p class='card-text'>Policy targets evolve faster than the Couple Model Intercomparison Project
cycles, complicating adaptation and mitigation planning that must often contend
with outdated projections. Climate model output emulators address this gap by
offering inexpensive surrogates that can rapidly explore alternative futures
while staying close to Earth System Model (ESM) behavior. We focus on emulators
designed to provide inputs to impact models. Using monthly ESM fields of
near-surface temperature, precipitation, relative humidity, and wind speed, we
show that deep generative models have the potential to model jointly the
distribution of variables relevant for impacts. The specific model we propose
uses score-based diffusion on a spherical mesh and runs on a single mid-range
graphical processing unit. We introduce a thorough suite of diagnostics to
compare emulator outputs with their parent ESMs, including their probability
densities, cross-variable correlations, time of emergence, or tail behavior. We
evaluate performance across three distinct ESMs in both pre-industrial and
forced regimes. The results show that the emulator produces distributions that
closely match the ESM outputs and captures key forced responses. They also
reveal important failure cases, notably for variables with a strong regime
shift in the seasonal cycle. Although not a perfect match to the ESM, the
inaccuracies of the emulator are small relative to the scale of internal
variability in ESM projections. We therefore argue that it shows potential to
be useful in supporting impact assessment. We discuss priorities for future
development toward daily resolution, finer spatial scales, and bias-aware
training. Code is made available at https://github.com/shahineb/climemu.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04355v1' target='_blank'>Quantizer Design for Finite Model Approximations, Model Learning, and
  Quantized Q-Learning for MDPs with Unbounded Spaces</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Osman Bicer, Ali D. Kara, Serdar Yuksel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 20:39:52</h6>
<p class='card-text'>In this paper, for Markov decision processes (MDPs) with unbounded state
spaces we present refined upper bounds presented in [Kara et. al. JMLR'23] on
finite model approximation errors via optimizing the quantizers used for finite
model approximations. We also consider implications on quantizer design for
quantized Q-learning and empirical model learning, and the performance of
policies obtained via Q-learning where the quantized state is treated as the
state itself. We highlight the distinctions between planning, where
approximating MDPs can be independently designed, and learning (either via
Q-learning or empirical model learning), where approximating MDPs are
restricted to be defined by invariant measures of Markov chains under
exploration policies, leading to significant subtleties on quantizer design
performance, even though asymptotic near optimality can be established under
both setups. In particular, under Lyapunov growth conditions, we obtain
explicit upper bounds which decay to zero as the number of bins approaches
infinity.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04346v1' target='_blank'>Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression
  Comparisons, Shadow Fading, and Calibrated Fade Margins</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nahshon Mokua Obiri, Kristof Van Laerhoven</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 20:14:48</h6>
<p class='card-text'>Indoor LoRaWAN propagation is shaped by structural and time-varying context
factors, which challenge log-distance models and the assumption of log-normal
shadowing. We present an environment-aware, statistically disciplined path loss
framework evaluated using leakage-safe cross-validation on a 12-month campaign
in an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is
augmented with environmental covariates (relative humidity, temperature, carbon
dioxide, particulate matter, and barometric pressure), as well as the
signal-to-noise ratio. We compare multiple linear regression with regularized
variants, Bayesian linear regression, and a selective second-order polynomial
applied to continuous drivers. Predictor relevance is established using
heteroscedasticity-robust Type II and III analysis of variance and nested
partial F tests. Shadow fading is profiled with kernel density estimation and
non-parametric families, including Normal, Skew-Normal, Student's t, and
Gaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07
to 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are
non-Gaussian; a 3-component mixture captures a sharp core with a light, broad
tail. We convert accuracy into reliability by prescribing the fade margin as
the upper-tail quantile of cross-validated residuals, quantifying uncertainty
via a moving-block bootstrap, and validating on a held-out set. At 99% packet
delivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7
to 27.9 dB for linear baselines. This result presents a deployment-ready,
interpretable workflow with calibrated reliability control for indoor Internet
of Things planning, aligned with 6G targets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04333v1' target='_blank'>RAP: 3D Rasterization Augmented End-to-End Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lan Feng, Yang Gao, Eloi Zablocki, Quanyi Li, Wuyang Li, Sichao Liu, Matthieu Cord, Alexandre Alahi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 19:31:24</h6>
<p class='card-text'>Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04313v1' target='_blank'>Convex Formulation of the Zero Emission Vessel Route Planning Problem</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Antti Ritari, Jani Romanoff, Kari Tammi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 18:14:52</h6>
<p class='card-text'>This paper focuses on the zero emission vessel route planning problem, which
deals with cost-effective planning of battery-electric vessel services for
predetermined routes. Vessel characteristics (including battery capacity),
fleet size, cyclic schedule frequencies, sailing leg speeds, and shore charging
infrastructure are jointly optimized. The problem is nonlinear and nonconvex in
its original form, which makes it intractable for most real-world instances.
The conventional approach in the literature is to solve a linear approximation
by restricting vessel designs and sailing leg speeds to a small finite set.
Contrary to the conventional linearization approach, this paper deals with the
nonlinearities directly. We show that the problem exhibits a hidden convex
structure uncovered by nonlinear changes of variables. By exploiting the
favorable convex form of the transformed problem, we solve it in a few seconds
using a free off-the-shelf solver that requires no initial guesses, variable
bounds, or parameter tuning. We then easily recover the exact solution to the
original nonconvex problem by reversing the variable changes. We provide an
open-source implementation of our method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04280v1' target='_blank'>A KL-regularization framework for learning to plan with adaptive priors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ãlvaro Serra-Gomez, Daniel Jarne Ornia, Dhruva Tirumala, Thomas Moerland</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 16:45:38</h6>
<p class='card-text'>Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04278v1' target='_blank'>Integrated Planning and Control on Manifolds: Factor Graph
  Representation and Toolkit</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Peiwen Yang, Weisong Wen, Runqiu Yang, Yuanyuan Zhang, Jiahao Hu, Yingming Chen, Naigui Xiao, Jiaqi Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 16:36:33</h6>
<p class='card-text'>Model predictive control (MPC) faces significant limitations when applied to
systems evolving on nonlinear manifolds, such as robotic attitude dynamics and
constrained motion planning, where traditional Euclidean formulations struggle
with singularities, over-parameterization, and poor convergence. To overcome
these challenges, this paper introduces FactorMPC, a factor-graph based MPC
toolkit that unifies system dynamics, constraints, and objectives into a
modular, user-friendly, and efficient optimization structure. Our approach
natively supports manifold-valued states with Gaussian uncertainties modeled in
tangent spaces. By exploiting the sparsity and probabilistic structure of
factor graphs, the toolkit achieves real-time performance even for
high-dimensional systems with complex constraints. The velocity-extended
on-manifold control barrier function (CBF)-based obstacle avoidance factors are
designed for safety-critical applications. By bridging graphical models with
safety-critical MPC, our work offers a scalable and geometrically consistent
framework for integrated planning and control. The simulations and experimental
results on the quadrotor demonstrate superior trajectory tracking and obstacle
avoidance performance compared to baseline methods. To foster research
reproducibility, we have provided open-source implementation offering
plug-and-play factors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04243v1' target='_blank'>The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast):
  Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and
  Test-Time Adaptation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jincan Lou, Jingkun Chen, Haoquan Li, Hang Li, Wenjian Huang, Weihua Chen, Fan Wang, Jianguo Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 15:18:53</h6>
<p class='card-text'>Accurate liver segmentation from contrast-enhanced MRI is essential for
diagnosis, treatment planning, and disease monitoring. However, it remains
challenging due to limited annotated data, heterogeneous enhancement protocols,
and significant domain shifts across scanners and institutions. Traditional
image-to-image translation frameworks have made great progress in domain
generalization, but their application is not straightforward. For example,
Pix2Pix requires image registration, and cycle-GAN cannot be integrated
seamlessly into segmentation pipelines. Meanwhile, these methods are originally
used to deal with cross-modality scenarios, and often introduce structural
distortions and suffer from unstable training, which may pose drawbacks in our
single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a
compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary
phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised
mean teacher scheme to exploit large amounts of unlabeled volumes. A domain
adaptation module, incorporating a randomized histogram-based style appearance
transfer function and a trainable contrast-aware network, enriches domain
diversity and mitigates cross-center variability. Furthermore, a continual
test-time adaptation strategy is employed to improve robustness during
inference. Extensive experiments demonstrate that our framework consistently
outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff
Distance while exhibiting strong generalization to unseen domains under
low-annotation conditions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04234v1' target='_blank'>Flexible Locomotion Learning with Diffusion Model Predictive Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Runhan Huang, Haldun Balim, Heng Yang, Yilun Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 14:51:13</h6>
<p class='card-text'>Legged locomotion demands controllers that are both robust and adaptable,
while remaining compatible with task and safety considerations. However,
model-free reinforcement learning (RL) methods often yield a fixed policy that
can be difficult to adapt to new behaviors at test time. In contrast, Model
Predictive Control (MPC) provides a natural approach to flexible behavior
synthesis by incorporating different objectives and constraints directly into
its optimization process. However, classical MPC relies on accurate dynamics
models, which are often difficult to obtain in complex environments and
typically require simplifying assumptions. We present Diffusion-MPC, which
leverages a learned generative diffusion model as an approximate dynamics prior
for planning, enabling flexible test-time adaptation through reward and
constraint based optimization. Diffusion-MPC jointly predicts future states and
actions; at each reverse step, we incorporate reward planning and impose
constraint projection, yielding trajectories that satisfy task objectives while
remaining within physical limits. To obtain a planning model that adapts beyond
imitation pretraining, we introduce an interactive training algorithm for
diffusion based planner: we execute our reward-and-constraint planner in
environment, then filter and reweight the collected trajectories by their
realized returns before updating the denoiser. Our design enables strong
test-time adaptability, allowing the planner to adjust to new reward
specifications without retraining. We validate Diffusion-MPC on real world,
demonstrating strong locomotion and flexible adaptation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04218v1' target='_blank'>Pedestrian collision avoidance in hemianopia during natural walking in
  immersive virtual reality</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jonathan K. Doyon, Sujin Kim, Alex D. Hwang, Jae-Hyun Jung</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 14:20:32</h6>
<p class='card-text'>Homonymous hemianopia (HH) patients report difficulties in avoiding
collisions with other pedestrians. We evaluated pedestrian collision detection
and avoidance behaviors in HH patients and healthy controls using a novel
virtual reality (VR) walking with pedestrians, which enables natural walking
behavior in an empty real-world corridor while viewing an immersive VR
environment (shopping mall with colliding and other pedestrians) presented in a
head-mounted display (HMD). Critically, it measures avoidance maneuvers in
addition to collision detection. Colliding and non-colliding pedestrian
scenarios were developed for Meta Quest 2 using Unity. Ten normal vision (NV)
subjects and 12 HH subjects detected and avoided collisions with virtual
approaching and overtaken pedestrians initialized at bearing angles of 20, 40,
and 60 degrees, with planned time-to-collision of 6 seconds in each trial. HH
subjects were less likely to detect and more likely to collide with pedestrians
than NV, particularly for blind-side targets. Response times did not differ
between groups but were faster for overtaken pedestrians. HH subjects also
biased their head rotations toward the blind side and more after detection
compared to before. Collision avoidance difficulties as reported by HH
subjects, which clinical measures fail to capture, were recorded and analyzed
with objective measures. These metrics may offer further insights into the
underlying mechanisms driving collision avoidance behaviors. Our HMD-VR
collision detection and avoidance paradigm enables natural walking behaviors
and offers an affordable, objective assessment tool that may be adopted by
clinicians for mobility enhancement and rehabilitation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04171v1' target='_blank'>VBM-NET: Visual Base Pose Learning for Mobile Manipulation using
  Equivariant TransporterNet and GNNs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lakshadeep Naik, Adam Fischer, Daniel Duberg, Danica Kragic</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 12:17:56</h6>
<p class='card-text'>In Mobile Manipulation, selecting an optimal mobile base pose is essential
for successful object grasping. Previous works have addressed this problem
either through classical planning methods or by learning state-based policies.
They assume access to reliable state information, such as the precise object
poses and environment models. In this work, we study base pose planning
directly from top-down orthographic projections of the scene, which provide a
global overview of the scene while preserving spatial structure. We propose
VBM-NET, a learning-based method for base pose selection using such top-down
orthographic projections. We use equivariant TransporterNet to exploit spatial
symmetries and efficiently learn candidate base poses for grasping. Further, we
use graph neural networks to represent a varying number of candidate base poses
and use Reinforcement Learning to determine the optimal base pose among them.
We show that VBM-NET can produce comparable solutions to the classical methods
in significantly less computation time. Furthermore, we validate sim-to-real
transfer by successfully deploying a policy trained in simulation to real-world
mobile manipulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04161v1' target='_blank'>HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of
  Unknown Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Longrui Yang, Yiyu Wang, Jingfan Tang, Yunpeng Lv, Shizhe Zhao, Chao Cao, Zhongqiang Ren</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 11:30:39</h6>
<p class='card-text'>This paper considers the path planning problem for autonomous exploration of
an unknown environment using multiple heterogeneous robots such as drones,
wheeled, and legged robots, which have different capabilities to traverse
complex terrains. A key challenge there is to intelligently allocate the robots
to the unknown areas to be explored and determine the visiting order of those
spaces subject to traversablity constraints, which leads to a large scale
constrained optimization problem that needs to be quickly and iteratively
solved every time when new space are explored. To address the challenge, we
propose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging
a recent hierarchical method that decompose the exploration into global
planning and local planning. The major contribution in HEHA is its global
planning, where we propose a new routing algorithm PEAF (Partial Anytime Focal
search) that can quickly find bounded sub-optimal solutions to minimize the
maximum path length among the agents subject to traversability constraints.
Additionally, the local planner in HEHA also considers heterogeneity to avoid
repeated and duplicated exploration among the robots. The experimental results
show that, our HEHA can reduce up to 30% of the exploration time than the
baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04074v1' target='_blank'>Feedback Matters: Augmenting Autonomous Dissection with Visual and
  Topological Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chung-Pang Wang, Changwei Chen, Xiao Liang, Soofiyan Atar, Florian Richter, Michael Yip</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 07:26:40</h6>
<p class='card-text'>Autonomous surgical systems must adapt to highly dynamic environments where
tissue properties and visual cues evolve rapidly. Central to such adaptability
is feedback: the ability to sense, interpret, and respond to changes during
execution. While feedback mechanisms have been explored in surgical robotics,
ranging from tool and tissue tracking to error detection, existing methods
remain limited in handling the topological and perceptual challenges of tissue
dissection. In this work, we propose a feedback-enabled framework for
autonomous tissue dissection that explicitly reasons about topological changes
from endoscopic images after each dissection action. This structured feedback
guides subsequent actions, enabling the system to localize dissection progress
and adapt policies online. To improve the reliability of such feedback, we
introduce visibility metrics that quantify tissue exposure and formulate
optimal controller designs that actively manipulate tissue to maximize
visibility. Finally, we integrate these feedback mechanisms with both
planning-based and learning-based dissection methods, and demonstrate
experimentally that they significantly enhance autonomy, reduce errors, and
improve robustness in complex surgical scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04050v1' target='_blank'>A Dynamic Programming Approach to Evader Pathfinding in Static Pursuit
  Scenarios</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sukanya Samanta, Manohar Reddy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 06:12:42</h6>
<p class='card-text'>The interdiction of escaping adversaries in urban networks is a critical
security challenge. State-of-the-art game-theoretic models, such as the Escape
Interdiction Game (EIG), provide comprehensive frameworks but assume a highly
dynamic interaction and entail significant computational complexity, which can
be prohibitive for real-time applications. This paper investigates a crucial
sub-problem: an evader's optimal pathfinding calculus when faced with a static
or pre-determined defender deployment. We propose the Dynamic Programming for
Evader Route Optimization (DPERO) algorithm, which models the environment as a
graph with probabilistic risks at various nodes. By transforming the
multiplicative survival objective into an additive cost function using
logarithms, we frame the task as a shortest path problem solvable with value
iteration. This approach allows for the efficient computation of a path that
optimally balances safety and distance. Experimental results on simulated grid
networks demonstrate that DPERO identifies routes with significantly higher
survival probabilities compared to naive shortest-path baselines, validating
its efficacy as a practical tool for vulnerability analysis and strategic
planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.04041v1' target='_blank'>SITCOM: Scaling Inference-Time COMpute for VLAs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ayudh Saxena, Harsh Shah, Sandeep Routray, Rishi Rajesh Shah, Esha Pahwa</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-05 05:24:08</h6>
<p class='card-text'>Learning robust robotic control policies remains a major challenge due to the
high cost of collecting labeled data, limited generalization to unseen
environments, and difficulties in planning over long horizons. While
Vision-Language-Action (VLA) models offer a promising solution by grounding
natural language instructions into single-step control commands, they often
lack mechanisms for lookahead and struggle with compounding errors in dynamic
tasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs
(SITCOM), a framework that augments any pretrained VLA with model-based
rollouts and reward-based trajectory selection, inspired by Model Predictive
Control algorithm. SITCOM leverages a learned dynamics model to simulate
multi-step action rollouts to select the best candidate plan for real-world
execution, transforming one-shot VLAs into robust long-horizon planners. We
develop an efficient transformer-based dynamics model trained on large-scale
BridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim
gap, and score candidate rollouts using rewards from simulator. Through
comprehensive evaluation across multiple tasks and settings in the SIMPLER
environment, we demonstrate that SITCOM when combined with a good reward
function can significantly improve task completion rate from 48% to 72% using
trained dynamics model.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>