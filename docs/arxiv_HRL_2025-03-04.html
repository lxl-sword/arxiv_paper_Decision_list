<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>HRL - 2025-03-04</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>HRL - 2025-03-04</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20380v1' target='_blank'>Multi-Turn Code Generation Through Single-Step Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arnav Kumar Jain, Gonzalo Gonzalez-Pumariega, Wayne Chen, Alexander M Rush, Wenting Zhao, Sanjiban Choudhury</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:55:05</h6>
<p class='card-text'>We address the problem of code generation from multi-turn execution feedback.
Existing methods either generate code without feedback or use complex,
hierarchical reinforcement learning to optimize multi-turn rewards. We propose
a simple yet scalable approach, $\mu$Code, that solves multi-turn code
generation using only single-step rewards. Our key insight is that code
generation is a one-step recoverable MDP, where the correct code can be
recovered from any intermediate code state in a single turn. $\mu$Code
iteratively trains both a generator to provide code solutions conditioned on
multi-turn execution feedback and a verifier to score the newly generated code.
Experimental evaluations show that our approach achieves significant
improvements over the state-of-the-art baselines. We provide analysis of the
design choices of the reward models and policy, and show the efficacy of
$\mu$Code at utilizing the execution feedback. Our code is available at
https://github.com/portal-cornell/muCode.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.15425v3' target='_blank'>TAG: A Decentralized Framework for Multi-Agent Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Giuseppe Paolo, Abdelhakim Benechehab, Hamza Cherkaoui, Albert Thomas, Balázs Kégl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-21 12:52:16</h6>
<p class='card-text'>Hierarchical organization is fundamental to biological systems and human
societies, yet artificial intelligence systems often rely on monolithic
architectures that limit adaptability and scalability. Current hierarchical
reinforcement learning (HRL) approaches typically restrict hierarchies to two
levels or require centralized training, which limits their practical
applicability. We introduce TAME Agent Framework (TAG), a framework for
constructing fully decentralized hierarchical multi-agent systems.TAG enables
hierarchies of arbitrary depth through a novel LevelEnv concept, which
abstracts each hierarchy level as the environment for the agents above it. This
approach standardizes information flow between levels while preserving loose
coupling, allowing for seamless integration of diverse agent types. We
demonstrate the effectiveness of TAG by implementing hierarchical architectures
that combine different RL agents across multiple levels, achieving improved
performance over classical multi-agent RL baselines on standard benchmarks. Our
results show that decentralized hierarchical organization enhances both
learning speed and final performance, positioning TAG as a promising direction
for scalable multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06772v1' target='_blank'>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 18:51:47</h6>
<p class='card-text'>We present that hierarchical LLM reasoning via scaling thought templates can
effectively optimize the reasoning search space and outperform the mathematical
reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.
We train our ReasonFlux-32B model with only 8 GPUs and introduces three
innovations: (i) a structured and generic thought template library, containing
around 500 high-level thought templates capable of generalizing to similar or
relevant reasoning problems; (ii) performing hierarchical reinforcement
learning on a sequence of thought templates instead of long CoTs, optimizing a
base LLM to plan out an optimal template trajectory for gradually handling
complex problems; (iii) a brand new inference scaling system that enables
hierarchical LLM reasoning by adaptively scaling thought templates at inference
time. With a template trajectory containing sequential thought templates, our
ReasonFlux-32B significantly advances math reasoning capabilities to
state-of-the-art levels. Notably, on the MATH benchmark, it achieves an
accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad
(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,
surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:
https://github.com/Gen-Verse/ReasonFlux</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05537v1' target='_blank'>Sequential Stochastic Combinatorial Optimization Using Hierarchal
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinsong Feng, Zihan Yu, Yanhai Xiong, Haipeng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 12:00:30</h6>
<p class='card-text'>Reinforcement learning (RL) has emerged as a promising tool for combinatorial
optimization (CO) problems due to its ability to learn fast, effective, and
generalizable solutions. Nonetheless, existing works mostly focus on one-shot
deterministic CO, while sequential stochastic CO (SSCO) has rarely been studied
despite its broad applications such as adaptive influence maximization (IM) and
infectious disease intervention. In this paper, we study the SSCO problem where
we first decide the budget (e.g., number of seed nodes in adaptive IM)
allocation for all time steps, and then select a set of nodes for each time
step. The few existing studies on SSCO simplify the problems by assuming a
uniformly distributed budget allocation over the time horizon, yielding
suboptimal solutions. We propose a generic hierarchical RL (HRL) framework
called wake-sleep option (WS-option), a two-layer option-based framework that
simultaneously decides adaptive budget allocation on the higher layer and node
selection on the lower layer. WS-option starts with a coherent formulation of
the two-layer Markov decision processes (MDPs), capturing the interdependencies
between the two layers of decisions. Building on this, WS-option employs
several innovative designs to balance the model's training stability and
computational efficiency, preventing the vicious cyclic interference issue
between the two layers. Empirical results show that WS-option exhibits
significantly improved effectiveness and generalizability compared to
traditional methods. Moreover, the learned model can be generalized to larger
graphs, which significantly reduces the overhead of computational resources.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03960v1' target='_blank'>Bilevel Multi-Armed Bandit-Based Hierarchical Reinforcement Learning for
  Interaction-Aware Self-Driving at Unsignalized Intersections</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zengqi Peng, Yubin Wang, Lei Zheng, Jun Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 10:50:59</h6>
<p class='card-text'>In this work, we present BiM-ACPPO, a bilevel multi-armed bandit-based
hierarchical reinforcement learning framework for interaction-aware
decision-making and planning at unsignalized intersections. Essentially, it
proactively takes the uncertainties associated with surrounding vehicles (SVs)
into consideration, which encompass those stemming from the driver's intention,
interactive behaviors, and the varying number of SVs. Intermediate decision
variables are introduced to enable the high-level RL policy to provide an
interaction-aware reference, for guiding low-level model predictive control
(MPC) and further enhancing the generalization ability of the proposed
framework. By leveraging the structured nature of self-driving at unsignalized
intersections, the training problem of the RL policy is modeled as a bilevel
curriculum learning task, which is addressed by the proposed Exp3.S-based BiMAB
algorithm. It is noteworthy that the training curricula are dynamically
adjusted, thereby facilitating the sample efficiency of the RL training
process. Comparative experiments are conducted in the high-fidelity CARLA
simulator, and the results indicate that our approach achieves superior
performance compared to all baseline methods. Furthermore, experimental results
in two new urban driving scenarios clearly demonstrate the commendable
generalization performance of the proposed method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01956v1' target='_blank'>DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement
  Learning Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shashank Sharma, Janina Hoffmann, Vinay Namboodiri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-04 03:05:55</h6>
<p class='card-text'>In this paper, we address the challenge of long-horizon visual planning tasks
using Hierarchical Reinforcement Learning (HRL). Our key contribution is a
Discrete Hierarchical Planning (DHP) method, an alternative to traditional
distance-based approaches. We provide theoretical foundations for the method
and demonstrate its effectiveness through extensive empirical evaluations.
  Our agent recursively predicts subgoals in the context of a long-term goal
and receives discrete rewards for constructing plans as compositions of
abstract actions. The method introduces a novel advantage estimation strategy
for tree trajectories, which inherently encourages shorter plans and enables
generalization beyond the maximum tree depth. The learned policy function
allows the agent to plan efficiently, requiring only $\log N$ computational
steps, making re-planning highly efficient. The agent, based on a soft-actor
critic (SAC) framework, is trained using on-policy imagination data.
Additionally, we propose a novel exploration strategy that enables the agent to
generate relevant training examples for the planning modules. We evaluate our
method on long-horizon visual planning tasks in a 25-room environment, where it
significantly outperforms previous benchmarks at success rate and average
episode length. Furthermore, an ablation study highlights the individual
contributions of key modules to the overall performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.17424v1' target='_blank'>Certificated Actor-Critic: Hierarchical Reinforcement Learning with
  Control Barrier Functions for Safe Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junjun Xie, Shuhao Zhao, Liang Hu, Huijun Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-29 05:37:47</h6>
<p class='card-text'>Control Barrier Functions (CBFs) have emerged as a prominent approach to
designing safe navigation systems of robots. Despite their popularity, current
CBF-based methods exhibit some limitations: optimization-based safe control
techniques tend to be either myopic or computationally intensive, and they rely
on simplified system models; conversely, the learning-based methods suffer from
the lack of quantitative indication in terms of navigation performance and
safety. In this paper, we present a new model-free reinforcement learning
algorithm called Certificated Actor-Critic (CAC), which introduces a
hierarchical reinforcement learning framework and well-defined reward functions
derived from CBFs. We carry out theoretical analysis and proof of our
algorithm, and propose several improvements in algorithm implementation. Our
analysis is validated by two simulation experiments, showing the effectiveness
of our proposed CAC algorithm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.14992v1' target='_blank'>Extensive Exploration in Complex Traffic Scenarios using Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhihao Zhang, Ekim Yurtsever, Keith A. Redmill</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-25 00:00:11</h6>
<p class='card-text'>Developing an automated driving system capable of navigating complex traffic
environments remains a formidable challenge. Unlike rule-based or supervised
learning-based methods, Deep Reinforcement Learning (DRL) based controllers
eliminate the need for domain-specific knowledge and datasets, thus providing
adaptability to various scenarios. Nonetheless, a common limitation of existing
studies on DRL-based controllers is their focus on driving scenarios with
simple traffic patterns, which hinders their capability to effectively handle
complex driving environments with delayed, long-term rewards, thus compromising
the generalizability of their findings. In response to these limitations, our
research introduces a pioneering hierarchical framework that efficiently
decomposes intricate decision-making problems into manageable and interpretable
subtasks. We adopt a two step training process that trains the high-level
controller and low-level controller separately. The high-level controller
exhibits an enhanced exploration potential with long-term delayed rewards, and
the low-level controller provides longitudinal and lateral control ability
using short-term instantaneous rewards. Through simulation experiments, we
demonstrate the superiority of our hierarchical controller in managing complex
highway driving situations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13084v1' target='_blank'>Attention-Driven Hierarchical Reinforcement Learning with Particle
  Filtering for Source Localization in Dynamic Fields</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiwei Shi, Mengyue Yang, Qi Zhang, Weinan Zhang, Cunjia Liu, Weiru Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 18:45:29</h6>
<p class='card-text'>In many real-world scenarios, such as gas leak detection or environmental
pollutant tracking, solving the Inverse Source Localization and
Characterization problem involves navigating complex, dynamic fields with
sparse and noisy observations. Traditional methods face significant challenges,
including partial observability, temporal and spatial dynamics,
out-of-distribution generalization, and reward sparsity. To address these
issues, we propose a hierarchical framework that integrates Bayesian inference
and reinforcement learning. The framework leverages an attention-enhanced
particle filtering mechanism for efficient and accurate belief updates, and
incorporates two complementary execution strategies: Attention Particle
Filtering Planning and Attention Particle Filtering Reinforcement Learning.
These approaches optimize exploration and adaptation under uncertainty.
Theoretical analysis proves the convergence of the attention-enhanced particle
filter, while extensive experiments across diverse scenarios validate the
framework's superior accuracy, adaptability, and computational efficiency. Our
results highlight the framework's potential for broad applications in dynamic
field estimation tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13132v1' target='_blank'>A Hierarchical Reinforcement Learning Framework for Multi-UAV Combat
  Using Leader-Follower Strategy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinhui Pang, Jinglin He, Noureldin Mohamed Abdelaal Ahmed Mohamed, Changqing Lin, Zhihui Zhang, Xiaoshuai Hao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 02:41:36</h6>
<p class='card-text'>Multi-UAV air combat is a complex task involving multiple autonomous UAVs, an
evolving field in both aerospace and artificial intelligence. This paper aims
to enhance adversarial performance through collaborative strategies. Previous
approaches predominantly discretize the action space into predefined actions,
limiting UAV maneuverability and complex strategy implementation. Others
simplify the problem to 1v1 combat, neglecting the cooperative dynamics among
multiple UAVs. To address the high-dimensional challenges inherent in
six-degree-of-freedom space and improve cooperation, we propose a hierarchical
framework utilizing the Leader-Follower Multi-Agent Proximal Policy
Optimization (LFMAPPO) strategy. Specifically, the framework is structured into
three levels. The top level conducts a macro-level assessment of the
environment and guides execution policy. The middle level determines the angle
of the desired action. The bottom level generates precise action commands for
the high-dimensional action space. Moreover, we optimize the state-value
functions by assigning distinct roles with the leader-follower strategy to
train the top-level policy, followers estimate the leader's utility, promoting
effective cooperation among agents. Additionally, the incorporation of a target
selector, aligned with the UAVs' posture, assesses the threat level of targets.
Finally, simulation experiments validate the effectiveness of our proposed
method.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>