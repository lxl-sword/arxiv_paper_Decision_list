<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>multi-agent - 2025-08-22</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>multi-agent - 2025-08-22</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.15652v1' target='_blank'>Understanding Action Effects through Instrumental Empowerment in
  Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ardian Selmonaj, Miroslav Strupl, Oleg Szehr, Alessandro Antonucci</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-21 15:35:59</h6>
<p class='card-text'>To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is
crucial to understand individual agent behaviors within a team. While prior
work typically evaluates overall team performance based on explicit reward
signals or learned value functions, it is unclear how to infer agent
contributions in the absence of any value feedback. In this work, we
investigate whether meaningful insights into agent behaviors can be extracted
that are consistent with the underlying value functions, solely by analyzing
the policy distribution. Inspired by the phenomenon that intelligent agents
tend to pursue convergent instrumental values, which generally increase the
likelihood of task success, we introduce Intended Cooperation Values (ICVs), a
method based on information-theoretic Shapley values for quantifying each
agent's causal influence on their co-players' instrumental empowerment.
Specifically, ICVs measure an agent's action effect on its teammates' policies
by assessing their decision uncertainty and preference alignment. The analysis
across cooperative and competitive MARL environments reveals the extent to
which agents adopt similar or diverse strategies. By comparing action effects
between policies and value functions, our method identifies which agent
behaviors are beneficial to team success, either by fostering deterministic
decisions or by preserving flexibility for future action choices. Our proposed
method offers novel insights into cooperation dynamics and enhances
explainability in MARL systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.14676v1' target='_blank'>Adaptive Vision-Based Coverage Optimization in Mobile Wireless Sensor
  Networks: A Multi-Agent Deep Reinforcement Learning Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Parham Soltani, Mehrshad Eskandarpour, Sina Heidari, Farnaz Alizadeh, Hossein Soleimani</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-20 12:48:21</h6>
<p class='card-text'>Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of
the target area, network size, and sensor coverage to determine initial
deployment. This often results in significant overlap to ensure continued
network operation despite sensor energy depletion. With the emergence of Mobile
Wireless Sensor Networks (MWSNs), issues such as sensor failure and static
coverage limitations can be more effectively addressed through mobility. This
paper proposes a novel deployment strategy in which mobile sensors autonomously
position themselves to maximize area coverage, eliminating the need for
predefined policies. A live camera system, combined with deep reinforcement
learning (DRL), monitors the network by detecting sensor LED indicators and
evaluating real-time coverage. Rewards based on coverage efficiency and sensor
movement are computed at each learning step and shared across the network
through a Multi-Agent Reinforcement Learning (MARL) framework, enabling
decentralized, cooperative sensor control. Key contributions include a
vision-based, low-cost coverage evaluation method; a scalable MARL-DRL
framework for autonomous deployment; and a self-reconfigurable system that
adjusts sensor positioning in response to energy depletion. Compared to
traditional distance-based localization, the proposed method achieves a 26.5%
improvement in coverage, a 32% reduction in energy consumption, and a 22%
decrease in redundancy, extending network lifetime by 45%. This approach
significantly enhances adaptability, energy efficiency, and robustness in
MWSNs, offering a practical deployment solution within the IoT framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.13661v1' target='_blank'>MACTAS: Self-Attention-Based Module for Inter-Agent Communication in
  Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maciej Wojtala, Bogusz Stefańczyk, Dominik Bogucki, Łukasz Lepak, Jakub Strykowski, Paweł Wawrzyński</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-19 09:08:48</h6>
<p class='card-text'>Communication is essential for the collective execution of complex tasks by
human agents, motivating interest in communication mechanisms for multi-agent
reinforcement learning (MARL). However, existing communication protocols in
MARL are often complex and non-differentiable. In this work, we introduce a
self-attention-based communication module that exchanges information between
the agents in MARL. Our proposed approach is fully differentiable, allowing
agents to learn to generate messages in a reward-driven manner. The module can
be seamlessly integrated with any action-value function decomposition method
and can be viewed as an extension of such decompositions. Notably, it includes
a fixed number of trainable parameters, independent of the number of agents.
Experimental results on the SMAC benchmark demonstrate the effectiveness of our
approach, which achieves state-of-the-art performance on several maps.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.12845v1' target='_blank'>CAMAR: Continuous Actions Multi-Agent Routing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Artem Pshenitsyn, Aleksandr Panov, Alexey Skrynnik</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-18 11:32:26</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving
cooperative and competitive decision-making problems. While many MARL
benchmarks have been proposed, few combine continuous state and action spaces
with challenging coordination and planning tasks. We introduce CAMAR, a new
MARL benchmark designed explicitly for multi-agent pathfinding in environments
with continuous actions. CAMAR supports cooperative and competitive
interactions between agents and runs efficiently at up to 100,000 environment
steps per second. We also propose a three-tier evaluation protocol to better
track algorithmic progress and enable deeper analysis of performance. In
addition, CAMAR allows the integration of classical planning methods such as
RRT and RRT* into MARL pipelines. We use them as standalone baselines and
combine RRT* with popular MARL algorithms to create hybrid approaches. We
provide a suite of test scenarios and benchmarking tools to ensure
reproducibility and fair comparison. Experiments show that CAMAR presents a
challenging and realistic testbed for the MARL community.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.12633v2' target='_blank'>DCT-MARL: A Dynamic Communication Topology-Based MARL Algorithm for
  Connected Vehicle Platoon Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaqi Xu, Yan Shi, Jin Tian, Fanzeng Xia, Tongxin Li, Shanzhi Chen, Yuming Ge</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-18 05:34:01</h6>
<p class='card-text'>With the rapid advancement of vehicular communication facilities and
autonomous driving technologies, connected vehicle platooning has emerged as a
promising approach to improve traffic efficiency and driving safety. Reliable
Vehicle-to-Vehicle (V2V) communication is critical to achieving efficient
cooperative control. However, in the real-world traffic environment, V2V
communication may suffer from time-varying delay and packet loss, leading to
degraded control performance and even safety risks. To mitigate the adverse
effects of non-ideal communication, this paper proposes a Dynamic Communication
Topology based Multi-Agent Reinforcement Learning (DCT-MARL) algorithm for
robust cooperative platoon control. Specifically, the state space is augmented
with historical control action and delay to enhance robustness against
communication delay. To mitigate the impact of packet loss, a multi-key gated
communication mechanism is introduced, which dynamically adjusts the
communication topology based on the correlation between vehicles and their
current communication status. Simulation results demonstrate that the proposed
DCT-MARL significantly outperforms state-of-the-art methods in terms of string
stability and driving comfort, validating its superior robustness and
effectiveness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.10423v1' target='_blank'>MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for
  Single Humanoid Robot Locomotion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qi Liu, Xiaopeng Zhang, Mingshan Tan, Shuaikang Ma, Jinliang Ding, Yanjie Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-14 07:54:31</h6>
<p class='card-text'>This paper proposes a novel method to enhance locomotion for a single
humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement
learning (MARL). While most existing methods typically employ single-agent
reinforcement learning algorithms for a single humanoid robot or MARL
algorithms for multi-robot system tasks, we propose a distinct paradigm:
applying cooperative-heterogeneous MARL to optimize locomotion for a single
humanoid robot. The proposed method, multi-agent reinforcement learning for
single humanoid locomotion (MASH), treats each limb (legs and arms) as an
independent agent that explores the robot's action space while sharing a global
critic for cooperative learning. Experiments demonstrate that MASH accelerates
training convergence and improves whole-body cooperation ability, outperforming
conventional single-agent reinforcement learning methods. This work advances
the integration of MARL into single-humanoid-robot control, offering new
insights into efficient locomotion strategies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.10340v1' target='_blank'>Multi-Agent Trust Region Policy Optimisation: A Joint Constraint
  Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chak Lam Shek, Guangyao Shi, Pratap Tokekar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-14 04:48:46</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.11706v1' target='_blank'>Centralized Permutation Equivariant Policy for Cooperative Multi-Agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuofan Xu, Benedikt Bollig, Matthias Függer, Thomas Nowak, Vincent Le Dréau</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-13 22:10:37</h6>
<p class='card-text'>The Centralized Training with Decentralized Execution (CTDE) paradigm has
gained significant attention in multi-agent reinforcement learning (MARL) and
is the foundation of many recent algorithms. However, decentralized policies
operate under partial observability and often yield suboptimal performance
compared to centralized policies, while fully centralized approaches typically
face scalability challenges as the number of agents increases.
  We propose Centralized Permutation Equivariant (CPE) learning, a centralized
training and execution framework that employs a fully centralized policy to
overcome these limitations. Our approach leverages a novel permutation
equivariant architecture, Global-Local Permutation Equivariant (GLPE) networks,
that is lightweight, scalable, and easy to implement. Experiments show that CPE
integrates seamlessly with both value decomposition and actor-critic methods,
substantially improving the performance of standard CTDE algorithms across
cooperative benchmarks including MPE, SMAC, and RWARE, and matching the
performance of state-of-the-art RWARE implementations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.09541v1' target='_blank'>Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing
  a Joint Objective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gang Chen, Guoxin Wang, Anton van Beek, Zhenjun Ming, Yan Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-13 06:50:03</h6>
<p class='card-text'>Multi-agent self-organizing systems (MASOS) exhibit key characteristics
including scalability, adaptability, flexibility, and robustness, which have
contributed to their extensive application across various fields. However, the
self-organizing nature of MASOS also introduces elements of unpredictability in
their emergent behaviors. This paper focuses on the emergence of dependency
hierarchies during task execution, aiming to understand how such hierarchies
arise from agents' collective pursuit of the joint objective, how they evolve
dynamically, and what factors govern their development. To investigate this
phenomenon, multi-agent reinforcement learning (MARL) is employed to train
MASOS for a collaborative box-pushing task. By calculating the gradients of
each agent's actions in relation to the states of other agents, the inter-agent
dependencies are quantified, and the emergence of hierarchies is analyzed
through the aggregation of these dependencies. Our results demonstrate that
hierarchies emerge dynamically as agents work towards a joint objective, with
these hierarchies evolving in response to changing task requirements. Notably,
these dependency hierarchies emerge organically in response to the shared
objective, rather than being a consequence of pre-configured rules or
parameters that can be fine-tuned to achieve specific results. Furthermore, the
emergence of hierarchies is influenced by the task environment and network
initialization conditions. Additionally, hierarchies in MASOS emerge from the
dynamic interplay between agents' "Talent" and "Effort" within the
"Environment." "Talent" determines an agent's initial influence on collective
decision-making, while continuous "Effort" within the "Environment" enables
agents to shift their roles and positions within the system.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.08800v1' target='_blank'>Fault Tolerant Multi-Agent Learning with Adversarial Budget Constraints</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:David Mguni, Yaqi Sun, Haojun Chen, Amir Darabi, Larry Olanrewaju Orimoloye, Yaodong Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-12 09:57:05</h6>
<p class='card-text'>In multi-agent systems, the safe and reliable execution of tasks often
depends on agents correctly coordinating their actions. However, in real-world
deployments, failures of computational components are inevitable, presenting a
critical challenge: ensuring that multi-agent reinforcement learning (MARL)
policies remain effective even when some agents malfunction. We propose the
Multi-Agent Robust Training Algorithm (MARTA), a plug-and-play framework for
training MARL agents to be resilient to potentially severe faults. MARTA
operates in cooperative multi-agent settings where agents may lose the ability
to execute their intended actions. It learns to identify failure scenarios that
are especially detrimental to system performance and equips agents with
strategies to mitigate their impact. At the heart of MARTA is a novel
adversarial Markov game in which an adversary -- modelled via \emph{Markov
switching controls} -- learns to disable agents in high-risk state regions,
while the remaining agents are trained to \emph{jointly} best-respond to such
targeted malfunctions. To ensure practicality, MARTA enforces a malfunction
budget, constraining the adversary to a fixed number of failures and learning
robust policies accordingly. We provide theoretical guarantees that MARTA
converges to a Markov perfect equilibrium, ensuring agents optimally counteract
worst-case faults. Empirically, we show that MARTA achieves state-of-the-art
fault-tolerant performance across benchmark environments, including Multi-Agent
Particle World and Level-Based Foraging.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>