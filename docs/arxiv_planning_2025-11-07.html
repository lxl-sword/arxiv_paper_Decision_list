<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-11-07</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-11-07</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.04573v1' target='_blank'>ARETE: an R package for Automated REtrieval from TExt with large
  language models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vasco V. Branco, Jandó Benedek, Lidia Pivovarova, Luís Correia, Pedro Cardoso</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-06 17:26:48</h6>
<p class='card-text'>1. A hard stop for the implementation of rigorous conservation initiatives is
our lack of key species data, especially occurrence data. Furthermore,
researchers have to contend with an accelerated speed at which new information
must be collected and processed due to anthropogenic activity. Publications
ranging from scientific papers to gray literature contain this crucial
information but their data are often not machine-readable, requiring extensive
human work to be retrieved. 2. We present the ARETE R package, an open-source
software aiming to automate data extraction of species occurrences powered by
large language models, namely using the chatGPT Application Programming
Interface. This R package integrates all steps of the data extraction and
validation process, from Optical Character Recognition to detection of outliers
and output in tabular format. Furthermore, we validate ARETE through systematic
comparison between what is modelled and the work of human annotators. 3. We
demonstrate the usefulness of the approach by comparing range maps produced
using GBIF data and with those automatically extracted for 100 species of
spiders. Newly extracted data allowed to expand the known Extent of Occurrence
by a mean three orders of magnitude, revealing new areas where the species were
found in the past, which mayhave important implications for spatial
conservation planning and extinction risk assessments. 4. ARETE allows faster
access to hitherto untapped occurrence data, a potential game changer in
projects requiring such data. Researchers will be able to better prioritize
resources, manually verifying selected species while maintaining automated
extraction for the majority. This workflow also allows predicting available
bibliographic data during project planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.04523v1' target='_blank'>A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting
  Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Silvia Bonomi, Giovanni Farina, Roy Friedman, Eviatar B. Procaccia, Sebastien Tixeuil</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-06 16:38:43</h6>
<p class='card-text'>Modern distributed systems face growing security threats, as attackers
continuously enhance their skills and vulnerabilities span across the entire
system stack, from hardware to the application layer. In the system design
phase, fault tolerance techniques can be employed to safeguard systems. From a
theoretical perspective, an attacker attempting to compromise a system can be
abstracted by considering the presence of Byzantine processes in the system.
Although this approach enhances the resilience of the distributed system, it
introduces certain limitations regarding the accuracy of the model in
reflecting real-world scenarios. In this paper, we consider a self-protecting
distributed system based on the \emph{Monitoring-Analyse-Plan-Execute over a
shared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic
Mobile Byzantine Failure (MBF) that can be plugged into the Analysis component.
Our new model captures the dynamics of evolving attacks and can be used to
drive the self-protection and reconfiguration strategy. We analyze
mathematically the time that it takes until the number of Byzantine nodes
crosses given thresholds, or for the system to self-recover back into a safe
state, depending on the rates of Byzantine infection spreading \emph{vs.} the
rate of self-recovery. We also provide simulation results that illustrate the
behavior of the system under such assumptions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.04515v1' target='_blank'>Robust mean-field control under common noise uncertainty</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mathieu Laurière, Ariel Neufeld, Kyunghyun Park</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-06 16:31:49</h6>
<p class='card-text'>We propose and analyze a framework for discrete-time robust mean-field
control problems under common noise uncertainty. In this framework, the
mean-field interaction describes the collective behavior of infinitely many
cooperative agents' state and action, while the common noise -- a random
disturbance affecting all agents' state dynamics -- is uncertain. A social
planner optimizes over open-loop controls on an infinite horizon to maximize
the representative agent's worst-case expected reward, where worst-case
corresponds to the most adverse probability measure among all candidates
inducing the unknown true law of the common noise process. We refer to this
optimization as a robust mean-field control problem under common noise
uncertainty. We first show that this problem arises as the asymptotic limit of
a cooperative $N$-agent robust optimization problem, commonly known as
propagation of chaos. We then prove the existence of an optimal open-loop
control by linking the robust mean field control problem to a lifted robust
Markov decision problem on the space of probability measures and by
establishing the dynamic programming principle and Bellman--Isaac fixed point
theorem for the lifted robust Markov decision problem. Finally, we complement
our theoretical results with numerical experiments motivated by distribution
planning and systemic risk in finance, highlighting the advantages of
accounting for common noise uncertainty.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.04357v1' target='_blank'>GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon
  Planning with VLA Policies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maëlic Neau, Zoe Falomir, Paulo E. Santos, Anne-Gwenn Bosser, Cédric Buche</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-06 13:39:38</h6>
<p class='card-text'>Deploying autonomous robots that can learn new skills from demonstrations is
an important challenge of modern robotics. Existing solutions often apply
end-to-end imitation learning with Vision-Language Action (VLA) models or
symbolic approaches with Action Model Learning (AML). On the one hand, current
VLA models are limited by the lack of high-level symbolic planning, which
hinders their abilities in long-horizon tasks. On the other hand, symbolic
approaches in AML lack generalization and scalability perspectives. In this
paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that
uses a Continuous Scene Graph representation to generate a symbolic
representation of human demonstrations. This representation is used to generate
new planning domains during inference and serves as an orchestrator for
low-level VLA policies, scaling up the number of actions that can be reproduced
in a row. Our results show that GraSP-VLA is effective for modeling symbolic
representations on the task of automatic planning domain generation from
observations. In addition, results on real-world experiments show the potential
of our Continuous Scene Graph representation to orchestrate low-level VLA
policies in long-horizon tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.04246v1' target='_blank'>Differential Flatness of Quasi-Static Slider-Pusher Models with
  Applications in Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sander De Witte, Tom Lefebvre, Thomas Neve, Andras Retzler, Guillaume Crevecoeur</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-06 10:33:24</h6>
<p class='card-text'>This paper investigates the dynamic properties of planar slider-pusher
systems as a motion primitive in manipulation tasks. To that end, we construct
a differential kinematic model deriving from the limit surface approach under
the quasi-static assumption and with negligible contact friction. The
quasi-static model applies to generic slider shapes and circular pusher
geometries, enabling a differential kinematic representation of the system.
From this model, we analyze differential flatness - a property advantageous for
control synthesis and planning - and find that slider-pusher systems with
polygon sliders and circular pushers exhibit flatness with the centre of mass
as a flat output. Leveraging this property, we propose two control strategies
for trajectory tracking: a cascaded quasi-static feedback strategy and a
dynamic feedback linearization approach. We validate these strategies through
closed-loop simulations incorporating perturbed models and input noise, as well
as experimental results using a physical setup with a finger-like pusher and
vision-based state detection. The real-world experiments confirm the
applicability of the simulation gains, highlighting the potential of the
proposed methods for</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.04172v1' target='_blank'>Transforming Mentorship: An AI Powered Chatbot Approach to University
  Guidance</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mashrur Rahman, Mantaqa abedin, Monowar Zamil Abir, Faizul Islam Ansari, Adib Reza, Farig Yousuf Sadeque, Niloy Farhan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-06 08:24:52</h6>
<p class='card-text'>University students face immense challenges during their undergraduate lives,
often being deprived of personalized on-demand guidance that mentors fail to
provide at scale. Digital tools exist, but there is a serious lack of
customized coaching for newcomers. This paper presents an AI-powered chatbot
that will serve as a mentor for the students of BRAC University. The main
component is a data ingestion pipeline that efficiently processes and updates
information from diverse sources, such as CSV files and university webpages.
The chatbot retrieves information through a hybrid approach, combining BM25
lexical ranking with ChromaDB semantic retrieval, and uses a Large Language
Model, LLaMA-3.3-70B, to generate conversational responses. The generated text
was found to be semantically highly relevant, with a BERTScore of 0.831 and a
METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82
seconds for updates, compared to 368.62 seconds for new data. This chatbot will
be able to help students by responding to their queries, helping them to get a
better understanding of university life, and assisting them to plan better
routines for their semester in the open-credit university.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.04126v1' target='_blank'>Automated Tennis Player and Ball Tracking with Court Keypoints Detection
  (Hawk Eye System)</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Venkata Manikanta Desu, Syed Fawaz Ali</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-06 07:18:54</h6>
<p class='card-text'>This study presents a complete pipeline for automated tennis match analysis.
Our framework integrates multiple deep learning models to detect and track
players and the tennis ball in real time, while also identifying court
keypoints for spatial reference. Using YOLOv8 for player detection, a
custom-trained YOLOv5 model for ball tracking, and a ResNet50-based
architecture for court keypoint detection, our system provides detailed
analytics including player movement patterns, ball speed, shot accuracy, and
player reaction times. The experimental results demonstrate robust performance
in varying court conditions and match scenarios. The model outputs an annotated
video along with detailed performance metrics, enabling coaches, broadcasters,
and players to gain actionable insights into the dynamics of the game.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.04094v1' target='_blank'>KoTaP: A Panel Dataset for Corporate Tax Avoidance, Performance, and
  Governance in Korea</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hyungjong Na, Wonho Song, Seungyong Han, Donghyeon Jo, Sejin Myung, Hyungjoon Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-06 06:13:53</h6>
<p class='card-text'>This study introduces the Korean Tax Avoidance Panel (KoTaP), a long-term
panel dataset of non-financial firms listed on KOSPI and KOSDAQ between 2011
and 2024. After excluding financial firms, firms with non-December fiscal year
ends, capital impairment, and negative pre-tax income, the final dataset
consists of 12,653 firm-year observations from 1,754 firms. KoTaP is designed
to treat corporate tax avoidance as a predictor variable and link it to
multiple domains, including earnings management (accrual- and activity-based),
profitability (ROA, ROE, CFO, LOSS), stability (LEV, CUR, SIZE, PPE, AGE,
INVREC), growth (GRW, MB, TQ), and governance (BIG4, FORN, OWN). Tax avoidance
itself is measured using complementary indicators cash effective tax rate
(CETR), GAAP effective tax rate (GETR), and book-tax difference measures (TSTA,
TSDA) with adjustments to ensure interpretability. A key strength of KoTaP is
its balanced panel structure with standardized variables and its consistency
with international literature on the distribution and correlation of core
indicators. At the same time, it reflects distinctive institutional features of
Korean firms, such as concentrated ownership, high foreign shareholding, and
elevated liquidity ratios, providing both international comparability and
contextual uniqueness. KoTaP enables applications in benchmarking econometric
and deep learning models, external validity checks, and explainable AI
analyses. It further supports policy evaluation, audit planning, and investment
analysis, making it a critical open resource for accounting, finance, and
interdisciplinary research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.04084v1' target='_blank'>When Swin Transformer Meets KANs: An Improved Transformer Architecture
  for Medical Image Segmentation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nishchal Sapkota, Haoyan Shi, Yejia Zhang, Xianshi Ma, Bofang Zheng, Danny Z. Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-06 05:44:57</h6>
<p class='card-text'>Medical image segmentation is critical for accurate diagnostics and treatment
planning, but remains challenging due to complex anatomical structures and
limited annotated training data. CNN-based segmentation methods excel at local
feature extraction, but struggle with modeling long-range dependencies.
Transformers, on the other hand, capture global context more effectively, but
are inherently data-hungry and computationally expensive. In this work, we
introduce UKAST, a U-Net like architecture that integrates rational-function
based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By
leveraging rational base functions and Group Rational KANs (GR-KANs) from the
Kolmogorov-Arnold Transformer (KAT), our architecture addresses the
inefficiencies of vanilla spline-based KANs, yielding a more expressive and
data-efficient framework with reduced FLOPs and only a very small increase in
parameter count compared to SwinUNETR. UKAST achieves state-of-the-art
performance on four diverse 2D and 3D medical image segmentation benchmarks,
consistently surpassing both CNN- and Transformer-based baselines. Notably, it
attains superior accuracy in data-scarce settings, alleviating the data-hungry
limitations of standard Vision Transformers. These results show the potential
of KAN-enhanced Transformers to advance data-efficient medical image
segmentation. Code is available at: https://github.com/nsapkota417/UKAST</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.04075v1' target='_blank'>Two Decades of Research at the University of Lagos (2004-2023): A
  Scientometric Analysis of Productivity, Collaboration, and Impact</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muneer Ahmad, Samuel Ibor Ubi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-06 05:26:17</h6>
<p class='card-text'>This paper presents a scientometric analysis of research output from the
University of Lagos, focusing on the two decades spanning 2004 to 2023. Using
bibliometric data retrieved from the Web of Science, we examine trends in
publication volume, collaboration patterns, citation impact, and the most
prolific authors, departments, and research domains at the university. The
study reveals a consistent increase in research productivity, with the highest
publication output recorded in 2023. Health Sciences, Engineering, and Social
Sciences are identified as dominant fields, reflecting the university's
interdisciplinary research strengths. Collaborative efforts, both locally and
internationally, show a positive correlation with higher citation impact, with
the United States and the United Kingdom being the leading international
collaborators. Notably, open-access publications account for a significant
portion of the university's research output, enhancing visibility and citation
rates. The findings offer valuable insights into the university's research
performance over the past two decades, providing a foundation for strategic
planning and policy formulation to foster research excellence and global
impact.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.04009v1' target='_blank'>Integrating Ergonomics and Manipulability for Upper Limb Postural
  Optimization in Bimanual Human-Robot Collaboration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenzui Li, Yiming Chen, Xi Wu, Giacinto Barresi, Fei Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-06 03:16:39</h6>
<p class='card-text'>This paper introduces an upper limb postural optimization method for
enhancing physical ergonomics and force manipulability during bimanual
human-robot co-carrying tasks. Existing research typically emphasizes human
safety or manipulative efficiency, whereas our proposed method uniquely
integrates both aspects to strengthen collaboration across diverse conditions
(e.g., different grasping postures of humans, and different shapes of objects).
Specifically, the joint angles of a simplified human skeleton model are
optimized by minimizing the cost function to prioritize safety and manipulative
capability. To guide humans towards the optimized posture, the reference
end-effector poses of the robot are generated through a transformation module.
A bimanual model predictive impedance controller (MPIC) is proposed for our
human-like robot, CURI, to recalibrate the end effector poses through planned
trajectories. The proposed method has been validated through various subjects
and objects during human-human collaboration (HHC) and human-robot
collaboration (HRC). The experimental results demonstrate significant
improvement in muscle conditions by comparing the activation of target muscles
before and after optimization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03928v1' target='_blank'>SynQuE: Estimating Synthetic Dataset Quality Without Annotations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arthur Chen, Victor Zhong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-06 00:09:33</h6>
<p class='card-text'>We introduce and formalize the Synthetic Dataset Quality Estimation (SynQuE)
problem: ranking synthetic datasets by their expected real-world task
performance using only limited unannotated real data. This addresses a critical
and open challenge where data is scarce due to collection costs or privacy
constraints. We establish the first comprehensive benchmarks for this problem
by introducing and evaluating proxy metrics that choose synthetic data for
training to maximize task performance on real data. We introduce the first
proxy metrics for SynQuE by adapting distribution and diversity-based distance
measures to our context via embedding models. To address the shortcomings of
these metrics on complex planning tasks, we propose LENS, a novel proxy that
leverages large language model reasoning. Our results show that SynQuE proxies
correlate with real task performance across diverse tasks, including sentiment
analysis, Text2SQL, web navigation, and image classification, with LENS
consistently outperforming others on complex tasks by capturing nuanced
characteristics. For instance, on text-to-SQL parsing, training on the top-3
synthetic datasets selected via SynQuE proxies can raise accuracy from 30.4% to
38.4 (+8.1)% on average compared to selecting data indiscriminately. This work
establishes SynQuE as a practical framework for synthetic data selection under
real-data scarcity and motivates future research on foundation model-based data
characterization and fine-grained data selection.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03890v1' target='_blank'>Shape Deformation Networks for Automated Aortic Valve Finite Element
  Meshing from 3D CT Images</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Linchen Qian, Jiasong Chen, Ruonan Gong, Wei Sun, Minliang Liu, Liang Liang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 22:33:32</h6>
<p class='card-text'>Accurate geometric modeling of the aortic valve from 3D CT images is
essential for biomechanical analysis and patient-specific simulations to assess
valve health or make a preoperative plan. However, it remains challenging to
generate aortic valve meshes with both high-quality and consistency across
different patients. Traditional approaches often produce triangular meshes with
irregular topologies, which can result in poorly shaped elements and
inconsistent correspondence due to inter-patient anatomical variation. In this
work, we address these challenges by introducing a template-fitting pipeline
with deep neural networks to generate structured quad (i.e., quadrilateral)
meshes from 3D CT images to represent aortic valve geometries. By remeshing
aortic valves of all patients with a common quad mesh template, we ensure a
uniform mesh topology with consistent node-to-node and element-to-element
correspondence across patients. This consistency enables us to simplify the
learning objective of the deep neural networks, by employing a loss function
with only two terms (i.e., a geometry reconstruction term and a smoothness
regularization term), which is sufficient to preserve mesh smoothness and
element quality. Our experiments demonstrate that the proposed approach
produces high-quality aortic valve surface meshes with improved smoothness and
shape quality, while requiring fewer explicit regularization terms compared to
the traditional methods. These results highlight that using structured quad
meshes for the template and neural network training not only ensures mesh
correspondence and quality but also simplifies the training process, thus
enhancing the effectiveness and efficiency of aortic valve modeling.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03883v1' target='_blank'>Physics Briefing Book: Input for the 2026 update of the European
  Strategy for Particle Physics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jorge de Blas, Monica Dunford, Emanuele Bagnaschi, Ayres Freitas, Pier Paolo Giardino, Christian Grefe, Michele Selvaggi, Angela Taliercio, Falk Bartels, Andrea Dainese, Cristinel Diaconu, Chiara Signorile-Signorile, Néstor Armesto, Roberta Arnaldi, Andy Buckley, David d'Enterria, Antoine Gérardin, Valentina Mantovani Sarti, Sven-Olaf Moch, Marco Pappagallo, Raimond Snellings, Urs Achim Wiedemann, Gino Isidori, Marie-Hélène Schune, Maria Laura Piscopo, Marta Calvi, Yuval Grossman, Thibaud Humair, Andreas Jüttner, Jernej F. Kamenik, Matthew Kenzie, Patrick Koppenburg, Radoslav Marchevski, Angela Papa, Guillaume Pignol, Justine Serrano, Pilar Hernandez, Sara Bolognesi, Ivan Esteban, Stephen Dolan, Valerie Domcke, Joseph Formaggio, M. C. Gonzalez-Garcia, Aart Heijboer, Aldo Ianni, Joachim Kopp, Elisa Resconi, Mark Scott, Viola Sordini, Fabio Maltoni, Rebeca Gonzalez Suarez, Benedikt Maier, Timothy Cohen, Annapaola de Cosa, Nathaniel Craig, Roberto Franceschini, Loukas Gouskos, Aurelio Juste, Sophie Renner, Lesya Shchutska, Jocelyn Monroe, Matthew McCullough, Yohei Ema, Paolo Agnes, Francesca Calore, Emanuele Castorina, Aaron Chou, Monica D'Onofrio, Maksym Ovchynnikov, Tina Pollman, Josef Pradler, Yotam Soreq, Julia Katharina Vogel, Gianluigi Arduini, Philip Burrows, Jacqueline Keintzel, Deepa Angal-Kalinin, Bernhard Auchmann, Massimo Ferrario, Angeles Faus Golfe, Roberto Losito, Anke-Susanne Mueller, Tor Raubenheimer, Marlene Turner, Pierre Vedrine, Hans Weise, Walter Wuensch, Chenghui Yu, Thomas Bergauer, Ulrich Husemann, Dorothea vom Bruch, Thea Aarrestad, Daniela Bortoletto, Shikma Bressler, Marcel Demarteau, Michael Doser, Gabriella Gaudio, Inés Gil-Botella, Andrea Giuliani, Fabrizio Palla, Rok Pestotnik, Felix Sefkow, Frank Simon, Maksym Titov, Tommaso Boccali, Borut Kersevan, Daniel Murnane, Gonzalo Merino Arevalo, John Derek Chapman, Frank-Dieter Gaede, Stefano Giagu, Maria Girone, Heather M. Gray, Giovanni Iadarola, Stephane Jezequel, Gregor Kasieczka, David Lange, Sinéad M. Ryan, Nicole Skidmore, Sofia Vallecorsa, Eric Laenen, Anadi Canepa, Xinchou Lou, Rogerio Rosenfeld, Yuji Yamazaki, Roger Forty, Karl Jakobs, Hugh Montgomery, Mike Seidel, Paris Sphicas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 22:06:23</h6>
<p class='card-text'>The European Strategy for Particle Physics (ESPP) reflects the vision and
presents concrete plans of the European particle physics community for
advancing human knowledge in fundamental physics. The ESPP is updated every
five-to-six years through a community-driven process. It commences with the
submission of specific proposals and other input from the community at large,
outlining projects envisioned for the near-, mid-, and long-term future. All
submitted contributions are evaluated by the Physics Preparatory Group (PPG),
and a preliminary analysis is presented at a Symposium meant to foster a broad
community discussion on the scientific value and feasibility of the various
ideas proposed. The outcomes of the analysis and the deliberations at the
Symposium are synthesized in the current Briefing Book, which provides an
important input in the deliberations of the Strategy recommendations by the
European Strategy Group (ESG).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03882v1' target='_blank'>Investigating Robot Control Policy Learning for Autonomous X-ray-guided
  Spine Procedures</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Florence Klitzner, Blanca Inigo, Benjamin D. Killeen, Lalithkumar Seenivasan, Michelle Song, Axel Krieger, Mathias Unberath</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 22:00:48</h6>
<p class='card-text'>Imitation learning-based robot control policies are enjoying renewed interest
in video-based robotics. However, it remains unclear whether this approach
applies to X-ray-guided procedures, such as spine instrumentation. This is
because interpretation of multi-view X-rays is complex. We examine
opportunities and challenges for imitation policy learning in bi-plane-guided
cannula insertion. We develop an in silico sandbox for scalable, automated
simulation of X-ray-guided spine procedures with a high degree of realism. We
curate a dataset of correct trajectories and corresponding bi-planar X-ray
sequences that emulate the stepwise alignment of providers. We then train
imitation learning policies for planning and open-loop control that iteratively
align a cannula solely based on visual information. This precisely controlled
setup offers insights into limitations and capabilities of this method. Our
policy succeeded on the first attempt in 68.5% of cases, maintaining safe
intra-pedicular trajectories across diverse vertebral levels. The policy
generalized to complex anatomy, including fractures, and remained robust to
varied initializations. Rollouts on real bi-planar X-rays further suggest that
the model can produce plausible trajectories, despite training exclusively in
simulation. While these preliminary results are promising, we also identify
limitations, especially in entry point precision. Full closed-look control will
require additional considerations around how to provide sufficiently frequent
feedback. With more robust priors and domain knowledge, such models may provide
a foundation for future efforts toward lightweight and CT-free robotic
intra-operative spinal navigation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03871v1' target='_blank'>Quantifying Compound Flood Risk and Transition Zones via an Extended
  Joint Probability Method</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mark S. Bartlett, Nathan Geldner, Zach Cobell, Luis Partida, Ovel Diaz, David R. Johnson, Hanbeen Kim, Brett McMann, Gabriele Villarini, Shubra Misra, Hugh J. Roberts, Muthukumar Narayanaswamy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 21:34:29</h6>
<p class='card-text'>Compound flooding from the combined effects of extreme storm surge, rainfall,
and river flows poses significant risks to infrastructure and communities -- as
demonstrated by hurricanes Isaac and Harvey. Yet, existing methods to quantify
compound flood risk lack a unified probabilistic basis. Copula-based models
capture the co-occurrence of flood drivers but not the likelihood of the flood
response, while coupled hydrodynamic models simulate interactions but lack a
probabilistic characterization of compound flood extremes. The Joint
Probability Method (JPM), the foundation of coastal surge risk analysis, has
never been formally extended to incorporate hydrologic drivers -- leaving a
critical gap in quantifying compound flood risk and the statistical structure
of compound flood transition zones (CFTZs). Here, we extend the JPM theory to
hydrologic processes for quantifying the likelihood of compound flood depths
across both tropical and non-tropical storms. This extended methodology
incorporates rainfall fields, antecedent soil moisture, and baseflow alongside
coastal storm surge, enabling: (1) a statistical description of the flood depth
as the response to the joint distribution of hydrologic and coastal drivers,
(2) a statistical delineation of the CFTZ based on exceedance probabilities,
and (3) a systematic identification of design storms for specified return
period flood depths, moving beyond design based solely on driver likelihoods.
We demonstrate this method around Lake Maurepas, Louisiana. Results show a CFTZ
more than double the area of prior event-specific delineations, with compound
interactions increasing flood depths by up to 2.25 feet. This extended JPM
provides a probabilistic foundation for compound flood risk assessment and
planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03652v1' target='_blank'>Motion Planning Under Temporal Logic Specifications In Semantically
  Unknown Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Azizollah Taheri, Derya Aksaray</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 17:09:43</h6>
<p class='card-text'>This paper addresses a motion planning problem to achieve
spatio-temporal-logical tasks, expressed by syntactically co-safe linear
temporal logic specifications (scLTL\next), in uncertain environments. Here,
the uncertainty is modeled as some probabilistic knowledge on the semantic
labels of the environment. For example, the task is "first go to region 1, then
go to region 2"; however, the exact locations of regions 1 and 2 are not known
a priori, instead a probabilistic belief is available. We propose a novel
automata-theoretic approach, where a special product automaton is constructed
to capture the uncertainty related to semantic labels, and a reward function is
designed for each edge of this product automaton. The proposed algorithm
utilizes value iteration for online replanning. We show some theoretical
results and present some simulations/experiments to demonstrate the efficacy of
the proposed approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03651v1' target='_blank'>Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andrei A. Korigodskii, Oleg D. Kalachev, Artem E. Vasiunik, Matvei V. Urvantsev, Georgii E. Bondar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 17:09:16</h6>
<p class='card-text'>This paper presents the innovative design and successful deployment of a
pioneering autonomous unmanned aerial system developed for executing the
world's largest mural painted by a drone. Addressing the dual challenges of
maintaining artistic precision and operational reliability under adverse
outdoor conditions such as wind and direct sunlight, our work introduces a
robust system capable of navigating and painting outdoors with unprecedented
accuracy. Key to our approach is a novel navigation system that combines an
infrared (IR) motion capture camera and LiDAR technology, enabling precise
location tracking tailored specifically for largescale artistic applications.
We employ a unique control architecture that uses different regulation in
tangential and normal directions relative to the planned path, enabling precise
trajectory tracking and stable line rendering. We also present algorithms for
trajectory planning and path optimization, allowing for complex curve drawing
and area filling. The system includes a custom-designed paint spraying
mechanism, specifically engineered to function effectively amidst the turbulent
airflow generated by the drone's propellers, which also protects the drone's
critical components from paint-related damage, ensuring longevity and
consistent performance. Experimental results demonstrate the system's
robustness and precision in varied conditions, showcasing its potential for
autonomous large-scale art creation and expanding the functional applications
of robotics in creative fields.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03770v1' target='_blank'>Deep Learning-Driven Downscaling for Climate Risk Assessment of
  Projected Temperature Extremes in the Nordic Region</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Parthiban Loganathan, Elias Zea, Ricardo Vinuesa, Evelyn Otero</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 17:08:32</h6>
<p class='card-text'>Rapid changes and increasing climatic variability across the widely varied
Koppen-Geiger regions of northern Europe generate significant needs for
adaptation. Regional planning needs high-resolution projected temperatures.
This work presents an integrative downscaling framework that incorporates
Vision Transformer (ViT), Convolutional Long Short-Term Memory (ConvLSTM), and
Geospatial Spatiotemporal Transformer with Attention and Imbalance-Aware
Network (GeoStaNet) models. The framework is evaluated with a multicriteria
decision system, Deep Learning-TOPSIS (DL-TOPSIS), for ten strategically chosen
meteorological stations encompassing the temperate oceanic (Cfb), subpolar
oceanic (Cfc), warm-summer continental (Dfb), and subarctic (Dfc) climate
regions. Norwegian Earth System Model (NorESM2-LM) Coupled Model
Intercomparison Project Phase 6 (CMIP6) outputs were bias-corrected during the
1951-2014 period and subsequently validated against earlier observations of
day-to-day temperature metrics and diurnal range statistics. The ViT showed
improved performance (Root Mean Squared Error (RMSE): 1.01 degrees C; R^2:
0.92), allowing for production of credible downscaled projections. Under the
SSP5-8.5 scenario, the Dfc and Dfb climate zones are projected to warm by 4.8
degrees C and 3.9 degrees C, respectively, by 2100, with expansion in the
diurnal temperature range by more than 1.5 degrees C. The Time of Emergence
signal first appears in subarctic winter seasons (Dfc: approximately 2032),
signifying an urgent need for adaptation measures. The presented framework
offers station-based, high-resolution estimates of uncertainties and extremes,
with direct uses for adaptation policy over high-latitude regions with fast
environmental change.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03642v1' target='_blank'>Generalized k-Cell Decomposition for Visibility Planning in Polygons</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yeganeh Bahoo, Sajad Saeedi, Roni Sherman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 17:02:35</h6>
<p class='card-text'>This paper introduces a novel $k$-cell decomposition method for
pursuit-evasion problems in polygonal environments, where a searcher is
equipped with a $k$-modem: a device capable of seeing through up to $k$ walls.
The proposed decomposition ensures that as the searcher moves within a cell,
the structure of unseen regions (shadows) remains unchanged, thereby preventing
any geometric events between or on invisible regions, that is, preventing the
appearance, disappearance, merge, or split of shadow regions. The method
extends existing work on $0$- and $2$-visibility by incorporating m-visibility
polygons for all even $0 \le m \le k$, constructing partition lines that enable
robust environment division. The correctness of the decomposition is proved via
three theorems. The decomposition enables reliable path planning for intruder
detection in simulated environments and opens new avenues for visibility-based
robotic surveillance. The difficulty in constructing the cells of the
decomposition consists in computing the $k$-visibility polygon from each vertex
and finding the intersection points of the partition lines to create the cells.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03596v1' target='_blank'>Adjusting for Heavy Censoring and Double-Dipping to Compare Risk
  Stratification Abilities of Existing Models for Time to Diagnosis of
  Huntington Disease</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kyle F. Grosser, Abigail G. Foes, Stellen Li, Vraj Parikh, Tanya P. Garcia, Sarah C. Lotspeich</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 16:16:48</h6>
<p class='card-text'>Huntington disease (HD) is a genetically inherited neurodegenerative disease
with progressively worsening symptoms. Accurately modeling time to HD diagnosis
is essential for clinical trial design and treatment planning. Langbehn's
model, the CAG-Age Product (CAP) model, the Prognostic Index Normed (PIN)
model, and the Multivariate Risk Score (MRS) model have all been proposed for
this task. However, differing in methodology, assumptions, and accuracy, these
models may yield conflicting predictions. Few studies have systematically
compared these models' performance, and those that have could be misleading due
to (i) testing the models on the same data used to train them and (ii) failing
to account for high rates of right censoring (80%+) in performance metrics. We
discuss the theoretical foundations of the four most common models of time to
HD diagnosis, offering intuitive comparisons about their practical feasibility.
Further, we externally validate their risk stratification abilities using data
from the ENROLL-HD study and performance metrics that adjust for censoring. Our
findings guide the selection of a model for HD clinical trial design. The MRS
model, which incorporates the most covariates, performed the best. However, the
simpler CAP and PIN models were not far behind and may be logistically simpler
to adopt. We also show how these models can be used to estimate sample sizes
for an HD clinical trial, emphasizing that previous estimates would lead to
underpowered trials.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03594v1' target='_blank'>Powered Descent Trajectory Optimization of Chandrayaan-3 using Radau
  Collocation and Controllable Sets</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Suraj Kumar, Aditya Rallapalli, Ashok Kumar Kakula, Bharat Kumar GVP</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 16:15:42</h6>
<p class='card-text'>India achieved a significant milestone on August $23^{\text{rd}}$ 2023,
becoming the fourth country to accomplish a soft landing on the Moon. This
paper presents the powered descent trajectory design for the Chandrayaan-3
mission. The optimization framework is based on pseudospectral Radau
collocation, and controllability-based waypoint refinement is employed to
further enhance the robustness of the trajectory against state and control
perturbations. Furthermore, the trade-off between fuel consumption and
robustness is explicitly quantified, providing insights into the practical
considerations of mission planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03591v1' target='_blank'>Manifold-constrained Hamilton-Jacobi Reachability Learning for
  Decentralized Multi-Agent Motion Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qingyi Chen, Ruiqi Ni, Jun Kim, Ahmed H. Qureshi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 16:11:12</h6>
<p class='card-text'>Safe multi-agent motion planning (MAMP) under task-induced constraints is a
critical challenge in robotics. Many real-world scenarios require robots to
navigate dynamic environments while adhering to manifold constraints imposed by
tasks. For example, service robots must carry cups upright while avoiding
collisions with humans or other robots. Despite recent advances in
decentralized MAMP for high-dimensional systems, incorporating manifold
constraints remains difficult. To address this, we propose a
manifold-constrained Hamilton-Jacobi reachability (HJR) learning framework for
decentralized MAMP. Our method solves HJR problems under manifold constraints
to capture task-aware safety conditions, which are then integrated into a
decentralized trajectory optimization planner. This enables robots to generate
motion plans that are both safe and task-feasible without requiring assumptions
about other agents' policies. Our approach generalizes across diverse
manifold-constrained tasks and scales effectively to high-dimensional
multi-agent manipulation problems. Experiments show that our method outperforms
existing constrained motion planners and operates at speeds suitable for
real-world applications. Video demonstrations are available at
https://youtu.be/RYcEHMnPTH8 .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03478v1' target='_blank'>SVG Decomposition for Enhancing Large Multimodal Models Visualization
  Comprehension: A Study with Floor Plans</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jeongah Lee, Ali Sarvghad</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 14:04:10</h6>
<p class='card-text'>Large multimodal models (LMMs) are increasingly capable of interpreting
visualizations, yet they continue to struggle with spatial reasoning. One
proposed strategy is decomposition, which breaks down complex visualizations
into structured components. In this work, we examine the efficacy of scalable
vector graphics (SVGs) as a decomposition strategy for improving LMMs'
performance on floor plans comprehension. Floor plans serve as a valuable
testbed because they combine geometry, topology, and semantics, and their
reliable comprehension has real-world applications, such as accessibility for
blind and low-vision individuals. We conducted an exploratory study with three
LMMs (GPT-4o, Claude 3.7 Sonnet, and Llama 3.2 11B Vision Instruct) across 75
floor plans. Results show that combining SVG with raster input (SVG+PNG)
improves performance on spatial understanding tasks but often hinders spatial
reasoning, particularly in pathfinding. These findings highlight both the
promise and limitations of decomposition as a strategy for advancing spatial
visualization comprehension.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03257v1' target='_blank'>Quantum-classical hybrid algorithm using quantum annealing for
  multi-objective job shop scheduling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kenta Sawamura, Kensuke Araki, Naoki Maruyama, Renichiro Haba, Masayuki Ohzeki</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 07:39:09</h6>
<p class='card-text'>Efficient production planning is essential in modern manufacturing to improve
performance indicators such as lead time and to reduce reliance on human
intuition. While mathematical optimization approaches, formulated as job shop
scheduling problems, have been applied to automate this process, solving
large-scale production planning problems remains computationally demanding.
Moreover, many practical scenarios involve conflicting objectives, making
traditional scalarization techniques ineffective in finding diverse and useful
Pareto-optimal solutions. To address these challenges, we developed a
quantum-classical hybrid algorithm that decomposes the problem into two
subproblems: resource allocation and task scheduling. Resource allocation is
formulated as a quadratic unconstrained binary optimization problem and solved
using annealing-based methods that efficiently explore complex solutions. Task
scheduling is modeled as a mixed-integer linear programming problem and solved
using conventional solvers to satisfy detailed scheduling constraints. We
validated the proposed method using benchmark instances based on foundry
production scenarios. Experimental results demonstrate that our hybrid approach
achieves superior solution quality and computational efficiency compared to
traditional monolithic methods. This work offers a promising direction for
high-speed, multi-objective scheduling in industrial applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03242v1' target='_blank'>Topography, climate, land cover, and biodiversity: Explaining endemic
  richness and management implications on a Mediterranean island</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aristides Moustakas, Ioannis N Vogiatzakis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 07:09:18</h6>
<p class='card-text'>Island endemism is shaped by complex interactions among environmental,
ecological, and evolutionary factors, yet the relative contributions of
topography, climate, and land cover remain incompletely quantified. We
investigated the drivers of endemic plant richness across Crete, a
Mediterranean biodiversity hotspot, using spatially explicit data on species
distributions, topographic complexity, climatic variability, land cover, and
soil characteristics. Artificial Neural Network models, a machine learning
tool, were employed to assess the relative importance of these predictors and
to identify hotspots of endemism. We found that total species richness,
elevation range, and climatic variability were the strongest predictors of
endemic richness, reflecting the role of biodiversity, topographic
heterogeneity, and climatic gradients in generating diverse habitats and
micro-refugia that promote speciation and buffer extinction risk. Endemic
hotspots only partially overlapped with areas of high total species richness,
indicating that total species richness was the optimal from the ones examined,
yet an imperfect surrogate. These environmentally heterogeneous areas also
provide critical ecosystem services, including soil stabilization, pollination,
and cultural value, which are increasingly threatened by tourism, renewable
energy development, land-use change, and climate impacts. Our findings
underscore the importance of prioritizing mountainous and climatically variable
regions in conservation planning, integrating ecosystem service considerations,
and accounting for within-island spatial heterogeneity. By explicitly linking
the environmental drivers of endemism to both biodiversity patterns and
ecosystem function, this study provides a framework for evidence-based
conservation planning in Crete and other Mediterranean islands with similar
geological and biogeographic contexts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03238v1' target='_blank'>Incorporating Quality of Life in Climate Adaptation Planning via
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Miguel Costa, Arthur Vandervoort, Martin Drews, Karyn Morrissey, Francisco C. Pereira</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 07:00:55</h6>
<p class='card-text'>Urban flooding is expected to increase in frequency and severity as a
consequence of climate change, causing wide-ranging impacts that include a
decrease in urban Quality of Life (QoL). Meanwhile, policymakers must devise
adaptation strategies that can cope with the uncertain nature of climate change
and the complex and dynamic nature of urban flooding. Reinforcement Learning
(RL) holds significant promise in tackling such complex, dynamic, and uncertain
problems. Because of this, we use RL to identify which climate adaptation
pathways lead to a higher QoL in the long term. We do this using an Integrated
Assessment Model (IAM) which combines a rainfall projection model, a flood
model, a transport accessibility model, and a quality of life index. Our
preliminary results suggest that this approach can be used to learn optimal
adaptation measures and it outperforms other realistic and real-world planning
strategies. Our framework is publicly available:
https://github.com/MLSM-at-DTU/maat_qol_framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03165v1' target='_blank'>SENT Map -- Semantically Enhanced Topological Maps with Foundation
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Raj Surya Rajendran Kathirvel, Zach A Chavis, Stephen J. Guy, Karthik Desingh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 04:22:04</h6>
<p class='card-text'>We introduce SENT-Map, a semantically enhanced topological map for
representing indoor environments, designed to support autonomous navigation and
manipulation by leveraging advancements in foundational models (FMs). Through
representing the environment in a JSON text format, we enable semantic
information to be added and edited in a format that both humans and FMs
understand, while grounding the robot to existing nodes during planning to
avoid infeasible states during deployment. Our proposed framework employs a two
stage approach, first mapping the environment alongside an operator with a
Vision-FM, then using the SENT-Map representation alongside a natural-language
query within an FM for planning. Our experimental results show that
semantic-enhancement enables even small locally-deployable FMs to successfully
plan over indoor environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03115v1' target='_blank'>Fast SDE-based Monte Carlo dose calculation for proton therapy validated
  against Geant4</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Christopher B. C. Dean, Maria L. Pérez-Lara, Emma Horton, Matthew Southerby, Jere Koskela, Andreas E. Kyprianou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-05 01:45:57</h6>
<p class='card-text'>Objective: To validate a newly proposed stochastic differential equation
(SDE)-based model for proton beam energy deposition by comparing its
predictions with those from Geant4 in simplified phantom scenarios. Approach:
Building on previous work in Crossley et al. (2025), where energy deposition
from a proton beam was modelled using an SDE framework, we implemented the
model with standard approximations to interaction cross sections and mean
excitation energies, which makes simulations easily adaptable to new materials
and configurations. The model was benchmarked against Geant4 in homogeneous and
heterogeneous phantoms. Main results: The SDE-based dose distributions agreed
well with Geant4, showing range differences within 0.4 mm and 3D gamma pass
rates exceeding 98% under 3%/2 mm criteria with a 1% dose threshold. The model
achieved a computational speed-up of approximately fivefold relative to Geant4,
consistent across different Geant4 physics lists. Significance: These results
demonstrate that the SDE approach can reproduce accuracy comparable to
high-fidelity Monte Carlo for proton therapy at a fraction of the computational
cost, highlighting its potential for accelerating dose calculations and
treatment planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2511.03077v1' target='_blank'>WorldPlanner: Monte Carlo Tree Search and MPC with Action-Conditioned
  Visual World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:R. Khorrambakht, Joaquim Ortiz-Haro, Joseph Amigo, Omar Mostafa, Daniel Dugas, Franziska Meier, Ludovic Righetti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-04 23:52:07</h6>
<p class='card-text'>Robots must understand their environment from raw sensory inputs and reason
about the consequences of their actions in it to solve complex tasks. Behavior
Cloning (BC) leverages task-specific human demonstrations to learn this
knowledge as end-to-end policies. However, these policies are difficult to
transfer to new tasks, and generating training data is challenging because it
requires careful demonstrations and frequent environment resets. In contrast to
such policy-based view, in this paper we take a model-based approach where we
collect a few hours of unstructured easy-to-collect play data to learn an
action-conditioned visual world model, a diffusion-based action sampler, and
optionally a reward model. The world model -- in combination with the action
sampler and a reward model -- is then used to optimize long sequences of
actions with a Monte Carlo Tree Search (MCTS) planner. The resulting plans are
executed on the robot via a zeroth-order Model Predictive Controller (MPC). We
show that the action sampler mitigates hallucinations of the world model during
planning and validate our approach on 3 real-world robotic tasks with varying
levels of planning and modeling complexity. Our experiments support the
hypothesis that planning leads to a significant improvement over BC baselines
on a standard manipulation test environment.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>