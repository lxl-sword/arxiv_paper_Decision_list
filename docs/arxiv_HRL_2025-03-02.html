<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>HRL - 2025-03-02</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>HRL - 2025-03-02</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20380v1' target='_blank'>Multi-Turn Code Generation Through Single-Step Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arnav Kumar Jain, Gonzalo Gonzalez-Pumariega, Wayne Chen, Alexander M Rush, Wenting Zhao, Sanjiban Choudhury</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:55:05</h6>
<p class='card-text'>We address the problem of code generation from multi-turn execution feedback.
Existing methods either generate code without feedback or use complex,
hierarchical reinforcement learning to optimize multi-turn rewards. We propose
a simple yet scalable approach, $\mu$Code, that solves multi-turn code
generation using only single-step rewards. Our key insight is that code
generation is a one-step recoverable MDP, where the correct code can be
recovered from any intermediate code state in a single turn. $\mu$Code
iteratively trains both a generator to provide code solutions conditioned on
multi-turn execution feedback and a verifier to score the newly generated code.
Experimental evaluations show that our approach achieves significant
improvements over the state-of-the-art baselines. We provide analysis of the
design choices of the reward models and policy, and show the efficacy of
$\mu$Code at utilizing the execution feedback. Our code is available at
https://github.com/portal-cornell/muCode.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.15425v2' target='_blank'>TAG: A Decentralized Framework for Multi-Agent Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Giuseppe Paolo, Abdelhakim Benechehab, Hamza Cherkaoui, Albert Thomas, Balázs Kégl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-21 12:52:16</h6>
<p class='card-text'>Hierarchical organization is fundamental to biological systems and human
societies, yet artificial intelligence systems often rely on monolithic
architectures that limit adaptability and scalability. Current hierarchical
reinforcement learning (HRL) approaches typically restrict hierarchies to two
levels or require centralized training, which limits their practical
applicability. We introduce TAME Agent Framework (TAG), a framework for
constructing fully decentralized hierarchical multi-agent systems.TAG enables
hierarchies of arbitrary depth through a novel LevelEnv concept, which
abstracts each hierarchy level as the environment for the agents above it. This
approach standardizes information flow between levels while preserving loose
coupling, allowing for seamless integration of diverse agent types. We
demonstrate the effectiveness of TAG by implementing hierarchical architectures
that combine different RL agents across multiple levels, achieving improved
performance over classical multi-agent RL baselines on standard benchmarks. Our
results show that decentralized hierarchical organization enhances both
learning speed and final performance, positioning TAG as a promising direction
for scalable multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06772v1' target='_blank'>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 18:51:47</h6>
<p class='card-text'>We present that hierarchical LLM reasoning via scaling thought templates can
effectively optimize the reasoning search space and outperform the mathematical
reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.
We train our ReasonFlux-32B model with only 8 GPUs and introduces three
innovations: (i) a structured and generic thought template library, containing
around 500 high-level thought templates capable of generalizing to similar or
relevant reasoning problems; (ii) performing hierarchical reinforcement
learning on a sequence of thought templates instead of long CoTs, optimizing a
base LLM to plan out an optimal template trajectory for gradually handling
complex problems; (iii) a brand new inference scaling system that enables
hierarchical LLM reasoning by adaptively scaling thought templates at inference
time. With a template trajectory containing sequential thought templates, our
ReasonFlux-32B significantly advances math reasoning capabilities to
state-of-the-art levels. Notably, on the MATH benchmark, it achieves an
accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad
(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,
surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:
https://github.com/Gen-Verse/ReasonFlux</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05537v1' target='_blank'>Sequential Stochastic Combinatorial Optimization Using Hierarchal
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinsong Feng, Zihan Yu, Yanhai Xiong, Haipeng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 12:00:30</h6>
<p class='card-text'>Reinforcement learning (RL) has emerged as a promising tool for combinatorial
optimization (CO) problems due to its ability to learn fast, effective, and
generalizable solutions. Nonetheless, existing works mostly focus on one-shot
deterministic CO, while sequential stochastic CO (SSCO) has rarely been studied
despite its broad applications such as adaptive influence maximization (IM) and
infectious disease intervention. In this paper, we study the SSCO problem where
we first decide the budget (e.g., number of seed nodes in adaptive IM)
allocation for all time steps, and then select a set of nodes for each time
step. The few existing studies on SSCO simplify the problems by assuming a
uniformly distributed budget allocation over the time horizon, yielding
suboptimal solutions. We propose a generic hierarchical RL (HRL) framework
called wake-sleep option (WS-option), a two-layer option-based framework that
simultaneously decides adaptive budget allocation on the higher layer and node
selection on the lower layer. WS-option starts with a coherent formulation of
the two-layer Markov decision processes (MDPs), capturing the interdependencies
between the two layers of decisions. Building on this, WS-option employs
several innovative designs to balance the model's training stability and
computational efficiency, preventing the vicious cyclic interference issue
between the two layers. Empirical results show that WS-option exhibits
significantly improved effectiveness and generalizability compared to
traditional methods. Moreover, the learned model can be generalized to larger
graphs, which significantly reduces the overhead of computational resources.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.03960v1' target='_blank'>Bilevel Multi-Armed Bandit-Based Hierarchical Reinforcement Learning for
  Interaction-Aware Self-Driving at Unsignalized Intersections</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zengqi Peng, Yubin Wang, Lei Zheng, Jun Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-06 10:50:59</h6>
<p class='card-text'>In this work, we present BiM-ACPPO, a bilevel multi-armed bandit-based
hierarchical reinforcement learning framework for interaction-aware
decision-making and planning at unsignalized intersections. Essentially, it
proactively takes the uncertainties associated with surrounding vehicles (SVs)
into consideration, which encompass those stemming from the driver's intention,
interactive behaviors, and the varying number of SVs. Intermediate decision
variables are introduced to enable the high-level RL policy to provide an
interaction-aware reference, for guiding low-level model predictive control
(MPC) and further enhancing the generalization ability of the proposed
framework. By leveraging the structured nature of self-driving at unsignalized
intersections, the training problem of the RL policy is modeled as a bilevel
curriculum learning task, which is addressed by the proposed Exp3.S-based BiMAB
algorithm. It is noteworthy that the training curricula are dynamically
adjusted, thereby facilitating the sample efficiency of the RL training
process. Comparative experiments are conducted in the high-fidelity CARLA
simulator, and the results indicate that our approach achieves superior
performance compared to all baseline methods. Furthermore, experimental results
in two new urban driving scenarios clearly demonstrate the commendable
generalization performance of the proposed method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.01956v1' target='_blank'>DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement
  Learning Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shashank Sharma, Janina Hoffmann, Vinay Namboodiri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-04 03:05:55</h6>
<p class='card-text'>In this paper, we address the challenge of long-horizon visual planning tasks
using Hierarchical Reinforcement Learning (HRL). Our key contribution is a
Discrete Hierarchical Planning (DHP) method, an alternative to traditional
distance-based approaches. We provide theoretical foundations for the method
and demonstrate its effectiveness through extensive empirical evaluations.
  Our agent recursively predicts subgoals in the context of a long-term goal
and receives discrete rewards for constructing plans as compositions of
abstract actions. The method introduces a novel advantage estimation strategy
for tree trajectories, which inherently encourages shorter plans and enables
generalization beyond the maximum tree depth. The learned policy function
allows the agent to plan efficiently, requiring only $\log N$ computational
steps, making re-planning highly efficient. The agent, based on a soft-actor
critic (SAC) framework, is trained using on-policy imagination data.
Additionally, we propose a novel exploration strategy that enables the agent to
generate relevant training examples for the planning modules. We evaluate our
method on long-horizon visual planning tasks in a 25-room environment, where it
significantly outperforms previous benchmarks at success rate and average
episode length. Furthermore, an ablation study highlights the individual
contributions of key modules to the overall performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.17424v1' target='_blank'>Certificated Actor-Critic: Hierarchical Reinforcement Learning with
  Control Barrier Functions for Safe Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junjun Xie, Shuhao Zhao, Liang Hu, Huijun Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-29 05:37:47</h6>
<p class='card-text'>Control Barrier Functions (CBFs) have emerged as a prominent approach to
designing safe navigation systems of robots. Despite their popularity, current
CBF-based methods exhibit some limitations: optimization-based safe control
techniques tend to be either myopic or computationally intensive, and they rely
on simplified system models; conversely, the learning-based methods suffer from
the lack of quantitative indication in terms of navigation performance and
safety. In this paper, we present a new model-free reinforcement learning
algorithm called Certificated Actor-Critic (CAC), which introduces a
hierarchical reinforcement learning framework and well-defined reward functions
derived from CBFs. We carry out theoretical analysis and proof of our
algorithm, and propose several improvements in algorithm implementation. Our
analysis is validated by two simulation experiments, showing the effectiveness
of our proposed CAC algorithm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.14992v1' target='_blank'>Extensive Exploration in Complex Traffic Scenarios using Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhihao Zhang, Ekim Yurtsever, Keith A. Redmill</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-25 00:00:11</h6>
<p class='card-text'>Developing an automated driving system capable of navigating complex traffic
environments remains a formidable challenge. Unlike rule-based or supervised
learning-based methods, Deep Reinforcement Learning (DRL) based controllers
eliminate the need for domain-specific knowledge and datasets, thus providing
adaptability to various scenarios. Nonetheless, a common limitation of existing
studies on DRL-based controllers is their focus on driving scenarios with
simple traffic patterns, which hinders their capability to effectively handle
complex driving environments with delayed, long-term rewards, thus compromising
the generalizability of their findings. In response to these limitations, our
research introduces a pioneering hierarchical framework that efficiently
decomposes intricate decision-making problems into manageable and interpretable
subtasks. We adopt a two step training process that trains the high-level
controller and low-level controller separately. The high-level controller
exhibits an enhanced exploration potential with long-term delayed rewards, and
the low-level controller provides longitudinal and lateral control ability
using short-term instantaneous rewards. Through simulation experiments, we
demonstrate the superiority of our hierarchical controller in managing complex
highway driving situations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13084v1' target='_blank'>Attention-Driven Hierarchical Reinforcement Learning with Particle
  Filtering for Source Localization in Dynamic Fields</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiwei Shi, Mengyue Yang, Qi Zhang, Weinan Zhang, Cunjia Liu, Weiru Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 18:45:29</h6>
<p class='card-text'>In many real-world scenarios, such as gas leak detection or environmental
pollutant tracking, solving the Inverse Source Localization and
Characterization problem involves navigating complex, dynamic fields with
sparse and noisy observations. Traditional methods face significant challenges,
including partial observability, temporal and spatial dynamics,
out-of-distribution generalization, and reward sparsity. To address these
issues, we propose a hierarchical framework that integrates Bayesian inference
and reinforcement learning. The framework leverages an attention-enhanced
particle filtering mechanism for efficient and accurate belief updates, and
incorporates two complementary execution strategies: Attention Particle
Filtering Planning and Attention Particle Filtering Reinforcement Learning.
These approaches optimize exploration and adaptation under uncertainty.
Theoretical analysis proves the convergence of the attention-enhanced particle
filter, while extensive experiments across diverse scenarios validate the
framework's superior accuracy, adaptability, and computational efficiency. Our
results highlight the framework's potential for broad applications in dynamic
field estimation tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.13132v1' target='_blank'>A Hierarchical Reinforcement Learning Framework for Multi-UAV Combat
  Using Leader-Follower Strategy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinhui Pang, Jinglin He, Noureldin Mohamed Abdelaal Ahmed Mohamed, Changqing Lin, Zhihui Zhang, Xiaoshuai Hao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-22 02:41:36</h6>
<p class='card-text'>Multi-UAV air combat is a complex task involving multiple autonomous UAVs, an
evolving field in both aerospace and artificial intelligence. This paper aims
to enhance adversarial performance through collaborative strategies. Previous
approaches predominantly discretize the action space into predefined actions,
limiting UAV maneuverability and complex strategy implementation. Others
simplify the problem to 1v1 combat, neglecting the cooperative dynamics among
multiple UAVs. To address the high-dimensional challenges inherent in
six-degree-of-freedom space and improve cooperation, we propose a hierarchical
framework utilizing the Leader-Follower Multi-Agent Proximal Policy
Optimization (LFMAPPO) strategy. Specifically, the framework is structured into
three levels. The top level conducts a macro-level assessment of the
environment and guides execution policy. The middle level determines the angle
of the desired action. The bottom level generates precise action commands for
the high-dimensional action space. Moreover, we optimize the state-value
functions by assigning distinct roles with the leader-follower strategy to
train the top-level policy, followers estimate the leader's utility, promoting
effective cooperation among agents. Additionally, the incorporation of a target
selector, aligned with the UAVs' posture, assesses the threat level of targets.
Finally, simulation experiments validate the effectiveness of our proposed
method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.07274v2' target='_blank'>Mining Intraday Risk Factor Collections via Hierarchical Reinforcement
  Learning based on Transferred Options</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenyan Xu, Jiayu Chen, Chen Li, Yonghong Hu, Zhonghua Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-13 12:38:05</h6>
<p class='card-text'>Traditional risk factors like beta, size/value, and momentum often lag behind
market dynamics in measuring and predicting stock return volatility.
Statistical models like PCA and factor analysis fail to capture hidden
nonlinear relationships. Genetic programming (GP) can identify nonlinear
factors but often lacks mechanisms for evaluating factor quality, and the
resulting formulas are complex. To address these challenges, we propose a
Hierarchical Proximal Policy Optimization (HPPO) framework for automated factor
generation and evaluation. HPPO uses two PPO models: a high-level policy
assigns weights to stock features, and a low-level policy identifies latent
nonlinear relationships. The Pearson correlation between generated factors and
return volatility serves as the reward signal. Transfer learning pre-trains the
high-level policy on large-scale historical data, fine-tuning it with the
latest data to adapt to new features and shifts. Experiments show the HPPO-TO
algorithm achieves a 25\% excess return in HFT markets across China (CSI
300/800), India (Nifty 100), and the US (S\&P 500). Code and data are available
at https://github.com/wencyxu/HRL-HF_risk_factor_set.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.06554v1' target='_blank'>Hierarchical Reinforcement Learning for Optimal Agent Grouping in
  Cooperative Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Liyuan Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-11 14:22:10</h6>
<p class='card-text'>This paper presents a hierarchical reinforcement learning (RL) approach to
address the agent grouping or pairing problem in cooperative multi-agent
systems. The goal is to simultaneously learn the optimal grouping and agent
policy. By employing a hierarchical RL framework, we distinguish between
high-level decisions of grouping and low-level agents' actions. Our approach
utilizes the CTDE (Centralized Training with Decentralized Execution) paradigm,
ensuring efficient learning and scalable execution. We incorporate
permutation-invariant neural networks to handle the homogeneity and cooperation
among agents, enabling effective coordination. The option-critic algorithm is
adapted to manage the hierarchical decision-making process, allowing for
dynamic and optimal policy adjustments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2501.02368v1' target='_blank'>Enhancing Workplace Productivity and Well-being Using AI Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ravirajan K, Arvind Sundarajan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-01-04 20:11:00</h6>
<p class='card-text'>This paper discusses the use of Artificial Intelligence (AI) to enhance
workplace productivity and employee well-being. By integrating machine learning
(ML) techniques with neurobiological data, the proposed approaches ensure
alignment with human ethical standards through value alignment models and
Hierarchical Reinforcement Learning (HRL) for autonomous task management. The
system utilizes biometric feedback from employees to generate personalized
health prompts, fostering a supportive work environment that encourages
physical activity. Additionally, we explore decentralized multi-agent systems
for improved collaboration and decision-making frameworks that enhance
transparency. Various approaches using ML techniques in conjunction with AI
implementations are discussed. Together, these innovations aim to create a more
productive and health-conscious workplace. These outcomes assist HR management
and organizations in launching more rational career progression streams for
employees and facilitating organizational transformation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.19538v1' target='_blank'>Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot
  Task Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-27 09:07:11</h6>
<p class='card-text'>To improve the efficiency of warehousing system and meet huge customer
orders, we aim to solve the challenges of dimension disaster and dynamic
properties in hyper scale multi-robot task planning (MRTP) for robotic mobile
fulfillment system (RMFS). Existing research indicates that hierarchical
reinforcement learning (HRL) is an effective method to reduce these challenges.
Based on that, we construct an efficient multi-stage HRL-based multi-robot task
planner for hyper scale MRTP in RMFS, and the planning process is represented
with a special temporal graph topology. To ensure optimality, the planner is
designed with a centralized architecture, but it also brings the challenges of
scaling up and generalization that require policies to maintain performance for
various unlearned scales and maps. To tackle these difficulties, we first
construct a hierarchical temporal attention network (HTAN) to ensure basic
ability of handling inputs with unfixed lengths, and then design multi-stage
curricula for hierarchical policy learning to further improve the scaling up
and generalization ability while avoiding catastrophic forgetting.
Additionally, we notice that policies with hierarchical structure suffer from
unfair credit assignment that is similar to that in multi-agent reinforcement
learning, inspired of which, we propose a hierarchical reinforcement learning
algorithm with counterfactual rollout baseline to improve learning performance.
Experimental results demonstrate that our planner outperform other
state-of-the-art methods on various MRTP instances in both simulated and
real-world RMFS. Also, our planner can successfully scale up to hyper scale
MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on
unlearned maps while keeping superior performance over other methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.16395v1' target='_blank'>Autonomous Option Invention for Continual Hierarchical Reinforcement
  Learning and Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rashmeet Kaur Nayyar, Siddharth Srivastava</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-20 23:04:52</h6>
<p class='card-text'>Abstraction is key to scaling up reinforcement learning (RL). However,
autonomously learning abstract state and action representations to enable
transfer and generalization remains a challenging open problem. This paper
presents a novel approach for inventing, representing, and utilizing options,
which represent temporally extended behaviors, in continual RL settings. Our
approach addresses streams of stochastic problems characterized by long
horizons, sparse rewards, and unknown transition and reward functions.
  Our approach continually learns and maintains an interpretable state
abstraction, and uses it to invent high-level options with abstract symbolic
representations. These options meet three key desiderata: (1) composability for
solving tasks effectively with lookahead planning, (2) reusability across
problem instances for minimizing the need for relearning, and (3) mutual
independence for reducing interference among options. Our main contributions
are approaches for continually learning transferable, generalizable options
with symbolic representations, and for integrating search techniques with RL to
efficiently plan over these learned options to solve new problems. Empirical
results demonstrate that the resulting approach effectively learns and
transfers abstract knowledge across problem instances, achieving superior
sample efficiency compared to state-of-the-art methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.17854v1' target='_blank'>Active Geospatial Search for Efficient Tenant Eviction Outreach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anindya Sarkar, Alex DiChristofano, Sanmay Das, Patrick J. Fowler, Nathan Jacobs, Yevgeniy Vorobeychik</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-19 23:40:36</h6>
<p class='card-text'>Tenant evictions threaten housing stability and are a major concern for many
cities. An open question concerns whether data-driven methods enhance outreach
programs that target at-risk tenants to mitigate their risk of eviction. We
propose a novel active geospatial search (AGS) modeling framework for this
problem. AGS integrates property-level information in a search policy that
identifies a sequence of rental units to canvas to both determine their
eviction risk and provide support if needed. We propose a hierarchical
reinforcement learning approach to learn a search policy for AGS that scales to
large urban areas containing thousands of parcels, balancing exploration and
exploitation and accounting for travel costs and a budget constraint.
Crucially, the search policy adapts online to newly discovered information
about evictions. Evaluation using eviction data for a large urban area
demonstrates that the proposed framework and algorithmic approach are
considerably more effective at sequentially identifying eviction cases than
baseline methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2412.14584v1' target='_blank'>Simulation-Free Hierarchical Latent Policy Planning for Proactive
  Dialogues</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Yiheng Sun, Zerui Chen, Ming Liu, Bing Qin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-12-19 07:06:01</h6>
<p class='card-text'>Recent advancements in proactive dialogues have garnered significant
attention, particularly for more complex objectives (e.g. emotion support and
persuasion). Unlike traditional task-oriented dialogues, proactive dialogues
demand advanced policy planning and adaptability, requiring rich scenarios and
comprehensive policy repositories to develop such systems. However, existing
approaches tend to rely on Large Language Models (LLMs) for user simulation and
online learning, leading to biases that diverge from realistic scenarios and
result in suboptimal efficiency. Moreover, these methods depend on manually
defined, context-independent, coarse-grained policies, which not only incur
high expert costs but also raise concerns regarding their completeness. In our
work, we highlight the potential for automatically discovering policies
directly from raw, real-world dialogue records. To this end, we introduce a
novel dialogue policy planning framework, LDPP. It fully automates the process
from mining policies in dialogue records to learning policy planning.
Specifically, we employ a variant of the Variational Autoencoder to discover
fine-grained policies represented as latent vectors. After automatically
annotating the data with these latent policy labels, we propose an Offline
Hierarchical Reinforcement Learning (RL) algorithm in the latent space to
develop effective policy planning capabilities. Our experiments demonstrate
that LDPP outperforms existing methods on two proactive scenarios, even
surpassing ChatGPT with only a 1.8-billion-parameter LLM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.02998v2' target='_blank'>Accelerating Task Generalisation with Multi-Level Hierarchical Options</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas P Cannon, Özgür Simsek</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-05 11:00:09</h6>
<p class='card-text'>Creating reinforcement learning agents that generalise effectively to new
tasks is a key challenge in AI research. This paper introduces Fracture Cluster
Options (FraCOs), a multi-level hierarchical reinforcement learning method that
achieves state-of-the-art performance on difficult generalisation tasks. FraCOs
identifies patterns in agent behaviour and forms options based on the expected
future usefulness of those patterns, enabling rapid adaptation to new tasks. In
tabular settings, FraCOs demonstrates effective transfer and improves
performance as it grows in hierarchical depth. We evaluate FraCOs against
state-of-the-art deep reinforcement learning algorithms in several complex
procedurally generated environments. Our results show that FraCOs achieves
higher in-distribution and out-of-distribution performance than competitors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.01184v1' target='_blank'>Guiding Multi-agent Multi-task Reinforcement Learning by a Hierarchical
  Framework with Logical Reward Shaping</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chanjuan Liu, Jinmiao Cong, Bingcai Chen, Yaochu Jin, Enqiang Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-02 09:03:23</h6>
<p class='card-text'>Multi-agent hierarchical reinforcement learning (MAHRL) has been studied as
an effective means to solve intelligent decision problems in complex and
large-scale environments. However, most current MAHRL algorithms follow the
traditional way of using reward functions in reinforcement learning, which
limits their use to a single task. This study aims to design a multi-agent
cooperative algorithm with logic reward shaping (LRS), which uses a more
flexible way of setting the rewards, allowing for the effective completion of
multi-tasks. LRS uses Linear Temporal Logic (LTL) to express the internal logic
relation of subtasks within a complex task. Then, it evaluates whether the
subformulae of the LTL expressions are satisfied based on a designed reward
structure. This helps agents to learn to effectively complete tasks by adhering
to the LTL expressions, thus enhancing the interpretability and credibility of
their decisions. To enhance coordination and cooperation among multiple agents,
a value iteration technique is designed to evaluate the actions taken by each
agent. Based on this evaluation, a reward function is shaped for coordination,
which enables each agent to evaluate its status and complete the remaining
subtasks through experiential learning. Experiments have been conducted on
various types of tasks in the Minecraft-like environment. The results
demonstrate that the proposed algorithm can improve the performance of
multi-agents when learning to complete multi-tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2411.00361v1' target='_blank'>Hierarchical Preference Optimization: Learning to achieve goals via
  feasible subgoals prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Utsav Singh, Souradip Chakraborty, Wesley A. Suttle, Brian M. Sadler, Anit Kumar Sahu, Mubarak Shah, Vinay P. Namboodiri, Amrit Singh Bedi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-11-01 04:58:40</h6>
<p class='card-text'>This work introduces Hierarchical Preference Optimization (HPO), a novel
approach to hierarchical reinforcement learning (HRL) that addresses
non-stationarity and infeasible subgoal generation issues when solving complex
robotic control tasks. HPO leverages maximum entropy reinforcement learning
combined with token-level Direct Preference Optimization (DPO), eliminating the
need for pre-trained reference policies that are typically unavailable in
challenging robotic scenarios. Mathematically, we formulate HRL as a bi-level
optimization problem and transform it into a primitive-regularized DPO
formulation, ensuring feasible subgoal generation and avoiding degenerate
solutions. Extensive experiments on challenging robotic navigation and
manipulation tasks demonstrate impressive performance of HPO, where it shows an
improvement of up to 35% over the baselines. Furthermore, ablation studies
validate our design choices, and quantitative analyses confirm the ability of
HPO to mitigate non-stationarity and infeasible subgoal generation issues in
HRL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.24089v1' target='_blank'>Demystifying Linear MDPs and Novel Dynamics Aggregation Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joongkyu Lee, Min-hwan Oh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-31 16:21:41</h6>
<p class='card-text'>In this work, we prove that, in linear MDPs, the feature dimension $d$ is
lower bounded by $S/U$ in order to aptly represent transition probabilities,
where $S$ is the size of the state space and $U$ is the maximum size of
directly reachable states. Hence, $d$ can still scale with $S$ depending on the
direct reachability of the environment. To address this limitation of linear
MDPs, we propose a novel structural aggregation framework based on dynamics,
named as the "dynamics aggregation". For this newly proposed framework, we
design a provably efficient hierarchical reinforcement learning algorithm in
linear function approximation that leverages aggregated sub-structures. Our
proposed algorithm exhibits statistical efficiency, achieving a regret of $
\tilde{O} ( d_{\psi}^{3/2} H^{3/2}\sqrt{ N T} )$, where $d_{\psi}$ represents
the feature dimension of aggregated subMDPs and $N$ signifies the number of
aggregated subMDPs. We establish that the condition $d_{\psi}^3 N \ll d^{3}$ is
readily met in most real-world environments with hierarchical structures,
enabling a substantial improvement in the regret bound compared to LSVI-UCB,
which enjoys a regret of $ \tilde{O} (d^{3/2} H^{3/2} \sqrt{ T})$. To the best
of our knowledge, this work presents the first HRL algorithm with linear
function approximation that offers provable guarantees.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.23156v1' target='_blank'>VisualPredicator: Learning Abstract World Models with Neuro-Symbolic
  Predicates for Robot Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B. Tenenbaum, Tom Silver, João F. Henriques, Kevin Ellis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-30 16:11:05</h6>
<p class='card-text'>Broadly intelligent agents should form task-specific abstractions that
selectively expose the essential elements of a task, while abstracting away the
complexity of the raw sensorimotor space. In this work, we present
Neuro-Symbolic Predicates, a first-order abstraction language that combines the
strengths of symbolic and neural knowledge representations. We outline an
online algorithm for inventing such predicates and learning abstract world
models. We compare our approach to hierarchical reinforcement learning,
vision-language model planning, and symbolic predicate invention approaches, on
both in- and out-of-distribution tasks across five simulated robotic domains.
Results show that our approach offers better sample complexity, stronger
out-of-distribution generalization, and improved interpretability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.20180v2' target='_blank'>Copyright-Aware Incentive Scheme for Generative Art Models Using
  Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuan Shi, Yifei Song, Xiaoli Tang, Lingjuan Lyu, Boi Faltings</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-26 13:29:43</h6>
<p class='card-text'>Generative art using Diffusion models has achieved remarkable performance in
image generation and text-to-image tasks. However, the increasing demand for
training data in generative art raises significant concerns about copyright
infringement, as models can produce images highly similar to copyrighted works.
Existing solutions attempt to mitigate this by perturbing Diffusion models to
reduce the likelihood of generating such images, but this often compromises
model performance. Another approach focuses on economically compensating data
holders for their contributions, yet it fails to address copyright loss
adequately. Our approach begin with the introduction of a novel copyright
metric grounded in copyright law and court precedents on infringement. We then
employ the TRAK method to estimate the contribution of data holders. To
accommodate the continuous data collection process, we divide the training into
multiple rounds. Finally, We designed a hierarchical budget allocation method
based on reinforcement learning to determine the budget for each round and the
remuneration of the data holder based on the data holder's contribution and
copyright loss in each round. Extensive experiments across three datasets show
that our method outperforms all eight benchmarks, demonstrating its
effectiveness in optimizing budget distribution in a copyright-aware manner. To
the best of our knowledge, this is the first technical work that introduces to
incentive contributors and protect their copyrights by compensating them.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.14927v1' target='_blank'>Hierarchical Reinforced Trader (HRT): A Bi-Level Approach for Optimizing
  Stock Selection and Execution</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijie Zhao, Roy E. Welsch</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-19 01:29:38</h6>
<p class='card-text'>Leveraging Deep Reinforcement Learning (DRL) in automated stock trading has
shown promising results, yet its application faces significant challenges,
including the curse of dimensionality, inertia in trading actions, and
insufficient portfolio diversification. Addressing these challenges, we
introduce the Hierarchical Reinforced Trader (HRT), a novel trading strategy
employing a bi-level Hierarchical Reinforcement Learning framework. The HRT
integrates a Proximal Policy Optimization (PPO)-based High-Level Controller
(HLC) for strategic stock selection with a Deep Deterministic Policy Gradient
(DDPG)-based Low-Level Controller (LLC) tasked with optimizing trade executions
to enhance portfolio value. In our empirical analysis, comparing the HRT agent
with standalone DRL models and the S&P 500 benchmark during both bullish and
bearish market conditions, we achieve a positive and higher Sharpe ratio. This
advancement not only underscores the efficacy of incorporating hierarchical
structures into DRL strategies but also mitigates the aforementioned
challenges, paving the way for designing more profitable and robust trading
algorithms in complex markets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.13979v1' target='_blank'>RecoveryChaining: Learning Local Recovery Policies for Robust
  Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shivam Vats, Devesh K. Jha, Maxim Likhachev, Oliver Kroemer, Diego Romeres</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-17 19:14:43</h6>
<p class='card-text'>Model-based planners and controllers are commonly used to solve complex
manipulation problems as they can efficiently optimize diverse objectives and
generalize to long horizon tasks. However, they are limited by the fidelity of
their model which oftentimes leads to failures during deployment. To enable a
robot to recover from such failures, we propose to use hierarchical
reinforcement learning to learn a separate recovery policy. The recovery policy
is triggered when a failure is detected based on sensory observations and seeks
to take the robot to a state from which it can complete the task using the
nominal model-based controllers. Our approach, called RecoveryChaining, uses a
hybrid action space, where the model-based controllers are provided as
additional \emph{nominal} options which allows the recovery policy to decide
how to recover, when to switch to a nominal controller and which controller to
switch to even with \emph{sparse rewards}. We evaluate our approach in three
multi-step manipulation tasks with sparse rewards, where it learns
significantly more robust recovery policies than those learned by baselines.
Finally, we successfully transfer recovery policies learned in simulation to a
physical robot to demonstrate the feasibility of sim-to-real transfer with our
method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.11251v1' target='_blank'>Disentangled Unsupervised Skill Discovery for Efficient Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiaheng Hu, Zizhao Wang, Peter Stone, Roberto Martín-Martín</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-15 04:13:20</h6>
<p class='card-text'>A hallmark of intelligent agents is the ability to learn reusable skills
purely from unsupervised interaction with the environment. However, existing
unsupervised skill discovery methods often learn entangled skills where one
skill variable simultaneously influences many entities in the environment,
making downstream skill chaining extremely challenging. We propose Disentangled
Unsupervised Skill Discovery (DUSDi), a method for learning disentangled skills
that can be efficiently reused to solve downstream tasks. DUSDi decomposes
skills into disentangled components, where each skill component only affects
one factor of the state space. Importantly, these skill components can be
concurrently composed to generate low-level actions, and efficiently chained to
tackle downstream tasks through hierarchical Reinforcement Learning. DUSDi
defines a novel mutual-information-based objective to enforce disentanglement
between the influences of different skill components, and utilizes value
factorization to optimize this objective efficiently. Evaluated in a set of
challenging environments, DUSDi successfully learns disentangled skills, and
significantly outperforms previous skill discovery methods when it comes to
applying the learned skills to solve downstream tasks. Code and skills
visualization at jiahenghu.github.io/DUSDi-site/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.09505v1' target='_blank'>HG2P: Hippocampus-inspired High-reward Graph and Model-Free Q-Gradient
  Penalty for Path Planning and Motion Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoran Wang, Yaoru Sun, Zeshen Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-12 11:46:31</h6>
<p class='card-text'>Goal-conditioned hierarchical reinforcement learning (HRL) decomposes complex
reaching tasks into a sequence of simple subgoal-conditioned tasks, showing
significant promise for addressing long-horizon planning in large-scale
environments. This paper bridges the goal-conditioned HRL based on graph-based
planning to brain mechanisms, proposing a hippocampus-striatum-like
dual-controller hypothesis. Inspired by the brain mechanisms of organisms
(i.e., the high-reward preferences observed in hippocampal replay) and
instance-based theory, we propose a high-return sampling strategy for
constructing memory graphs, improving sample efficiency. Additionally, we
derive a model-free lower-level Q-function gradient penalty to resolve the
model dependency issues present in prior work, improving the generalization of
Lipschitz constraints in applications. Finally, we integrate these two
extensions, High-reward Graph and model-free Gradient Penalty (HG2P), into the
state-of-the-art framework ACLG, proposing a novel goal-conditioned HRL
framework, HG2P+ACLG. Experimentally, the results demonstrate that our method
outperforms state-of-the-art goal-conditioned HRL algorithms on a variety of
long-horizon navigation tasks and robotic manipulation tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.08997v2' target='_blank'>Hierarchical Universal Value Function Approximators</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rushiv Arora</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-11 17:09:26</h6>
<p class='card-text'>There have been key advancements to building universal approximators for
multi-goal collections of reinforcement learning value functions -- key
elements in estimating long-term returns of states in a parameterized manner.
We extend this to hierarchical reinforcement learning, using the options
framework, by introducing hierarchical universal value function approximators
(H-UVFAs). This allows us to leverage the added benefits of scaling, planning,
and generalization expected in temporal abstraction settings. We develop
supervised and reinforcement learning methods for learning embeddings of the
states, goals, options, and actions in the two hierarchical value functions:
$Q(s, g, o; \theta)$ and $Q(s, g, o, a; \theta)$. Finally we demonstrate
generalization of the HUVFAs and show they outperform corresponding UVFAs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.07933v1' target='_blank'>Offline Hierarchical Reinforcement Learning via Inverse Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Carolin Schmidt, Daniele Gammelli, James Harrison, Marco Pavone, Filipe Rodrigues</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-10 14:00:21</h6>
<p class='card-text'>Hierarchical policies enable strong performance in many sequential
decision-making problems, such as those with high-dimensional action spaces,
those requiring long-horizon planning, and settings with sparse rewards.
However, learning hierarchical policies from static offline datasets presents a
significant challenge. Crucially, actions taken by higher-level policies may
not be directly observable within hierarchical controllers, and the offline
dataset might have been generated using a different policy structure, hindering
the use of standard offline learning algorithms. In this work, we propose OHIO:
a framework for offline reinforcement learning (RL) of hierarchical policies.
Our framework leverages knowledge of the policy structure to solve the inverse
problem, recovering the unobservable high-level actions that likely generated
the observed data under our hierarchical policy. This approach constructs a
dataset suitable for off-the-shelf offline training. We demonstrate our
framework on robotic and network optimization problems and show that it
substantially outperforms end-to-end RL methods and improves robustness. We
investigate a variety of instantiations of our framework, both in direct
deployment of policies trained offline and when online fine-tuning is
performed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.07921v1' target='_blank'>Meta-Learning Integration in Hierarchical Reinforcement Learning for
  Advanced Task Complexity</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arash Khajooeinejad, Masoumeh Chapariniya</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-10 13:47:37</h6>
<p class='card-text'>Hierarchical Reinforcement Learning (HRL) effectively tackles complex tasks
by decomposing them into structured policies. However, HRL agents often face
challenges with efficient exploration and rapid adaptation. To address this, we
integrate meta-learning into HRL to enhance the agent's ability to learn and
adapt hierarchical policies swiftly. Our approach employs meta-learning for
rapid task adaptation based on prior experience, while intrinsic motivation
mechanisms encourage efficient exploration by rewarding novel state visits.
Specifically, our agent uses a high-level policy to select among multiple
low-level policies operating within custom grid environments. We utilize
gradient-based meta-learning with differentiable inner-loop updates, enabling
optimization across a curriculum of increasingly difficult tasks. Experimental
results demonstrate that our meta-learned hierarchical agent significantly
outperforms traditional HRL agents without meta-learning and intrinsic
motivation. The agent exhibits accelerated learning, higher cumulative rewards,
and improved success rates in complex grid environments. These findings suggest
that integrating meta-learning with HRL, alongside curriculum learning and
intrinsic motivation, substantially enhances the agent's capability to handle
complex tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.04855v1' target='_blank'>Unsupervised Skill Discovery for Robotic Manipulation through Automatic
  Task Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Paul Jansonnie, Bingbing Wu, Julien Perez, Jan Peters</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-07 09:19:13</h6>
<p class='card-text'>Learning skills that interact with objects is of major importance for robotic
manipulation. These skills can indeed serve as an efficient prior for solving
various manipulation tasks. We propose a novel Skill Learning approach that
discovers composable behaviors by solving a large and diverse number of
autonomously generated tasks. Our method learns skills allowing the robot to
consistently and robustly interact with objects in its environment. The
discovered behaviors are embedded in primitives which can be composed with
Hierarchical Reinforcement Learning to solve unseen manipulation tasks. In
particular, we leverage Asymmetric Self-Play to discover behaviors and
Multiplicative Compositional Policies to embed them. We compare our method to
Skill Learning baselines and find that our skills are more interactive.
Furthermore, the learned skills can be used to solve a set of unseen
manipulation tasks, in simulation as well as on a real robotic platform.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.02389v1' target='_blank'>Diffusion Meets Options: Hierarchical Generative Skill Composition for
  Temporally-Extended Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zeyu Feng, Hao Luan, Kevin Yuchen Ma, Harold Soh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-03 11:10:37</h6>
<p class='card-text'>Safe and successful deployment of robots requires not only the ability to
generate complex plans but also the capacity to frequently replan and correct
execution errors. This paper addresses the challenge of long-horizon trajectory
planning under temporally extended objectives in a receding horizon manner. To
this end, we propose DOPPLER, a data-driven hierarchical framework that
generates and updates plans based on instruction specified by linear temporal
logic (LTL). Our method decomposes temporal tasks into chain of options with
hierarchical reinforcement learning from offline non-expert datasets. It
leverages diffusion models to generate options with low-level actions. We
devise a determinantal-guided posterior sampling technique during batch
generation, which improves the speed and diversity of diffusion generated
options, leading to more efficient querying. Experiments on robot navigation
and manipulation tasks demonstrate that DOPPLER can generate sequences of
trajectories that progressively satisfy the specified formulae for obstacle
avoidance and sequential visitation. Demonstration videos are available online
at: https://philiptheother.github.io/doppler/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2410.02141v3' target='_blank'>E2H: A Two-Stage Non-Invasive Neural Signal Driven Humanoid Robotic
  Whole-Body Control Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiqun Duan, Qiang Zhang, Jinzhao Zhou, Jingkai Sun, Xiaowei Jiang, Jiahang Cao, Jiaxu Wang, Yiqian Yang, Wen Zhao, Gang Han, Yijie Guo, Chin-Teng Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-10-03 01:58:34</h6>
<p class='card-text'>Recent advancements in humanoid robotics, including the integration of
hierarchical reinforcement learning-based control and the utilization of LLM
planning, have significantly enhanced the ability of robots to perform complex
tasks. In contrast to the highly developed humanoid robots, the human factors
involved remain relatively unexplored. Directly controlling humanoid robots
with the brain has already appeared in many science fiction novels, such as
Pacific Rim and Gundam. In this work, we present E2H (EEG-to-Humanoid), an
innovative framework that pioneers the control of humanoid robots using
high-frequency non-invasive neural signals. As the none-invasive signal quality
remains low in decoding precise spatial trajectory, we decompose the E2H
framework in an innovative two-stage formation: 1) decoding neural signals
(EEG) into semantic motion keywords, 2) utilizing LLM facilitated motion
generation with a precise motion imitation control policy to realize humanoid
robotics control. The method of directly driving robots with brainwave commands
offers a novel approach to human-machine collaboration, especially in
situations where verbal commands are impractical, such as in cases of speech
impairments, space exploration, or underwater exploration, unlocking
significant potential. E2H offers an exciting glimpse into the future, holding
immense potential for human-computer interaction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.19139v1' target='_blank'>Gaze-informed Signatures of Trust and Collaboration in Human-Autonomy
  Teams</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anthony J. Ries, Stéphane Aroca-Ouellette, Alessandro Roncone, Ewart J. de Visser</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-27 21:04:45</h6>
<p class='card-text'>In the evolving landscape of human-autonomy teaming (HAT), fostering
effective collaboration and trust between human and autonomous agents is
increasingly important. To explore this, we used the game Overcooked AI to
create dynamic teaming scenarios featuring varying agent behaviors (clumsy,
rigid, adaptive) and environmental complexities (low, medium, high). Our
objectives were to assess the performance of adaptive AI agents designed with
hierarchical reinforcement learning for better teamwork and measure eye
tracking signals related to changes in trust and collaboration. The results
indicate that the adaptive agent was more effective in managing teaming and
creating an equitable task distribution across environments compared to the
other agents. Working with the adaptive agent resulted in better coordination,
reduced collisions, more balanced task contributions, and higher trust ratings.
Reduced gaze allocation, across all agents, was associated with higher trust
levels, while blink count, scan path length, agent revisits and trust were
predictive of the humans contribution to the team. Notably, fixation revisits
on the agent increased with environmental complexity and decreased with agent
versatility, offering a unique metric for measuring teammate performance
monitoring. These findings underscore the importance of designing autonomous
teammates that not only excel in task performance but also enhance teamwork by
being more predictable and reducing the cognitive load on human team members.
Additionally, this study highlights the potential of eye-tracking as an
unobtrusive measure for evaluating and improving human-autonomy teams,
suggesting eye gaze could be used by agents to dynamically adapt their
behaviors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.13824v1' target='_blank'>Adaptive Task Allocation in Multi-Human Multi-Robot Teams under Team
  Heterogeneity and Dynamic Information Uncertainty</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziqin Yuan, Ruiqi Wang, Taehyeon Kim, Dezhong Zhao, Ike Obi, Byung-Cheol Min</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-20 18:05:42</h6>
<p class='card-text'>Task allocation in multi-human multi-robot (MH-MR) teams presents significant
challenges due to the inherent heterogeneity of team members, the dynamics of
task execution, and the information uncertainty of operational states. Existing
approaches often fail to address these challenges simultaneously, resulting in
suboptimal performance. To tackle this, we propose ATA-HRL, an adaptive task
allocation framework using hierarchical reinforcement learning (HRL), which
incorporates initial task allocation (ITA) that leverages team heterogeneity
and conditional task reallocation in response to dynamic operational states.
Additionally, we introduce an auxiliary state representation learning task to
manage information uncertainty and enhance task execution. Through an extensive
case study in large-scale environmental monitoring tasks, we demonstrate the
benefits of our approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.13445v1' target='_blank'>Selective Exploration and Information Gathering in Search and Rescue
  Using Hierarchical Learning Guided by Natural Language Input</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dimitrios Panagopoulos, Adolfo Perrusquia, Weisi Guo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-20 12:27:47</h6>
<p class='card-text'>In recent years, robots and autonomous systems have become increasingly
integral to our daily lives, offering solutions to complex problems across
various domains. Their application in search and rescue (SAR) operations,
however, presents unique challenges. Comprehensively exploring the
disaster-stricken area is often infeasible due to the vastness of the terrain,
transformed environment, and the time constraints involved. Traditional robotic
systems typically operate on predefined search patterns and lack the ability to
incorporate and exploit ground truths provided by human stakeholders, which can
be the key to speeding up the learning process and enhancing triage. Addressing
this gap, we introduce a system that integrates social interaction via large
language models (LLMs) with a hierarchical reinforcement learning (HRL)
framework. The proposed system is designed to translate verbal inputs from
human stakeholders into actionable RL insights and adjust its search strategy.
By leveraging human-provided information through LLMs and structuring task
execution through HRL, our approach not only bridges the gap between autonomous
capabilities and human intelligence but also significantly improves the agent's
learning efficiency and decision-making process in environments characterised
by long horizons and sparse rewards.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2409.07416v1' target='_blank'>Hierarchical Reinforcement Learning for Temporal Abstraction of Listwise
  Recommendation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Luo Ji, Gao Liu, Mingyang Yin, Hongxia Yang, Jingren Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-09-11 17:01:06</h6>
<p class='card-text'>Modern listwise recommendation systems need to consider both long-term user
perceptions and short-term interest shifts. Reinforcement learning can be
applied on recommendation to study such a problem but is also subject to large
search space, sparse user feedback and long interactive latency. Motivated by
recent progress in hierarchical reinforcement learning, we propose a novel
framework called mccHRL to provide different levels of temporal abstraction on
listwise recommendation. Within the hierarchical framework, the high-level
agent studies the evolution of user perception, while the low-level agent
produces the item selection policy by modeling the process as a sequential
decision-making problem. We argue that such framework has a well-defined
decomposition of the outra-session context and the intra-session context, which
are encoded by the high-level and low-level agents, respectively. To verify
this argument, we implement both a simulator-based environment and an
industrial dataset-based experiment. Results observe significant performance
improvement by our method, compared with several well-known baselines. Data and
codes have been made public.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.13333v1' target='_blank'>Mastering the Digital Art of War: Developing Intelligent Combat
  Simulation Agents for Wargaming Using Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Scotty Black</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-23 18:50:57</h6>
<p class='card-text'>In today's rapidly evolving military landscape, advancing artificial
intelligence (AI) in support of wargaming becomes essential. Despite
reinforcement learning (RL) showing promise for developing intelligent agents,
conventional RL faces limitations in handling the complexity inherent in combat
simulations. This dissertation proposes a comprehensive approach, including
targeted observation abstractions, multi-model integration, a hybrid AI
framework, and an overarching hierarchical reinforcement learning (HRL)
framework. Our localized observation abstraction using piecewise linear spatial
decay simplifies the RL problem, enhancing computational efficiency and
demonstrating superior efficacy over traditional global observation methods.
Our multi-model framework combines various AI methodologies, optimizing
performance while still enabling the use of diverse, specialized individual
behavior models. Our hybrid AI framework synergizes RL with scripted agents,
leveraging RL for high-level decisions and scripted agents for lower-level
tasks, enhancing adaptability, reliability, and performance. Our HRL
architecture and training framework decomposes complex problems into manageable
subproblems, aligning with military decision-making structures. Although
initial tests did not show improved performance, insights were gained to
improve future iterations. This study underscores AI's potential to
revolutionize wargaming, emphasizing the need for continued research in this
domain.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.11416v1' target='_blank'>Subgoal-based Hierarchical Reinforcement Learning for Multi-Agent
  Collaboration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Cheng Xu, Changtian Zhang, Yuchen Shi, Ran Wang, Shihong Duan, Yadong Wan, Xiaotong Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-21 08:22:11</h6>
<p class='card-text'>Recent advancements in reinforcement learning have made significant impacts
across various domains, yet they often struggle in complex multi-agent
environments due to issues like algorithm instability, low sampling efficiency,
and the challenges of exploration and dimensionality explosion. Hierarchical
reinforcement learning (HRL) offers a structured approach to decompose complex
tasks into simpler sub-tasks, which is promising for multi-agent settings. This
paper advances the field by introducing a hierarchical architecture that
autonomously generates effective subgoals without explicit constraints,
enhancing both flexibility and stability in training. We propose a dynamic goal
generation strategy that adapts based on environmental changes. This method
significantly improves the adaptability and sample efficiency of the learning
process. Furthermore, we address the critical issue of credit assignment in
multi-agent systems by synergizing our hierarchical architecture with a
modified QMIX network, thus improving overall strategy coordination and
efficiency. Comparative experiments with mainstream reinforcement learning
algorithms demonstrate the superior convergence speed and performance of our
approach in both single-agent and multi-agent environments, confirming its
effectiveness and flexibility in complex scenarios. Our code is open-sourced
at: \url{https://github.com/SICC-Group/GMAH}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.08516v1' target='_blank'>Multilevel Graph Reinforcement Learning for Consistent Cognitive
  Decision-making in Heterogeneous Mixed Autonomy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xin Gao, Zhaoyang Ma, Xueyuan Li, Xiaoqiang Meng, Zirui Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-16 04:14:06</h6>
<p class='card-text'>In the realm of heterogeneous mixed autonomy, vehicles experience dynamic
spatial correlations and nonlinear temporal interactions in a complex,
non-Euclidean space. These complexities pose significant challenges to
traditional decision-making frameworks. Addressing this, we propose a
hierarchical reinforcement learning framework integrated with multilevel graph
representations, which effectively comprehends and models the spatiotemporal
interactions among vehicles navigating through uncertain traffic conditions
with varying decision-making systems. Rooted in multilevel graph representation
theory, our approach encapsulates spatiotemporal relationships inherent in
non-Euclidean spaces. A weighted graph represents spatiotemporal features
between nodes, addressing the degree imbalance inherent in dynamic graphs. We
integrate asynchronous parallel hierarchical reinforcement learning with a
multilevel graph representation and a multi-head attention mechanism, which
enables connected autonomous vehicles (CAVs) to exhibit capabilities akin to
human cognition, facilitating consistent decision-making across various
critical dimensions. The proposed decision-making strategy is validated in
challenging environments characterized by high density, randomness, and
dynamism on highway roads. We assess the performance of our framework through
ablation studies, comparative analyses, and spatiotemporal trajectory
evaluations. This study presents a quantitative analysis of decision-making
mechanisms mirroring human cognitive functions in the realm of heterogeneous
mixed autonomy, promoting the development of multi-dimensional decision-making
strategies and a sophisticated distribution of attentional resources.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.07346v1' target='_blank'>Enabling microrobotic chemotaxis via reset-free hierarchical
  reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tongzhao Xiong, Zhaorong Liu, Chong Jin Ong, Lailai Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-14 07:41:04</h6>
<p class='card-text'>Microorganisms have evolved diverse strategies to propel in viscous fluids,
navigate complex environments, and exhibit taxis in response to stimuli. This
has inspired the development of synthetic microrobots, where machine learning
(ML) is playing an increasingly important role. Can ML endow these robots with
intelligence resembling that developed by their natural counterparts over
evolutionary timelines? Here, we demonstrate chemotactic navigation of a
multi-link articulated microrobot using two-level hierarchical reinforcement
learning (RL). The lower-level RL allows the robot -- featuring either a chain
or ring topology -- to acquire topology-specific swimming gaits: wave
propagation characteristic of flagella or body oscillation akin to an ameboid.
Such flagellar and ameboid microswimmers, further enabled by the higher-level
RL, accomplish chemotactic navigation in prototypical biologically-relevant
scenarios that feature conflicting chemoattractants, pursuing a swimming
bacterial mimic, steering in vortical flows, and squeezing through tight
constrictions. Additionally, we achieve reset-free, partially observable RL,
where the robot observes only its joint angles and local scalar quantities.
This advancement illuminates solutions for overcoming the persistent challenges
of manual resets and partial observability in real-world microrobotic RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.06520v2' target='_blank'>Retrieval-Augmented Hierarchical in-Context Reinforcement Learning and
  Hindsight Modular Reflections for Task Planning with LLMs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chuanneng Sun, Songjun Huang, Dario Pompili</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-12 22:40:01</h6>
<p class='card-text'>Large Language Models (LLMs) have demonstrated remarkable abilities in
various language tasks, making them promising candidates for decision-making in
robotics. Inspired by Hierarchical Reinforcement Learning (HRL), we propose
Retrieval-Augmented in-context reinforcement Learning (RAHL), a novel framework
that decomposes complex tasks into sub-tasks using an LLM-based high-level
policy, in which a complex task is decomposed into sub-tasks by a high-level
policy on-the-fly. The sub-tasks, defined by goals, are assigned to the
low-level policy to complete. To improve the agent's performance in
multi-episode execution, we propose Hindsight Modular Reflection (HMR), where,
instead of reflecting on the full trajectory, we let the agent reflect on
shorter sub-trajectories to improve reflection efficiency. We evaluated the
decision-making ability of the proposed RAHL in three benchmark
environments--ALFWorld, Webshop, and HotpotQA. The results show that RAHL can
achieve an improvement in performance in 9%, 42%, and 10% in 5 episodes of
execution in strong baselines. Furthermore, we also implemented RAHL on the
Boston Dynamics SPOT robot. The experiment shows that the robot can scan the
environment, find entrances, and navigate to new rooms controlled by the LLM
policy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2408.01880v4' target='_blank'>Walk Wisely on Graph: Knowledge Graph Reasoning with Dual Agents via
  Efficient Guidance-Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijian Wang, Bin Wang, Haifeng Jing, Huayu Li, Hongbo Dou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-08-03 23:15:57</h6>
<p class='card-text'>Recent years, multi-hop reasoning has been widely studied for knowledge graph
(KG) reasoning due to its efficacy and interpretability. However, previous
multi-hop reasoning approaches are subject to two primary shortcomings. First,
agents struggle to learn effective and robust policies at the early phase due
to sparse rewards. Second, these approaches often falter on specific datasets
like sparse knowledge graphs, where agents are required to traverse lengthy
reasoning paths. To address these problems, we propose a multi-hop reasoning
model with dual agents based on hierarchical reinforcement learning (HRL),
which is named FULORA. FULORA tackles the above reasoning challenges by
eFficient GUidance-ExpLORAtion between dual agents. The high-level agent walks
on the simplified knowledge graph to provide stage-wise hints for the low-level
agent walking on the original knowledge graph. In this framework, the low-level
agent optimizes a value function that balances two objectives: (1) maximizing
return, and (2) integrating efficient guidance from the high-level agent.
Experiments conducted on three real-word knowledge graph datasets demonstrate
that FULORA outperforms RL-based baselines, especially in the case of
long-distance reasoning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.15241v1' target='_blank'>Temporal Abstraction in Reinforcement Learning with Offline Data</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ranga Shaarad Ayyagari, Anurita Ghosh, Ambedkar Dukkipati</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-21 18:10:31</h6>
<p class='card-text'>Standard reinforcement learning algorithms with a single policy perform
poorly on tasks in complex environments involving sparse rewards, diverse
behaviors, or long-term planning. This led to the study of algorithms that
incorporate temporal abstraction by training a hierarchy of policies that plan
over different time scales. The options framework has been introduced to
implement such temporal abstraction by learning low-level options that act as
extended actions controlled by a high-level policy. The main challenge in
applying these algorithms to real-world problems is that they suffer from high
sample complexity to train multiple levels of the hierarchy, which is
impossible in online settings. Motivated by this, in this paper, we propose an
offline hierarchical RL method that can learn options from existing offline
datasets collected by other unknown agents. This is a very challenging problem
due to the distribution mismatch between the learned options and the policies
responsible for the offline dataset and to our knowledge, this is the first
work in this direction. In this work, we propose a framework by which an online
hierarchical reinforcement learning algorithm can be trained on an offline
dataset of transitions collected by an unknown behavior policy. We validate our
method on Gym MuJoCo locomotion environments and robotic gripper block-stacking
tasks in the standard as well as transfer and goal-conditioned settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.11939v3' target='_blank'>AI-Driven Physics-Informed Bio-Silicon Intelligence System: Integrating
  Hybrid Systems, Biocomputing, Neural Networks, and Machine Learning, for
  Advanced Neurotechnology</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vincent Jorgsson, Raghav Kumar, Mustaf Ahmed, Maxx Yung, Aryaman Pattnayak, Sri Pradhyumna Sridhar, Vaishnav Varma, Arun Ram Ponnambalam, Georg Weidlich, Dimitris Pinotsis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-12 20:21:13</h6>
<p class='card-text'>We present the Bio-Silicon Intelligence System (BSIS), an innovative hybrid
platform that integrates biological neural networks with silicon-based
computing. The BSIS, a Physics-Informed Hybrid Hierarchical Reinforcement
Learning State Machine, employs carbon nanotube-coated electrodes to interface
rat brains with computational systems, enabling high-fidelity neural
interfacing and bidirectional communication through self-organizing systems in
both biological and silicon forms. Our system leverages both analogue and
digital AI theory, incorporating concepts from computational theory, chaos
theory, dynamical systems theory, physics, and quantum mechanics. Additionally,
the BSIS replicates the neuronal dynamics typical of intelligent brain tissue,
employing nonlinear operations underlying learning and information storage.
Neural signals are read through the FreeEEG32 board and BrainFlow software,
then features are extracted and mapped to game actions by tracking feature
changes in continuous data. Metadata is encoded into both analogue and digital
brain stimulation signals at the microvolt level using our proprietary software
and hardware. The system employs a dual signaling approach for training the rat
brain, incorporating a reward solution and sound as well as human-inaudible
distress sounds. This paper details the design, theory, functionality, and
technical specifications of the BSIS, highlighting its interdisciplinary
approach and advanced technological integration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2407.06690v1' target='_blank'>Hierarchical Average-Reward Linearly-solvable Markov Decision Processes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guillermo Infante, Anders Jonsson, Vicenç Gómez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-07-09 09:06:44</h6>
<p class='card-text'>We introduce a novel approach to hierarchical reinforcement learning for
Linearly-solvable Markov Decision Processes (LMDPs) in the infinite-horizon
average-reward setting. Unlike previous work, our approach allows learning
low-level and high-level tasks simultaneously, without imposing limiting
restrictions on the low-level tasks. Our method relies on partitions of the
state space that create smaller subtasks that are easier to solve, and the
equivalence between such partitions to learn more efficiently. We then exploit
the compositionality of low-level tasks to exactly represent the value function
of the high-level task. Experiments show that our approach can outperform flat
average-reward reinforcement learning by one or several orders of magnitude.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.18310v1' target='_blank'>Spatial-temporal Hierarchical Reinforcement Learning for Interpretable
  Pathology Image Super-Resolution</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenting Chen, Jie Liu, Tommy W. S. Chow, Yixuan Yuan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-26 12:50:10</h6>
<p class='card-text'>Pathology image are essential for accurately interpreting lesion cells in
cytopathology screening, but acquiring high-resolution digital slides requires
specialized equipment and long scanning times. Though super-resolution (SR)
techniques can alleviate this problem, existing deep learning models recover
pathology image in a black-box manner, which can lead to untruthful biological
details and misdiagnosis. Additionally, current methods allocate the same
computational resources to recover each pixel of pathology image, leading to
the sub-optimal recovery issue due to the large variation of pathology image.
In this paper, we propose the first hierarchical reinforcement learning
framework named Spatial-Temporal hierARchical Reinforcement Learning (STAR-RL),
mainly for addressing the aforementioned issues in pathology image
super-resolution problem. We reformulate the SR problem as a Markov decision
process of interpretable operations and adopt the hierarchical recovery
mechanism in patch level, to avoid sub-optimal recovery. Specifically, the
higher-level spatial manager is proposed to pick out the most corrupted patch
for the lower-level patch worker. Moreover, the higher-level temporal manager
is advanced to evaluate the selected patch and determine whether the
optimization should be stopped earlier, thereby avoiding the over-processed
problem. Under the guidance of spatial-temporal managers, the lower-level patch
worker processes the selected patch with pixel-wise interpretable actions at
each time step. Experimental results on medical images degraded by different
kernels show the effectiveness of STAR-RL. Furthermore, STAR-RL validates the
promotion in tumor diagnosis with a large margin and shows generalizability
under various degradations. The source code is available at
https://github.com/CUHK-AIM-Group/STAR-RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.18053v1' target='_blank'>Bidirectional-Reachable Hierarchical Reinforcement Learning with
  Mutually Responsive Policies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu Luo, Fuchun Sun, Tianying Ji, Xianyuan Zhan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-26 04:05:04</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) addresses complex long-horizon
tasks by skillfully decomposing them into subgoals. Therefore, the
effectiveness of HRL is greatly influenced by subgoal reachability. Typical HRL
methods only consider subgoal reachability from the unilateral level, where a
dominant level enforces compliance to the subordinate level. However, we
observe that when the dominant level becomes trapped in local exploration or
generates unattainable subgoals, the subordinate level is negatively affected
and cannot follow the dominant level's actions. This can potentially make both
levels stuck in local optima, ultimately hindering subsequent subgoal
reachability. Allowing real-time bilateral information sharing and error
correction would be a natural cure for this issue, which motivates us to
propose a mutual response mechanism. Based on this, we propose the
Bidirectional-reachable Hierarchical Policy Optimization (BrHPO)--a simple yet
effective algorithm that also enjoys computation efficiency. Experiment results
on a variety of long-horizon tasks showcase that BrHPO outperforms other
state-of-the-art HRL baselines, coupled with a significantly higher exploration
efficiency and robustness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.17334v1' target='_blank'>Joint Admission Control and Resource Allocation of Virtual Network
  Embedding via Hierarchical Deep Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianfu Wang, Li Shen, Qilin Fan, Tong Xu, Tongliang Liu, Hui Xiong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-25 07:42:30</h6>
<p class='card-text'>As an essential resource management problem in network virtualization,
virtual network embedding (VNE) aims to allocate the finite resources of
physical network to sequentially arriving virtual network requests (VNRs) with
different resource demands. Since this is an NP-hard combinatorial optimization
problem, many efforts have been made to provide viable solutions. However, most
existing approaches have either ignored the admission control of VNRs, which
has a potential impact on long-term performances, or not fully exploited the
temporal and topological features of the physical network and VNRs. In this
paper, we propose a deep Hierarchical Reinforcement Learning approach to learn
a joint Admission Control and Resource Allocation policy for VNE, named
HRL-ACRA. Specifically, the whole VNE process is decomposed into an upper-level
policy for deciding whether to admit the arriving VNR or not and a lower-level
policy for allocating resources of the physical network to meet the requirement
of VNR through the HRL approach. Considering the proximal policy optimization
as the basic training algorithm, we also adopt the average reward method to
address the infinite horizon problem of the upper-level agent and design a
customized multi-objective intrinsic reward to alleviate the sparse reward
issue of the lower-level agent. Moreover, we develop a deep feature-aware graph
neural network to capture the features of VNR and physical network and exploit
a sequence-to-sequence model to generate embedding actions iteratively.
Finally, extensive experiments are conducted in various settings, and show that
HRL-ACRA outperforms state-of-the-art baselines in terms of both the acceptance
ratio and long-term average revenue. Our code is available at
\url{https://github.com/GeminiLight/hrl-acra}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.16707v1' target='_blank'>Probabilistic Subgoal Representations for Hierarchical Reinforcement
  learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vivienne Huiling Wang, Tinghuai Wang, Wenyan Yang, Joni-Kristian Kämäräinen, Joni Pajarinen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-24 15:09:22</h6>
<p class='card-text'>In goal-conditioned hierarchical reinforcement learning (HRL), a high-level
policy specifies a subgoal for the low-level policy to reach. Effective HRL
hinges on a suitable subgoal represen tation function, abstracting state space
into latent subgoal space and inducing varied low-level behaviors. Existing
methods adopt a subgoal representation that provides a deterministic mapping
from state space to latent subgoal space. Instead, this paper utilizes Gaussian
Processes (GPs) for the first probabilistic subgoal representation. Our method
employs a GP prior on the latent subgoal space to learn a posterior
distribution over the subgoal representation functions while exploiting the
long-range correlation in the state space through learnable kernels. This
enables an adaptive memory that integrates long-range subgoal information from
prior planning steps allowing to cope with stochastic uncertainties.
Furthermore, we propose a novel learning objective to facilitate the
simultaneous learning of probabilistic subgoal representations and policies
within a unified framework. In experiments, our approach outperforms
state-of-the-art baselines in standard benchmarks but also in environments with
stochastic elements and under diverse reward conditions. Additionally, our
model shows promising capabilities in transferring low-level policies across
different tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.16374v1' target='_blank'>KEHRL: Learning Knowledge-Enhanced Language Representations with
  Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongyang Li, Taolin Zhang, Longtao Huang, Chengyu Wang, Xiaofeng He, Hui Xue</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-24 07:32:35</h6>
<p class='card-text'>Knowledge-enhanced pre-trained language models (KEPLMs) leverage relation
triples from knowledge graphs (KGs) and integrate these external data sources
into language models via self-supervised learning. Previous works treat
knowledge enhancement as two independent operations, i.e., knowledge injection
and knowledge integration. In this paper, we propose to learn
Knowledge-Enhanced language representations with Hierarchical Reinforcement
Learning (KEHRL), which jointly addresses the problems of detecting positions
for knowledge injection and integrating external knowledge into the model in
order to avoid injecting inaccurate or irrelevant knowledge. Specifically, a
high-level reinforcement learning (RL) agent utilizes both internal and prior
knowledge to iteratively detect essential positions in texts for knowledge
injection, which filters out less meaningful entities to avoid diverting the
knowledge learning direction. Once the entity positions are selected, a
relevant triple filtration module is triggered to perform low-level RL to
dynamically refine the triples associated with polysemic entities through
binary-valued actions. Experiments validate KEHRL's effectiveness in probing
factual knowledge and enhancing the model's performance on various natural
language understanding tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.15124v2' target='_blank'>A Provably Efficient Option-Based Algorithm for both High-Level and
  Low-Level Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gianluca Drappo, Alberto Maria Metelli, Marcello Restelli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-21 13:17:33</h6>
<p class='card-text'>Hierarchical Reinforcement Learning (HRL) approaches have shown successful
results in solving a large variety of complex, structured, long-horizon
problems. Nevertheless, a full theoretical understanding of this empirical
evidence is currently missing. In the context of the \emph{option} framework,
prior research has devised efficient algorithms for scenarios where options are
fixed, and the high-level policy selecting among options only has to be
learned. However, the fully realistic scenario in which both the high-level and
the low-level policies are learned is surprisingly disregarded from a
theoretical perspective. This work makes a step towards the understanding of
this latter scenario. Focusing on the finite-horizon problem, we present a
meta-algorithm alternating between regret minimization algorithms instanced at
different (high and low) temporal abstractions. At the higher level, we treat
the problem as a Semi-Markov Decision Process (SMDP), with fixed low-level
policies, while at a lower level, inner option policies are learned with a
fixed high-level policy. The bounds derived are compared with the lower bound
for non-hierarchical finite-horizon problems, allowing to characterize when a
hierarchical approach is provably preferable, even without pre-trained options.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.14537v1' target='_blank'>MacroHFT: Memory Augmented Context-aware Reinforcement Learning On High
  Frequency Trading</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chuqiao Zong, Chaojie Wang, Molei Qin, Lei Feng, Xinrun Wang, Bo An</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-20 17:48:24</h6>
<p class='card-text'>High-frequency trading (HFT) that executes algorithmic trading in short time
scales, has recently occupied the majority of cryptocurrency market. Besides
traditional quantitative trading methods, reinforcement learning (RL) has
become another appealing approach for HFT due to its terrific ability of
handling high-dimensional financial data and solving sophisticated sequential
decision-making problems, \emph{e.g.,} hierarchical reinforcement learning
(HRL) has shown its promising performance on second-level HFT by training a
router to select only one sub-agent from the agent pool to execute the current
transaction. However, existing RL methods for HFT still have some defects: 1)
standard RL-based trading agents suffer from the overfitting issue, preventing
them from making effective policy adjustments based on financial context; 2)
due to the rapid changes in market conditions, investment decisions made by an
individual agent are usually one-sided and highly biased, which might lead to
significant loss in extreme markets. To tackle these problems, we propose a
novel Memory Augmented Context-aware Reinforcement learning method On HFT,
\emph{a.k.a.} MacroHFT, which consists of two training phases: 1) we first
train multiple types of sub-agents with the market data decomposed according to
various financial indicators, specifically market trend and volatility, where
each agent owns a conditional adapter to adjust its trading policy according to
market conditions; 2) then we train a hyper-agent to mix the decisions from
these sub-agents and output a consistently profitable meta-policy to handle
rapid market fluctuations, equipped with a memory mechanism to enhance the
capability of decision-making. Extensive experiments on various cryptocurrency
markets demonstrate that MacroHFT can achieve state-of-the-art performance on
minute-level trading tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.10892v3' target='_blank'>DIPPER: Direct Preference Optimization to Accelerate Primitive-Enabled
  Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Utsav Singh, Souradip Chakraborty, Wesley A. Suttle, Brian M. Sadler, Vinay P Namboodiri, Amrit Singh Bedi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-16 10:49:41</h6>
<p class='card-text'>Learning control policies to perform complex robotics tasks from human
preference data presents significant challenges. On the one hand, the
complexity of such tasks typically requires learning policies to perform a
variety of subtasks, then combining them to achieve the overall goal. At the
same time, comprehensive, well-engineered reward functions are typically
unavailable in such problems, while limited human preference data often is;
making efficient use of such data to guide learning is therefore essential.
Methods for learning to perform complex robotics tasks from human preference
data must overcome both these challenges simultaneously. In this work, we
introduce DIPPER: Direct Preference Optimization to Accelerate
Primitive-Enabled Hierarchical Reinforcement Learning, an efficient
hierarchical approach that leverages direct preference optimization to learn a
higher-level policy and reinforcement learning to learn a lower-level policy.
DIPPER enjoys improved computational efficiency due to its use of direct
preference optimization instead of standard preference-based approaches such as
reinforcement learning from human feedback, while it also mitigates the
well-known hierarchical reinforcement learning issues of non-stationarity and
infeasible subgoal generation due to our use of primitive-informed
regularization inspired by a novel bi-level optimization formulation of the
hierarchical reinforcement learning problem. To validate our approach, we
perform extensive experimental analysis on a variety of challenging robotics
tasks, demonstrating that DIPPER outperforms hierarchical and non-hierarchical
baselines, while ameliorating the non-stationarity and infeasible subgoal
generation issues of hierarchical reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.07877v2' target='_blank'>Hierarchical Reinforcement Learning for Swarm Confrontation with High
  Uncertainty</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qizhen Wu, Kexin Liu, Lei Chen, Jinhu Lü</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-12 05:12:10</h6>
<p class='card-text'>In swarm robotics, confrontation including the pursuit-evasion game is a key
scenario. High uncertainty caused by unknown opponents' strategies, dynamic
obstacles, and insufficient training complicates the action space into a hybrid
decision process. Although the deep reinforcement learning method is
significant for swarm confrontation since it can handle various sizes, as an
end-to-end implementation, it cannot deal with the hybrid process. Here, we
propose a novel hierarchical reinforcement learning approach consisting of a
target allocation layer, a path planning layer, and the underlying dynamic
interaction mechanism between the two layers, which indicates the quantified
uncertainty. It decouples the hybrid process into discrete allocation and
continuous planning layers, with a probabilistic ensemble model to quantify the
uncertainty and regulate the interaction frequency adaptively. Furthermore, to
overcome the unstable training process introduced by the two layers, we design
an integration training method including pre-training and cross-training, which
enhances the training efficiency and stability. Experiment results in both
comparison, ablation, and real-robot studies validate the effectiveness and
generalization performance of our proposed approach. In our defined experiments
with twenty to forty agents, the win rate of the proposed method reaches around
ninety percent, outperforming other traditional methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.17795v1' target='_blank'>RACon: Retrieval-Augmented Simulated Character Locomotion Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxuan Mu, Shihao Zou, Kangning Yin, Zheng Tian, Li Cheng, Weinan Zhang, Jun Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-11 16:21:28</h6>
<p class='card-text'>In computer animation, driving a simulated character with lifelike motion is
challenging. Current generative models, though able to generalize to diverse
motions, often pose challenges to the responsiveness of end-user control. To
address these issues, we introduce RACon: Retrieval-Augmented Simulated
Character Locomotion Control. Our end-to-end hierarchical reinforcement
learning method utilizes a retriever and a motion controller. The retriever
searches motion experts from a user-specified database in a task-oriented
fashion, which boosts the responsiveness to the user's control. The selected
motion experts and the manipulation signal are then transferred to the
controller to drive the simulated character. In addition, a retrieval-augmented
discriminator is designed to stabilize the training process. Our method
surpasses existing techniques in both quality and quantity in locomotion
control, as demonstrated in our empirical study. Moreover, by switching
extensive databases for retrieval, it can adapt to distinctive motion types at
run time.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.06059v2' target='_blank'>LLM-Based Intent Processing and Network Optimization Using
  Attention-Based Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Md Arafat Habib, Pedro Enrique Iturria Rivera, Yigit Ozcan, Medhat Elsayed, Majid Bavand, Raimundus Gaigalas, Melike Erol-Kantarci</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-10 07:13:54</h6>
<p class='card-text'>Intent-based network automation is a promising tool to enable easier network
management however certain challenges need to be effectively addressed. These
are: 1) processing intents, i.e., identification of logic and necessary
parameters to fulfill an intent, 2) validating an intent to align it with
current network status, and 3) satisfying intents via network optimizing
functions like xApps and rApps in O-RAN. This paper addresses these points via
a three-fold strategy to introduce intent-based automation for O-RAN. First,
intents are processed via a lightweight Large Language Model (LLM). Secondly,
once an intent is processed, it is validated against future incoming traffic
volume profiles (high or low). Finally, a series of network optimization
applications (rApps and xApps) have been developed. With their machine
learning-based functionalities, they can improve certain key performance
indicators such as throughput, delay, and energy efficiency. In this final
stage, using an attention-based hierarchical reinforcement learning algorithm,
these applications are optimally initiated to satisfy the intent of an
operator. Our simulations show that the proposed method can achieve at least
12% increase in throughput, 17.1% increase in energy efficiency, and 26.5%
decrease in network delay compared to the baseline algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.05881v2' target='_blank'>LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-09 18:40:24</h6>
<p class='card-text'>Developing interactive systems that leverage natural language instructions to
solve complex robotic control tasks has been a long-desired goal in the
robotics community. Large Language Models (LLMs) have demonstrated exceptional
abilities in handling complex tasks, including logical reasoning, in-context
learning, and code generation. However, predicting low-level robotic actions
using LLMs poses significant challenges. Additionally, the complexity of such
tasks usually demands the acquisition of policies to execute diverse subtasks
and combine them to attain the ultimate objective. Hierarchical Reinforcement
Learning (HRL) is an elegant approach for solving such tasks, which provides
the intuitive benefits of temporal abstraction and improved exploration.
However, HRL faces the recurring issue of non-stationarity due to unstable
lower primitive behaviour. In this work, we propose LGR2, a novel HRL framework
that leverages language instructions to generate a stationary reward function
for the higher-level policy. Since the language-guided reward is unaffected by
the lower primitive behaviour, LGR2 mitigates non-stationarity and is thus an
elegant method for leveraging language instructions to solve robotic control
tasks. To analyze the efficacy of our approach, we perform empirical analysis
and demonstrate that LGR2 effectively alleviates non-stationarity in HRL. Our
approach attains success rates exceeding 70$\%$ in challenging, sparse-reward
robotic navigation and manipulation environments where the baselines fail to
achieve any significant progress. Additionally, we conduct real-world robotic
manipulation experiments and demonstrate that CRISP shows impressive
generalization in real-world scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2406.00483v1' target='_blank'>Exploring the limits of Hierarchical World Models in Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Robin Schiewer, Anand Subramoney, Laurenz Wiskott</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-06-01 16:29:03</h6>
<p class='card-text'>Hierarchical model-based reinforcement learning (HMBRL) aims to combine the
benefits of better sample efficiency of model based reinforcement learning
(MBRL) with the abstraction capability of hierarchical reinforcement learning
(HRL) to solve complex tasks efficiently. While HMBRL has great potential, it
still lacks wide adoption. In this work we describe a novel HMBRL framework and
evaluate it thoroughly. To complement the multi-layered decision making idiom
characteristic for HRL, we construct hierarchical world models that simulate
environment dynamics at various levels of temporal abstraction. These models
are used to train a stack of agents that communicate in a top-down manner by
proposing goals to their subordinate agents. A significant focus of this study
is the exploration of a static and environment agnostic temporal abstraction,
which allows concurrent training of models and agents throughout the hierarchy.
Unlike most goal-conditioned H(MB)RL approaches, it also leads to comparatively
low dimensional abstract actions. Although our HMBRL approach did not
outperform traditional methods in terms of final episode returns, it
successfully facilitated decision making across two levels of abstraction using
compact, low dimensional abstract actions. A central challenge in enhancing our
method's performance, as uncovered through comprehensive experimentation, is
model exploitation on the abstract level of our world model stack. We provide
an in depth examination of this issue, discussing its implications for the
field and suggesting directions for future research to overcome this challenge.
By sharing these findings, we aim to contribute to the broader discourse on
refining HMBRL methodologies and to assist in the development of more effective
autonomous learning systems for complex decision-making environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.19883v2' target='_blank'>From Words to Actions: Unveiling the Theoretical Underpinnings of
  LLM-Driven Autonomous Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianliang He, Siyu Chen, Fengzhuo Zhang, Zhuoran Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-30 09:42:54</h6>
<p class='card-text'>In this work, from a theoretical lens, we aim to understand why large
language model (LLM) empowered agents are able to solve decision-making
problems in the physical world. To this end, consider a hierarchical
reinforcement learning (RL) model where the LLM Planner and the Actor perform
high-level task planning and low-level execution, respectively. Under this
model, the LLM Planner navigates a partially observable Markov decision process
(POMDP) by iteratively generating language-based subgoals via prompting. Under
proper assumptions on the pretraining data, we prove that the pretrained LLM
Planner effectively performs Bayesian aggregated imitation learning (BAIL)
through in-context learning. Additionally, we highlight the necessity for
exploration beyond the subgoals derived from BAIL by proving that naively
executing the subgoals returned by LLM leads to a linear regret. As a remedy,
we introduce an $\epsilon$-greedy exploration strategy to BAIL, which is proven
to incur sublinear regret when the pretraining error is small. Finally, we
extend our theoretical framework to include scenarios where the LLM Planner
serves as a world model for inferring the transition model of the environment
and to multi-agent settings, enabling coordination among multiple Actors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.11352v1' target='_blank'>Hierarchical Reinforcement Learning Empowered Task Offloading in V2I
  Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinyu You, Haojie Yan, Yuedong Xu, Lifeng Wang, Liangui Dai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-18 17:44:02</h6>
<p class='card-text'>Edge computing plays an essential role in the vehicle-to-infrastructure (V2I)
networks, where vehicles offload their intensive computation tasks to the
road-side units for saving energy and reduce the latency. This paper designs
the optimal task offloading policy to address the concerns involving processing
delay, energy consumption and edge computing cost. Each computation task
consisting of some interdependent sub-tasks is characterized as a directed
acyclic graph (DAG). In such dynamic networks, a novel hierarchical Offloading
scheme is proposed by leveraging deep reinforcement learning (DRL). The
inter-dependencies among the DAGs of the computation tasks are extracted using
a graph neural network with attention mechanism. A parameterized DRL algorithm
is developed to deal with the hierarchical action space containing both
discrete and continuous actions. Simulation results with a real-world car speed
dataset demonstrate that the proposed scheme can effectively reduce the system
overhead.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2405.17439v2' target='_blank'>An Overview of Machine Learning-Enabled Optimization for Reconfigurable
  Intelligent Surfaces-Aided 6G Networks: From Reinforcement Learning to Large
  Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Zhou, Chengming Hu, Xue Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-05-09 03:07:59</h6>
<p class='card-text'>Reconfigurable intelligent surface (RIS) becomes a promising technique for 6G
networks by reshaping signal propagation in smart radio environments. However,
it also leads to significant complexity for network management due to the large
number of elements and dedicated phase-shift optimization. In this work, we
provide an overview of machine learning (ML)-enabled optimization for RIS-aided
6G networks. In particular, we focus on various reinforcement learning (RL)
techniques, e.g., deep Q-learning, multi-agent reinforcement learning, transfer
reinforcement learning, hierarchical reinforcement learning, and offline
reinforcement learning. Different from existing studies, this work further
discusses how large language models (LLMs) can be combined with RL to handle
network optimization problems. It shows that LLM offers new opportunities to
enhance the capabilities of RL algorithms in terms of generalization, reward
function design, multi-modal information processing, etc. Finally, we identify
the future challenges and directions of ML-enabled optimization for RIS-aided
6G networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.13423v2' target='_blank'>PIPER: Primitive-Informed Preference-based Hierarchical Reinforcement
  Learning via Hindsight Relabeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Utsav Singh, Wesley A. Suttle, Brian M. Sadler, Vinay P. Namboodiri, Amrit Singh Bedi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-20 17:06:00</h6>
<p class='card-text'>In this work, we introduce PIPER: Primitive-Informed Preference-based
Hierarchical reinforcement learning via Hindsight Relabeling, a novel approach
that leverages preference-based learning to learn a reward model, and
subsequently uses this reward model to relabel higher-level replay buffers.
Since this reward is unaffected by lower primitive behavior, our
relabeling-based approach is able to mitigate non-stationarity, which is common
in existing hierarchical approaches, and demonstrates impressive performance
across a range of challenging sparse-reward tasks. Since obtaining human
feedback is typically impractical, we propose to replace the human-in-the-loop
approach with our primitive-in-the-loop approach, which generates feedback
using sparse rewards provided by the environment. Moreover, in order to prevent
infeasible subgoal prediction and avoid degenerate solutions, we propose
primitive-informed regularization that conditions higher-level policies to
generate feasible subgoals for lower-level policies. We perform extensive
experiments to show that PIPER mitigates non-stationarity in hierarchical
reinforcement learning and achieves greater than 50$\%$ success rates in
challenging, sparse-reward robotic environments, where most other baselines
fail to achieve any significant progress.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2404.02486v2' target='_blank'>Joint Optimization on Uplink OFDMA and MU-MIMO for IEEE 802.11ax: Deep
  Hierarchical Reinforcement Learning Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hyeonho Noh, Harim Lee, Hyun Jong Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-04-03 05:47:55</h6>
<p class='card-text'>This letter tackles a joint user scheduling, frequency resource allocation
(USRA), multi-input-multi-output mode selection (MIMO MS) between single-user
MIMO and multi-user (MU) MIMO, and MU-MIMO user selection problem, integrating
uplink orthogonal frequency division multiple access (OFDMA) in IEEE 802.11ax.
Specifically, we focus on \textit{unsaturated traffic conditions} where users'
data demands fluctuate. In unsaturated traffic conditions, considering packet
volumes per user introduces a combinatorial problem, requiring the simultaneous
optimization of MU-MIMO user selection and RA along the time-frequency-space
axis. Consequently, dealing with the combinatorial nature of this problem,
characterized by a large cardinality of unknown variables, poses a challenge
that conventional optimization methods find nearly impossible to address. In
response, this letter proposes an approach with deep hierarchical reinforcement
learning (DHRL) to solve the joint problem. Rather than simply adopting
off-the-shelf DHRL, we \textit{tailor} the DHRL to the joint USRA and MS
problem, thereby significantly improving the convergence speed and throughput.
Extensive simulation results show that the proposed algorithm achieves
significantly improved throughput compared to the existing schemes under
various unsaturated traffic conditions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.16015v1' target='_blank'>MQE: Unleashing the Power of Interaction with Multi-agent Quadruped
  Environment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyan Xiong, Bo Chen, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, Yang Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-24 05:13:37</h6>
<p class='card-text'>The advent of deep reinforcement learning (DRL) has significantly advanced
the field of robotics, particularly in the control and coordination of
quadruped robots. However, the complexity of real-world tasks often
necessitates the deployment of multi-robot systems capable of sophisticated
interaction and collaboration. To address this need, we introduce the
Multi-agent Quadruped Environment (MQE), a novel platform designed to
facilitate the development and evaluation of multi-agent reinforcement learning
(MARL) algorithms in realistic and dynamic scenarios. MQE emphasizes complex
interactions between robots and objects, hierarchical policy structures, and
challenging evaluation scenarios that reflect real-world applications. We
present a series of collaborative and competitive tasks within MQE, ranging
from simple coordination to complex adversarial interactions, and benchmark
state-of-the-art MARL algorithms. Our findings indicate that hierarchical
reinforcement learning can simplify task learning, but also highlight the need
for advanced algorithms capable of handling the intricate dynamics of
multi-agent interactions. MQE serves as a stepping stone towards bridging the
gap between simulation and practical deployment, offering a rich environment
for future research in multi-agent systems and robot learning. For open-sourced
code and more details of MQE, please refer to
https://ziyanx02.github.io/multiagent-quadruped-environment/ .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.14879v1' target='_blank'>Learning to Change: Choreographing Mixed Traffic Through Lateral Control
  and Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dawei Wang, Weizi Li, Lei Zhu, Jia Pan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-21 23:00:10</h6>
<p class='card-text'>The management of mixed traffic that consists of robot vehicles (RVs) and
human-driven vehicles (HVs) at complex intersections presents a multifaceted
challenge. Traditional signal controls often struggle to adapt to dynamic
traffic conditions and heterogeneous vehicle types. Recent advancements have
turned to strategies based on reinforcement learning (RL), leveraging its
model-free nature, real-time operation, and generalizability over different
scenarios. We introduce a hierarchical RL framework to manage mixed traffic
through precise longitudinal and lateral control of RVs. Our proposed
hierarchical framework combines the state-of-the-art mixed traffic control
algorithm as a high level decision maker to improve the performance and
robustness of the whole system. Our experiments demonstrate that the framework
can reduce the average waiting time by up to 54% compared to the
state-of-the-art mixed traffic control method. When the RV penetration rate
exceeds 60%, our technique consistently outperforms conventional traffic signal
control programs in terms of the average waiting time for all vehicles at the
intersection.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.10855v2' target='_blank'>Reinforcement Learning with Options and State Representation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ayoub Ghriss, Masashi Sugiyama, Alessandro Lazaric</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-16 08:30:55</h6>
<p class='card-text'>The current thesis aims to explore the reinforcement learning field and build
on existing methods to produce improved ones to tackle the problem of learning
in high-dimensional and complex environments. It addresses such goals by
decomposing learning tasks in a hierarchical fashion known as Hierarchical
Reinforcement Learning.
  We start in the first chapter by getting familiar with the Markov Decision
Process framework and presenting some of its recent techniques that the
following chapters use. We then proceed to build our Hierarchical Policy
learning as an answer to the limitations of a single primitive policy. The
hierarchy is composed of a manager agent at the top and employee agents at the
lower level.
  In the last chapter, which is the core of this thesis, we attempt to learn
lower-level elements of the hierarchy independently of the manager level in
what is known as the "Eigenoption". Based on the graph structure of the
environment, Eigenoptions allow us to build agents that are aware of the
geometric and dynamic properties of the environment. Their decision-making has
a special property: it is invariant to symmetric transformations of the
environment, allowing as a consequence to greatly reduce the complexity of the
learning task.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.01816v1' target='_blank'>SMAUG: A Sliding Multidimensional Task Window-Based MARL Framework for
  Adaptive Real-Time Subtask Recognition</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenjing Zhang, Wei Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-03-04 08:04:41</h6>
<p class='card-text'>Instead of making behavioral decisions directly from the exponentially
expanding joint observational-action space, subtask-based multi-agent
reinforcement learning (MARL) methods enable agents to learn how to tackle
different subtasks. Most existing subtask-based MARL methods are based on
hierarchical reinforcement learning (HRL). However, these approaches often
limit the number of subtasks, perform subtask recognition periodically, and can
only identify and execute a specific subtask within the predefined fixed time
period, which makes them inflexible and not suitable for diverse and dynamic
scenarios with constantly changing subtasks. To break through above
restrictions, a \textbf{S}liding \textbf{M}ultidimensional t\textbf{A}sk window
based m\textbf{U}ti-agent reinforcement learnin\textbf{G} framework (SMAUG) is
proposed for adaptive real-time subtask recognition. It leverages a sliding
multidimensional task window to extract essential information of subtasks from
trajectory segments concatenated based on observed and predicted trajectories
in varying lengths. An inference network is designed to iteratively predict
future trajectories with the subtask-oriented policy network. Furthermore,
intrinsic motivation rewards are defined to promote subtask exploration and
behavior diversity. SMAUG can be integrated with any Q-learning-based approach.
Experiments on StarCraft II show that SMAUG not only demonstrates performance
superiority in comparison with all baselines but also presents a more prominent
and swift rise in rewards during the initial training stage.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2403.00832v1' target='_blank'>Explainable Session-based Recommendation via Path Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yang Cao, Shuo Shang, Jun Wang, Wei Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-28 12:11:08</h6>
<p class='card-text'>This paper explores providing explainability for session-based recommendation
(SR) by path reasoning. Current SR models emphasize accuracy but lack
explainability, while traditional path reasoning prioritizes knowledge graph
exploration, ignoring sequential patterns present in the session history.
Therefore, we propose a generalized hierarchical reinforcement learning
framework for SR, which improves the explainability of existing SR models via
Path Reasoning, namely PR4SR. Considering the different importance of items to
the session, we design the session-level agent to select the items in the
session as the starting point for path reasoning and the path-level agent to
perform path reasoning. In particular, we design a multi-target reward
mechanism to adapt to the skip behaviors of sequential patterns in SR, and
introduce path midpoint reward to enhance the exploration efficiency in
knowledge graphs. To improve the completeness of the knowledge graph and to
diversify the paths of explanation, we incorporate extracted feature
information from images into the knowledge graph. We instantiate PR4SR in five
state-of-the-art SR models (i.e., GRU4REC, NARM, GCSAN, SR-GNN, SASRec) and
compare it with other explainable SR frameworks, to demonstrate the
effectiveness of PR4SR for recommendation and explanation tasks through
extensive experiments with these approaches on four datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.14244v2' target='_blank'>MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback
  and Dynamic Distance Constraint</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinglin Zhou, Yifu Yuan, Shaofu Yang, Jianye Hao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-22 03:11:09</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) provides a promising solution for
complex tasks with sparse rewards of intelligent agents, which uses a
hierarchical framework that divides tasks into subgoals and completes them
sequentially. However, current methods struggle to find suitable subgoals for
ensuring a stable learning process. Without additional guidance, it is
impractical to rely solely on exploration or heuristics methods to determine
subgoals in a large goal space. To address the issue, We propose a general
hierarchical reinforcement learning framework incorporating human feedback and
dynamic distance constraints (MENTOR). MENTOR acts as a "mentor", incorporating
human feedback into high-level policy learning, to find better subgoals. As for
low-level policy, MENTOR designs a dual policy for exploration-exploitation
decoupling respectively to stabilize the training. Furthermore, although humans
can simply break down tasks into subgoals to guide the right learning
direction, subgoals that are too difficult or too easy can still hinder
downstream learning efficiency. We propose the Dynamic Distance Constraint
(DDC) mechanism dynamically adjusting the space of optional subgoals. Thus
MENTOR can generate subgoals matching the low-level policy learning process
from easy to hard. Extensive experiments demonstrate that MENTOR uses a small
amount of human feedback to achieve significant improvement in complex tasks
with sparse rewards.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.06694v1' target='_blank'>Scaling Intelligent Agents in Combat Simulations for Wargaming</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Scotty Black, Christian Darken</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-08 21:57:10</h6>
<p class='card-text'>Remaining competitive in future conflicts with technologically-advanced
competitors requires us to accelerate our research and development in
artificial intelligence (AI) for wargaming. More importantly, leveraging
machine learning for intelligent combat behavior development will be key to one
day achieving superhuman performance in this domain--elevating the quality and
accelerating the speed of our decisions in future wars. Although deep
reinforcement learning (RL) continues to show promising results in intelligent
agent behavior development in games, it has yet to perform at or above the
human level in the long-horizon, complex tasks typically found in combat
modeling and simulation. Capitalizing on the proven potential of RL and recent
successes of hierarchical reinforcement learning (HRL), our research is
investigating and extending the use of HRL to create intelligent agents capable
of performing effectively in these large and complex simulation environments.
Our ultimate goal is to develop an agent capable of superhuman performance that
could then serve as an AI advisor to military planners and decision-makers.
This papers covers our ongoing approach and the first three of our five
research areas aimed at managing the exponential growth of computations that
have thus far limited the use of AI in combat simulations: (1) developing an
HRL training framework and agent architecture for combat units; (2) developing
a multi-model framework for agent decision-making; (3) developing
dimension-invariant observation abstractions of the state space to manage the
exponential growth of computations; (4) developing an intrinsic rewards engine
to enable long-term planning; and (5) implementing this framework into a
higher-fidelity combat simulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.06075v1' target='_blank'>Scaling Artificial Intelligence for Digital Wargaming in Support of
  Decision-Making</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Scotty Black, Christian Darken</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-08 21:51:07</h6>
<p class='card-text'>In this unprecedented era of technology-driven transformation, it becomes
more critical than ever that we aggressively invest in developing robust
artificial intelligence (AI) for wargaming in support of decision-making. By
advancing AI-enabled systems and pairing these with human judgment, we will be
able to enhance all-domain awareness, improve the speed and quality of our
decision cycles, offer recommendations for novel courses of action, and more
rapidly counter our adversary's actions. It therefore becomes imperative that
we accelerate the development of AI to help us better address the complexity of
modern challenges and dilemmas that currently requires human intelligence and,
if possible, attempt to surpass human intelligence--not to replace humans, but
to augment and better inform human decision-making at machine speed. Although
deep reinforcement learning continues to show promising results in intelligent
agent behavior development for the long-horizon, complex tasks typically found
in combat modeling and simulation, further research is needed to enable the
scaling of AI to deal with these intricate and expansive state-spaces
characteristic of wargaming for either concept development, education, or
analysis. To help address this challenge, in our research, we are developing
and implementing a hierarchical reinforcement learning framework that includes
a multi-model approach and dimension-invariant observation abstractions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.04061v3' target='_blank'>TopoNav: Topological Navigation for Efficient Exploration in Sparse
  Reward Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy, Jade Freeman, Timothy Gregory, Theron T. Trout</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-06 15:05:25</h6>
<p class='card-text'>Autonomous robots exploring unknown environments face a significant
challenge: navigating effectively without prior maps and with limited external
feedback. This challenge intensifies in sparse reward environments, where
traditional exploration techniques often fail. In this paper, we present
TopoNav, a novel topological navigation framework that integrates active
mapping, hierarchical reinforcement learning, and intrinsic motivation to
enable efficient goal-oriented exploration and navigation in sparse-reward
settings. TopoNav dynamically constructs a topological map of the environment,
capturing key locations and pathways. A two-level hierarchical policy
architecture, comprising a high-level graph traversal policy and low-level
motion control policies, enables effective navigation and obstacle avoidance
while maintaining focus on the overall goal. Additionally, TopoNav incorporates
intrinsic motivation to guide exploration toward relevant regions and frontier
nodes in the topological map, addressing the challenges of sparse extrinsic
rewards. We evaluate TopoNav both in the simulated and real-world off-road
environments using a Clearpath Jackal robot, across three challenging
navigation scenarios: goal-reaching, feature-based navigation, and navigation
in complex terrains. We observe an increase in exploration coverage by 7- 20%,
in success rates by 9-19%, and reductions in navigation times by 15-36% across
various scenarios, compared to state-of-the-art methods</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.02146v1' target='_blank'>Emergency Computing: An Adaptive Collaborative Inference Method Based on
  Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weiqi Fu, Lianming Xu, Xin Wu, Li Wang, Aiguo Fei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-03 13:28:35</h6>
<p class='card-text'>In achieving effective emergency response, the timely acquisition of
environmental information, seamless command data transmission, and prompt
decision-making are crucial. This necessitates the establishment of a resilient
emergency communication dedicated network, capable of providing communication
and sensing services even in the absence of basic infrastructure. In this
paper, we propose an Emergency Network with Sensing, Communication,
Computation, Caching, and Intelligence (E-SC3I). The framework incorporates
mechanisms for emergency computing, caching, integrated communication and
sensing, and intelligence empowerment. E-SC3I ensures rapid access to a large
user base, reliable data transmission over unstable links, and dynamic network
deployment in a changing environment. However, these advantages come at the
cost of significant computation overhead. Therefore, we specifically
concentrate on emergency computing and propose an adaptive collaborative
inference method (ACIM) based on hierarchical reinforcement learning.
Experimental results demonstrate our method's ability to achieve rapid
inference of AI models with constrained computational and communication
resources.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2402.00823v2' target='_blank'>SLIM: Skill Learning with Multiple Critics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:David Emukpere, Bingbing Wu, Julien Perez, Jean-Michel Renders</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-02-01 18:07:33</h6>
<p class='card-text'>Self-supervised skill learning aims to acquire useful behaviors that leverage
the underlying dynamics of the environment. Latent variable models, based on
mutual information maximization, have been successful in this task but still
struggle in the context of robotic manipulation. As it requires impacting a
possibly large set of degrees of freedom composing the environment, mutual
information maximization fails alone in producing useful and safe manipulation
behaviors. Furthermore, tackling this by augmenting skill discovery rewards
with additional rewards through a naive combination might fail to produce
desired behaviors. To address this limitation, we introduce SLIM, a
multi-critic learning approach for skill discovery with a particular focus on
robotic manipulation. Our main insight is that utilizing multiple critics in an
actor-critic framework to gracefully combine multiple reward functions leads to
a significant improvement in latent-variable skill discovery for robotic
manipulation while overcoming possible interference occurring among rewards
which hinders convergence to useful skills. Furthermore, in the context of
tabletop manipulation, we demonstrate the applicability of our novel skill
discovery approach to acquire safe and efficient motor primitives in a
hierarchical reinforcement learning fashion and leverage them through planning,
significantly surpassing baseline approaches for skill discovery.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.10368v2' target='_blank'>HRL-TSCH: A Hierarchical Reinforcement Learning-based TSCH Scheduler for
  IIoT</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:F. Fernando Jurado-Lasso, Charalampos Orfanidis, J. F. Jurado, Xenofon Fafoutis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-18 20:25:27</h6>
<p class='card-text'>The Industrial Internet of Things (IIoT) demands adaptable Networked Embedded
Systems (NES) for optimal performance. Combined with recent advances in
Artificial Intelligence (AI), tailored solutions can be developed to meet
specific application requirements. This study introduces HRL-TSCH, an approach
rooted in Hierarchical Reinforcement Learning (HRL), to devise Time Slotted
Channel Hopping (TSCH) schedules provisioning IIoT demand. HRL-TSCH employs
dual policies: one at a higher level for TSCH schedule link management, and
another at a lower level for timeslot and channel assignments. The proposed RL
agents address a multi-objective problem, optimizing throughput, power
efficiency, and network delay based on predefined application requirements.
Simulation experiments demonstrate HRL-TSCH superiority over existing
state-of-art approaches, effectively achieving an optimal balance between
throughput, power consumption, and delay, thereby enhancing IIoT network
performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.09870v2' target='_blank'>Reconciling Spatial and Temporal Abstractions for Goal Representation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mehdi Zadem, Sergio Mover, Sao Mai Nguyen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2024-01-18 10:33:30</h6>
<p class='card-text'>Goal representation affects the performance of Hierarchical Reinforcement
Learning (HRL) algorithms by decomposing the complex learning problem into
easier subtasks. Recent studies show that representations that preserve
temporally abstract environment dynamics are successful in solving difficult
problems and provide theoretical guarantees for optimality. These methods
however cannot scale to tasks where environment dynamics increase in complexity
i.e. the temporally abstract transition relations depend on larger number of
variables. On the other hand, other efforts have tried to use spatial
abstraction to mitigate the previous issues. Their limitations include
scalability to high dimensional environments and dependency on prior knowledge.
  In this paper, we propose a novel three-layer HRL algorithm that introduces,
at different levels of the hierarchy, both a spatial and a temporal goal
abstraction. We provide a theoretical study of the regret bounds of the learned
policies. We evaluate the approach on complex continuous control tasks,
demonstrating the effectiveness of spatial and temporal abstractions learned by
this approach. Find open-source code at https://github.com/cosynus-lix/STAR.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.15717v1' target='_blank'>Spatial-Temporal Interplay in Human Mobility: A Hierarchical
  Reinforcement Learning Approach with Hypergraph Representation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhaofan Zhang, Yanan Xiao, Lu Jiang, Dingqi Yang, Minghao Yin, Pengyang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-25 13:00:05</h6>
<p class='card-text'>In the realm of human mobility, the decision-making process for selecting the
next-visit location is intricately influenced by a trade-off between spatial
and temporal constraints, which are reflective of individual needs and
preferences. This trade-off, however, varies across individuals, making the
modeling of these spatial-temporal dynamics a formidable challenge. To address
the problem, in this work, we introduce the "Spatial-temporal Induced
Hierarchical Reinforcement Learning" (STI-HRL) framework, for capturing the
interplay between spatial and temporal factors in human mobility
decision-making. Specifically, STI-HRL employs a two-tiered decision-making
process: the low-level focuses on disentangling spatial and temporal
preferences using dedicated agents, while the high-level integrates these
considerations to finalize the decision. To complement the hierarchical
decision setting, we construct a hypergraph to organize historical data,
encapsulating the multi-aspect semantics of human mobility. We propose a
cross-channel hypergraph embedding module to learn the representations as the
states to facilitate the decision-making cycle. Our extensive experiments on
two real-world datasets validate the superiority of STI-HRL over
state-of-the-art methods in predicting users' next visits across various
performance metrics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.03225v1' target='_blank'>Snake Robot with Tactile Perception Navigates on Large-scale Challenging
  Terrain</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuo Jiang, Adarsh Salagame, Alireza Ramezani, Lawson Wong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-06 01:50:47</h6>
<p class='card-text'>Along with the advancement of robot skin technology, there has been notable
progress in the development of snake robots featuring body-surface tactile
perception. In this study, we proposed a locomotion control framework for snake
robots that integrates tactile perception to augment their adaptability to
various terrains. Our approach embraces a hierarchical reinforcement learning
(HRL) architecture, wherein the high-level orchestrates global navigation
strategies while the low-level uses curriculum learning for local navigation
maneuvers. Due to the significant computational demands of collision detection
in whole-body tactile sensing, the efficiency of the simulator is severely
compromised. Thus a distributed training pattern to mitigate the efficiency
reduction was adopted. We evaluated the navigation performance of the snake
robot in complex large-scale cave exploration with challenging terrains to
exhibit improvements in motion efficiency, evidencing the efficacy of tactile
perception in terrain-adaptive locomotion of snake robots.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.02697v1' target='_blank'>Hierarchical Visual Policy Learning for Long-Horizon Robot Manipulation
  in Densely Cluttered Scenes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hecheng Wang, Lizhe Qi, Bin Fang, Yunquan Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-05 11:57:39</h6>
<p class='card-text'>In this work, we focus on addressing the long-horizon manipulation tasks in
densely cluttered scenes. Such tasks require policies to effectively manage
severe occlusions among objects and continually produce actions based on visual
observations. We propose a vision-based Hierarchical policy for Cluttered-scene
Long-horizon Manipulation (HCLM). It employs a high-level policy and three
options to select and instantiate three parameterized action primitives: push,
pick, and place. We first train the pick and place options by behavior cloning
(BC). Subsequently, we use hierarchical reinforcement learning (HRL) to train
the high-level policy and push option. During HRL, we propose a Spatially
Extended Q-update (SEQ) to augment the updates for the push option and a
Two-Stage Update Scheme (TSUS) to alleviate the non-stationary transition
problem in updating the high-level policy. We demonstrate that HCLM
significantly outperforms baseline methods in terms of success rate and
efficiency in diverse tasks. We also highlight our method's ability to
generalize to more cluttered environments with more additional blocks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2312.01071v1' target='_blank'>Hybrid Hierarchical DRL Enabled Resource Allocation for Secure
  Transmission in Multi-IRS-Assisted Sensing-Enhanced Spectrum Sharing Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lingyi Wang, Wei Wu, Fuhui Zhou, Qihui Wu, Octavia A. Dobre, Tony Q. S. Quek</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-12-02 08:49:31</h6>
<p class='card-text'>Secure communications are of paramount importance in spectrum sharing
networks due to the allocation and sharing characteristics of spectrum
resources. To further explore the potential of intelligent reflective surfaces
(IRSs) in enhancing spectrum sharing and secure transmission performance, a
multiple intelligent reflection surface (multi-IRS)-assisted sensing-enhanced
wideband spectrum sharing network is investigated by considering physical layer
security techniques. An intelligent resource allocation scheme based on double
deep Q networks (D3QN) algorithm and soft Actor-Critic (SAC) algorithm is
proposed to maximize the secure transmission rate of the secondary network by
jointly optimizing IRS pairings, subchannel assignment, transmit beamforming of
the secondary base station, reflection coefficients of IRSs and the sensing
time. To tackle the sparse reward problem caused by a significant amount of
reflection elements of multiple IRSs, the method of hierarchical reinforcement
learning is exploited. An alternative optimization (AO)-based conventional
mathematical scheme is introduced to verify the computational complexity
advantage of our proposed intelligent scheme. Simulation results demonstrate
the efficiency of our proposed intelligent scheme as well as the superiority of
multi-IRS design in enhancing secrecy rate and spectrum utilization. It is
shown that inappropriate deployment of IRSs can reduce the security performance
with the presence of multiple eavesdroppers (Eves), and the arrangement of IRSs
deserves further consideration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.12548v1' target='_blank'>Multi-Session Budget Optimization for Forward Auction-based Federated
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoli Tang, Han Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-21 11:57:41</h6>
<p class='card-text'>Auction-based Federated Learning (AFL) has emerged as an important research
field in recent years. The prevailing strategies for FL model users (MUs)
assume that the entire team of the required data owners (DOs) for an FL task
must be assembled before training can commence. In practice, an MU can trigger
the FL training process multiple times. DOs can thus be gradually recruited
over multiple FL model training sessions. Existing bidding strategies for AFL
MUs are not designed to handle such scenarios. Therefore, the problem of
multi-session AFL remains open. To address this problem, we propose the
Multi-session Budget Optimization Strategy for forward Auction-based Federated
Learning (MultiBOS-AFL). Based on hierarchical reinforcement learning,
MultiBOS-AFL jointly optimizes inter-session budget pacing and intra-session
bidding for AFL MUs, with the objective of maximizing the total utility.
Extensive experiments on six benchmark datasets show that it significantly
outperforms seven state-of-the-art approaches. On average, MultiBOS-AFL
achieves 12.28% higher utility, 14.52% more data acquired through auctions for
a given budget, and 1.23% higher test accuracy achieved by the resulting FL
model compared to the best baseline. To the best of our knowledge, it is the
first budget optimization decision support method with budget pacing capability
designed for MUs in multi-session forward auction-based federated learning</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.10309v2' target='_blank'>Imagination-Augmented Hierarchical Reinforcement Learning for Safe and
  Interactive Autonomous Driving in Urban Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sang-Hyun Lee, Yoonjae Jung, Seung-Woo Seo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-17 03:41:22</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) incorporates temporal abstraction
into reinforcement learning (RL) by explicitly taking advantage of hierarchical
structure. Modern HRL typically designs a hierarchical agent composed of a
high-level policy and low-level policies. The high-level policy selects which
low-level policy to activate at a lower frequency and the activated low-level
policy selects an action at each time step. Recent HRL algorithms have achieved
performance gains over standard RL algorithms in synthetic navigation tasks.
However, we cannot apply these HRL algorithms to real-world navigation tasks.
One of the main challenges is that real-world navigation tasks require an agent
to perform safe and interactive behaviors in dynamic environments. In this
paper, we propose imagination-augmented HRL (IAHRL) that efficiently integrates
imagination into HRL to enable an agent to learn safe and interactive behaviors
in real-world navigation tasks. Imagination is to predict the consequences of
actions without interactions with actual environments. The key idea behind
IAHRL is that the low-level policies imagine safe and structured behaviors, and
then the high-level policy infers interactions with surrounding objects by
interpreting the imagined behaviors. We also introduce a new attention
mechanism that allows our high-level policy to be permutation-invariant to the
order of surrounding objects and to prioritize our agent over them. To evaluate
IAHRL, we introduce five complex urban driving tasks, which are among the most
challenging real-world navigation tasks. The experimental results indicate that
IAHRL enables an agent to perform safe and interactive behaviors, achieving
higher success rates and lower average episode steps than baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.02129v1' target='_blank'>Hierarchical Reinforcement Learning for Power Network Topology Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Blazej Manczak, Jan Viebahn, Herke van Hoof</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-03 12:33:00</h6>
<p class='card-text'>Learning in high-dimensional action spaces is a key challenge in applying
reinforcement learning (RL) to real-world systems. In this paper, we study the
possibility of controlling power networks using RL methods. Power networks are
critical infrastructures that are complex to control. In particular, the
combinatorial nature of the action space poses a challenge to both conventional
optimizers and learned controllers. Hierarchical reinforcement learning (HRL)
represents one approach to address this challenge. More precisely, a HRL
framework for power network topology control is proposed. The HRL framework
consists of three levels of action abstraction. At the highest level, there is
the overall long-term task of power network operation, namely, keeping the
power grid state within security constraints at all times, which is decomposed
into two temporally extended actions: 'do nothing' versus 'propose a topology
change'. At the intermediate level, the action space consists of all
controllable substations. Finally, at the lowest level, the action space
consists of all configurations of the chosen substation. By employing this HRL
framework, several hierarchical power network agents are trained for the IEEE
14-bus network. Whereas at the highest level a purely rule-based policy is
still chosen for all agents in this study, at the intermediate level the policy
is trained using different state-of-the-art RL algorithms. At the lowest level,
either an RL algorithm or a greedy algorithm is used. The performance of the
different 3-level agents is compared with standard baseline (RL or greedy)
approaches. A key finding is that the 3-level agent that employs RL both at the
intermediate and the lowest level outperforms all other agents on the most
difficult task. Our code is publicly available.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2401.03337v1' target='_blank'>MTAC: Hierarchical Reinforcement Learning-based Multi-gait
  Terrain-adaptive Quadruped Controller</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nishaant Shah, Kshitij Tiwari, Aniket Bera</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-01 18:17:47</h6>
<p class='card-text'>Urban search and rescue missions require rapid first response to minimize
loss of life and damage. Often, such efforts are assisted by humanitarian
robots which need to handle dynamic operational conditions such as uneven and
rough terrains, especially during mass casualty incidents like an earthquake.
Quadruped robots, owing to their versatile design, have the potential to assist
in such scenarios. However, control of quadruped robots in dynamic and rough
terrain environments is a challenging problem due to the many degrees of
freedom of these robots. Current locomotion controllers for quadrupeds are
limited in their ability to produce multiple adaptive gaits, solve tasks in a
time and resource-efficient manner, and require tedious training and manual
tuning procedures. To address these challenges, we propose MTAC: a multi-gait
terrain-adaptive controller, which utilizes a Hierarchical reinforcement
learning (HRL) approach while being time and memory-efficient. We show that our
proposed method scales well to a diverse range of environments with similar
compute times as state-of-the-art methods. Our method showed greater than 75%
on most tasks, outperforming previous work on the majority of test cases.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2311.00267v1' target='_blank'>Rethinking Decision Transformer via Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yi Ma, Chenjun Xiao, Hebin Liang, Jianye Hao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-11-01 03:32:13</h6>
<p class='card-text'>Decision Transformer (DT) is an innovative algorithm leveraging recent
advances of the transformer architecture in reinforcement learning (RL).
However, a notable limitation of DT is its reliance on recalling trajectories
from datasets, losing the capability to seamlessly stitch sub-optimal
trajectories together. In this work we introduce a general sequence modeling
framework for studying sequential decision making through the lens of
Hierarchical RL. At the time of making decisions, a high-level policy first
proposes an ideal prompt for the current state, a low-level policy subsequently
generates an action conditioned on the given prompt. We show DT emerges as a
special case of this framework with certain choices of high-level and low-level
policies, and discuss the potential failure of these choices. Inspired by these
observations, we study how to jointly optimize the high-level and low-level
policies to enable the stitching ability, which further leads to the
development of new offline RL algorithms. Our empirical results clearly show
that the proposed algorithms significantly surpass DT on several control and
navigation benchmarks. We hope our contributions can inspire the integration of
transformer architectures within the field of RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.17922v2' target='_blank'>Chain-of-Choice Hierarchical Policy Learning for Conversational
  Recommendation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Fan, Weijia Zhang, Weiqi Wang, Yangqiu Song, Hao Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-27 06:36:31</h6>
<p class='card-text'>Conversational Recommender Systems (CRS) illuminate user preferences via
multi-round interactive dialogues, ultimately navigating towards precise and
satisfactory recommendations. However, contemporary CRS are limited to
inquiring binary or multi-choice questions based on a single attribute type
(e.g., color) per round, which causes excessive rounds of interaction and
diminishes the user's experience. To address this, we propose a more realistic
and efficient conversational recommendation problem setting, called
Multi-Type-Attribute Multi-round Conversational Recommendation (MTAMCR), which
enables CRS to inquire about multi-choice questions covering multiple types of
attributes in each round, thereby improving interactive efficiency. Moreover,
by formulating MTAMCR as a hierarchical reinforcement learning task, we propose
a Chain-of-Choice Hierarchical Policy Learning (CoCHPL) framework to enhance
both the questioning efficiency and recommendation effectiveness in MTAMCR.
Specifically, a long-term policy over options (i.e., ask or recommend)
determines the action type, while two short-term intra-option policies
sequentially generate the chain of attributes or items through multi-step
reasoning and selection, optimizing the diversity and interdependence of
questioning attributes. Finally, extensive experiments on four benchmarks
demonstrate the superior performance of CoCHPL over prevailing state-of-the-art
methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.17785v3' target='_blank'>Learning Extrinsic Dexterity with Parameterized Manipulation Primitives</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shih-Min Yang, Martin Magnusson, Johannes A. Stork, Todor Stoyanov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-26 21:28:23</h6>
<p class='card-text'>Many practically relevant robot grasping problems feature a target object for
which all grasps are occluded, e.g., by the environment. Single-shot grasp
planning invariably fails in such scenarios. Instead, it is necessary to first
manipulate the object into a configuration that affords a grasp. We solve this
problem by learning a sequence of actions that utilize the environment to
change the object's pose. Concretely, we employ hierarchical reinforcement
learning to combine a sequence of learned parameterized manipulation
primitives. By learning the low-level manipulation policies, our approach can
control the object's state through exploiting interactions between the object,
the gripper, and the environment. Designing such a complex behavior
analytically would be infeasible under uncontrolled conditions, as an analytic
approach requires accurate physical modeling of the interaction and contact
dynamics. In contrast, we learn a hierarchical policy model that operates
directly on depth perception data, without the need for object detection, pose
estimation, or manual design of controllers. We evaluate our approach on
picking box-shaped objects of various weight, shape, and friction properties
from a constrained table-top workspace. Our method transfers to a real robot
and is able to successfully complete the object picking task in 98\% of
experimental trials. Supplementary information and videos can be found at
https://shihminyang.github.io/ED-PMP/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.09997v1' target='_blank'>Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas Jiralerspong, Flemming Kondrup, Doina Precup, Khimya Khetarpal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-16 01:13:26</h6>
<p class='card-text'>The ability to plan at many different levels of abstraction enables agents to
envision the long-term repercussions of their decisions and thus enables
sample-efficient learning. This becomes particularly beneficial in complex
environments from high-dimensional state space such as pixels, where the goal
is distant and the reward sparse. We introduce Forecaster, a deep hierarchical
reinforcement learning approach which plans over high-level goals leveraging a
temporally abstract world model. Forecaster learns an abstract model of its
environment by modelling the transitions dynamics at an abstract level and
training a world model on such transition. It then uses this world model to
choose optimal high-level goals through a tree-search planning procedure. It
additionally trains a low-level policy that learns to reach those goals. Our
method not only captures building world models with longer horizons, but also,
planning with such models in downstream tasks. We empirically demonstrate
Forecaster's potential in both single-task learning and generalization to new
tasks in the AntMaze domain.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.05695v1' target='_blank'>Hierarchical Reinforcement Learning for Temporal Pattern Prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Faith Johnson, Kristin Dana</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-09 13:15:57</h6>
<p class='card-text'>In this work, we explore the use of hierarchical reinforcement learning (HRL)
for the task of temporal sequence prediction. Using a combination of deep
learning and HRL, we develop a stock agent to predict temporal price sequences
from historical stock price data and a vehicle agent to predict steering angles
from first person, dash cam images. Our results in both domains indicate that a
type of HRL, called feudal reinforcement learning, provides significant
improvements to training speed and stability and prediction accuracy over
standard RL. A key component to this success is the multi-resolution structure
that introduces both temporal and spatial abstraction into the network
hierarchy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2310.04979v1' target='_blank'>Initial Task Assignment in Multi-Human Multi-Robot Teams: An
  Attention-enhanced Hierarchical Reinforcement Learning Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruiqi Wang, Dezhong Zhao, Arjun Gupte, Byung-Cheol Min</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-10-08 03:03:59</h6>
<p class='card-text'>Multi-human multi-robot teams (MH-MR) obtain tremendous potential in tackling
intricate and massive missions by merging distinct strengths and expertise of
individual members. The inherent heterogeneity of these teams necessitates
advanced initial task assignment (ITA) methods that align tasks with the
intrinsic capabilities of team members from the outset. While existing
reinforcement learning approaches show encouraging results, they might fall
short in addressing the nuances of long-horizon ITA problems, particularly in
settings with large-scale MH-MR teams or multifaceted tasks. To bridge this
gap, we propose an attention-enhanced hierarchical reinforcement learning
approach that decomposes the complex ITA problem into structured sub-problems,
facilitating more efficient allocations. To bolster sub-policy learning, we
introduce a hierarchical cross-attribute attention (HCA) mechanism, encouraging
each sub-policy within the hierarchy to discern and leverage the specific
nuances in the state space that are crucial for its respective decision-making
phase. Through an extensive environmental surveillance case study, we
demonstrate the benefits of our model and the HCA inside.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.17011v2' target='_blank'>Feature Interaction Aware Automated Data Representation Transformation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ehtesamul Azim, Dongjie Wang, Kunpeng Liu, Wei Zhang, Yanjie Fu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-29 06:48:16</h6>
<p class='card-text'>Creating an effective representation space is crucial for mitigating the
curse of dimensionality, enhancing model generalization, addressing data
sparsity, and leveraging classical models more effectively. Recent advancements
in automated feature engineering (AutoFE) have made significant progress in
addressing various challenges associated with representation learning, issues
such as heavy reliance on intensive labor and empirical experiences, lack of
explainable explicitness, and inflexible feature space reconstruction embedded
into downstream tasks. However, these approaches are constrained by: 1)
generation of potentially unintelligible and illogical reconstructed feature
spaces, stemming from the neglect of expert-level cognitive processes; 2) lack
of systematic exploration, which subsequently results in slower model
convergence for identification of optimal feature space. To address these, we
introduce an interaction-aware reinforced generation perspective. We redefine
feature space reconstruction as a nested process of creating meaningful
features and controlling feature set size through selection. We develop a
hierarchical reinforcement learning structure with cascading Markov Decision
Processes to automate feature and operation selection, as well as feature
crossing. By incorporating statistical measures, we reward agents based on the
interaction strength between selected features, resulting in intelligent and
efficient exploration of the feature space that emulates human decision-making.
Extensive experiments are conducted to validate our proposed approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.14237v2' target='_blank'>Hierarchical Reinforcement Learning Based on Planning Operators</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jing Zhang, Emmanuel Dean, Karinne Ramirez-Amaro</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-25 15:54:32</h6>
<p class='card-text'>Long-horizon manipulation tasks such as stacking represent a longstanding
challenge in the field of robotic manipulation, particularly when using
reinforcement learning (RL) methods which often struggle to learn the correct
sequence of actions for achieving these complex goals. To learn this sequence,
symbolic planning methods offer a good solution based on high-level reasoning,
however, planners often fall short in addressing the low-level control
specificity needed for precise execution. This paper introduces a novel
framework that integrates symbolic planning with hierarchical RL through the
cooperation of high-level operators and low-level policies. Our contribution
integrates planning operators (e.g. preconditions and effects) as part of the
hierarchical RL algorithm based on the Scheduled Auxiliary Control (SAC-X)
method. We developed a dual-purpose high-level operator, which can be used both
in holistic planning and as independent, reusable policies. Our approach offers
a flexible solution for long-horizon tasks, e.g., stacking a cube. The
experimental results show that our proposed method obtained an average of 97.2%
success rate for learning and executing the whole stack sequence, and the
success rate for learning independent policies, e.g. reach (98.9%), lift
(99.7%), stack (85%), etc. The training time is also reduced by 68% when using
our proposed approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.13508v2' target='_blank'>Guided Cooperation in Hierarchical Reinforcement Learning via
  Model-based Rollout</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoran Wang, Zeshen Tang, Leya Yang, Yaoru Sun, Fang Wang, Siyu Zhang, Yeming Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-24 00:13:16</h6>
<p class='card-text'>Goal-conditioned hierarchical reinforcement learning (HRL) presents a
promising approach for enabling effective exploration in complex, long-horizon
reinforcement learning (RL) tasks through temporal abstraction. Empirically,
heightened inter-level communication and coordination can induce more stable
and robust policy improvement in hierarchical systems. Yet, most existing
goal-conditioned HRL algorithms have primarily focused on the subgoal
discovery, neglecting inter-level cooperation. Here, we propose a
goal-conditioned HRL framework named Guided Cooperation via Model-based Rollout
(GCMR), aiming to bridge inter-layer information synchronization and
cooperation by exploiting forward dynamics. Firstly, the GCMR mitigates the
state-transition error within off-policy correction via model-based rollout,
thereby enhancing sample efficiency. Secondly, to prevent disruption by the
unseen subgoals and states, lower-level Q-function gradients are constrained
using a gradient penalty with a model-inferred upper bound, leading to a more
stable behavioral policy conducive to effective exploration. Thirdly, we
propose a one-step rollout-based planning, using higher-level critics to guide
the lower-level policy. Specifically, we estimate the value of future states of
the lower-level policy using the higher-level critic function, thereby
transmitting global task information downwards to avoid local pitfalls. These
three critical components in GCMR are expected to facilitate inter-level
cooperation significantly. Experimental results demonstrate that incorporating
the proposed GCMR framework with a disentangled variant of HIGL, namely ACLG,
yields more stable and robust policy improvement compared to various baselines
and significantly outperforms previous state-of-the-art algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.12891v1' target='_blank'>EarnHFT: Efficient Hierarchical Reinforcement Learning for High
  Frequency Trading</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Molei Qin, Shuo Sun, Wentao Zhang, Haochong Xia, Xinrun Wang, Bo An</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-22 14:25:03</h6>
<p class='card-text'>High-frequency trading (HFT) uses computer algorithms to make trading
decisions in short time scales (e.g., second-level), which is widely used in
the Cryptocurrency (Crypto) market (e.g., Bitcoin). Reinforcement learning (RL)
in financial research has shown stellar performance on many quantitative
trading tasks. However, most methods focus on low-frequency trading, e.g.,
day-level, which cannot be directly applied to HFT because of two challenges.
First, RL for HFT involves dealing with extremely long trajectories (e.g., 2.4
million steps per month), which is hard to optimize and evaluate. Second, the
dramatic price fluctuations and market trend changes of Crypto make existing
algorithms fail to maintain satisfactory performance. To tackle these
challenges, we propose an Efficient hieArchical Reinforcement learNing method
for High Frequency Trading (EarnHFT), a novel three-stage hierarchical RL
framework for HFT. In stage I, we compute a Q-teacher, i.e., the optimal action
value based on dynamic programming, for enhancing the performance and training
efficiency of second-level RL agents. In stage II, we construct a pool of
diverse RL agents for different market trends, distinguished by return rates,
where hundreds of RL agents are trained with different preferences of return
rates and only a tiny fraction of them will be selected into the pool based on
their profitability. In stage III, we train a minute-level router which
dynamically picks a second-level agent from the pool to achieve stable
performance across different markets. Through extensive experiments in various
market trends on Crypto markets in a high-fidelity simulation trading
environment, we demonstrate that EarnHFT significantly outperforms 6
state-of-art baselines in 6 popular financial criteria, exceeding the runner-up
by 30% in profitability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.12004v1' target='_blank'>Safe Hierarchical Reinforcement Learning for CubeSat Task Scheduling
  Based on Energy Consumption</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mahya Ramezani, M. Amin Alandihallaj, Jose Luis Sanchez-Lopez, Andreas Hein</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-21 12:22:11</h6>
<p class='card-text'>This paper presents a Hierarchical Reinforcement Learning methodology
tailored for optimizing CubeSat task scheduling in Low Earth Orbits (LEO).
Incorporating a high-level policy for global task distribution and a low-level
policy for real-time adaptations as a safety mechanism, our approach integrates
the Similarity Attention-based Encoder (SABE) for task prioritization and an
MLP estimator for energy consumption forecasting. Integrating this mechanism
creates a safe and fault-tolerant system for CubeSat task scheduling.
Simulation results validate the Hierarchical Reinforcement Learning superior
convergence and task success rate, outperforming both the MADDPG model and
traditional random scheduling across multiple CubeSat configurations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.11564v1' target='_blank'>Hierarchical reinforcement learning with natural language subgoals</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arun Ahuja, Kavya Kopparapu, Rob Fergus, Ishita Dasgupta</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-20 18:03:04</h6>
<p class='card-text'>Hierarchical reinforcement learning has been a compelling approach for
achieving goal directed behavior over long sequences of actions. However, it
has been challenging to implement in realistic or open-ended environments. A
main challenge has been to find the right space of sub-goals over which to
instantiate a hierarchy. We present a novel approach where we use data from
humans solving these tasks to softly supervise the goal space for a set of long
range tasks in a 3D embodied environment. In particular, we use unconstrained
natural language to parameterize this space. This has two advantages: first, it
is easy to generate this data from naive human participants; second, it is
flexible enough to represent a vast range of sub-goals in human-relevant tasks.
Our approach outperforms agents that clone expert behavior on these tasks, as
well as HRL from scratch without this supervised sub-goal space. Our work
presents a novel approach to combining human expert supervision with the
benefits and flexibility of reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.07675v2' target='_blank'>Goal Space Abstraction in Hierarchical Reinforcement Learning via
  Set-Based Reachability Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mehdi Zadem, Sergio Mover, Sao Mai Nguyen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-14 12:39:26</h6>
<p class='card-text'>Open-ended learning benefits immensely from the use of symbolic methods for
goal representation as they offer ways to structure knowledge for efficient and
transferable learning. However, the existing Hierarchical Reinforcement
Learning (HRL) approaches relying on symbolic reasoning are often limited as
they require a manual goal representation. The challenge in autonomously
discovering a symbolic goal representation is that it must preserve critical
information, such as the environment dynamics. In this paper, we propose a
developmental mechanism for goal discovery via an emergent representation that
abstracts (i.e., groups together) sets of environment states that have similar
roles in the task. We introduce a Feudal HRL algorithm that concurrently learns
both the goal representation and a hierarchical policy. The algorithm uses
symbolic reachability analysis for neural networks to approximate the
transition relation among sets of states and to refine the goal representation.
We evaluate our approach on complex navigation tasks, showing the learned
representation is interpretable, transferrable and results in data efficient
learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.07168v1' target='_blank'>Goal Space Abstraction in Hierarchical Reinforcement Learning via
  Reachability Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mehdi Zadem, Sergio Mover, Sao Mai Nguyen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-12 06:53:11</h6>
<p class='card-text'>Open-ended learning benefits immensely from the use of symbolic methods for
goal representation as they offer ways to structure knowledge for efficient and
transferable learning. However, the existing Hierarchical Reinforcement
Learning (HRL) approaches relying on symbolic reasoning are often limited as
they require a manual goal representation. The challenge in autonomously
discovering a symbolic goal representation is that it must preserve critical
information, such as the environment dynamics. In this work, we propose a
developmental mechanism for subgoal discovery via an emergent representation
that abstracts (i.e., groups together) sets of environment states that have
similar roles in the task. We create a HRL algorithm that gradually learns this
representation along with the policies and evaluate it on navigation tasks to
show the learned representation is interpretable and results in data
efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2309.01099v1' target='_blank'>Enhancing Infrared Small Target Detection Robustness with Bi-Level
  Adversarial Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhu Liu, Zihang Chen, Jinyuan Liu, Long Ma, Xin Fan, Risheng Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-09-03 06:35:07</h6>
<p class='card-text'>The detection of small infrared targets against blurred and cluttered
backgrounds has remained an enduring challenge. In recent years, learning-based
schemes have become the mainstream methodology to establish the mapping
directly. However, these methods are susceptible to the inherent complexities
of changing backgrounds and real-world disturbances, leading to unreliable and
compromised target estimations. In this work, we propose a bi-level adversarial
framework to promote the robustness of detection in the presence of distinct
corruptions. We first propose a bi-level optimization formulation to introduce
dynamic adversarial learning. Specifically, it is composited by the learnable
generation of corruptions to maximize the losses as the lower-level objective
and the robustness promotion of detectors as the upper-level one. We also
provide a hierarchical reinforced learning strategy to discover the most
detrimental corruptions and balance the performance between robustness and
accuracy. To better disentangle the corruptions from salient features, we also
propose a spatial-frequency interaction network for target detection. Extensive
experiments demonstrate our scheme remarkably improves 21.96% IOU across a wide
array of corruptions and notably promotes 4.97% IOU on the general benchmark.
The source codes are available at https://github.com/LiuZhu-CV/BALISTD.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.14311v3' target='_blank'>Spread Control Method on Unknown Networks Based on Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenxiang Dong, Zhanjiang Chen, H. Vicky Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-28 05:29:49</h6>
<p class='card-text'>Epidemics such as COVID-19 pose serious threats to public health and our
society, and it is critical to investigate effective methods to control the
spread of epidemics over networks. Prior works on epidemic control often assume
complete knowledge of network structures, a presumption seldom valid in
real-world situations. In this paper, we study epidemic control on networks
with unknown structures, and propose a hierarchical reinforcement learning
framework for joint network structure exploration and epidemic control. To
reduce the action space and achieve computation tractability, our proposed
framework contains three modules: the Policy Selection Module, which determines
whether to explore the structure or remove nodes to control the epidemic; the
Explore Module, responsible for selecting nodes to explore; and the Remove
Module, which decides which nodes to remove to stop the epidemic spread.
Simulation results show that our proposed method outperforms baseline methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.13202v1' target='_blank'>Joint Band Assignment and Beam Management using Hierarchical
  Reinforcement Learning for Multi-Band Communication</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dohyun Kim, Miguel R. Castellanos, Robert W. Heath Jr</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-25 06:53:32</h6>
<p class='card-text'>Multi-band operation in wireless networks can improve data rates by
leveraging the benefits of propagation in different frequency ranges.
Distinctive beam management procedures in different bands complicate band
assignment because they require considering not only the channel quality but
also the associated beam management overhead. Reinforcement learning (RL) is a
promising approach for multi-band operation as it enables the system to learn
and adjust its behavior through environmental feedback. In this paper, we
formulate a sequential decision problem to jointly perform band assignment and
beam management. We propose a method based on hierarchical RL (HRL) to handle
the complexity of the problem by separating the policies for band selection and
beam management. We evaluate the proposed HRL-based algorithm on a realistic
channel generated based on ray-tracing simulators. Our results show that the
proposed approach outperforms traditional RL approaches in terms of reduced
beam training overhead and increased data rates under a realistic vehicular
channel.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2308.00989v1' target='_blank'>Wasserstein Diversity-Enriched Regularizer for Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haorui Li, Jiaqi Liang, Linjing Li, Daniel Zeng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-08-02 07:45:24</h6>
<p class='card-text'>Hierarchical reinforcement learning composites subpolicies in different
hierarchies to accomplish complex tasks.Automated subpolicies discovery, which
does not depend on domain knowledge, is a promising approach to generating
subpolicies.However, the degradation problem is a challenge that existing
methods can hardly deal with due to the lack of consideration of diversity or
the employment of weak regularizers. In this paper, we propose a novel
task-agnostic regularizer called the Wasserstein Diversity-Enriched Regularizer
(WDER), which enlarges the diversity of subpolicies by maximizing the
Wasserstein distances among action distributions. The proposed WDER can be
easily incorporated into the loss function of existing methods to boost their
performance further.Experimental results demonstrate that our WDER improves
performance and sample efficiency in comparison with prior work without
modifying hyperparameters, which indicates the applicability and robustness of
the WDER.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.13415v1' target='_blank'>Communication-Efficient Orchestrations for URLLC Service via
  Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Shi, Milad Ganjalizadeh, Hossein Shokri Ghadikolaei, Marina Petrova</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-25 11:23:38</h6>
<p class='card-text'>Ultra-reliable low latency communications (URLLC) service is envisioned to
enable use cases with strict reliability and latency requirements in 5G. One
approach for enabling URLLC services is to leverage Reinforcement Learning (RL)
to efficiently allocate wireless resources. However, with conventional RL
methods, the decision variables (though being deployed at various network
layers) are typically optimized in the same control loop, leading to
significant practical limitations on the control loop's delay as well as
excessive signaling and energy consumption. In this paper, we propose a
multi-agent Hierarchical RL (HRL) framework that enables the implementation of
multi-level policies with different control loop timescales. Agents with faster
control loops are deployed closer to the base station, while the ones with
slower control loops are at the edge or closer to the core network providing
high-level guidelines for low-level actions. On a use case from the prior art,
with our HRL framework, we optimized the maximum number of retransmissions and
transmission power of industrial devices. Our extensive simulation results on
the factory automation scenario show that the HRL framework achieves better
performance as the baseline single-agent RL method, with significantly less
overhead of signal transmissions and delay compared to the one-agent RL
methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.12063v1' target='_blank'>Balancing Exploration and Exploitation in Hierarchical Reinforcement
  Learning via Latent Landmark Graphs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qingyang Zhang, Yiming Yang, Jingqing Ruan, Xuantang Xiong, Dengpeng Xing, Bo Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-22 12:10:23</h6>
<p class='card-text'>Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising
paradigm to address the exploration-exploitation dilemma in reinforcement
learning. It decomposes the source task into subgoal conditional subtasks and
conducts exploration and exploitation in the subgoal space. The effectiveness
of GCHRL heavily relies on subgoal representation functions and subgoal
selection strategy. However, existing works often overlook the temporal
coherence in GCHRL when learning latent subgoal representations and lack an
efficient subgoal selection strategy that balances exploration and
exploitation. This paper proposes HIerarchical reinforcement learning via
dynamically building Latent Landmark graphs (HILL) to overcome these
limitations. HILL learns latent subgoal representations that satisfy temporal
coherence using a contrastive representation learning objective. Based on these
representations, HILL dynamically builds latent landmark graphs and employs a
novelty measure on nodes and a utility measure on edges. Finally, HILL develops
a subgoal selection strategy that balances exploration and exploitation by
jointly considering both measures. Experimental results demonstrate that HILL
outperforms state-of-the-art baselines on continuous control tasks with sparse
rewards in sample efficiency and asymptotic performance. Our code is available
at https://github.com/papercode2022/HILL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.06742v2' target='_blank'>Vehicle Dispatching and Routing of On-Demand Intercity Ride-Pooling
  Services: A Multi-Agent Hierarchical Reinforcement Learning Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinhua Si, Fang He, Xi Lin, Xindi Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-13 13:31:01</h6>
<p class='card-text'>The integrated development of city clusters has given rise to an increasing
demand for intercity travel. Intercity ride-pooling service exhibits
considerable potential in upgrading traditional intercity bus services by
implementing demand-responsive enhancements. Nevertheless, its online
operations suffer the inherent complexities due to the coupling of vehicle
resource allocation among cities and pooled-ride vehicle routing. To tackle
these challenges, this study proposes a two-level framework designed to
facilitate online fleet management. Specifically, a novel multi-agent feudal
reinforcement learning model is proposed at the upper level of the framework to
cooperatively assign idle vehicles to different intercity lines, while the
lower level updates the routes of vehicles using an adaptive large neighborhood
search heuristic. Numerical studies based on the realistic dataset of Xiamen
and its surrounding cities in China show that the proposed framework
effectively mitigates the supply and demand imbalances, and achieves
significant improvement in both the average daily system profit and order
fulfillment ratio.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.06125v3' target='_blank'>Learning Hierarchical Interactive Multi-Object Search for Mobile
  Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fabian Schmalstieg, Daniel Honerkamp, Tim Welschehold, Abhinav Valada</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-12 12:25:33</h6>
<p class='card-text'>Existing object-search approaches enable robots to search through free
pathways, however, robots operating in unstructured human-centered environments
frequently also have to manipulate the environment to their needs. In this
work, we introduce a novel interactive multi-object search task in which a
robot has to open doors to navigate rooms and search inside cabinets and
drawers to find target objects. These new challenges require combining
manipulation and navigation skills in unexplored environments. We present
HIMOS, a hierarchical reinforcement learning approach that learns to compose
exploration, navigation, and manipulation skills. To achieve this, we design an
abstract high-level action space around a semantic map memory and leverage the
explored environment as instance navigation points. We perform extensive
experiments in simulation and the real world that demonstrate that, with
accurate perception, the decision making of HIMOS effectively transfers to new
environments in a zero-shot manner. It shows robustness to unseen subpolicies,
failures in their execution, and different robot kinematics. These capabilities
open the door to a wide range of downstream tasks across embodied AI and
real-world use cases.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.02754v1' target='_blank'>Intent-driven Intelligent Control and Orchestration in O-RAN Via
  Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Md Arafat Habib, Hao Zhou, Pedro Enrique Iturria-Rivera, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-06 03:26:11</h6>
<p class='card-text'>rApps and xApps need to be controlled and orchestrated well in the open radio
access network (O-RAN) so that they can deliver a guaranteed network
performance in a complex multi-vendor environment. This paper proposes a novel
intent-driven intelligent control and orchestration scheme based on
hierarchical reinforcement learning (HRL). The proposed scheme can orchestrate
multiple rApps or xApps according to the operator's intent of optimizing
certain key performance indicators (KPIs), such as throughput, energy
efficiency, and latency. Specifically, we propose a bi-level architecture with
a meta-controller and a controller. The meta-controller provides the target
performance in terms of KPIs, while the controller performs xApp orchestration
at the lower level. Our simulation results show that the proposed HRL-based
intent-driven xApp orchestration mechanism achieves 7.5% and 21.4% increase in
average system throughput with respect to two baselines, i.e., a single xApp
baseline and a non-machine learning-based algorithm, respectively. Similarly,
17.3% and 37.9% increase in energy efficiency are observed in comparison to the
same baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2307.02728v2' target='_blank'>Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andrew Levy, Sreehari Rammohan, Alessandro Allievi, Scott Niekum, George Konidaris</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-07-06 02:27:05</h6>
<p class='card-text'>General purpose agents will require large repertoires of skills. Empowerment
-- the maximum mutual information between skills and states -- provides a
pathway for learning large collections of distinct skills, but mutual
information is difficult to optimize. We introduce a new framework,
Hierarchical Empowerment, that makes computing empowerment more tractable by
integrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning.
Our framework makes two specific contributions. First, we introduce a new
variational lower bound on mutual information that can be used to compute
empowerment over short horizons. Second, we introduce a hierarchical
architecture for computing empowerment over exponentially longer time scales.
We verify the contributions of the framework in a series of simulated robotics
tasks. In a popular ant navigation domain, our four level agents are able to
learn skills that cover a surface area over two orders of magnitude larger than
prior work.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.17484v2' target='_blank'>Landmark Guided Active Exploration with State-specific Balance
  Coefficient</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fei Cui, Jiaojiao Fang, Mengke Yang, Guizhong Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-30 08:54:47</h6>
<p class='card-text'>Goal-conditioned hierarchical reinforcement learning (GCHRL) decomposes
long-horizon tasks into sub-tasks through a hierarchical framework and it has
demonstrated promising results across a variety of domains. However, the
high-level policy's action space is often excessively large, presenting a
significant challenge to effective exploration and resulting in potentially
inefficient training. In this paper, we design a measure of prospect for
sub-goals by planning in the goal space based on the goal-conditioned value
function. Building upon the measure of prospect, we propose a landmark-guided
exploration strategy by integrating the measures of prospect and novelty which
aims to guide the agent to explore efficiently and improve sample efficiency.
In order to dynamically consider the impact of prospect and novelty on
exploration, we introduce a state-specific balance coefficient to balance the
significance of prospect and novelty. The experimental results demonstrate that
our proposed exploration strategy significantly outperforms the baseline
methods across multiple tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.15968v1' target='_blank'>Action and Trajectory Planning for Urban Autonomous Driving with
  Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinyang Lu, Flint Xiaofeng Fan, Tianying Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-28 07:11:02</h6>
<p class='card-text'>Reinforcement Learning (RL) has made promising progress in planning and
decision-making for Autonomous Vehicles (AVs) in simple driving scenarios.
However, existing RL algorithms for AVs fail to learn critical driving skills
in complex urban scenarios. First, urban driving scenarios require AVs to
handle multiple driving tasks of which conventional RL algorithms are
incapable. Second, the presence of other vehicles in urban scenarios results in
a dynamically changing environment, which challenges RL algorithms to plan the
action and trajectory of the AV. In this work, we propose an action and
trajectory planner using Hierarchical Reinforcement Learning (atHRL) method,
which models the agent behavior in a hierarchical model by using the perception
of the lidar and birdeye view. The proposed atHRL method learns to make
decisions about the agent's future trajectory and computes target waypoints
under continuous settings based on a hierarchical DDPG algorithm. The waypoints
planned by the atHRL model are then sent to a low-level controller to generate
the steering and throttle commands required for the vehicle maneuver. We
empirically verify the efficacy of atHRL through extensive experiments in
complex urban driving scenarios that compose multiple tasks with the presence
of other vehicles in the CARLA simulator. The experimental results suggest a
significant performance improvement compared to the state-of-the-art RL
methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.11483v1' target='_blank'>Int-HRL: Towards Intention-based Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anna Penzkofer, Simon Schaefer, Florian Strohm, Mihai Bâce, Stefan Leutenegger, Andreas Bulling</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-20 12:12:16</h6>
<p class='card-text'>While deep reinforcement learning (RL) agents outperform humans on an
increasing number of tasks, training them requires data equivalent to decades
of human gameplay. Recent hierarchical RL methods have increased sample
efficiency by incorporating information inherent to the structure of the
decision problem but at the cost of having to discover or use human-annotated
sub-goals that guide the learning process. We show that intentions of human
players, i.e. the precursor of goal-oriented decisions, can be robustly
predicted from eye gaze even for the long-horizon sparse rewards task of
Montezuma's Revenge - one of the most challenging RL tasks in the Atari2600
game suite. We propose Int-HRL: Hierarchical RL with intention-based sub-goals
that are inferred from human eye gaze. Our novel sub-goal extraction pipeline
is fully automatic and replaces the need for manual sub-goal annotation by
human experts. Our evaluations show that replacing hand-crafted sub-goals with
automatically extracted intentions leads to a HRL agent that is significantly
more sample efficient than previous methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.10083v1' target='_blank'>Automatic Deduction Path Learning via Reinforcement Learning with
  Environmental Correction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuai Xiao, Chen Pan, Min Wang, Xinxin Zhu, Siqiao Xue, Jing Wang, Yunhua Hu, James Zhang, Jinghua Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-16 11:43:49</h6>
<p class='card-text'>Automatic bill payment is an important part of business operations in fintech
companies. The practice of deduction was mainly based on the total amount or
heuristic search by dividing the bill into smaller parts to deduct as much as
possible. This article proposes an end-to-end approach of automatically
learning the optimal deduction paths (deduction amount in order), which reduces
the cost of manual path design and maximizes the amount of successful
deduction. Specifically, in view of the large search space of the paths and the
extreme sparsity of historical successful deduction records, we propose a deep
hierarchical reinforcement learning approach which abstracts the action into a
two-level hierarchical space: an upper agent that determines the number of
steps of deductions each day and a lower agent that decides the amount of
deduction at each step. In such a way, the action space is structured via prior
knowledge and the exploration space is reduced. Moreover, the inherited
information incompleteness of the business makes the environment just partially
observable. To be precise, the deducted amounts indicate merely the lower
bounds of the available account balance. To this end, we formulate the problem
as a partially observable Markov decision problem (POMDP) and employ an
environment correction algorithm based on the characteristics of the business.
In the world's largest electronic payment business, we have verified the
effectiveness of this scheme offline and deployed it online to serve millions
of users.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.08388v3' target='_blank'>Skill-Critic: Refining Learned Skills for Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ce Hao, Catherine Weaver, Chen Tang, Kenta Kawamoto, Masayoshi Tomizuka, Wei Zhan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-14 09:24:32</h6>
<p class='card-text'>Hierarchical reinforcement learning (RL) can accelerate long-horizon
decision-making by temporally abstracting a policy into multiple levels.
Promising results in sparse reward environments have been seen with skills,
i.e. sequences of primitive actions. Typically, a skill latent space and policy
are discovered from offline data. However, the resulting low-level policy can
be unreliable due to low-coverage demonstrations or distribution shifts. As a
solution, we propose the Skill-Critic algorithm to fine-tune the low-level
policy in conjunction with high-level skill selection. Our Skill-Critic
algorithm optimizes both the low-level and high-level policies; these policies
are initialized and regularized by the latent space learned from offline
demonstrations to guide the parallel policy optimization. We validate
Skill-Critic in multiple sparse-reward RL environments, including a new
sparse-reward autonomous racing task in Gran Turismo Sport. The experiments
show that Skill-Critic's low-level policy fine-tuning and demonstration-guided
regularization are essential for good performance. Code and videos are
available at our website: https://sites.google.com/view/skill-critic.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.06394v6' target='_blank'>PEAR: Primitive Enabled Adaptive Relabeling for Boosting Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Utsav Singh, Vinay P. Namboodiri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-10 09:41:30</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) has the potential to solve complex
long horizon tasks using temporal abstraction and increased exploration.
However, hierarchical agents are difficult to train due to inherent
non-stationarity. We present primitive enabled adaptive relabeling (PEAR), a
two-phase approach where we first perform adaptive relabeling on a few expert
demonstrations to generate efficient subgoal supervision, and then jointly
optimize HRL agents by employing reinforcement learning (RL) and imitation
learning (IL). We perform theoretical analysis to bound the sub-optimality of
our approach and derive a joint optimization framework using RL and IL. Since
PEAR utilizes only a few expert demonstrations and considers minimal limiting
assumptions on the task structure, it can be easily integrated with typical
off-policy RL algorithms to produce a practical HRL approach. We perform
extensive experiments on challenging environments and show that PEAR is able to
outperform various hierarchical and non-hierarchical baselines and achieve upto
$80\%$ success rates in complex sparse robotic control tasks where other
baselines typically fail to show significant progress. We also perform
ablations to thoroughly analyse the importance of our various design choices.
Finally, we perform real world robotic experiments on complex tasks and
demonstrate that PEAR consistently outperforms the baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.01476v1' target='_blank'>Hierarchical Reinforcement Learning for Modeling User Novelty-Seeking
  Intent in Recommender Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pan Li, Yuyan Wang, Ed H. Chi, Minmin Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-02 12:02:23</h6>
<p class='card-text'>Recommending novel content, which expands user horizons by introducing them
to new interests, has been shown to improve users' long-term experience on
recommendation platforms \cite{chen2021values}. Users however are not
constantly looking to explore novel content. It is therefore crucial to
understand their novelty-seeking intent and adjust the recommendation policy
accordingly. Most existing literature models a user's propensity to choose
novel content or to prefer a more diverse set of recommendations at individual
interactions. Hierarchical structure, on the other hand, exists in a user's
novelty-seeking intent, which is manifested as a static and intrinsic user
preference for seeking novelty along with a dynamic session-based propensity.
To this end, we propose a novel hierarchical reinforcement learning-based
method to model the hierarchical user novelty-seeking intent, and to adapt the
recommendation policy accordingly based on the extracted user novelty-seeking
propensity. We further incorporate diversity and novelty-related measurement in
the reward function of the hierarchical RL (HRL) agent to encourage user
exploration \cite{chen2021values}. We demonstrate the benefits of explicitly
modeling hierarchical user novelty-seeking intent in recommendations through
extensive experiments on simulated and real-world datasets. In particular, we
demonstrate that the effectiveness of our proposed hierarchical RL-based method
lies in its ability to capture such hierarchically-structured intent. As a
result, the proposed HRL model achieves superior performance on several public
datasets, compared with state-of-art baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2306.00416v4' target='_blank'>Interactive Character Control with Auto-Regressive Motion Diffusion
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yi Shi, Jingbo Wang, Xuekun Jiang, Bingkun Lin, Bo Dai, Xue Bin Peng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-06-01 07:48:34</h6>
<p class='card-text'>Real-time character control is an essential component for interactive
experiences, with a broad range of applications, including physics simulations,
video games, and virtual reality. The success of diffusion models for image
synthesis has led to the use of these models for motion synthesis. However, the
majority of these motion diffusion models are primarily designed for offline
applications, where space-time models are used to synthesize an entire sequence
of frames simultaneously with a pre-specified length. To enable real-time
motion synthesis with diffusion model that allows time-varying controls, we
propose A-MDM (Auto-regressive Motion Diffusion Model). Our conditional
diffusion model takes an initial pose as input, and auto-regressively generates
successive motion frames conditioned on the previous frame. Despite its
streamlined network architecture, which uses simple MLPs, our framework is
capable of generating diverse, long-horizon, and high-fidelity motion
sequences. Furthermore, we introduce a suite of techniques for incorporating
interactive controls into A-MDM, such as task-oriented sampling, in-painting,
and hierarchical reinforcement learning. These techniques enable a pre-trained
A-MDM to be efficiently adapted for a variety of new downstream tasks. We
conduct a comprehensive suite of experiments to demonstrate the effectiveness
of A-MDM, and compare its performance against state-of-the-art auto-regressive
methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.19077v1' target='_blank'>DHRL-FNMR: An Intelligent Multicast Routing Approach Based on Deep
  Hierarchical Reinforcement Learning in SDN</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Miao Ye, Chenwei Zhao, Xingsi Xue, Jinqiang Li, Hongwen Hu, Yejin Yang, Qiuxiang Jiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-30 14:40:40</h6>
<p class='card-text'>The optimal multicast tree problem in the Software-Defined Networking (SDN)
multicast routing is an NP-hard combinatorial optimization problem. Although
existing SDN intelligent solution methods, which are based on deep
reinforcement learning, can dynamically adapt to complex network link state
changes, these methods are plagued by problems such as redundant branches,
large action space, and slow agent convergence. In this paper, an SDN
intelligent multicast routing algorithm based on deep hierarchical
reinforcement learning is proposed to circumvent the aforementioned problems.
First, the multicast tree construction problem is decomposed into two
sub-problems: the fork node selection problem and the construction of the
optimal path from the fork node to the destination node. Second, based on the
information characteristics of SDN global network perception, the multicast
tree state matrix, link bandwidth matrix, link delay matrix, link packet loss
rate matrix, and sub-goal matrix are designed as the state space of intrinsic
and meta controllers. Then, in order to mitigate the excessive action space,
our approach constructs different action spaces at the upper and lower levels.
The meta-controller generates an action space using network nodes to select the
fork node, and the intrinsic controller uses the adjacent edges of the current
node as its action space, thus implementing four different action selection
strategies in the construction of the multicast tree. To facilitate the
intelligent agent in constructing the optimal multicast tree with greater
speed, we developed alternative reward strategies that distinguish between
single-step node actions and multi-step actions towards multiple destination
nodes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.16708v1' target='_blank'>A Hierarchical Approach to Population Training for Human-AI
  Collaboration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yi Loo, Chen Gong, Malika Meghjani</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-26 07:53:12</h6>
<p class='card-text'>A major challenge for deep reinforcement learning (DRL) agents is to
collaborate with novel partners that were not encountered by them during the
training phase. This is specifically worsened by an increased variance in
action responses when the DRL agents collaborate with human partners due to the
lack of consistency in human behaviors. Recent work have shown that training a
single agent as the best response to a diverse population of training partners
significantly increases an agent's robustness to novel partners. We further
enhance the population-based training approach by introducing a Hierarchical
Reinforcement Learning (HRL) based method for Human-AI Collaboration. Our agent
is able to learn multiple best-response policies as its low-level policy while
at the same time, it learns a high-level policy that acts as a manager which
allows the agent to dynamically switch between the low-level best-response
policies based on its current partner. We demonstrate that our method is able
to dynamically adapt to novel partners of different play styles and skill
levels in the 2-player collaborative Overcooked game environment. We also
conducted a human study in the same environment to test the effectiveness of
our method when partnering with real human subjects.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2305.06936v1' target='_blank'>An Option-Dependent Analysis of Regret Minimization Algorithms in
  Finite-Horizon Semi-Markov Decision Processes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gianluca Drappo, Alberto Maria Metelli, Marcello Restelli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-05-10 15:00:05</h6>
<p class='card-text'>A large variety of real-world Reinforcement Learning (RL) tasks is
characterized by a complex and heterogeneous structure that makes end-to-end
(or flat) approaches hardly applicable or even infeasible. Hierarchical
Reinforcement Learning (HRL) provides general solutions to address these
problems thanks to a convenient multi-level decomposition of the tasks, making
their solution accessible. Although often used in practice, few works provide
theoretical guarantees to justify this outcome effectively. Thus, it is not yet
clear when to prefer such approaches compared to standard flat ones. In this
work, we provide an option-dependent upper bound to the regret suffered by
regret minimization algorithms in finite-horizon problems. We illustrate that
the performance improvement derives from the planning horizon reduction induced
by the temporal abstraction enforced by the hierarchical structure. Then,
focusing on a sub-setting of HRL approaches, the options framework, we
highlight how the average duration of the available options affects the
planning horizon and, consequently, the regret itself. Finally, we relax the
assumption of having pre-trained options to show how in particular situations,
learning hierarchically from scratch could be preferable to using a standard
approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2304.09395v1' target='_blank'>H-TSP: Hierarchically Solving the Large-Scale Travelling Salesman
  Problem</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuanhao Pan, Yan Jin, Yuandong Ding, Mingxiao Feng, Li Zhao, Lei Song, Jiang Bian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-04-19 03:10:30</h6>
<p class='card-text'>We propose an end-to-end learning framework based on hierarchical
reinforcement learning, called H-TSP, for addressing the large-scale Travelling
Salesman Problem (TSP). The proposed H-TSP constructs a solution of a TSP
instance starting from the scratch relying on two components: the upper-level
policy chooses a small subset of nodes (up to 200 in our experiment) from all
nodes that are to be traversed, while the lower-level policy takes the chosen
nodes as input and outputs a tour connecting them to the existing partial route
(initially only containing the depot). After jointly training the upper-level
and lower-level policies, our approach can directly generate solutions for the
given TSP instances without relying on any time-consuming search procedures. To
demonstrate effectiveness of the proposed approach, we have conducted extensive
experiments on randomly generated TSP instances with different numbers of
nodes. We show that H-TSP can achieve comparable results (gap 3.42% vs. 7.32%)
as SOTA search-based approaches, and more importantly, we reduce the time
consumption up to two orders of magnitude (3.32s vs. 395.85s). To the best of
our knowledge, H-TSP is the first end-to-end deep reinforcement learning
approach that can scale to TSP instances of up to 10000 nodes. Although there
are still gaps to SOTA results with respect to solution quality, we believe
that H-TSP will be useful for practical applications, particularly those that
are time-sensitive e.g., on-call routing and ride hailing service.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2304.07337v1' target='_blank'>Learning to Learn Group Alignment: A Self-Tuning Credo Framework with
  Multiagent Teams</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:David Radke, Kyle Tilbury</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-04-14 18:16:19</h6>
<p class='card-text'>Mixed incentives among a population with multiagent teams has been shown to
have advantages over a fully cooperative system; however, discovering the best
mixture of incentives or team structure is a difficult and dynamic problem. We
propose a framework where individual learning agents self-regulate their
configuration of incentives through various parts of their reward function.
This work extends previous work by giving agents the ability to dynamically
update their group alignment during learning and by allowing teammates to have
different group alignment. Our model builds on ideas from hierarchical
reinforcement learning and meta-learning to learn the configuration of a reward
function that supports the development of a behavioral policy. We provide
preliminary results in a commonly studied multiagent environment and find that
agents can achieve better global outcomes by self-tuning their respective group
alignment parameters.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2304.03535v5' target='_blank'>CRISP: Curriculum Inducing Primitive Informed Subgoal Prediction for
  Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Utsav Singh, Vinay P. Namboodiri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-04-07 08:22:50</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) is a promising approach that uses
temporal abstraction to solve complex long horizon problems. However,
simultaneously learning a hierarchy of policies is unstable as it is
challenging to train higher-level policy when the lower-level primitive is
non-stationary. In this paper, we present CRISP, a novel HRL algorithm that
effectively generates a curriculum of achievable subgoals for evolving
lower-level primitives using reinforcement learning and imitation learning.
CRISP uses the lower level primitive to periodically perform data relabeling on
a handful of expert demonstrations, using a novel primitive informed parsing
(PIP) approach, thereby mitigating non-stationarity. Since our approach only
assumes access to a handful of expert demonstrations, it is suitable for most
robotic control tasks. Experimental evaluations on complex robotic maze
navigation and robotic manipulation tasks demonstrate that inducing
hierarchical curriculum learning significantly improves sample efficiency, and
results in efficient goal conditioned policies for solving temporally extended
tasks. Additionally, we perform real world robotic experiments on complex
manipulation tasks and demonstrate that CRISP demonstrates impressive
generalization in real world scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2302.14451v1' target='_blank'>Hierarchical Reinforcement Learning in Complex 3D Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bernardo Avila Pires, Feryal Behbahani, Hubert Soyer, Kyriacos Nikiforou, Thomas Keck, Satinder Singh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-02-28 09:56:36</h6>
<p class='card-text'>Hierarchical Reinforcement Learning (HRL) agents have the potential to
demonstrate appealing capabilities such as planning and exploration with
abstraction, transfer, and skill reuse. Recent successes with HRL across
different domains provide evidence that practical, effective HRL agents are
possible, even if existing agents do not yet fully realize the potential of
HRL. Despite these successes, visually complex partially observable 3D
environments remained a challenge for HRL agents. We address this issue with
Hierarchical Hybrid Offline-Online (H2O2), a hierarchical deep reinforcement
learning agent that discovers and learns to use options from scratch using its
own experience. We show that H2O2 is competitive with a strong non-hierarchical
Muesli baseline in the DeepMind Hard Eight tasks and we shed new light on the
problem of learning hierarchical agents in complex environments. Our empirical
study of H2O2 reveals previously unnoticed practical challenges and brings new
perspective to the current understanding of hierarchical agents in complex
domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2302.10639v2' target='_blank'>Handling Long and Richly Constrained Tasks through Constrained
  Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxiao Lu, Arunesh Sinha, Pradeep Varakantham</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-02-21 12:57:12</h6>
<p class='card-text'>Safety in goal directed Reinforcement Learning (RL) settings has typically
been handled through constraints over trajectories and have demonstrated good
performance in primarily short horizon tasks. In this paper, we are
specifically interested in the problem of solving temporally extended decision
making problems such as robots cleaning different areas in a house while
avoiding slippery and unsafe areas (e.g., stairs) and retaining enough charge
to move to a charging dock; in the presence of complex safety constraints. Our
key contribution is a (safety) Constrained Search with Hierarchical
Reinforcement Learning (CoSHRL) mechanism that combines an upper level
constrained search agent (which computes a reward maximizing policy from a
given start to a far away goal state while satisfying cost constraints) with a
low-level goal conditioned RL agent (which estimates cost and reward values to
move between nearby states). A major advantage of CoSHRL is that it can handle
constraints on the cost value distribution (e.g., on Conditional Value at Risk,
CVaR) and can adjust to flexible constraint thresholds without retraining. We
perform extensive experiments with different types of safety constraints to
demonstrate the utility of our approach over leading approaches in constrained
and hierarchical RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2302.04094v1' target='_blank'>Learning Graph-Enhanced Commander-Executor for Multi-Agent Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinyi Yang, Shiyu Huang, Yiwen Sun, Yuxiang Yang, Chao Yu, Wei-Wei Tu, Huazhong Yang, Yu Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-02-08 14:44:21</h6>
<p class='card-text'>This paper investigates the multi-agent navigation problem, which requires
multiple agents to reach the target goals in a limited time. Multi-agent
reinforcement learning (MARL) has shown promising results for solving this
issue. However, it is inefficient for MARL to directly explore the (nearly)
optimal policy in the large search space, which is exacerbated as the agent
number increases (e.g., 10+ agents) or the environment is more complex (e.g.,
3D simulator). Goal-conditioned hierarchical reinforcement learning (HRL)
provides a promising direction to tackle this challenge by introducing a
hierarchical structure to decompose the search space, where the low-level
policy predicts primitive actions in the guidance of the goals derived from the
high-level policy. In this paper, we propose Multi-Agent Graph-Enhanced
Commander-Executor (MAGE-X), a graph-based goal-conditioned hierarchical method
for multi-agent navigation tasks. MAGE-X comprises a high-level Goal Commander
and a low-level Action Executor. The Goal Commander predicts the probability
distribution of goals and leverages them to assign each agent the most
appropriate final target. The Action Executor utilizes graph neural networks
(GNN) to construct a subgraph for each agent that only contains crucial
partners to improve cooperation. Additionally, the Goal Encoder in the Action
Executor captures the relationship between the agent and the designated goal to
encourage the agent to reach the final target. The results show that MAGE-X
outperforms the state-of-the-art MARL baselines with a 100% success rate with
only 3 million training steps in multi-agent particle environments (MPE) with
50 agents, and at least a 12% higher success rate and 2x higher data efficiency
in a more complicated quadrotor 3D navigation task.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2302.02179v2' target='_blank'>Developing Driving Strategies Efficiently: A Skill-Based Hierarchical
  Reinforcement Learning Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yigit Gurses, Kaan Buyukdemirci, Yildiray Yildiz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-02-04 15:09:51</h6>
<p class='card-text'>Driving in dense traffic with human and autonomous drivers is a challenging
task that requires high-level planning and reasoning. Human drivers can achieve
this task comfortably, and there has been many efforts to model human driver
strategies. These strategies can be used as inspirations for developing
autonomous driving algorithms or to create high-fidelity simulators.
Reinforcement learning is a common tool to model driver policies, but
conventional training of these models can be computationally expensive and
time-consuming. To address this issue, in this paper, we propose ``skill-based"
hierarchical driving strategies, where motion primitives, i.e. skills, are
designed and used as high-level actions. This reduces the training time for
applications that require multiple models with varying behavior. Simulation
results in a merging scenario demonstrate that the proposed approach yields
driver models that achieve higher performance with less training compared to
baseline reinforcement learning methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2301.10724v2' target='_blank'>Select and Trade: Towards Unified Pair Trading with Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weiguang Han, Boyi Zhang, Qianqian Xie, Min Peng, Yanzhao Lai, Jimin Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-01-25 17:32:21</h6>
<p class='card-text'>Pair trading is one of the most effective statistical arbitrage strategies
which seeks a neutral profit by hedging a pair of selected assets. Existing
methods generally decompose the task into two separate steps: pair selection
and trading. However, the decoupling of two closely related subtasks can block
information propagation and lead to limited overall performance. For pair
selection, ignoring the trading performance results in the wrong assets being
selected with irrelevant price movements, while the agent trained for trading
can overfit to the selected assets without any historical information of other
assets. To address it, in this paper, we propose a paradigm for automatic pair
trading as a unified task rather than a two-step pipeline. We design a
hierarchical reinforcement learning framework to jointly learn and optimize two
subtasks. A high-level policy would select two assets from all possible
combinations and a low-level policy would then perform a series of trading
actions. Experimental results on real-world stock data demonstrate the
effectiveness of our method on pair trading compared with both existing pair
selection and trading methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2301.07818v1' target='_blank'>Hierarchical Reinforcement Learning Based Traffic Steering in Multi-RAT
  5G Deployments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Md Arafat Habib, Hao Zhou, Pedro Enrique Iturria-Rivera, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-01-18 23:05:58</h6>
<p class='card-text'>In 5G non-standalone mode, an intelligent traffic steering mechanism can
vastly aid in ensuring smooth user experience by selecting the best radio
access technology (RAT) from a multi-RAT environment for a specific traffic
flow. In this paper, we propose a novel load-aware traffic steering algorithm
based on hierarchical reinforcement learning (HRL) while satisfying diverse QoS
requirements of different traffic types. HRL can significantly increase system
performance using a bi-level architecture having a meta-controller and a
controller. In our proposed method, the meta-controller provides an appropriate
threshold for load balancing, while the controller performs traffic admission
to an appropriate RAT in the lower level. Simulation results show that HRL
outperforms a Deep Q-Learning (DQN) and a threshold-based heuristic baseline
with 8.49%, 12.52% higher average system throughput and 27.74%, 39.13% lower
network delay, respectively.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2301.02771v1' target='_blank'>Hierarchical Reinforcement Learning for RIS-Assisted Energy-Efficient
  RAN</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Zhou, Long Kong, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Steve Furr, Melike Erol-Kantarci</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2023-01-07 03:00:23</h6>
<p class='card-text'>Reconfigurable intelligent surface (RIS) is emerging as a promising
technology to boost the energy efficiency (EE) of 5G beyond and 6G networks.
Inspired by this potential, in this paper, we investigate the RIS-assisted
energy-efficient radio access networks (RAN). In particular, we combine RIS
with sleep control techniques, and develop a hierarchical reinforcement
learning (HRL) algorithm for network management. In HRL, the meta-controller
decides the on/off status of the small base stations (SBSs) in heterogeneous
networks, while the sub-controller can change the transmission power levels of
SBSs to save energy. The simulations show that the RIS-assisted sleep control
can achieve significantly lower energy consumption, higher throughput, and more
than doubled energy efficiency than no-RIS conditions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2212.12786v1' target='_blank'>SHIRO: Soft Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kandai Watanabe, Mathew Strong, Omer Eldar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-12-24 17:21:58</h6>
<p class='card-text'>Hierarchical Reinforcement Learning (HRL) algorithms have been demonstrated
to perform well on high-dimensional decision making and robotic control tasks.
However, because they solely optimize for rewards, the agent tends to search
the same space redundantly. This problem reduces the speed of learning and
achieved reward. In this work, we present an Off-Policy HRL algorithm that
maximizes entropy for efficient exploration. The algorithm learns a temporally
abstracted low-level policy and is able to explore broadly through the addition
of entropy to the high-level. The novelty of this work is the theoretical
motivation of adding entropy to the RL objective in the HRL setting. We
empirically show that the entropy can be added to both levels if the
Kullback-Leibler (KL) divergence between consecutive updates of the low-level
policy is sufficiently small. We performed an ablative study to analyze the
effects of entropy on hierarchy, in which adding entropy to high-level emerged
as the most desirable configuration. Furthermore, a higher temperature in the
low-level leads to Q-value overestimation and increases the stochasticity of
the environment that the high-level operates on, making learning more
challenging. Our method, SHIRO, surpasses state-of-the-art performance on a
range of simulated robotic control benchmark tasks and requires minimal tuning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2212.10690v1' target='_blank'>METEOR Guided Divergence for Video Captioning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel Lukas Rothenpieler, Shahin Amiriparian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-12-20 23:30:47</h6>
<p class='card-text'>Automatic video captioning aims for a holistic visual scene understanding. It
requires a mechanism for capturing temporal context in video frames and the
ability to comprehend the actions and associations of objects in a given
timeframe. Such a system should additionally learn to abstract video sequences
into sensible representations as well as to generate natural written language.
While the majority of captioning models focus solely on the visual inputs,
little attention has been paid to the audiovisual modality. To tackle this
issue, we propose a novel two-fold approach. First, we implement a
reward-guided KL Divergence to train a video captioning model which is
resilient towards token permutations. Second, we utilise a Bi-Modal
Hierarchical Reinforcement Learning (BMHRL) Transformer architecture to capture
long-term temporal dependencies of the input data as a foundation for our
hierarchical captioning module. Using our BMHRL, we show the suitability of the
HRL agent in the generation of content-complete and grammatically sound
sentences by achieving $4.91$, $2.23$, and $10.80$ in BLEU3, BLEU4, and METEOR
scores, respectively on the ActivityNet Captions dataset. Finally, we make our
BMHRL framework and trained models publicly available for users and developers
at https://github.com/d-rothen/bmhrl.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2212.06967v1' target='_blank'>Explaining Agent's Decision-making in a Hierarchical Reinforcement
  Learning Scenario</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hugo Muñoz, Ernesto Portugal, Angel Ayala, Bruno Fernandes, Francisco Cruz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-12-14 01:18:45</h6>
<p class='card-text'>Reinforcement learning is a machine learning approach based on behavioral
psychology. It is focused on learning agents that can acquire knowledge and
learn to carry out new tasks by interacting with the environment. However, a
problem occurs when reinforcement learning is used in critical contexts where
the users of the system need to have more information and reliability for the
actions executed by an agent. In this regard, explainable reinforcement
learning seeks to provide to an agent in training with methods in order to
explain its behavior in such a way that users with no experience in machine
learning could understand the agent's behavior. One of these is the
memory-based explainable reinforcement learning method that is used to compute
probabilities of success for each state-action pair using an episodic memory.
In this work, we propose to make use of the memory-based explainable
reinforcement learning method in a hierarchical environment composed of
sub-tasks that need to be first addressed to solve a more complex task. The end
goal is to verify if it is possible to provide to the agent the ability to
explain its actions in the global task as well as in the sub-tasks. The results
obtained showed that it is possible to use the memory-based method in
hierarchical environments with high-level tasks and compute the probabilities
of success to be used as a basis for explaining the agent's behavior.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2212.14670v1' target='_blank'>Hierarchical Deep Reinforcement Learning for VWAP Strategy Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaodong Li, Pangjing Wu, Chenxin Zou, Qing Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-12-11 07:35:26</h6>
<p class='card-text'>Designing an intelligent volume-weighted average price (VWAP) strategy is a
critical concern for brokers, since traditional rule-based strategies are
relatively static that cannot achieve a lower transaction cost in a dynamic
market. Many studies have tried to minimize the cost via reinforcement
learning, but there are bottlenecks in improvement, especially for
long-duration strategies such as the VWAP strategy. To address this issue, we
propose a deep learning and hierarchical reinforcement learning jointed
architecture termed Macro-Meta-Micro Trader (M3T) to capture market patterns
and execute orders from different temporal scales. The Macro Trader first
allocates a parent order into tranches based on volume profiles as the
traditional VWAP strategy does, but a long short-term memory neural network is
used to improve the forecasting accuracy. Then the Meta Trader selects a
short-term subgoal appropriate to instant liquidity within each tranche to form
a mini-tranche. The Micro Trader consequently extracts the instant market state
and fulfils the subgoal with the lowest transaction cost. Our experiments over
stocks listed on the Shanghai stock exchange demonstrate that our approach
outperforms baselines in terms of VWAP slippage, with an average cost saving of
1.16 base points compared to the optimal baseline.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2212.01763v1' target='_blank'>Learning Bifunctional Push-grasping Synergistic Strategy for
  Goal-agnostic and Goal-oriented Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dafa Ren, Shuang Wu, Xiaofan Wang, Yan Peng, Xiaoqiang Ren</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-12-04 08:16:04</h6>
<p class='card-text'>Both goal-agnostic and goal-oriented tasks have practical value for robotic
grasping: goal-agnostic tasks target all objects in the workspace, while
goal-oriented tasks aim at grasping pre-assigned goal objects. However, most
current grasping methods are only better at coping with one task. In this work,
we propose a bifunctional push-grasping synergistic strategy for goal-agnostic
and goal-oriented grasping tasks. Our method integrates pushing along with
grasping to pick up all objects or pre-assigned goal objects with high action
efficiency depending on the task requirement. We introduce a bifunctional
network, which takes in visual observations and outputs dense pixel-wise maps
of Q values for pushing and grasping primitive actions, to increase the
available samples in the action space. Then we propose a hierarchical
reinforcement learning framework to coordinate the two tasks by considering the
goal-agnostic task as a combination of multiple goal-oriented tasks. To reduce
the training difficulty of the hierarchical framework, we design a two-stage
training method to train the two types of tasks separately. We perform
pre-training of the model in simulation, and then transfer the learned model to
the real world without any additional real-world fine-tuning. Experimental
results show that the proposed approach outperforms existing methods in task
completion rate and grasp success rate with less motion number. Supplementary
material is available at https:
//github.com/DafaRen/Learning_Bifunctional_Push-grasping_Synergistic_Strategy_for_Goal-agnostic_and_Goal-oriented_Tasks</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2211.09382v1' target='_blank'>Planning Irregular Object Packing via Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sichao Huang, Ziwei Wang, Jie Zhou, Jiwen Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-11-17 07:16:37</h6>
<p class='card-text'>Object packing by autonomous robots is an im-portant challenge in warehouses
and logistics industry. Most conventional data-driven packing planning
approaches focus on regular cuboid packing, which are usually heuristic and
limit the practical use in realistic applications with everyday objects. In
this paper, we propose a deep hierarchical reinforcement learning approach to
simultaneously plan packing sequence and placement for irregular object
packing. Specifically, the top manager network infers packing sequence from six
principal view heightmaps of all objects, and then the bottom worker network
receives heightmaps of the next object to predict the placement position and
orientation. The two networks are trained hierarchically in a self-supervised
Q-Learning framework, where the rewards are provided by the packing results
based on the top height , object volume and placement stability in the box. The
framework repeats sequence and placement planning iteratively until all objects
have been packed into the box or no space is remained for unpacked items. We
compare our approach with existing robotic packing methods for irregular
objects in a physics simulator. Experiments show that our approach can pack
more objects with less time cost than the state-of-the-art packing methods of
irregular objects. We also implement our packing plan with a robotic
manipulator to show the generalization ability in the real world.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2211.06351v1' target='_blank'>Emergency action termination for immediate reaction in hierarchical
  reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Michał Bortkiewicz, Jakub Łyskawa, Paweł Wawrzyński, Mateusz Ostaszewski, Artur Grudkowski, Tomasz Trzciński</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-11-11 16:56:02</h6>
<p class='card-text'>Hierarchical decomposition of control is unavoidable in large dynamical
systems. In reinforcement learning (RL), it is usually solved with subgoals
defined at higher policy levels and achieved at lower policy levels. Reaching
these goals can take a substantial amount of time, during which it is not
verified whether they are still worth pursuing. However, due to the randomness
of the environment, these goals may become obsolete. In this paper, we address
this gap in the state-of-the-art approaches and propose a method in which the
validity of higher-level actions (thus lower-level goals) is constantly
verified at the higher level. If the actions, i.e. lower level goals, become
inadequate, they are replaced by more appropriate ones. This way we combine the
advantages of hierarchical RL, which is fast training, and flat RL, which is
immediate reactivity. We study our approach experimentally on seven benchmark
environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2210.10431v1' target='_blank'>Hierarchical Reinforcement Learning for Furniture Layout in Virtual
  Indoor Scenes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinhan Di, Pengqian Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-10-19 09:58:10</h6>
<p class='card-text'>In real life, the decoration of 3D indoor scenes through designing furniture
layout provides a rich experience for people. In this paper, we explore the
furniture layout task as a Markov decision process (MDP) in virtual reality,
which is solved by hierarchical reinforcement learning (HRL). The goal is to
produce a proper two-furniture layout in the virtual reality of the indoor
scenes. In particular, we first design a simulation environment and introduce
the HRL formulation for a two-furniture layout. We then apply a hierarchical
actor-critic algorithm with curriculum learning to solve the MDP. We conduct
our experiments on a large-scale real-world interior layout dataset that
contains industrial designs from professional designers. Our numerical results
demonstrate that the proposed model yields higher-quality layouts as compared
with the state-of-art models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2210.07940v1' target='_blank'>AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sudipta Paul, Amit K. Roy-Chowdhury, Anoop Cherian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-10-14 16:35:06</h6>
<p class='card-text'>Recent years have seen embodied visual navigation advance in two distinct
directions: (i) in equipping the AI agent to follow natural language
instructions, and (ii) in making the navigable world multimodal, e.g.,
audio-visual navigation. However, the real world is not only multimodal, but
also often complex, and thus in spite of these advances, agents still need to
understand the uncertainty in their actions and seek instructions to navigate.
To this end, we present AVLEN~ -- an interactive agent for
Audio-Visual-Language Embodied Navigation. Similar to audio-visual navigation
tasks, the goal of our embodied agent is to localize an audio event via
navigating the 3D visual world; however, the agent may also seek help from a
human (oracle), where the assistance is provided in free-form natural language.
To realize these abilities, AVLEN uses a multimodal hierarchical reinforcement
learning backbone that learns: (a) high-level policies to choose either
audio-cues for navigation or to query the oracle, and (b) lower-level policies
to select navigation actions based on its audio-visual and language inputs. The
policies are trained via rewarding for the success on the navigation task while
minimizing the number of queries to the oracle. To empirically evaluate AVLEN,
we present experiments on the SoundSpaces framework for semantic audio-visual
navigation tasks. Our results show that equipping the agent to ask for help
leads to a clear improvement in performance, especially in challenging cases,
e.g., when the sound is unheard during training or in the presence of
distractor sounds.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2210.06964v1' target='_blank'>Causality-driven Hierarchical Structure Discovery for Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shaohui Peng, Xing Hu, Rui Zhang, Ke Tang, Jiaming Guo, Qi Yi, Ruizhi Chen, Xishan Zhang, Zidong Du, Ling Li, Qi Guo, Yunji Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-10-13 12:42:48</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) effectively improves agents'
exploration efficiency on tasks with sparse reward, with the guide of
high-quality hierarchical structures (e.g., subgoals or options). However, how
to automatically discover high-quality hierarchical structures is still a great
challenge. Previous HRL methods can hardly discover the hierarchical structures
in complex environments due to the low exploration efficiency by exploiting the
randomness-driven exploration paradigm. To address this issue, we propose
CDHRL, a causality-driven hierarchical reinforcement learning framework,
leveraging a causality-driven discovery instead of a randomness-driven
exploration to effectively build high-quality hierarchical structures in
complicated environments. The key insight is that the causalities among
environment variables are naturally fit for modeling reachable subgoals and
their dependencies and can perfectly guide to build high-quality hierarchical
structures. The results in two complex environments, 2D-Minecraft and Eden,
show that CDHRL significantly boosts exploration efficiency with the
causality-driven paradigm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2210.05150v3' target='_blank'>DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Seungjae Lee, Jigang Kim, Inkyu Jang, H. Jin Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-10-11 05:09:34</h6>
<p class='card-text'>Hierarchical Reinforcement Learning (HRL) has made notable progress in
complex control tasks by leveraging temporal abstraction. However, previous HRL
algorithms often suffer from serious data inefficiency as environments get
large. The extended components, $i.e.$, goal space and length of episodes,
impose a burden on either one or both high-level and low-level policies since
both levels share the total horizon of the episode. In this paper, we present a
method of Decoupling Horizons Using a Graph in Hierarchical Reinforcement
Learning (DHRL) which can alleviate this problem by decoupling the horizons of
high-level and low-level policies and bridging the gap between the length of
both horizons using a graph. DHRL provides a freely stretchable high-level
action interval, which facilitates longer temporal abstraction and faster
training in complex tasks. Our method outperforms state-of-the-art HRL
algorithms in typical HRL environments. Moreover, DHRL achieves long and
complex locomotion and manipulation tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2210.00795v2' target='_blank'>Hierarchical reinforcement learning for in-hand robotic manipulation
  using Davenport chained rotations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Francisco Roldan Sanchez, Qiang Wang, David Cordova Bulens, Kevin McGuinness, Stephen Redmond, Noel O'Connor</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-10-03 10:05:07</h6>
<p class='card-text'>End-to-end reinforcement learning techniques are among the most successful
methods for robotic manipulation tasks. However, the training time required to
find a good policy capable of solving complex tasks is prohibitively large.
Therefore, depending on the computing resources available, it might not be
feasible to use such techniques. The use of domain knowledge to decompose
manipulation tasks into primitive skills, to be performed in sequence, could
reduce the overall complexity of the learning problem, and hence reduce the
amount of training required to achieve dexterity. In this paper, we propose the
use of Davenport chained rotations to decompose complex 3D rotation goals into
a concatenation of a smaller set of more simple rotation skills.
State-of-the-art reinforcement-learning-based methods can then be trained using
less overall simulated experience. We compare its performance with the popular
Hindsight Experience Replay method, trained in an end-to-end fashion using the
same amount of experience in a simulated robotic hand environment. Despite a
general decrease in performance of the primitive skills when being sequentially
executed, we find that decomposing arbitrary 3D rotations into elementary
rotations is beneficial when computing resources are limited, obtaining
increases of success rates of approximately 10% on the most complex 3D
rotations with respect to the success rates obtained by HER trained in an
end-to-end fashion, and increases of success rates between 20% and 40% on the
most simple rotations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2209.08112v1' target='_blank'>Optimizing Industrial HVAC Systems with Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:William Wong, Praneet Dutta, Octavian Voicu, Yuri Chervonyi, Cosmin Paduraru, Jerry Luo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-09-16 18:00:46</h6>
<p class='card-text'>Reinforcement learning (RL) techniques have been developed to optimize
industrial cooling systems, offering substantial energy savings compared to
traditional heuristic policies. A major challenge in industrial control
involves learning behaviors that are feasible in the real world due to
machinery constraints. For example, certain actions can only be executed every
few hours while other actions can be taken more frequently. Without extensive
reward engineering and experimentation, an RL agent may not learn realistic
operation of machinery. To address this, we use hierarchical reinforcement
learning with multiple agents that control subsets of actions according to
their operation time scales. Our hierarchical approach achieves energy savings
over existing baselines while maintaining constraints such as operating
chillers within safe bounds in a simulated HVAC control environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2209.07368v1' target='_blank'>Causal Coupled Mechanisms: A Control Method with Cooperation and
  Competition for Complex System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuehui Yu, Jingchi Jiang, Xinmiao Yu, Yi Guan, Xue Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-09-15 15:32:16</h6>
<p class='card-text'>Complex systems are ubiquitous in the real world and tend to have complicated
and poorly understood dynamics. For their control issues, the challenge is to
guarantee accuracy, robustness, and generalization in such bloated and troubled
environments. Fortunately, a complex system can be divided into multiple
modular structures that human cognition appears to exploit. Inspired by this
cognition, a novel control method, Causal Coupled Mechanisms (CCMs), is
proposed that explores the cooperation in division and competition in
combination. Our method employs the theory of hierarchical reinforcement
learning (HRL), in which 1) the high-level policy with competitive awareness
divides the whole complex system into multiple functional mechanisms, and 2)
the low-level policy finishes the control task of each mechanism. Specifically
for cooperation, a cascade control module helps the series operation of CCMs,
and a forward coupled reasoning module is used to recover the coupling
information lost in the division process. On both synthetic systems and a
real-world biological regulatory system, the CCM method achieves robust and
state-of-the-art control results even with unpredictable random noise.
Moreover, generalization results show that reusing prepared specialized CCMs
helps to perform well in environments with different confounders and dynamics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2208.12433v1' target='_blank'>Towards Automated Imbalanced Learning with Deep Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daochen Zha, Kwei-Herng Lai, Qiaoyu Tan, Sirui Ding, Na Zou, Xia Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-08-26 04:28:01</h6>
<p class='card-text'>Imbalanced learning is a fundamental challenge in data mining, where there is
a disproportionate ratio of training samples in each class. Over-sampling is an
effective technique to tackle imbalanced learning through generating synthetic
samples for the minority class. While numerous over-sampling algorithms have
been proposed, they heavily rely on heuristics, which could be sub-optimal
since we may need different sampling strategies for different datasets and base
classifiers, and they cannot directly optimize the performance metric.
Motivated by this, we investigate developing a learning-based over-sampling
algorithm to optimize the classification performance, which is a challenging
task because of the huge and hierarchical decision space. At the high level, we
need to decide how many synthetic samples to generate. At the low level, we
need to determine where the synthetic samples should be located, which depends
on the high-level decision since the optimal locations of the samples may
differ for different numbers of samples. To address the challenges, we propose
AutoSMOTE, an automated over-sampling algorithm that can jointly optimize
different levels of decisions. Motivated by the success of
SMOTE~\cite{chawla2002smote} and its extensions, we formulate the generation
process as a Markov decision process (MDP) consisting of three levels of
policies to generate synthetic samples within the SMOTE search space. Then we
leverage deep hierarchical reinforcement learning to optimize the performance
metric on the validation data. Extensive experiments on six real-world datasets
demonstrate that AutoSMOTE significantly outperforms the state-of-the-art
resampling algorithms. The code is at https://github.com/daochenzha/autosmote</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2208.11529v1' target='_blank'>Hierarchical Reinforcement Learning Based Video Semantic Coding for
  Segmentation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guangqi Xie, Xin Li, Shiqi Lin, Li Zhang, Kai Zhang, Yue Li, Zhibo Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-08-24 13:22:22</h6>
<p class='card-text'>The rapid development of intelligent tasks, e.g., segmentation, detection,
classification, etc, has brought an urgent need for semantic compression, which
aims to reduce the compression cost while maintaining the original semantic
information. However, it is impractical to directly integrate the semantic
metric into the traditional codecs since they cannot be optimized in an
end-to-end manner. To solve this problem, some pioneering works have applied
reinforcement learning to implement image-wise semantic compression.
Nevertheless, video semantic compression has not been explored since its
complex reference architectures and compression modes. In this paper, we take a
step forward to video semantic compression and propose the Hierarchical
Reinforcement Learning based task-driven Video Semantic Coding, named as
HRLVSC. Specifically, to simplify the complex mode decision of video semantic
coding, we divided the action space into frame-level and CTU-level spaces in a
hierarchical manner, and then explore the best mode selection for them
progressively with the cooperation of frame-level and CTU-level agents.
Moreover, since the modes of video semantic coding will exponentially increase
with the number of frames in a Group of Pictures (GOP), we carefully
investigate the effects of different mode selections for video semantic coding
and design a simple but effective mode simplification strategy for it. We have
validated our HRLVSC on the video segmentation task with HEVC reference
software HM16.19. Extensive experimental results demonstrated that our HRLVSC
can achieve over 39% BD-rate saving for video semantic coding under the Low
Delay P configuration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2208.08731v1' target='_blank'>Intelligent problem-solving as integrated hierarchical reinforcement
  learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Manfred Eppe, Christian Gumbsch, Matthias Kerzel, Phuong D. H. Nguyen, Martin V. Butz, Stefan Wermter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-08-18 09:28:03</h6>
<p class='card-text'>According to cognitive psychology and related disciplines, the development of
complex problem-solving behaviour in biological agents depends on hierarchical
cognitive mechanisms. Hierarchical reinforcement learning is a promising
computational approach that may eventually yield comparable problem-solving
behaviour in artificial agents and robots. However, to date the problem-solving
abilities of many human and non-human animals are clearly superior to those of
artificial systems. Here, we propose steps to integrate biologically inspired
hierarchical mechanisms to enable advanced problem-solving skills in artificial
agents. Therefore, we first review the literature in cognitive psychology to
highlight the importance of compositional abstraction and predictive
processing. Then we relate the gained insights with contemporary hierarchical
reinforcement learning methods. Interestingly, our results suggest that all
identified cognitive mechanisms have been implemented individually in isolated
computational architectures, raising the question of why there exists no single
unifying architecture that integrates them. As our final contribution, we
address this question by providing an integrative perspective on the
computational challenges to develop such a unifying architecture. We expect our
results to guide the development of more sophisticated cognitively inspired
hierarchical machine learning architectures.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2208.04833v1' target='_blank'>From Scratch to Sketch: Deep Decoupled Hierarchical Reinforcement
  Learning for Robotic Sketching Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ganghun Lee, Minji Kim, Minsu Lee, Byoung-Tak Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-08-09 15:18:55</h6>
<p class='card-text'>We present an automated learning framework for a robotic sketching agent that
is capable of learning stroke-based rendering and motor control simultaneously.
We formulate the robotic sketching problem as a deep decoupled hierarchical
reinforcement learning; two policies for stroke-based rendering and motor
control are learned independently to achieve sub-tasks for drawing, and form a
hierarchy when cooperating for real-world drawing. Without hand-crafted
features, drawing sequences or trajectories, and inverse kinematics, the
proposed method trains the robotic sketching agent from scratch. We performed
experiments with a 6-DoF robot arm with 2F gripper to sketch doodles. Our
experimental results show that the two policies successfully learned the
sub-tasks and collaborated to sketch the target images. Also, the robustness
and flexibility were examined by varying drawing tools and surfaces.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2208.01160v1' target='_blank'>Hierarchical Reinforcement Learning for Precise Soccer Shooting Skills
  using a Quadrupedal Robot</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yandong Ji, Zhongyu Li, Yinan Sun, Xue Bin Peng, Sergey Levine, Glen Berseth, Koushil Sreenath</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-08-01 22:34:51</h6>
<p class='card-text'>We address the problem of enabling quadrupedal robots to perform precise
shooting skills in the real world using reinforcement learning. Developing
algorithms to enable a legged robot to shoot a soccer ball to a given target is
a challenging problem that combines robot motion control and planning into one
task. To solve this problem, we need to consider the dynamics limitation and
motion stability during the control of a dynamic legged robot. Moreover, we
need to consider motion planning to shoot the hard-to-model deformable ball
rolling on the ground with uncertain friction to a desired location. In this
paper, we propose a hierarchical framework that leverages deep reinforcement
learning to train (a) a robust motion control policy that can track arbitrary
motions and (b) a planning policy to decide the desired kicking motion to shoot
a soccer ball to a target. We deploy the proposed framework on an A1
quadrupedal robot and enable it to accurately shoot the ball to random targets
in the real world.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2207.12051v1' target='_blank'>Flowsheet synthesis through hierarchical reinforcement learning and
  graph neural networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Laura Stops, Roel Leenhouts, Qinghe Gao, Artur M. Schweidtmann</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-07-25 10:42:15</h6>
<p class='card-text'>Process synthesis experiences a disruptive transformation accelerated by
digitization and artificial intelligence. We propose a reinforcement learning
algorithm for chemical process design based on a state-of-the-art actor-critic
logic. Our proposed algorithm represents chemical processes as graphs and uses
graph convolutional neural networks to learn from process graphs. In
particular, the graph neural networks are implemented within the agent
architecture to process the states and make decisions. Moreover, we implement a
hierarchical and hybrid decision-making process to generate flowsheets, where
unit operations are placed iteratively as discrete decisions and corresponding
design variables are selected as continuous decisions. We demonstrate the
potential of our method to design economically viable flowsheets in an
illustrative case study comprising equilibrium reactions, azeotropic
separation, and recycles. The results show quick learning in discrete,
continuous, and hybrid action spaces. Due to the flexible architecture of the
proposed reinforcement learning agent, the method is predestined to include
large action-state spaces and an interface to process simulators in future
research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2207.11724v1' target='_blank'>Adaptive Decision Making at the Intersection for Autonomous Vehicles
  Based on Skill Discovery</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xianqi He, Lin Yang, Chao Lu, Zirui Li, Jianwei Gong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-07-24 11:56:45</h6>
<p class='card-text'>In urban environments, the complex and uncertain intersection scenarios are
challenging for autonomous driving. To ensure safety, it is crucial to develop
an adaptive decision making system that can handle the interaction with other
vehicles. Manually designed model-based methods are reliable in common
scenarios. But in uncertain environments, they are not reliable, so
learning-based methods are proposed, especially reinforcement learning (RL)
methods. However, current RL methods need retraining when the scenarios change.
In other words, current RL methods cannot reuse accumulated knowledge. They
forget learned knowledge when new scenarios are given. To solve this problem,
we propose a hierarchical framework that can autonomously accumulate and reuse
knowledge. The proposed method combines the idea of motion primitives (MPs)
with hierarchical reinforcement learning (HRL). It decomposes complex problems
into multiple basic subtasks to reduce the difficulty. The proposed method and
other baseline methods are tested in a challenging intersection scenario based
on the CARLA simulator. The intersection scenario contains three different
subtasks that can reflect the complexity and uncertainty of real traffic flow.
After offline learning and testing, the proposed method is proved to have the
best performance among all methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2207.05018v3' target='_blank'>Learning Temporally Extended Skills in Continuous Domains as Symbolic
  Actions for Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jan Achterhold, Markus Krimmel, Joerg Stueckler</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-07-11 17:13:10</h6>
<p class='card-text'>Problems which require both long-horizon planning and continuous control
capabilities pose significant challenges to existing reinforcement learning
agents. In this paper we introduce a novel hierarchical reinforcement learning
agent which links temporally extended skills for continuous control with a
forward model in a symbolic discrete abstraction of the environment's state for
planning. We term our agent SEADS for Symbolic Effect-Aware Diverse Skills. We
formulate an objective and corresponding algorithm which leads to unsupervised
learning of a diverse set of skills through intrinsic motivation given a known
state abstraction. The skills are jointly learned with the symbolic forward
model which captures the effect of skill execution in the state abstraction.
After training, we can leverage the skills as symbolic actions using the
forward model for long-horizon planning and subsequently execute the plan using
the learned continuous-action control skills. The proposed algorithm learns
skills and forward models that can be used to solve complex tasks which require
both continuous control and long-horizon planning capabilities with high
success rate. It compares favorably with other flat and hierarchical
reinforcement learning baseline agents and is successfully demonstrated with a
real robot.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2206.12718v1' target='_blank'>Hierarchical Reinforcement Learning with Opponent Modeling for
  Distributed Multi-agent Cooperation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhixuan Liang, Jiannong Cao, Shan Jiang, Divya Saxena, Huafeng Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-06-25 19:09:29</h6>
<p class='card-text'>Many real-world applications can be formulated as multi-agent cooperation
problems, such as network packet routing and coordination of autonomous
vehicles. The emergence of deep reinforcement learning (DRL) provides a
promising approach for multi-agent cooperation through the interaction of the
agents and environments. However, traditional DRL solutions suffer from the
high dimensions of multiple agents with continuous action space during policy
search. Besides, the dynamicity of agents' policies makes the training
non-stationary. To tackle the issues, we propose a hierarchical reinforcement
learning approach with high-level decision-making and low-level individual
control for efficient policy search. In particular, the cooperation of multiple
agents can be learned in high-level discrete action space efficiently. At the
same time, the low-level individual control can be reduced to single-agent
reinforcement learning. In addition to hierarchical reinforcement learning, we
propose an opponent modeling network to model other agents' policies during the
learning process. In contrast to end-to-end DRL approaches, our approach
reduces the learning complexity by decomposing the overall task into sub-tasks
in a hierarchical way. To evaluate the efficiency of our approach, we conduct a
real-world case study in the cooperative lane change scenario. Both simulation
and real-world experiments show the superiority of our approach in the
collision rate and convergence speed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2206.08088v1' target='_blank'>Reinforcement Learning-enhanced Shared-account Cross-domain Sequential
  Recommendation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lei Guo, Jinyu Zhang, Tong Chen, Xinhua Wang, Hongzhi Yin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-06-16 11:06:32</h6>
<p class='card-text'>Shared-account Cross-domain Sequential Recommendation (SCSR) is an emerging
yet challenging task that simultaneously considers the shared-account and
cross-domain characteristics in the sequential recommendation. Existing works
on SCSR are mainly based on Recurrent Neural Network (RNN) and Graph Neural
Network (GNN) but they ignore the fact that although multiple users share a
single account, it is mainly occupied by one user at a time. This observation
motivates us to learn a more accurate user-specific account representation by
attentively focusing on its recent behaviors. Furthermore, though existing
works endow lower weights to irrelevant interactions, they may still dilute the
domain information and impede the cross-domain recommendation. To address the
above issues, we propose a reinforcement learning-based solution, namely
RL-ISN, which consists of a basic cross-domain recommender and a reinforcement
learning-based domain filter. Specifically, to model the account representation
in the shared-account scenario, the basic recommender first clusters users'
mixed behaviors as latent users, and then leverages an attention model over
them to conduct user identification. To reduce the impact of irrelevant domain
information, we formulate the domain filter as a hierarchical reinforcement
learning task, where a high-level task is utilized to decide whether to revise
the whole transferred sequence or not, and if it does, a low-level task is
further performed to determine whether to remove each interaction within it or
not. To evaluate the performance of our solution, we conduct extensive
experiments on two real-world datasets, and the experimental results
demonstrate the superiority of our RL-ISN method compared with the
state-of-the-art recommendation methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2206.05750v1' target='_blank'>Matching options to tasks using Option-Indexed Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kushal Chauhan, Soumya Chatterjee, Akash Reddy, Balaraman Ravindran, Pradeep Shenoy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-06-12 14:39:02</h6>
<p class='card-text'>The options framework in Hierarchical Reinforcement Learning breaks down
overall goals into a combination of options or simpler tasks and associated
policies, allowing for abstraction in the action space. Ideally, these options
can be reused across different higher-level goals; indeed, such reuse is
necessary to realize the vision of a continual learning agent that can
effectively leverage its prior experience. Previous approaches have only
proposed limited forms of transfer of prelearned options to new task settings.
We propose a novel option indexing approach to hierarchical learning (OI-HRL),
where we learn an affinity function between options and the items present in
the environment. This allows us to effectively reuse a large library of
pretrained options, in zero-shot generalization at test time, by restricting
goal-directed learning to only those options relevant to the task at hand. We
develop a meta-training loop that learns the representations of options and
environments over a series of HRL problems, by incorporating feedback about the
relevance of retrieved options to the higher-level goal. We evaluate OI-HRL in
two simulated settings - the CraftWorld and AI2THOR environments - and show
that we achieve performance competitive with oracular baselines, and
substantial gains over a baseline that has the entire option pool available for
learning the hierarchical policy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2206.04114v1' target='_blank'>Deep Hierarchical Planning from Pixels</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Danijar Hafner, Kuang-Huei Lee, Ian Fischer, Pieter Abbeel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-06-08 18:20:15</h6>
<p class='card-text'>Intelligent agents need to select long sequences of actions to solve complex
tasks. While humans easily break down tasks into subgoals and reach them
through millions of muscle commands, current artificial intelligence is limited
to tasks with horizons of a few hundred decisions, despite large compute
budgets. Research on hierarchical reinforcement learning aims to overcome this
limitation but has proven to be challenging, current methods rely on manually
specified goal spaces or subtasks, and no general solution exists. We introduce
Director, a practical method for learning hierarchical behaviors directly from
pixels by planning inside the latent space of a learned world model. The
high-level policy maximizes task and exploration rewards by selecting latent
goals and the low-level policy learns to achieve the goals. Despite operating
in latent space, the decisions are interpretable because the world model can
decode goals into images for visualization. Director outperforms exploration
methods on tasks with sparse rewards, including 3D maze traversal with a
quadruped robot from an egocentric camera and proprioception, without access to
the global position or top-down view that was used by prior work. Director also
learns successful behaviors across a wide range of environments, including
visual control, Atari games, and DMLab levels.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2205.12648v1' target='_blank'>Fast Inference and Transfer of Compositional Task Structures for
  Few-shot Task Generalization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sungryull Sohn, Hyunjae Woo, Jongwook Choi, lyubing qiang, Izzeddin Gur, Aleksandra Faust, Honglak Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-05-25 10:44:25</h6>
<p class='card-text'>We tackle real-world problems with complex structures beyond the pixel-based
game or simulator. We formulate it as a few-shot reinforcement learning problem
where a task is characterized by a subtask graph that defines a set of subtasks
and their dependencies that are unknown to the agent. Different from the
previous meta-rl methods trying to directly infer the unstructured task
embedding, our multi-task subtask graph inferencer (MTSGI) first infers the
common high-level task structure in terms of the subtask graph from the
training tasks, and use it as a prior to improve the task inference in testing.
Our experiment results on 2D grid-world and complex web navigation domains show
that the proposed method can learn and leverage the common underlying structure
of the tasks for faster adaptation to the unseen tasks than various existing
algorithms such as meta reinforcement learning, hierarchical reinforcement
learning, and other heuristic agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2205.07070v1' target='_blank'>A Learning Approach for Joint Design of Event-triggered Control and
  Power-Efficient Resource Allocation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Atefeh Termehchi, Mehdi Rasti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-05-14 14:16:11</h6>
<p class='card-text'>In emerging Industrial Cyber-Physical Systems (ICPSs), the joint design of
communication and control sub-systems is essential, as these sub-systems are
interconnected. In this paper, we study the joint design problem of an
event-triggered control and an energy-efficient resource allocation in a fifth
generation (5G) wireless network. We formally state the problem as a
multi-objective optimization one, aiming to minimize the number of updates on
the actuators' input and the power consumption in the downlink transmission. To
address the problem, we propose a model-free hierarchical reinforcement
learning approach \textcolor{blue}{with uniformly ultimate boundedness
stability guarantee} that learns four policies simultaneously. These policies
contain an update time policy on the actuators' input, a control policy, and
energy-efficient sub-carrier and power allocation policies. Our simulation
results show that the proposed approach can properly control a simulated ICPS
and significantly decrease the number of updates on the actuators' input as
well as the downlink power consumption.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2205.05230v1' target='_blank'>Developing cooperative policies for multi-stage reinforcement learning
  tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jordan Erskine, Chris Lehnert</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-05-11 01:31:04</h6>
<p class='card-text'>Many hierarchical reinforcement learning algorithms utilise a series of
independent skills as a basis to solve tasks at a higher level of reasoning.
These algorithms don't consider the value of using skills that are cooperative
instead of independent. This paper proposes the Cooperative Consecutive
Policies (CCP) method of enabling consecutive agents to cooperatively solve
long time horizon multi-stage tasks. This method is achieved by modifying the
policy of each agent to maximise both the current and next agent's critic.
Cooperatively maximising critics allows each agent to take actions that are
beneficial for its task as well as subsequent tasks. Using this method in a
multi-room maze domain and a peg in hole manipulation domain, the cooperative
policies were able to outperform a set of naive policies, a single agent
trained across the entire domain, as well as another sequential HRL algorithm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2204.10374v1' target='_blank'>Learning how to Interact with a Complex Interface using Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gheorghe Comanici, Amelia Glaese, Anita Gergely, Daniel Toyama, Zafarali Ahmed, Tyler Jackson, Philippe Hamel, Doina Precup</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-04-21 19:07:50</h6>
<p class='card-text'>Hierarchical Reinforcement Learning (HRL) allows interactive agents to
decompose complex problems into a hierarchy of sub-tasks. Higher-level tasks
can invoke the solutions of lower-level tasks as if they were primitive
actions. In this work, we study the utility of hierarchical decompositions for
learning an appropriate way to interact with a complex interface. Specifically,
we train HRL agents that can interface with applications in a simulated Android
device. We introduce a Hierarchical Distributed Deep Reinforcement Learning
architecture that learns (1) subtasks corresponding to simple finger gestures,
and (2) how to combine these gestures to solve several Android tasks. Our
approach relies on goal conditioning and can be used more generally to convert
any base RL agent into an HRL agent. We use the AndroidEnv environment to
evaluate our approach. For the experiments, the HRL agent uses a distributed
version of the popular DQN algorithm to train different components of the
hierarchy. While the native action space is completely intractable for simple
DQN agents, our architecture can be used to establish an effective way to
interact with different tasks, significantly improving the performance of the
same DQN agent over different levels of abstraction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2204.03196v3' target='_blank'>A Framework for Following Temporal Logic Instructions with Unknown
  Causal Dependencies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Duo Xu, Faramarz Fekri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-04-07 04:01:17</h6>
<p class='card-text'>Teaching a deep reinforcement learning (RL) agent to follow instructions in
multi-task environments is a challenging problem. We consider that user defines
every task by a linear temporal logic (LTL) formula. However, some causal
dependencies in complex environments may be unknown to the user in advance.
Hence, when human user is specifying instructions, the robot cannot solve the
tasks by simply following the given instructions. In this work, we propose a
hierarchical reinforcement learning (HRL) framework in which a symbolic
transition model is learned to efficiently produce high-level plans that can
guide the agent efficiently solve different tasks. Specifically, the symbolic
transition model is learned by inductive logic programming (ILP) to capture
logic rules of state transitions. By planning over the product of the symbolic
transition model and the automaton derived from the LTL formula, the agent can
resolve causal dependencies and break a causally complex problem down into a
sequence of simpler low-level sub-tasks. We evaluate the proposed framework on
three environments in both discrete and continuous domains, showing advantages
over previous representative methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2204.00898v3' target='_blank'>Hierarchical Reinforcement Learning under Mixed Observability</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hai Nguyen, Zhihan Yang, Andrea Baisero, Xiao Ma, Robert Platt, Christopher Amato</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-04-02 16:41:17</h6>
<p class='card-text'>The framework of mixed observable Markov decision processes (MOMDP) models
many robotic domains in which some state variables are fully observable while
others are not. In this work, we identify a significant subclass of MOMDPs
defined by how actions influence the fully observable components of the state
and how those, in turn, influence the partially observable components and the
rewards. This unique property allows for a two-level hierarchical approach we
call HIerarchical Reinforcement Learning under Mixed Observability (HILMO),
which restricts partial observability to the top level while the bottom level
remains fully observable, enabling higher learning efficiency. The top level
produces desired goals to be reached by the bottom level until the task is
solved. We further develop theoretical guarantees to show that our approach can
achieve optimal and quasi-optimal behavior under mild assumptions. Empirical
results on long-horizon continuous control tasks demonstrate the efficacy and
efficiency of our approach in terms of improved success rate, sample
efficiency, and wall-clock training time. We also deploy policies learned in
simulation on a real robot.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2203.12686v1' target='_blank'>Possibility Before Utility: Learning And Using Hierarchical Affordances</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Robby Costales, Shariq Iqbal, Fei Sha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-03-23 19:17:22</h6>
<p class='card-text'>Reinforcement learning algorithms struggle on tasks with complex hierarchical
dependency structures. Humans and other intelligent agents do not waste time
assessing the utility of every high-level action in existence, but instead only
consider ones they deem possible in the first place. By focusing only on what
is feasible, or "afforded", at the present moment, an agent can spend more time
both evaluating the utility of and acting on what matters. To this end, we
present Hierarchical Affordance Learning (HAL), a method that learns a model of
hierarchical affordances in order to prune impossible subtasks for more
effective learning. Existing works in hierarchical reinforcement learning
provide agents with structural representations of subtasks but are not
affordance-aware, and by grounding our definition of hierarchical affordances
in the present state, our approach is more flexible than the multitude of
approaches that ground their subtask dependencies in a symbolic history. While
these logic-based methods often require complete knowledge of the subtask
hierarchy, our approach is able to utilize incomplete and varying symbolic
specifications. Furthermore, we demonstrate that relative to
non-affordance-aware methods, HAL agents are better able to efficiently learn
complex tasks, navigate environment stochasticity, and acquire diverse skills
in the absence of extrinsic supervision -- all of which are hallmarks of human
learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2203.10616v1' target='_blank'>Hierarchical Reinforcement Learning of Locomotion Policies in Response
  to Approaching Objects: A Preliminary Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shangqun Yu, Sreehari Rammohan, Kaiyu Zheng, George Konidaris</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-03-20 18:24:18</h6>
<p class='card-text'>Animals such as rabbits and birds can instantly generate locomotion behavior
in reaction to a dynamic, approaching object, such as a person or a rock,
despite having possibly never seen the object before and having limited
perception of the object's properties. Recently, deep reinforcement learning
has enabled complex kinematic systems such as humanoid robots to successfully
move from point A to point B. Inspired by the observation of the innate
reactive behavior of animals in nature, we hope to extend this progress in
robot locomotion to settings where external, dynamic objects are involved whose
properties are partially observable to the robot. As a first step toward this
goal, we build a simulation environment in MuJoCo where a legged robot must
avoid getting hit by a ball moving toward it. We explore whether prior
locomotion experiences that animals typically possess benefit the learning of a
reactive control policy under a proposed hierarchical reinforcement learning
framework. Preliminary results support the claim that the learning becomes more
efficient using this hierarchical reinforcement learning method, even when
partial observability (radius-based object visibility) is taken into account.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2203.03292v1' target='_blank'>On Credit Assignment in Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joery A. de Vries, Thomas M. Moerland, Aske Plaat</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-03-07 11:13:09</h6>
<p class='card-text'>Hierarchical Reinforcement Learning (HRL) has held longstanding promise to
advance reinforcement learning. Yet, it has remained a considerable challenge
to develop practical algorithms that exhibit some of these promises. To improve
our fundamental understanding of HRL, we investigate hierarchical credit
assignment from the perspective of conventional multistep reinforcement
learning. We show how e.g., a 1-step `hierarchical backup' can be seen as a
conventional multistep backup with $n$ skip connections over time connecting
each subsequent state to the first independent of actions inbetween.
Furthermore, we find that generalizing hierarchy to multistep return estimation
methods requires us to consider how to partition the environment trace, in
order to construct backup paths. We leverage these insight to develop a new
hierarchical algorithm Hier$Q_k(\lambda)$, for which we demonstrate that
hierarchical credit assignment alone can already boost agent performance (i.e.,
when eliminating generalization or exploration). Altogether, our work yields
fundamental insight into the nature of hierarchical backups and distinguishes
this as an additional basis for reinforcement learning research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2203.00669v2' target='_blank'>Hierarchical Reinforcement Learning with AI Planning Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junkyu Lee, Michael Katz, Don Joven Agravante, Miao Liu, Geraud Nangue Tasse, Tim Klinger, Shirin Sohrabi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-03-01 18:38:41</h6>
<p class='card-text'>Two common approaches to sequential decision-making are AI planning (AIP) and
reinforcement learning (RL). Each has strengths and weaknesses. AIP is
interpretable, easy to integrate with symbolic knowledge, and often efficient,
but requires an up-front logical domain specification and is sensitive to
noise; RL only requires specification of rewards and is robust to noise but is
sample inefficient and not easily supplied with external knowledge. We propose
an integrative approach that combines high-level planning with RL, retaining
interpretability, transfer, and efficiency, while allowing for robust learning
of the lower-level plan actions. Our approach defines options in hierarchical
reinforcement learning (HRL) from AIP operators by establishing a
correspondence between the state transition model of AI planning problem and
the abstract state transition system of a Markov Decision Process (MDP).
Options are learned by adding intrinsic rewards to encourage consistency
between the MDP and AIP transition models. We demonstrate the benefit of our
integrated approach by comparing the performance of RL and HRL algorithms in
both MiniGrid and N-rooms environments, showing the advantage of our method
over the existing ones.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2202.10222v1' target='_blank'>Robots Learn Increasingly Complex Tasks with Intrinsic Motivation and
  Automatic Curriculum Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sao Mai Nguyen, Nicolas Duminy, Alexandre Manoury, Dominique Duhaut, Cédric Buche</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-02-11 08:14:16</h6>
<p class='card-text'>Multi-task learning by robots poses the challenge of the domain knowledge:
complexity of tasks, complexity of the actions required, relationship between
tasks for transfer learning. We demonstrate that this domain knowledge can be
learned to address the challenges in life-long learning. Specifically, the
hierarchy between tasks of various complexities is key to infer a curriculum
from simple to composite tasks. We propose a framework for robots to learn
sequences of actions of unbounded complexity in order to achieve multiple
control tasks of various complexity. Our hierarchical reinforcement learning
framework, named SGIM-SAHT, offers a new direction of research, and tries to
unify partial implementations on robot arms and mobile robots. We outline our
contributions to enable robots to map multiple control tasks to sequences of
actions: representations of task dependencies, an intrinsically motivated
exploration to learn task hierarchies, and active imitation learning. While
learning the hierarchy of tasks, it infers its curriculum by deciding which
tasks to explore first, how to transfer knowledge, and when, how and whom to
imitate.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2201.09653v1' target='_blank'>The Paradox of Choice: Using Attention in Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andrei Nica, Khimya Khetarpal, Doina Precup</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-01-24 13:18:02</h6>
<p class='card-text'>Decision-making AI agents are often faced with two important challenges: the
depth of the planning horizon, and the branching factor due to having many
choices. Hierarchical reinforcement learning methods aim to solve the first
problem, by providing shortcuts that skip over multiple time steps. To cope
with the breadth, it is desirable to restrict the agent's attention at each
step to a reasonable number of possible choices. The concept of affordances
(Gibson, 1977) suggests that only certain actions are feasible in certain
states. In this work, we model "affordances" through an attention mechanism
that limits the available choices of temporally extended options. We present an
online, model-free algorithm to learn affordances that can be used to further
learn subgoal options. We investigate the role of hard versus soft attention in
training data collection, abstract value learning in long-horizon tasks, and
handling a growing number of choices. We identify and empirically illustrate
the settings in which the paradox of choice arises, i.e. when having fewer but
more meaningful choices improves the learning speed and performance of a
reinforcement learning agent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2201.09635v4' target='_blank'>State-Conditioned Adversarial Subgoal Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vivienne Huiling Wang, Joni Pajarinen, Tinghuai Wang, Joni-Kristian Kämäräinen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-01-24 12:30:38</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) proposes to solve difficult tasks
by performing decision-making and control at successively higher levels of
temporal abstraction. However, off-policy HRL often suffers from the problem of
a non-stationary high-level policy since the low-level policy is constantly
changing. In this paper, we propose a novel HRL approach for mitigating the
non-stationarity by adversarially enforcing the high-level policy to generate
subgoals compatible with the current instantiation of the low-level policy. In
practice, the adversarial learning is implemented by training a simple
state-conditioned discriminator network concurrently with the high-level policy
which determines the compatibility level of subgoals. Comparison to
state-of-the-art algorithms shows that our approach improves both learning
efficiency and performance in challenging continuous control tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2201.02135v5' target='_blank'>Deep Reinforcement Learning, a textbook</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aske Plaat</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2022-01-04 11:47:21</h6>
<p class='card-text'>Deep reinforcement learning has gathered much attention recently. Impressive
results were achieved in activities as diverse as autonomous driving, game
playing, molecular recombination, and robotics. In all these fields, computer
programs have taught themselves to solve difficult problems. They have learned
to fly model helicopters and perform aerobatic manoeuvers such as loops and
rolls. In some applications they have even become better than the best humans,
such as in Atari, Go, poker and StarCraft. The way in which deep reinforcement
learning explores complex environments reminds us of how children learn, by
playfully trying out things, getting feedback, and trying again. The computer
seems to truly possess aspects of human learning; this goes to the heart of the
dream of artificial intelligence. The successes in research have not gone
unnoticed by educators, and universities have started to offer courses on the
subject. The aim of this book is to provide a comprehensive overview of the
field of deep reinforcement learning. The book is written for graduate students
of artificial intelligence, and for researchers and practitioners who wish to
better understand deep reinforcement learning methods and their challenges. We
assume an undergraduate-level of understanding of computer science and
artificial intelligence; the programming language of this book is Python. We
describe the foundations, the algorithms and the applications of deep
reinforcement learning. We cover the established model-free and model-based
methods that form the basis of the field. Developments go quickly, and we also
cover advanced topics: deep multi-agent reinforcement learning, deep
hierarchical reinforcement learning, and deep meta learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2112.04741v1' target='_blank'>Learning multiple gaits of quadruped robot using hierarchical
  reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yunho Kim, Bukun Son, Dongjun Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-12-09 07:45:25</h6>
<p class='card-text'>There is a growing interest in learning a velocity command tracking
controller of quadruped robot using reinforcement learning due to its
robustness and scalability. However, a single policy, trained end-to-end,
usually shows a single gait regardless of the command velocity. This could be a
suboptimal solution considering the existence of optimal gait according to the
velocity for quadruped animals. In this work, we propose a hierarchical
controller for quadruped robot that could generate multiple gaits (i.e. pace,
trot, bound) while tracking velocity command. Our controller is composed of two
policies, each working as a central pattern generator and local feedback
controller, and trained with hierarchical reinforcement learning. Experiment
results show 1) the existence of optimal gait for specific velocity range 2)
the efficiency of our hierarchical controller compared to a controller composed
of a single policy, which usually shows a single gait. Codes are publicly
available.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2112.04907v1' target='_blank'>JueWu-MC: Playing Minecraft with Sample-efficient Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-12-07 09:24:49</h6>
<p class='card-text'>Learning rational behaviors in open-world games like Minecraft remains to be
challenging for Reinforcement Learning (RL) research due to the compound
challenge of partial observability, high-dimensional visual perception and
delayed reward. To address this, we propose JueWu-MC, a sample-efficient
hierarchical RL approach equipped with representation learning and imitation
learning to deal with perception and exploration. Specifically, our approach
includes two levels of hierarchy, where the high-level controller learns a
policy to control over options and the low-level workers learn to solve each
sub-task. To boost the learning of sub-tasks, we propose a combination of
techniques including 1) action-aware representation learning which captures
underlying relations between action and representation, 2) discriminator-based
self-imitation learning for efficient exploration, and 3) ensemble behavior
cloning with consistency filtering for policy robustness. Extensive experiments
show that JueWu-MC significantly improves sample efficiency and outperforms a
set of baselines by a large margin. Notably, we won the championship of the
NeurIPS MineRL 2021 research competition and achieved the highest performance
score ever.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2112.03100v1' target='_blank'>Hierarchical Reinforcement Learning with Timed Subgoals</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nico Gürtler, Dieter Büchler, Georg Martius</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-12-06 15:11:19</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) holds great potential for
sample-efficient learning on challenging long-horizon tasks. In particular,
letting a higher level assign subgoals to a lower level has been shown to
enable fast learning on difficult problems. However, such subgoal-based methods
have been designed with static reinforcement learning environments in mind and
consequently struggle with dynamic elements beyond the immediate control of the
agent even though they are ubiquitous in real-world problems. In this paper, we
introduce Hierarchical reinforcement learning with Timed Subgoals (HiTS), an
HRL algorithm that enables the agent to adapt its timing to a dynamic
environment by not only specifying what goal state is to be reached but also
when. We discuss how communicating with a lower level in terms of such timed
subgoals results in a more stable learning problem for the higher level. Our
experiments on a range of standard benchmarks and three new challenging dynamic
reinforcement learning environments show that our method is capable of
sample-efficient learning where an existing state-of-the-art subgoal-based HRL
method fails to learn stable solutions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2112.03097v1' target='_blank'>Flexible Option Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Martin Klissarov, Doina Precup</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-12-06 15:07:48</h6>
<p class='card-text'>Temporal abstraction in reinforcement learning (RL), offers the promise of
improving generalization and knowledge transfer in complex environments, by
propagating information more efficiently over time. Although option learning
was initially formulated in a way that allows updating many options
simultaneously, using off-policy, intra-option learning (Sutton, Precup &
Singh, 1999), many of the recent hierarchical reinforcement learning approaches
only update a single option at a time: the option currently executing. We
revisit and extend intra-option learning in the context of deep reinforcement
learning, in order to enable updating all options consistent with current
primitive action choices, without introducing any additional estimates. Our
method can therefore be naturally adopted in most hierarchical RL frameworks.
When we combine our approach with the option-critic algorithm for option
discovery, we obtain significant improvements in performance and
data-efficiency across a wide variety of domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2112.02862v1' target='_blank'>SelectAugment: Hierarchical Deterministic Sample Selection for Data
  Augmentation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shiqi Lin, Zhizheng Zhang, Xin Li, Wenjun Zeng, Zhibo Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-12-06 08:38:38</h6>
<p class='card-text'>Data augmentation (DA) has been widely investigated to facilitate model
optimization in many tasks. However, in most cases, data augmentation is
randomly performed for each training sample with a certain probability, which
might incur content destruction and visual ambiguities. To eliminate this, in
this paper, we propose an effective approach, dubbed SelectAugment, to select
samples to be augmented in a deterministic and online manner based on the
sample contents and the network training status. Specifically, in each batch,
we first determine the augmentation ratio, and then decide whether to augment
each training sample under this ratio. We model this process as a two-step
Markov decision process and adopt Hierarchical Reinforcement Learning (HRL) to
learn the augmentation policy. In this way, the negative effects of the
randomness in selecting samples to augment can be effectively alleviated and
the effectiveness of DA is improved. Extensive experiments demonstrate that our
proposed SelectAugment can be adapted upon numerous commonly used DA methods,
e.g., Mixup, Cutmix, AutoAugment, etc, and improve their performance on
multiple benchmark datasets of image classification and fine-grained image
recognition.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2112.00971v4' target='_blank'>Towards Personalization of User Preferences in Partially Observable
  Smart Home Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shashi Suman, Francois Rivest, Ali Etemad</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-12-02 04:01:44</h6>
<p class='card-text'>The technologies used in smart homes have recently improved to learn the user
preferences from feedback in order to enhance the user convenience and quality
of experience. Most smart homes learn a uniform model to represent the thermal
preferences of users, which generally fails when the pool of occupants includes
people with different sensitivities to temperature, for instance due to age and
physiological factors. Thus, a smart home with a single optimal policy may fail
to provide comfort when a new user with a different preference is integrated
into the home. In this paper, we propose a Bayesian Reinforcement learning
framework that can approximate the current occupant state in a partially
observable smart home environment using its thermal preference, and then
identify the occupant as a new user or someone is already known to the system.
Our proposed framework can be used to identify users based on the temperature
and humidity preferences of the occupant when performing different activities
to enable personalization and improve comfort. We then compare the proposed
framework with a baseline long short-term memory learner that learns the
thermal preference of the user from the sequence of actions which it takes. We
perform these experiments with up to 5 simulated human models each based on
hierarchical reinforcement learning. The results show that our framework can
approximate the belief state of the current user just by its temperature and
humidity preferences across different activities with a high degree of
accuracy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2111.05479v1' target='_blank'>Spatially and Seamlessly Hierarchical Reinforcement Learning for State
  Space and Policy space in Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jaehyun Kim, Jaeseung Jeong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-11-10 01:35:14</h6>
<p class='card-text'>Despite advances in hierarchical reinforcement learning, its applications to
path planning in autonomous driving on highways are challenging. One reason is
that conventional hierarchical reinforcement learning approaches are not
amenable to autonomous driving due to its riskiness: the agent must move
avoiding multiple obstacles such as other agents that are highly unpredictable,
thus safe regions are small, scattered, and changeable over time. To overcome
this challenge, we propose a spatially hierarchical reinforcement learning
method for state space and policy space. The high-level policy selects not only
behavioral sub-policy but also regions to pay mind to in state space and for
outline in policy space. Subsequently, the low-level policy elaborates the
short-term goal position of the agent within the outline of the region selected
by the high-level command. The network structure and optimization suggested in
our method are as concise as those of single-level methods. Experiments on the
environment with various shapes of roads showed that our method finds the
nearly optimal policies from early episodes, outperforming a baseline
hierarchical reinforcement learning method, especially in narrow and complex
roads. The resulting trajectories on the roads were similar to those of human
strategies on the behavioral planning level.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2111.03189v2' target='_blank'>Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon
  Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dhruv Shah, Peng Xu, Yao Lu, Ted Xiao, Alexander Toshev, Sergey Levine, Brian Ichter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-11-04 22:46:16</h6>
<p class='card-text'>Reinforcement learning can train policies that effectively perform complex
tasks. However for long-horizon tasks, the performance of these methods
degrades with horizon, often necessitating reasoning over and chaining
lower-level skills. Hierarchical reinforcement learning aims to enable this by
providing a bank of low-level skills as action abstractions. Hierarchies can
further improve on this by abstracting the space states as well. We posit that
a suitable state abstraction should depend on the capabilities of the available
lower-level policies. We propose Value Function Spaces: a simple approach that
produces such a representation by using the value functions corresponding to
each lower-level skill. These value functions capture the affordances of the
scene, thus forming a representation that compactly abstracts task relevant
information and robustly ignores distractors. Empirical evaluations for
maze-solving and robotic manipulation tasks demonstrate that our approach
improves long-horizon performance and enables better zero-shot generalization
than alternative model-free and model-based methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2111.00213v4' target='_blank'>Adjacency constraint for efficient hierarchical reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, Feng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-10-30 09:26:45</h6>
<p class='card-text'>Goal-conditioned Hierarchical Reinforcement Learning (HRL) is a promising
approach for scaling up reinforcement learning (RL) techniques. However, it
often suffers from training inefficiency as the action space of the high-level,
i.e., the goal space, is large. Searching in a large goal space poses
difficulty for both high-level subgoal generation and low-level policy
learning. In this paper, we show that this problem can be effectively
alleviated by restricting the high-level action space from the whole goal space
to a $k$-step adjacent region of the current state using an adjacency
constraint. We theoretically prove that in a deterministic Markov Decision
Process (MDP), the proposed adjacency constraint preserves the optimal
hierarchical policy, while in a stochastic MDP the adjacency constraint induces
a bounded state-value suboptimality determined by the MDP's transition
structure. We further show that this constraint can be practically implemented
by training an adjacency network that can discriminate between adjacent and
non-adjacent subgoals. Experimental results on discrete and continuous control
tasks including challenging simulated robot locomotion and manipulation tasks
show that incorporating the adjacency constraint significantly boosts the
performance of state-of-the-art goal-conditioned HRL approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2110.13625v3' target='_blank'>Landmark-Guided Subgoal Generation in Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junsu Kim, Younggyo Seo, Jinwoo Shin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-10-26 12:16:19</h6>
<p class='card-text'>Goal-conditioned hierarchical reinforcement learning (HRL) has shown
promising results for solving complex and long-horizon RL tasks. However, the
action space of high-level policy in the goal-conditioned HRL is often large,
so it results in poor exploration, leading to inefficiency in training. In this
paper, we present HIerarchical reinforcement learning Guided by Landmarks
(HIGL), a novel framework for training a high-level policy with a reduced
action space guided by landmarks, i.e., promising states to explore. The key
component of HIGL is twofold: (a) sampling landmarks that are informative for
exploration and (b) encouraging the high-level policy to generate a subgoal
towards a selected landmark. For (a), we consider two criteria: coverage of the
entire visited state space (i.e., dispersion of states) and novelty of states
(i.e., prediction error of a state). For (b), we select a landmark as the very
first landmark in the shortest path in a graph whose nodes are landmarks. Our
experiments demonstrate that our framework outperforms prior-arts across a
variety of control tasks, thanks to efficient exploration guided by landmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2110.10809v1' target='_blank'>Hierarchical Skills for Efficient Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jonas Gehring, Gabriel Synnaeve, Andreas Krause, Nicolas Usunier</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-10-20 22:29:32</h6>
<p class='card-text'>In reinforcement learning, pre-trained low-level skills have the potential to
greatly facilitate exploration. However, prior knowledge of the downstream task
is required to strike the right balance between generality (fine-grained
control) and specificity (faster learning) in skill design. In previous work on
continuous control, the sensitivity of methods to this trade-off has not been
addressed explicitly, as locomotion provides a suitable prior for navigation
tasks, which have been of foremost interest. In this work, we analyze this
trade-off for low-level policy pre-training with a new benchmark suite of
diverse, sparse-reward tasks for bipedal robots. We alleviate the need for
prior knowledge by proposing a hierarchical skill learning framework that
acquires skills of varying complexity in an unsupervised manner. For
utilization on downstream tasks, we present a three-layered hierarchical
learning algorithm to automatically trade off between general and specific
skills as required by the respective task. In our experiments, we show that our
approach performs this trade-off effectively and achieves better results than
current state-of-the-art methods for end- to-end hierarchical reinforcement
learning and unsupervised skill discovery. Code and videos are available at
https://facebookresearch.github.io/hsd3 .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2110.10655v2' target='_blank'>Socialbots on Fire: Modeling Adversarial Behaviors of Socialbots via
  Multi-Agent Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thai Le, Long Tran-Thanh, Dongwon Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-10-20 16:49:26</h6>
<p class='card-text'>Socialbots are software-driven user accounts on social platforms, acting
autonomously (mimicking human behavior), with the aims to influence the
opinions of other users or spread targeted misinformation for particular goals.
As socialbots undermine the ecosystem of social platforms, they are often
considered harmful. As such, there have been several computational efforts to
auto-detect the socialbots. However, to our best knowledge, the adversarial
nature of these socialbots has not yet been studied. This begs a question "can
adversaries, controlling socialbots, exploit AI techniques to their advantage?"
To this question, we successfully demonstrate that indeed it is possible for
adversaries to exploit computational learning mechanism such as reinforcement
learning (RL) to maximize the influence of socialbots while avoiding being
detected. We first formulate the adversarial socialbot learning as a
cooperative game between two functional hierarchical RL agents. While one agent
curates a sequence of activities that can avoid the detection, the other agent
aims to maximize network influence by selectively connecting with right users.
Our proposed policy networks train with a vast amount of synthetic graphs and
generalize better than baselines on unseen real-life graphs both in terms of
maximizing network influence (up to +18%) and sustainable stealthiness (up to
+40% undetectability) under a strong bot detector (with 90% detection
accuracy). During inference, the complexity of our approach scales linearly,
independent of a network's structure and the virality of news. This makes our
approach a practical adversarial attack when deployed in a real-life setting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2110.09507v1' target='_blank'>Provable Hierarchy-Based Meta-Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kurtland Chua, Qi Lei, Jason D. Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-10-18 17:56:02</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) has seen widespread interest as an
approach to tractable learning of complex modular behaviors. However, existing
work either assume access to expert-constructed hierarchies, or use
hierarchy-learning heuristics with no provable guarantees. To address this gap,
we analyze HRL in the meta-RL setting, where a learner learns latent
hierarchical structure during meta-training for use in a downstream task. We
consider a tabular setting where natural hierarchical structure is embedded in
the transition dynamics. Analogous to supervised meta-learning theory, we
provide "diversity conditions" which, together with a tractable optimism-based
algorithm, guarantee sample-efficient recovery of this natural hierarchy.
Furthermore, we provide regret bounds on a learner using the recovered
hierarchy to solve a meta-test task. Our bounds incorporate common notions in
HRL literature such as temporal and state/action abstractions, suggesting that
our setting and analysis capture important features of HRL in practice.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2110.07940v1' target='_blank'>Wasserstein Unsupervised Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuncheng He, Yuhang Jiang, Hongchang Zhang, Jianzhun Shao, Xiangyang Ji</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-10-15 08:41:51</h6>
<p class='card-text'>Unsupervised reinforcement learning aims to train agents to learn a handful
of policies or skills in environments without external reward. These
pre-trained policies can accelerate learning when endowed with external reward,
and can also be used as primitive options in hierarchical reinforcement
learning. Conventional approaches of unsupervised skill discovery feed a latent
variable to the agent and shed its empowerment on agent's behavior by mutual
information (MI) maximization. However, the policies learned by MI-based
methods cannot sufficiently explore the state space, despite they can be
successfully identified from each other. Therefore we propose a new framework
Wasserstein unsupervised reinforcement learning (WURL) where we directly
maximize the distance of state distributions induced by different policies.
Additionally, we overcome difficulties in simultaneously training N(N >2)
policies, and amortizing the overall reward to each step. Experiments show
policies learned by our approach outperform MI-based methods on the metric of
Wasserstein distance while keeping high discriminability. Furthermore, the
agents trained by WURL can sufficiently explore the state space in mazes and
MuJoCo tasks and the pre-trained policies can be applied to downstream tasks by
hierarchical learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2110.07246v3' target='_blank'>HAVEN: Hierarchical Cooperative Multi-Agent Reinforcement Learning with
  Dual Coordination Mechanism</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiwei Xu, Yunpeng Bai, Bin Zhang, Dapeng Li, Guoliang Fan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-10-14 10:43:47</h6>
<p class='card-text'>Recently, some challenging tasks in multi-agent systems have been solved by
some hierarchical reinforcement learning methods. Inspired by the intra-level
and inter-level coordination in the human nervous system, we propose a novel
value decomposition framework HAVEN based on hierarchical reinforcement
learning for fully cooperative multi-agent problems. To address the instability
arising from the concurrent optimization of policies between various levels and
agents, we introduce the dual coordination mechanism of inter-level and
inter-agent strategies by designing reward functions in a two-level hierarchy.
HAVEN does not require domain knowledge and pre-training, and can be applied to
any value decomposition variant. Our method achieves desirable results on
different decentralized partially observable Markov decision process domains
and outperforms other popular multi-agent hierarchical reinforcement learning
algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2110.04357v2' target='_blank'>Training Transition Policies via Distribution Matching for Complex Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ju-Seung Byun, Andrew Perrault</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-10-08 19:57:37</h6>
<p class='card-text'>Humans decompose novel complex tasks into simpler ones to exploit previously
learned skills. Analogously, hierarchical reinforcement learning seeks to
leverage lower-level policies for simple tasks to solve complex ones. However,
because each lower-level policy induces a different distribution of states,
transitioning from one lower-level policy to another may fail due to an
unexpected starting state. We introduce transition policies that smoothly
connect lower-level policies by producing a distribution of states and actions
that matches what is expected by the next policy. Training transition policies
is challenging because the natural reward signal -- whether the next policy can
execute its subtask successfully -- is sparse. By training transition policies
via adversarial inverse reinforcement learning to match the distribution of
expected states and actions, we avoid relying on task-based reward. To further
improve performance, we use deep Q-learning with a binary action space to
determine when to switch from a transition policy to the next pre-trained
policy, using the success or failure of the next subtask as the reward.
Although the reward is still sparse, the problem is less severe due to the
simple binary action space. We demonstrate our method on continuous bipedal
locomotion and arm manipulation tasks that require diverse skills. We show that
it smoothly connects the lower-level policies, achieving higher success rates
than previous methods that search for successful trajectories based on a reward
function, but do not match the state distribution.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2110.00650v1' target='_blank'>Multi-lane Cruising Using Hierarchical Planning and Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kasra Rezaee, Peyman Yadmellat, Masoud S. Nosrati, Elmira Amirloo Abolfathi, Mohammed Elmahgiubi, Jun Luo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-10-01 21:03:39</h6>
<p class='card-text'>Competent multi-lane cruising requires using lane changes and within-lane
maneuvers to achieve good speed and maintain safety. This paper proposes a
design for autonomous multi-lane cruising by combining a hierarchical
reinforcement learning framework with a novel state-action space abstraction.
While the proposed solution follows the classical hierarchy of behavior
decision, motion planning and control, it introduces a key intermediate
abstraction within the motion planner to discretize the state-action space
according to high level behavioral decisions. We argue that this design allows
principled modular extension of motion planning, in contrast to using either
monolithic behavior cloning or a large set of hand-written rules. Moreover, we
demonstrate that our state-action space abstraction allows transferring of the
trained models without retraining from a simulated environment with virtually
no dynamics to one with significantly more realistic dynamics. Together, these
results suggest that our proposed hierarchical architecture is a promising way
to allow reinforcement learning to be applied to complex multi-lane cruising in
the real world.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2109.12798v2' target='_blank'>From internal models toward metacognitive AI</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mitsuo Kawato, Aurelio Cortese</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-09-27 05:00:56</h6>
<p class='card-text'>In several papers published in Biological Cybernetics in the 1980s and 1990s,
Kawato and colleagues proposed computational models explaining how internal
models are acquired in the cerebellum. These models were later supported by
neurophysiological experiments using monkeys and neuroimaging experiments
involving humans. These early studies influenced neuroscience from basic,
sensory-motor control to higher cognitive functions. One of the most perplexing
enigmas related to internal models is to understand the neural mechanisms that
enable animals to learn large-dimensional problems with so few trials.
Consciousness and metacognition -- the ability to monitor one's own thoughts,
may be part of the solution to this enigma. Based on literature reviews of the
past 20 years, here we propose a computational neuroscience model of
metacognition. The model comprises a modular hierarchical
reinforcement-learning architecture of parallel and layered, generative-inverse
model pairs. In the prefrontal cortex, a distributed executive network called
the "cognitive reality monitoring network" (CRMN) orchestrates conscious
involvement of generative-inverse model pairs in perception and action. Based
on mismatches between computations by generative and inverse models, as well as
reward prediction errors, CRMN computes a "responsibility signal" that gates
selection and learning of pairs in perception, action, and reinforcement
learning. A high responsibility signal is given to the pairs that best capture
the external world, that are competent in movements (small mismatch), and that
are capable of reinforcement learning (small reward prediction error). CRMN
selects pairs with higher responsibility signals as objects of metacognition,
and consciousness is determined by the entropy of responsibility signals across
all pairs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2109.09968v1' target='_blank'>Generalization in Text-based Games via Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Chengqi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-09-21 05:27:33</h6>
<p class='card-text'>Deep reinforcement learning provides a promising approach for text-based
games in studying natural language communication between humans and artificial
agents. However, the generalization still remains a big challenge as the agents
depend critically on the complexity and variety of training tasks. In this
paper, we address this problem by introducing a hierarchical framework built
upon the knowledge graph-based RL agent. In the high level, a meta-policy is
executed to decompose the whole game into a set of subtasks specified by
textual goals, and select one of them based on the KG. Then a sub-policy in the
low level is executed to conduct goal-conditioned reinforcement learning. We
carry out experiments on games with various difficulty levels and show that the
proposed method enjoys favorable generalizability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2109.09876v2' target='_blank'>Context-Specific Representation Abstraction for Deep Option Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marwa Abdulhai, Dong-Ki Kim, Matthew Riemer, Miao Liu, Gerald Tesauro, Jonathan P. How</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-09-20 22:50:01</h6>
<p class='card-text'>Hierarchical reinforcement learning has focused on discovering temporally
extended actions, such as options, that can provide benefits in problems
requiring extensive exploration. One promising approach that learns these
options end-to-end is the option-critic (OC) framework. We examine and show in
this paper that OC does not decompose a problem into simpler sub-problems, but
instead increases the size of the search over policy space with each option
considering the entire state space during learning. This issue can result in
practical limitations of this method, including sample inefficient learning. To
address this problem, we introduce Context-Specific Representation Abstraction
for Deep Option Learning (CRADOL), a new framework that considers both temporal
abstraction and context-specific representation abstraction to effectively
reduce the size of the search over policy space. Specifically, our method
learns a factored belief state representation that enables each option to learn
a policy over only a subsection of the state space. We test our method against
hierarchical, non-hierarchical, and modular recurrent neural network baselines,
demonstrating significant sample efficiency improvements in challenging
partially observable environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2108.13268v3' target='_blank'>Sensor-Based Navigation Using Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Christopher Gebauer, Nils Dengler, Maren Bennewitz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-08-30 14:29:57</h6>
<p class='card-text'>Robotic systems are nowadays capable of solving complex navigation tasks.
However, their capabilities are limited to the knowledge of the designer and
consequently lack generalizability to initially unconsidered situations. This
makes deep reinforcement learning (DRL) especially interesting, as these
algorithms promise a self-learning system only relying on feedback from the
environment. In this paper, we consider the problem of lidar-based robot
navigation in continuous action space using DRL without providing any
goal-oriented or global information. By relying solely on local sensor data to
solve navigation tasks, we design an agent that assigns its own waypoints based
on intrinsic motivation. Our agent is able to learn goal-directed navigation
behavior even when facing only sparse feedback, i.e., delayed rewards when
reaching the target. To address this challenge and the complexity of the
continuous action space, we deploy a hierarchical agent structure in which the
exploration is distributed across multiple layers. Within the hierarchical
structure, our agent self-assigns internal goals and learns to extract
reasonable waypoints to reach the desired target position only based on local
sensor data. In our experiments, we demonstrate the navigation capabilities of
our agent in two environments and show that the hierarchical structure
seriously improves the performance in terms of success rate and success
weighted by path length in comparison to a flat structure. Furthermore, we
provide a real-robot experiment to illustrate that the trained agent can be
easily transferred to a real-world scenario.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2108.06107v1' target='_blank'>Aspect Sentiment Triplet Extraction Using Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Samson Yu Bai Jian, Tapas Nayak, Navonil Majumder, Soujanya Poria</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-08-13 07:38:48</h6>
<p class='card-text'>Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting triplets
of aspect terms, their associated sentiments, and the opinion terms that
provide evidence for the expressed sentiments. Previous approaches to ASTE
usually simultaneously extract all three components or first identify the
aspect and opinion terms, then pair them up to predict their sentiment
polarities. In this work, we present a novel paradigm, ASTE-RL, by regarding
the aspect and opinion terms as arguments of the expressed sentiment in a
hierarchical reinforcement learning (RL) framework. We first focus on
sentiments expressed in a sentence, then identify the target aspect and opinion
terms for that sentiment. This takes into account the mutual interactions among
the triplet's components while improving exploration and sample efficiency.
Furthermore, this hierarchical RLsetup enables us to deal with multiple and
overlapping triplets. In our experiments, we evaluate our model on existing
datasets from laptop and restaurant domains and show that it achieves
state-of-the-art performance. The implementation of this work is publicly
available at https://github.com/declare-lab/ASTE-RL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2108.05872v1' target='_blank'>HAC Explore: Accelerating Exploration with Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Willie McClinton, Andrew Levy, George Konidaris</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-08-12 17:42:12</h6>
<p class='card-text'>Sparse rewards and long time horizons remain challenging for reinforcement
learning algorithms. Exploration bonuses can help in sparse reward settings by
encouraging agents to explore the state space, while hierarchical approaches
can assist with long-horizon tasks by decomposing lengthy tasks into shorter
subtasks. We propose HAC Explore (HACx), a new method that combines these
approaches by integrating the exploration bonus method Random Network
Distillation (RND) into the hierarchical approach Hierarchical Actor-Critic
(HAC). HACx outperforms either component method on its own, as well as an
existing approach to combining hierarchy and exploration, in a set of difficult
simulated robotics tasks. HACx is the first RL method to solve a sparse reward,
continuous-control task that requires over 1,000 actions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2107.08183v1' target='_blank'>Hierarchical Reinforcement Learning with Optimal Level Synchronization
  based on a Deep Generative Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:JaeYoon Kim, Junyu Xuan, Christy Liang, Farookh Hussain</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-07-17 05:02:25</h6>
<p class='card-text'>The high-dimensional or sparse reward task of a reinforcement learning (RL)
environment requires a superior potential controller such as hierarchical
reinforcement learning (HRL) rather than an atomic RL because it absorbs the
complexity of commands to achieve the purpose of the task in its hierarchical
structure. One of the HRL issues is how to train each level policy with the
optimal data collection from its experience. That is to say, how to synchronize
adjacent level policies optimally. Our research finds that a HRL model through
the off-policy correction technique of HRL, which trains a higher-level policy
with the goal of reflecting a lower-level policy which is newly trained using
the off-policy method, takes the critical role of synchronizing both level
policies at all times while they are being trained. We propose a novel HRL
model supporting the optimal level synchronization using the off-policy
correction technique with a deep generative model. This uses the advantage of
the inverse operation of a flow-based deep generative model (FDGM) to achieve
the goal corresponding to the current state of the lower-level policy. The
proposed model also considers the freedom of the goal dimension between HRL
policies which makes it the generalized inverse model of the model-free RL in
HRL with the optimal synchronization method. The comparative experiment results
show the performance of our proposed model.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2107.03685v1' target='_blank'>Towards Autonomous Pipeline Inspection with Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nicolò Botteghi, Luuk Grefte, Mannes Poel, Beril Sirmacek, Christoph Brune, Edwin Dertien, Stefano Stramigioli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-07-08 08:58:59</h6>
<p class='card-text'>Inspection and maintenance are two crucial aspects of industrial pipeline
plants. While robotics has made tremendous progress in the mechanic design of
in-pipe inspection robots, the autonomous control of such robots is still a big
open challenge due to the high number of actuators and the complex manoeuvres
required. To address this problem, we investigate the usage of Deep
Reinforcement Learning for achieving autonomous navigation of in-pipe robots in
pipeline networks with complex topologies. Moreover, we introduce a
hierarchical policy decomposition based on Hierarchical Reinforcement Learning
to learn robust high-level navigation skills. We show that the hierarchical
structure introduced in the policy is fundamental for solving the navigation
task through pipes and necessary for achieving navigation performances superior
to human-level control.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2107.01518v3' target='_blank'>Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lirui Wang, Xiangyun Meng, Yu Xiang, Dieter Fox</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-07-04 01:26:48</h6>
<p class='card-text'>6D grasping in cluttered scenes is a longstanding problem in robotic
manipulation. Open-loop manipulation pipelines may fail due to inaccurate state
estimation, while most end-to-end grasping methods have not yet scaled to
complex scenes with obstacles. In this work, we propose a new method for
end-to-end learning of 6D grasping in cluttered scenes. Our hierarchical
framework learns collision-free target-driven grasping based on partial point
cloud observations. We learn an embedding space to encode expert grasping plans
during training and a variational autoencoder to sample diverse grasping
trajectories at test time. Furthermore, we train a critic network for plan
selection and an option classifier for switching to an instance grasping policy
through hierarchical reinforcement learning. We evaluate our method and compare
against several baselines in simulation, as well as demonstrate that our latent
planning can generalize to real-world cluttered-scene grasping tasks. Our
videos and code can be found at https://sites.google.com/view/latent-grasping .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2106.15380v3' target='_blank'>Globally Optimal Hierarchical Reinforcement Learning for
  Linearly-Solvable Markov Decision Processes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guillermo Infante, Anders Jonsson, Vicenç Gómez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-06-29 13:10:08</h6>
<p class='card-text'>In this work we present a novel approach to hierarchical reinforcement
learning for linearly-solvable Markov decision processes. Our approach assumes
that the state space is partitioned, and the subtasks consist in moving between
the partitions. We represent value functions on several levels of abstraction,
and use the compositionality of subtasks to estimate the optimal values of the
states in each partition. The policy is implicitly defined on these optimal
value estimates, rather than being decomposed among the subtasks. As a
consequence, our approach can learn the globally optimal policy, and does not
suffer from the non-stationarity of high-level decisions. If several partitions
have equivalent dynamics, the subtasks of those partitions can be shared. If
the set of boundary states is smaller than the entire state space, our approach
can have significantly smaller sample complexity than that of a flat learner,
and we validate this empirically in several experiments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2106.13935v1' target='_blank'>Discovering Generalizable Skills via Automated Generation of Diverse
  Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kuan Fang, Yuke Zhu, Silvio Savarese, Li Fei-Fei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-06-26 03:41:51</h6>
<p class='card-text'>The learning efficiency and generalization ability of an intelligent agent
can be greatly improved by utilizing a useful set of skills. However, the
design of robot skills can often be intractable in real-world applications due
to the prohibitive amount of effort and expertise that it requires. In this
work, we introduce Skill Learning In Diversified Environments (SLIDE), a method
to discover generalizable skills via automated generation of a diverse set of
tasks. As opposed to prior work on unsupervised discovery of skills which
incentivizes the skills to produce different outcomes in the same environment,
our method pairs each skill with a unique task produced by a trainable task
generator. To encourage generalizable skills to emerge, our method trains each
skill to specialize in the paired task and maximizes the diversity of the
generated tasks. A task discriminator defined on the robot behaviors in the
generated tasks is jointly trained to estimate the evidence lower bound of the
diversity objective. The learned skills can then be composed in a hierarchical
reinforcement learning algorithm to solve unseen target tasks. We demonstrate
that the proposed method can effectively learn a variety of robot skills in two
tabletop manipulation domains. Our results suggest that the learned skills can
effectively improve the robot's performance in various unseen target tasks
compared to existing reinforcement learning and skill learning methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2106.11417v1' target='_blank'>Interpretable Model-based Hierarchical Reinforcement Learning using
  Inductive Logic Programming</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Duo Xu, Faramarz Fekri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-06-21 21:30:08</h6>
<p class='card-text'>Recently deep reinforcement learning has achieved tremendous success in wide
ranges of applications. However, it notoriously lacks data-efficiency and
interpretability. Data-efficiency is important as interacting with the
environment is expensive. Further, interpretability can increase the
transparency of the black-box-style deep RL models and hence gain trust from
the users. In this work, we propose a new hierarchical framework via symbolic
RL, leveraging a symbolic transition model to improve the data-efficiency and
introduce the interpretability for learned policy. This framework consists of a
high-level agent, a subtask solver and a symbolic transition model. Without
assuming any prior knowledge on the state transition, we adopt inductive logic
programming (ILP) to learn the rules of symbolic state transitions, introducing
interpretability and making the learned behavior understandable to users. In
empirical experiments, we confirmed that the proposed framework offers
approximately between 30\% to 40\% more data efficiency over previous methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2106.03853v1' target='_blank'>DisTop: Discovering a Topological representation to learn diverse and
  rewarding skills</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arthur Aubret, Laetitia matignon, Salima Hassas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-06-06 10:09:05</h6>
<p class='card-text'>The optimal way for a deep reinforcement learning (DRL) agent to explore is
to learn a set of skills that achieves a uniform distribution of states.
Following this,we introduce DisTop, a new model that simultaneously learns
diverse skills and focuses on improving rewarding skills. DisTop progressively
builds a discrete topology of the environment using an unsupervised contrastive
loss, a growing network and a goal-conditioned policy. Using this topology, a
state-independent hierarchical policy can select where the agent has to keep
discovering skills in the state space. In turn, the newly visited states allows
an improved learnt representation and the learning loop continues. Our
experiments emphasize that DisTop is agnostic to the ground state
representation and that the agent can discover the topology of its environment
whether the states are high-dimensional binary data, images, or proprioceptive
inputs. We demonstrate that this paradigm is competitiveon MuJoCo benchmarks
with state-of-the-art algorithms on both single-task dense rewards and diverse
skill discovery. By combining these two aspects, we showthat DisTop achieves
state-of-the-art performance in comparison with hierarchical reinforcement
learning (HRL) when rewards are sparse. We believe DisTop opens new
perspectives by showing that bottom-up skill discovery combined with
representation learning can unlock the exploration challenge in DRL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2105.14750v3' target='_blank'>Active Hierarchical Exploration with Stable Subgoal Representation
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Siyuan Li, Jin Zhang, Jianhao Wang, Yang Yu, Chongjie Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-05-31 07:28:59</h6>
<p class='card-text'>Goal-conditioned hierarchical reinforcement learning (GCHRL) provides a
promising approach to solving long-horizon tasks. Recently, its success has
been extended to more general settings by concurrently learning hierarchical
policies and subgoal representations. Although GCHRL possesses superior
exploration ability by decomposing tasks via subgoals, existing GCHRL methods
struggle in temporally extended tasks with sparse external rewards, since the
high-level policy learning relies on external rewards. As the high-level policy
selects subgoals in an online learned representation space, the dynamic change
of the subgoal space severely hinders effective high-level exploration. In this
paper, we propose a novel regularization that contributes to both stable and
efficient subgoal representation learning. Building upon the stable
representation, we design measures of novelty and potential for subgoals, and
develop an active hierarchical exploration strategy that seeks out new
promising subgoals and states without intrinsic rewards. Experimental results
show that our approach significantly outperforms state-of-the-art baselines in
continuous control tasks with sparse rewards.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2105.11328v1' target='_blank'>Room Clearance with Feudal Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Henry Charlesworth, Adrian Millea, Eddie Pottrill, Rich Riley</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-05-24 15:05:58</h6>
<p class='card-text'>Reinforcement learning (RL) is a general framework that allows systems to
learn autonomously through trial-and-error interaction with their environment.
In recent years combining RL with expressive, high-capacity neural network
models has led to impressive performance in a diverse range of domains.
However, dealing with the large state and action spaces often required for
problems in the real world still remains a significant challenge. In this paper
we introduce a new simulation environment, "Gambit", designed as a tool to
build scenarios that can drive RL research in a direction useful for military
analysis. Using this environment we focus on an abstracted and simplified room
clearance scenario, where a team of blue agents have to make their way through
a building and ensure that all rooms are cleared of (and remain clear) of enemy
red agents. We implement a multi-agent version of feudal hierarchical RL that
introduces a command hierarchy where a commander at the higher level sends
orders to multiple agents at the lower level who simply have to learn to follow
these orders. We find that breaking the task down in this way allows us to
solve a number of non-trivial floorplans that require the coordination of
multiple agents much more efficiently than the standard baseline RL algorithms
we compare with. We then go on to explore how qualitatively different behaviour
can emerge depending on what we prioritise in the agent's reward function (e.g.
clearing the building quickly vs. prioritising rescuing civilians).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2105.00990v2' target='_blank'>Hierarchical Reinforcement Learning for Air-to-Air Combat</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Adrian P. Pope, Jaime S. Ide, Daria Micovic, Henry Diaz, David Rosenbluth, Lee Ritholtz, Jason C. Twedt, Thayne T. Walker, Kevin Alcedo, Daniel Javorsek</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-05-03 16:40:00</h6>
<p class='card-text'>Artificial Intelligence (AI) is becoming a critical component in the defense
industry, as recently demonstrated by DARPA`s AlphaDogfight Trials (ADT). ADT
sought to vet the feasibility of AI algorithms capable of piloting an F-16 in
simulated air-to-air combat. As a participant in ADT, Lockheed Martin`s (LM)
approach combines a hierarchical architecture with maximum-entropy
reinforcement learning (RL), integrates expert knowledge through reward
shaping, and supports modularity of policies. This approach achieved a $2^{nd}$
place finish in the final ADT event (among eight total competitors) and
defeated a graduate of the US Air Force's (USAF) F-16 Weapons Instructor Course
in match play.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2103.12197v1' target='_blank'>Online Baum-Welch algorithm for Hierarchical Imitation Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vittorio Giammarino, Ioannis Ch. Paschalidis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-03-22 22:03:25</h6>
<p class='card-text'>The options framework for hierarchical reinforcement learning has increased
its popularity in recent years and has made improvements in tackling the
scalability problem in reinforcement learning. Yet, most of these recent
successes are linked with a proper options initialization or discovery. When an
expert is available, the options discovery problem can be addressed by learning
an options-type hierarchical policy directly from expert demonstrations. This
problem is referred to as hierarchical imitation learning and can be handled as
an inference problem in a Hidden Markov Model, which is done via an
Expectation-Maximization type algorithm. In this work, we propose a novel
online algorithm to perform hierarchical imitation learning in the options
framework. Further, we discuss the benefits of such an algorithm and compare it
with its batch version in classical reinforcement learning benchmarks. We show
that this approach works well in both discrete and continuous environments and,
under certain conditions, it outperforms the batch version.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2103.08981v2' target='_blank'>Hierarchical Reinforcement Learning Framework for Stochastic Spaceflight
  Campaign Design</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuji Takubo, Hao Chen, Koki Ho</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-03-16 11:17:02</h6>
<p class='card-text'>This paper develops a hierarchical reinforcement learning architecture for
multimission spaceflight campaign design under uncertainty, including vehicle
design, infrastructure deployment planning, and space transportation
scheduling. This problem involves a high-dimensional design space and is
challenging especially with uncertainty present. To tackle this challenge, the
developed framework has a hierarchical structure with reinforcement learning
and network-based mixed-integer linear programming (MILP), where the former
optimizes campaign-level decisions (e.g., design of the vehicle used throughout
the campaign, destination demand assigned to each mission in the campaign),
whereas the latter optimizes the detailed mission-level decisions (e.g., when
to launch what from where to where). The framework is applied to a set of human
lunar exploration campaign scenarios with uncertain in situ resource
utilization performance as a case study. The main value of this work is its
integration of the rapidly growing reinforcement learning research and the
existing MILP-based space logistics methods through a hierarchical framework to
handle the otherwise intractable complexity of space mission design under
uncertainty. This unique framework is expected to be a critical steppingstone
for the emerging research direction of artificial intelligence for space
mission design.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2103.05405v3' target='_blank'>Efficient learning of goal-oriented push-grasping synergy in clutter</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kechun Xu, Hongxiang Yu, Qianen Lai, Yue Wang, Rong Xiong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-03-09 13:03:33</h6>
<p class='card-text'>We focus on the task of goal-oriented grasping, in which a robot is supposed
to grasp a pre-assigned goal object in clutter and needs some pre-grasp actions
such as pushes to enable stable grasps. However, in this task, the robot gets
positive rewards from environment only when successfully grasping the goal
object. Besides, joint pushing and grasping elongates the action sequence,
compounding the problem of reward delay. Thus, sample inefficiency remains a
main challenge in this task. In this paper, a goal-conditioned hierarchical
reinforcement learning formulation with high sample efficiency is proposed to
learn a push-grasping policy for grasping a specific object in clutter. In our
work, sample efficiency is improved by two means. First, we use a
goal-conditioned mechanism by goal relabeling to enrich the replay buffer.
Second, the pushing and grasping policies are respectively regarded as a
generator and a discriminator and the pushing policy is trained with
supervision of the grasping discriminator, thus densifying pushing rewards. To
deal with the problem of distribution mismatch caused by different training
settings of two policies, an alternating training stage is added to learn
pushing and grasping in turn. A series of experiments carried out in simulation
and real world indicate that our method can quickly learn effective pushing and
grasping policies and outperforms existing methods in task completion rate and
goal grasp success rate by less times of motion. Furthermore, we validate that
our system can also adapt to goal-agnostic conditions with better performance.
Note that our system can be transferred to the real world without any
fine-tuning. Our code is available at
https://github.com/xukechun/Efficient_goal-oriented_push-grasping_synergy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2103.01350v2' target='_blank'>Hierarchical and Partially Observable Goal-driven Policy Learning with
  Goals Relational Graph</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xin Ye, Yezhou Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-03-01 23:21:46</h6>
<p class='card-text'>We present a novel two-layer hierarchical reinforcement learning approach
equipped with a Goals Relational Graph (GRG) for tackling the partially
observable goal-driven task, such as goal-driven visual navigation. Our GRG
captures the underlying relations of all goals in the goal space through a
Dirichlet-categorical process that facilitates: 1) the high-level network
raising a sub-goal towards achieving a designated final goal; 2) the low-level
network towards an optimal policy; and 3) the overall system generalizing
unseen environments and goals. We evaluate our approach with two settings of
partially observable goal-driven tasks -- a grid-world domain and a robotic
object search task. Our experimental results show that our approach exhibits
superior generalization performance on both unseen environments and new goals.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2102.13307v3' target='_blank'>Potential Impacts of Smart Homes on Human Behavior: A Reinforcement
  Learning Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shashi Suman, Ali Etemad, Francois Rivest</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-02-26 05:33:46</h6>
<p class='card-text'>We aim to investigate the potential impacts of smart homes on human behavior.
To this end, we simulate a series of human models capable of performing various
activities inside a reinforcement learning-based smart home. We then
investigate the possibility of human behavior being altered as a result of the
smart home and the human model adapting to one-another. We design a semi-Markov
decision process human task interleaving model based on hierarchical
reinforcement learning that learns to make decisions to either pursue or leave
an activity. We then integrate our human model in the smart home which is based
on Q-learning. We show that a smart home trained on a generic human model is
able to anticipate and learn the thermal preferences of human models with
intrinsic rewards similar to the generic model. The hierarchical human model
learns to complete each activity and set optimal thermal settings for maximum
comfort. With the smart home, the number of time steps required to change the
thermal settings are reduced for the human models. Interestingly, we observe
that small variations in the human model reward structures can lead to the
opposite behavior in the form of unexpected switching between activities which
signals changes in human behavior due to the presence of the smart home.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2102.12571v1' target='_blank'>The Logical Options Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Brandon Araki, Xiao Li, Kiran Vodrahalli, Jonathan DeCastro, Micah J. Fry, Daniela Rus</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-02-24 21:43:16</h6>
<p class='card-text'>Learning composable policies for environments with complex rules and tasks is
a challenging problem. We introduce a hierarchical reinforcement learning
framework called the Logical Options Framework (LOF) that learns policies that
are satisfying, optimal, and composable. LOF efficiently learns policies that
satisfy tasks by representing the task as an automaton and integrating it into
learning and planning. We provide and prove conditions under which LOF will
learn satisfying, optimal policies. And lastly, we show how LOF's learned
policies can be composed to satisfy unseen tasks with only 10-50 retraining
steps. We evaluate LOF on four tasks in discrete and continuous domains,
including a 3D pick-and-place environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2102.09854v1' target='_blank'>Intrinsically Motivated Open-Ended Multi-Task Learning Using Transfer
  Learning to Discover Task Hierarchy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nicolas Duminy, Sao Mai Nguyen, Junshuai Zhu, Dominique Duhaut, Jerome Kerdreux</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-02-19 10:44:08</h6>
<p class='card-text'>In open-ended continuous environments, robots need to learn multiple
parameterised control tasks in hierarchical reinforcement learning. We
hypothesise that the most complex tasks can be learned more easily by
transferring knowledge from simpler tasks, and faster by adapting the
complexity of the actions to the task. We propose a task-oriented
representation of complex actions, called procedures, to learn online task
relationships and unbounded sequences of action primitives to control the
different observables of the environment. Combining both goal-babbling with
imitation learning, and active learning with transfer of knowledge based on
intrinsic motivation, our algorithm self-organises its learning process. It
chooses at any given time a task to focus on; and what, how, when and from whom
to transfer knowledge. We show with a simulation and a real industrial robot
arm, in cross-task and cross-learner transfer settings, that task composition
is key to tackle highly complex tasks. Task decomposition is also efficiently
transferred across different embodied learners and by active imitation, where
the robot requests just a small amount of demonstrations and the adequate type
of information. The robot learns and exploits task dependencies so as to learn
tasks of every complexity.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2104.00620v1' target='_blank'>TradeR: Practical Deep Hierarchical Reinforcement Learning for Trade
  Execution</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Karush Suri, Xiao Qi Shi, Konstantinos Plataniotis, Yuri Lawryshyn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-02-16 19:52:52</h6>
<p class='card-text'>Advances in Reinforcement Learning (RL) span a wide variety of applications
which motivate development in this area. While application tasks serve as
suitable benchmarks for real world problems, RL is seldomly used in practical
scenarios consisting of abrupt dynamics. This allows one to rethink the problem
setup in light of practical challenges. We present Trade Execution using
Reinforcement Learning (TradeR) which aims to address two such practical
challenges of catastrophy and surprise minimization by formulating trading as a
real-world hierarchical RL problem. Through this lens, TradeR makes use of
hierarchical RL to execute trade bids on high frequency real market experiences
comprising of abrupt price variations during the 2019 fiscal year COVID19 stock
market crash. The framework utilizes an energy-based scheme in conjunction with
surprise value function for estimating and minimizing surprise. In a
large-scale study of 35 stock symbols from the S&P500 index, TradeR
demonstrates robustness to abrupt price changes and catastrophic losses while
maintaining profitable outcomes. We hope that our work serves as a motivating
example for application of RL to practical problems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2102.04211v4' target='_blank'>Challenging Social Media Threats using Collective Well-being Aware
  Recommendation Algorithms and an Educational Virtual Companion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dimitri Ognibene, Davide Taibi, Udo Kruschwitz, Rodrigo Souza Wilkens, Davinia Hernandez-Leo, Emily Theophilou, Lidia Scifo, Rene Alejandro Lobo, Francesco Lomonaco, Sabrina Eimler, H. Ulrich Hoppe, Nils Malzahn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-01-25 15:58:18</h6>
<p class='card-text'>Social media have become an integral part of our lives, expanding our
interlinking capabilities to new levels. There is plenty to be said about their
positive effects. On the other hand, however, some serious negative
implications of social media have been repeatedly highlighted in recent years,
pointing at various threats to society and its more vulnerable members, such as
teenagers. We thus propose a theoretical framework based on an adaptive "Social
Media Virtual Companion" for educating and supporting an entire community,
teenage students, to interact in social media environments in order to achieve
desirable conditions, defined in terms of a community-specific and
participatory designed measure of Collective Well-Being (CWB). This Companion
combines automatic processing with expert intervention and guidance. The
virtual Companion will be powered by a Recommender System (CWB-RS) that will
optimize a CWB metric instead of engagement or platform profit, which currently
largely drives recommender systems thereby disregarding any societal collateral
effect.We put an emphasis on experts and educators in the educationally managed
social media community of the Companion. They play five key roles: (a) use the
Companion in classroom-based educational activities; (b) guide the definition
of the CWB; (c) provide a hierarchical structure of learning strategies,
objectives and activities that will support and contain the adaptive sequencing
algorithms of the CWB-RS based on hierarchical reinforcement learning; (d) act
as moderators of direct conflicts between the members of the community; and,
finally, (e) monitor and address ethical and educational issues that are beyond
the intelligent agent's competence and control. Preliminary results on the
performance of the Companion's components and studies of the educational and
psychological underlying principles are presented.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2101.06521v3' target='_blank'>Hierarchical Reinforcement Learning By Discovering Intrinsic Options</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jesse Zhang, Haonan Yu, Wei Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2021-01-16 20:54:31</h6>
<p class='card-text'>We propose a hierarchical reinforcement learning method, HIDIO, that can
learn task-agnostic options in a self-supervised manner while jointly learning
to utilize them to solve sparse-reward tasks. Unlike current hierarchical RL
approaches that tend to formulate goal-reaching low-level tasks or pre-define
ad hoc lower-level policies, HIDIO encourages lower-level option learning that
is independent of the task at hand, requiring few assumptions or little
knowledge about the task structure. These options are learned through an
intrinsic entropy minimization objective conditioned on the option
sub-trajectories. The learned options are diverse and task-agnostic. In
experiments on sparse-reward robotic manipulation and navigation tasks, HIDIO
achieves higher success rates with greater sample efficiency than regular RL
baselines and two state-of-the-art hierarchical RL methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2012.12620v2' target='_blank'>Deep Stock Trading: A Hierarchical Reinforcement Learning Framework for
  Portfolio Optimization and Order Execution</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rundong Wang, Hongxin Wei, Bo An, Zhouyan Feng, Jun Yao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-12-23 12:09:26</h6>
<p class='card-text'>Portfolio management via reinforcement learning is at the forefront of
fintech research, which explores how to optimally reallocate a fund into
different financial assets over the long term by trial-and-error. Existing
methods are impractical since they usually assume each reallocation can be
finished immediately and thus ignoring the price slippage as part of the
trading cost. To address these issues, we propose a hierarchical reinforced
stock trading system for portfolio management (HRPM). Concretely, we decompose
the trading process into a hierarchy of portfolio management over trade
execution and train the corresponding policies. The high-level policy gives
portfolio weights at a lower frequency to maximize the long term profit and
invokes the low-level policy to sell or buy the corresponding shares within a
short time window at a higher frequency to minimize the trading cost. We train
two levels of policies via pre-training scheme and iterative training scheme
for data efficiency. Extensive experimental results in the U.S. market and the
China market demonstrate that HRPM achieves significant improvement against
many state-of-the-art approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2012.10147v2' target='_blank'>Hierarchical principles of embodied reinforcement learning: A review</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Manfred Eppe, Christian Gumbsch, Matthias Kerzel, Phuong D. H. Nguyen, Martin V. Butz, Stefan Wermter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-12-18 10:19:38</h6>
<p class='card-text'>Cognitive Psychology and related disciplines have identified several critical
mechanisms that enable intelligent biological agents to learn to solve complex
problems. There exists pressing evidence that the cognitive mechanisms that
enable problem-solving skills in these species build on hierarchical mental
representations. Among the most promising computational approaches to provide
comparable learning-based problem-solving abilities for artificial agents and
robots is hierarchical reinforcement learning. However, so far the existing
computational approaches have not been able to equip artificial agents with
problem-solving abilities that are comparable to intelligent animals, including
human and non-human primates, crows, or octopuses. Here, we first survey the
literature in Cognitive Psychology, and related disciplines, and find that many
important mental mechanisms involve compositional abstraction, curiosity, and
forward models. We then relate these insights with contemporary hierarchical
reinforcement learning methods, and identify the key machine intelligence
approaches that realise these mechanisms. As our main result, we show that all
important cognitive mechanisms have been implemented independently in isolated
computational architectures, and there is simply a lack of approaches that
integrate them appropriately. We expect our results to guide the development of
more sophisticated cognitively inspired hierarchical methods, so that future
artificial agents achieve a problem-solving performance on the level of
intelligent animals.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2012.07827v1' target='_blank'>Relative Variational Intrinsic Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kate Baumli, David Warde-Farley, Steven Hansen, Volodymyr Mnih</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-12-14 18:59:23</h6>
<p class='card-text'>In the absence of external rewards, agents can still learn useful behaviors
by identifying and mastering a set of diverse skills within their environment.
Existing skill learning methods use mutual information objectives to
incentivize each skill to be diverse and distinguishable from the rest.
However, if care is not taken to constrain the ways in which the skills are
diverse, trivially diverse skill sets can arise. To ensure useful skill
diversity, we propose a novel skill learning objective, Relative Variational
Intrinsic Control (RVIC), which incentivizes learning skills that are
distinguishable in how they change the agent's relationship to its environment.
The resulting set of skills tiles the space of affordances available to the
agent. We qualitatively analyze skill behaviors on multiple environments and
show how RVIC skills are more useful than skills discovered by existing methods
when used in hierarchical reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2012.02880v1' target='_blank'>A Hierarchical Deep Actor-Critic Learning Method for Joint Distribution
  System State Estimation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxuan Yuan, Kaveh Dehghanpour, Zhaoyu Wang, Fankun Bu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-12-04 22:38:21</h6>
<p class='card-text'>Due to increasing penetration of volatile distributed photovoltaic (PV)
resources, real-time monitoring of customers at the grid-edge has become a
critical task. However, this requires solving the distribution system state
estimation (DSSE) jointly for both primary and secondary levels of distribution
grids, which is computationally complex and lacks scalability to large systems.
To achieve near real-time solutions for DSSE, we present a novel hierarchical
reinforcement learning-aided framework: at the first layer, a weighted least
squares (WLS) algorithm solves the DSSE over primary medium-voltage feeders; at
the second layer, deep actor-critic (A-C) modules are trained for each
secondary transformer using measurement residuals to estimate the states of
low-voltage circuits and capture the impact of PVs at the grid-edge. While the
A-C parameter learning process takes place offline, the trained A-C modules are
deployed online for fast secondary grid state estimation; this is the key
factor in scalability and computational efficiency of the framework. To
maintain monitoring accuracy, the two levels exchange boundary information with
each other at the secondary nodes, including transformer voltages (first layer
to second layer) and active/reactive total power injection (second layer to
first layer). This interactive information passing strategy results in a
closed-loop structure that is able to track optimal solutions at both layers in
few iterations. Moreover, our model can handle the topology changes using the
Jacobian matrices of the first layer. We have performed numerical experiments
using real utility data and feeder models to verify the performance of the
proposed framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2011.11722v1' target='_blank'>From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Deepali Jain, Atil Iscen, Ken Caluwaerts</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-11-23 20:55:54</h6>
<p class='card-text'>Legged robots navigating crowded scenes and complex terrains in the real
world are required to execute dynamic leg movements while processing visual
input for obstacle avoidance and path planning. We show that a quadruped robot
can acquire both of these skills by means of hierarchical reinforcement
learning (HRL). By virtue of their hierarchical structure, our policies learn
to implicitly break down this joint problem by concurrently learning High Level
(HL) and Low Level (LL) neural network policies. These two levels are connected
by a low dimensional hidden layer, which we call latent command. HL receives a
first-person camera view, whereas LL receives the latent command from HL and
the robot's on-board sensors to control its actuators. We train policies to
walk in two different environments: a curved cliff and a maze. We show that
hierarchical policies can concurrently learn to locomote and navigate in these
environments, and show they are more efficient than non-hierarchical neural
network policies. This architecture also allows for knowledge reuse across
tasks. LL networks trained on one task can be transferred to a new task in a
new environment. Finally HL, which processes camera images, can be evaluated at
much lower and varying frequencies compared to LL, thus reducing computation
times and bandwidth requirements.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2011.06335v1' target='_blank'>Hierarchical reinforcement learning for efficient exploration and
  transfer</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lorenzo Steccanella, Simone Totaro, Damien Allonsius, Anders Jonsson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-11-12 12:09:13</h6>
<p class='card-text'>Sparse-reward domains are challenging for reinforcement learning algorithms
since significant exploration is needed before encountering reward for the
first time. Hierarchical reinforcement learning can facilitate exploration by
reducing the number of decisions necessary before obtaining a reward. In this
paper, we present a novel hierarchical reinforcement learning framework based
on the compression of an invariant state space that is common to a range of
tasks. The algorithm introduces subtasks which consist of moving between the
state partitions induced by the compression. Results indicate that the
algorithm can successfully solve complex sparse-reward domains, and transfer
knowledge to solve new, previously unseen tasks more quickly.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2011.04891v2' target='_blank'>Hierarchical Reinforcement Learning for Relay Selection and Power
  Optimization in Two-Hop Cooperative Relay Network</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuanzhe Geng, Erwu Liu, Rui Wang, Yiming Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-11-10 04:47:41</h6>
<p class='card-text'>Cooperative communication is an effective approach to improve spectrum
utilization. In order to reduce outage probability of communication system,
most studies propose various schemes for relay selection and power allocation,
which are based on the assumption of channel state information (CSI). However,
it is difficult to get an accurate CSI in practice. In this paper, we study the
outage probability minimizing problem subjected to a total transmission power
constraint in a two-hop cooperative relay network. We use reinforcement
learning (RL) methods to learn strategies for relay selection and power
allocation, which do not need any prior knowledge of CSI but simply rely on the
interaction with communication environment. It is noted that conventional RL
methods, including most deep reinforcement learning (DRL) methods, cannot
perform well when the search space is too large. Therefore, we first propose a
DRL framework with an outage-based reward function, which is then used as a
baseline. Then, we further propose a hierarchical reinforcement learning (HRL)
framework and training algorithm. A key difference from other RL-based methods
in existing literatures is that, our proposed HRL approach decomposes relay
selection and power allocation into two hierarchical optimization objectives,
which are trained in different levels. With the simplification of search space,
the HRL approach can solve the problem of sparse reward, while the conventional
RL method fails. Simulation results reveal that compared with traditional DRL
method, the HRL training algorithm can reach convergence 30 training iterations
earlier and reduce the outage probability by 5% in two-hop relay network with
the same outage threshold.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2011.04752v1' target='_blank'>Trajectory Planning for Autonomous Vehicles Using Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaleb Ben Naveed, Zhiqian Qiao, John M. Dolan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-11-09 20:49:54</h6>
<p class='card-text'>Planning safe trajectories under uncertain and dynamic conditions makes the
autonomous driving problem significantly complex. Current sampling-based
methods such as Rapidly Exploring Random Trees (RRTs) are not ideal for this
problem because of the high computational cost. Supervised learning methods
such as Imitation Learning lack generalization and safety guarantees. To
address these problems and in order to ensure a robust framework, we propose a
Hierarchical Reinforcement Learning (HRL) structure combined with a
Proportional-Integral-Derivative (PID) controller for trajectory planning. HRL
helps divide the task of autonomous vehicle driving into sub-goals and supports
the network to learn policies for both high-level options and low-level
trajectory planner choices. The introduction of sub-goals decreases convergence
time and enables the policies learned to be reused for other scenarios. In
addition, the proposed planner is made robust by guaranteeing smooth
trajectories and by handling the noisy perception system of the ego-car. The
PID controller is used for tracking the waypoints, which ensures smooth
trajectories and reduces jerk. The problem of incomplete observations is
handled by using a Long-Short-Term-Memory (LSTM) layer in the network. Results
from the high-fidelity CARLA simulator indicate that the proposed method
reduces convergence time, generates smoother trajectories, and is able to
handle dynamic surroundings and noisy observations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2011.04697v1' target='_blank'>Behavior Planning at Urban Intersections through Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiqian Qiao, Jeff Schneider, John M. Dolan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-11-09 19:23:26</h6>
<p class='card-text'>For autonomous vehicles, effective behavior planning is crucial to ensure
safety of the ego car. In many urban scenarios, it is hard to create
sufficiently general heuristic rules, especially for challenging scenarios that
some new human drivers find difficult. In this work, we propose a behavior
planning structure based on reinforcement learning (RL) which is capable of
performing autonomous vehicle behavior planning with a hierarchical structure
in simulated urban environments. Application of the hierarchical structure
allows the various layers of the behavior planning system to be satisfied. Our
algorithms can perform better than heuristic-rule-based methods for elective
decisions such as when to turn left between vehicles approaching from the
opposite direction or possible lane-change when approaching an intersection due
to lane blockage or delay in front of the ego car. Such behavior is hard to
evaluate as correct or incorrect, but for some aggressive expert human drivers
handle such scenarios effectively and quickly. On the other hand, compared to
traditional RL methods, our algorithm is more sample-efficient, due to the use
of a hybrid reward mechanism and heuristic exploration during the training
process. The results also show that the proposed method converges to an optimal
policy faster than traditional RL methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2011.01046v1' target='_blank'>NEARL: Non-Explicit Action Reinforcement Learning for Robotic Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nan Lin, Yuxuan Li, Yujun Zhu, Ruolin Wang, Xiayu Zhang, Jianmin Ji, Keke Tang, Xiaoping Chen, Xinming Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-11-02 15:28:19</h6>
<p class='card-text'>Traditionally, reinforcement learning methods predict the next action based
on the current state. However, in many situations, directly applying actions to
control systems or robots is dangerous and may lead to unexpected behaviors
because action is rather low-level. In this paper, we propose a novel
hierarchical reinforcement learning framework without explicit action. Our meta
policy tries to manipulate the next optimal state and actual action is produced
by the inverse dynamics model. To stabilize the training process, we integrate
adversarial learning and information bottleneck into our framework. Under our
framework, widely available state-only demonstrations can be exploited
effectively for imitation learning. Also, prior knowledge and constraints can
be applied to meta policy. We test our algorithm in simulation tasks and its
combination with imitation learning. The experimental results show the
reliability and robustness of our algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2010.15638v2' target='_blank'>Abstract Value Iteration for Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kishor Jothimurugan, Osbert Bastani, Rajeev Alur</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-10-29 14:41:42</h6>
<p class='card-text'>We propose a novel hierarchical reinforcement learning framework for control
with continuous state and action spaces. In our framework, the user specifies
subgoal regions which are subsets of states; then, we (i) learn options that
serve as transitions between these subgoal regions, and (ii) construct a
high-level plan in the resulting abstract decision process (ADP). A key
challenge is that the ADP may not be Markov, which we address by proposing two
algorithms for planning in the ADP. Our first algorithm is conservative,
allowing us to prove theoretical guarantees on its performance, which help
inform the design of subgoal regions. Our second algorithm is a practical one
that interweaves planning at the abstract level and learning at the concrete
level. In our experiments, we demonstrate that our approach outperforms
state-of-the-art hierarchical reinforcement learning algorithms on several
challenging benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2010.14274v1' target='_blank'>Behavior Priors for Efficient Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan Pascanu, Jonathan Schwarz, Guillaume Desjardins, Wojciech Marian Czarnecki, Arun Ahuja, Yee Whye Teh, Nicolas Heess</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-10-27 13:17:18</h6>
<p class='card-text'>As we deploy reinforcement learning agents to solve increasingly challenging
problems, methods that allow us to inject prior knowledge about the structure
of the world and effective solution strategies becomes increasingly important.
In this work we consider how information and architectural constraints can be
combined with ideas from the probabilistic modeling literature to learn
behavior priors that capture the common movement and interaction patterns that
are shared across a set of related tasks or contexts. For example the day-to
day behavior of humans comprises distinctive locomotion and manipulation
patterns that recur across many different situations and goals. We discuss how
such behavior patterns can be captured using probabilistic trajectory models
and how these can be integrated effectively into reinforcement learning
schemes, e.g.\ to facilitate multi-task and transfer learning. We then extend
these ideas to latent variable models and consider a formulation to learn
hierarchical priors that capture different aspects of the behavior in reusable
modules. We discuss how such latent variable formulations connect to related
work on hierarchical reinforcement learning (HRL) and mutual information and
curiosity based objectives, thereby offering an alternative perspective on
existing ideas. We demonstrate the effectiveness of our framework by applying
it to a range of simulated continuous control domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2010.03133v2' target='_blank'>Provable Hierarchical Imitation Learning via EM</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiyu Zhang, Ioannis Paschalidis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-10-07 03:21:57</h6>
<p class='card-text'>Due to recent empirical successes, the options framework for hierarchical
reinforcement learning is gaining increasing popularity. Rather than learning
from rewards which suffers from the curse of dimensionality, we consider
learning an options-type hierarchical policy from expert demonstrations. Such a
problem is referred to as hierarchical imitation learning. Converting this
problem to parameter inference in a latent variable model, we theoretically
characterize the EM approach proposed by Daniel et al. (2016). The population
level algorithm is analyzed as an intermediate step, which is nontrivial due to
the samples being correlated. If the expert policy can be parameterized by a
variant of the options framework, then under regularity conditions, we prove
that the proposed algorithm converges with high probability to a norm ball
around the true parameter. To our knowledge, this is the first performance
guarantee for an hierarchical imitation learning algorithm that only observes
primitive state-action pairs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2010.01351v2' target='_blank'>Disentangling causal effects for hierarchical reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Oriol Corcoll, Raul Vicente</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-10-03 13:19:16</h6>
<p class='card-text'>Exploration and credit assignment under sparse rewards are still challenging
problems. We argue that these challenges arise in part due to the intrinsic
rigidity of operating at the level of actions. Actions can precisely define how
to perform an activity but are ill-suited to describe what activity to perform.
Instead, causal effects are inherently composable and temporally abstract,
making them ideal for descriptive tasks. By leveraging a hierarchy of causal
effects, this study aims to expedite the learning of task-specific behavior and
aid exploration. Borrowing counterfactual and normality measures from causal
literature, we disentangle controllable effects from effects caused by other
dynamics of the environment. We propose CEHRL, a hierarchical method that
models the distribution of controllable effects using a Variational
Autoencoder. This distribution is used by a high-level policy to 1) explore the
environment via random effect exploration so that novel effects are
continuously discovered and learned, and to 2) learn task-specific behavior by
prioritizing the effects that maximize a given reward function. In comparison
to exploring with random actions, experimental results show that random effect
exploration is a more efficient mechanism and that by assigning credit to few
effects rather than many actions, CEHRL learns tasks more rapidly.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2008.07792v2' target='_blank'>ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for
  Mobile Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fei Xia, Chengshu Li, Roberto Martín-Martín, Or Litany, Alexander Toshev, Silvio Savarese</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-08-18 08:05:15</h6>
<p class='card-text'>Many Reinforcement Learning (RL) approaches use joint control signals
(positions, velocities, torques) as action space for continuous control tasks.
We propose to lift the action space to a higher level in the form of subgoals
for a motion generator (a combination of motion planner and trajectory
executor). We argue that, by lifting the action space and by leveraging
sampling-based motion planners, we can efficiently use RL to solve complex,
long-horizon tasks that could not be solved with existing RL methods in the
original action space. We propose ReLMoGen -- a framework that combines a
learned policy to predict subgoals and a motion generator to plan and execute
the motion needed to reach these subgoals. To validate our method, we apply
ReLMoGen to two types of tasks: 1) Interactive Navigation tasks, navigation
problems where interactions with the environment are required to reach the
destination, and 2) Mobile Manipulation tasks, manipulation tasks that require
moving the robot base. These problems are challenging because they are usually
long-horizon, hard to explore during training, and comprise alternating phases
of navigation and interaction. Our method is benchmarked on a diverse set of
seven robotics tasks in photo-realistic simulation environments. In all
settings, ReLMoGen outperforms state-of-the-art Reinforcement Learning and
Hierarchical Reinforcement Learning baselines. ReLMoGen also shows outstanding
transferability between different motion generators at test time, indicating a
great potential to transfer to real robots.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2008.04712v4' target='_blank'>Learning Event-triggered Control from Data through Joint Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Niklas Funk, Dominik Baumann, Vincent Berenz, Sebastian Trimpe</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-08-11 14:15:38</h6>
<p class='card-text'>We present a framework for model-free learning of event-triggered control
strategies. Event-triggered methods aim to achieve high control performance
while only closing the feedback loop when needed. This enables resource
savings, e.g., network bandwidth if control commands are sent via communication
networks, as in networked control systems. Event-triggered controllers consist
of a communication policy, determining when to communicate, and a control
policy, deciding what to communicate. It is essential to jointly optimize the
two policies since individual optimization does not necessarily yield the
overall optimal solution. To address this need for joint optimization, we
propose a novel algorithm based on hierarchical reinforcement learning. The
resulting algorithm is shown to accomplish high-performance control in line
with resource savings and scales seamlessly to nonlinear and high-dimensional
systems. The method's applicability to real-world scenarios is demonstrated
through experiments on a six degrees of freedom real-time controlled
manipulator. Further, we propose an approach towards evaluating the stability
of the learned neural network policies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2008.04403v1' target='_blank'>Comparison of Model Predictive and Reinforcement Learning Methods for
  Fault Tolerant Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ibrahim Ahmed, Hamed Khorasgani, Gautam Biswas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-08-10 20:22:15</h6>
<p class='card-text'>A desirable property in fault-tolerant controllers is adaptability to system
changes as they evolve during systems operations. An adaptive controller does
not require optimal control policies to be enumerated for possible faults.
Instead it can approximate one in real-time. We present two adaptive
fault-tolerant control schemes for a discrete time system based on hierarchical
reinforcement learning. We compare their performance against a model predictive
controller in presence of sensor noise and persistent faults. The controllers
are tested on a fuel tank model of a C-130 plane. Our experiments demonstrate
that reinforcement learning-based controllers perform more robustly than model
predictive controllers under faults, partially observable system models, and
varying sensor noise levels.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2008.03444v3' target='_blank'>Hierarchical Reinforcement Learning in StarCraft II with Human Expertise
  in Subgoals Selection</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinyi Xu, Tiancheng Huang, Pengfei Wei, Akshay Narayan, Tze-Yun Leong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-08-08 04:56:30</h6>
<p class='card-text'>This work is inspired by recent advances in hierarchical reinforcement
learning (HRL) (Barto and Mahadevan 2003; Hengst 2010), and improvements in
learning efficiency from heuristic-based subgoal selection, experience replay
(Lin 1993; Andrychowicz et al. 2017), and task-based curriculum learning
(Bengio et al. 2009; Zaremba and Sutskever 2014). We propose a new method to
integrate HRL, experience replay and effective subgoal selection through an
implicit curriculum design based on human expertise to support sample-efficient
learning and enhance interpretability of the agent's behavior. Human expertise
remains indispensable in many areas such as medicine (Buch, Ahmed, and
Maruthappu 2018) and law (Cath 2018), where interpretability, explainability
and transparency are crucial in the decision making process, for ethical and
legal reasons. Our method simplifies the complex task sets for achieving the
overall objectives by decomposing them into subgoals at different levels of
abstraction. Incorporating relevant subjective knowledge also significantly
reduces the computational resources spent in exploration for RL, especially in
high speed, changing, and complex environments where the transition dynamics
cannot be effectively learned and modelled in a short time. Experimental
results in two StarCraft II (SC2) (Vinyals et al. 2017) minigames demonstrate
that our method can achieve better sample efficiency than flat and end-to-end
RL methods, and provides an effective method for explaining the agent's
performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2007.06728v3' target='_blank'>On the Parallel Tower of Hanoi Puzzle: Acyclicity and a Conditional
  Triangle Inequality</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andrey Rukhin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-07-13 22:42:28</h6>
<p class='card-text'>A parallel variant of the Tower of Hanoi Puzzle is described herein. Within
this parallel context, two theorems on minimal walks in the state space of
configurations, along with their constructive proofs, are provided. These
proofs are used to describe a {\sl denoising method}: a method for identifying
and eliminating sub-optimal transfers within an arbitrary, valid sequence of
disk configurations (as per the rules of the Puzzle). We discuss potential
applications of this method to hierarchical reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2006.11704v1' target='_blank'>Hierarchical Reinforcement Learning for Deep Goal Reasoning: An
  Expressiveness Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weihang Yuan, Héctor Muñoz-Avila</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-06-21 03:29:05</h6>
<p class='card-text'>Hierarchical DQN (h-DQN) is a two-level architecture of feedforward neural
networks where the meta level selects goals and the lower level takes actions
to achieve the goals. We show tasks that cannot be solved by h-DQN,
exemplifying the limitation of this type of hierarchical framework (HF). We
describe the recurrent hierarchical framework (RHF), generalizing architectures
that use a recurrent neural network at the meta level. We analyze the
expressiveness of HF and RHF using context-sensitive grammars. We show that RHF
is more expressive than HF. We perform experiments comparing an implementation
of RHF with two HF baselines; the results corroborate our theoretical findings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2006.11485v4' target='_blank'>Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, Feng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-06-20 03:34:45</h6>
<p class='card-text'>Goal-conditioned hierarchical reinforcement learning (HRL) is a promising
approach for scaling up reinforcement learning (RL) techniques. However, it
often suffers from training inefficiency as the action space of the high-level,
i.e., the goal space, is often large. Searching in a large goal space poses
difficulties for both high-level subgoal generation and low-level policy
learning. In this paper, we show that this problem can be effectively
alleviated by restricting the high-level action space from the whole goal space
to a $k$-step adjacent region of the current state using an adjacency
constraint. We theoretically prove that the proposed adjacency constraint
preserves the optimal hierarchical policy in deterministic MDPs, and show that
this constraint can be practically implemented by training an adjacency network
that can discriminate between adjacent and non-adjacent subgoals. Experimental
results on discrete and continuous control tasks show that incorporating the
adjacency constraint improves the performance of state-of-the-art HRL
approaches in both deterministic and stochastic environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2006.10627v2' target='_blank'>Compositional Generalization by Learning Analytical Expressions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, Dongmei Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-06-18 15:50:57</h6>
<p class='card-text'>Compositional generalization is a basic and essential intellective capability
of human beings, which allows us to recombine known parts readily. However,
existing neural network based models have been proven to be extremely deficient
in such a capability. Inspired by work in cognition which argues
compositionality can be captured by variable slots with symbolic functions, we
present a refreshing view that connects a memory-augmented neural model with
analytical expressions, to achieve compositional generalization. Our model
consists of two cooperative neural modules, Composer and Solver, fitting well
with the cognitive argument while being able to be trained in an end-to-end
manner via a hierarchical reinforcement learning algorithm. Experiments on the
well-known benchmark SCAN demonstrate that our model seizes a great ability of
compositional generalization, solving all challenges addressed by previous
works with 100% accuracies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2006.09939v1' target='_blank'>Forgetful Experience Replay in Hierarchical Reinforcement Learning from
  Demonstrations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexey Skrynnik, Aleksey Staroverov, Ermek Aitygulov, Kirill Aksenov, Vasilii Davydov, Aleksandr I. Panov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-06-17 15:38:40</h6>
<p class='card-text'>Currently, deep reinforcement learning (RL) shows impressive results in
complex gaming and robotic environments. Often these results are achieved at
the expense of huge computational costs and require an incredible number of
episodes of interaction between the agent and the environment. There are two
main approaches to improving the sample efficiency of reinforcement learning
methods - using hierarchical methods and expert demonstrations. In this paper,
we propose a combination of these approaches that allow the agent to use
low-quality demonstrations in complex vision-based environments with multiple
related goals. Our forgetful experience replay (ForgER) algorithm effectively
handles errors in expert data and reduces quality losses when adapting the
action space and states representation to the agent's capabilities. Our
proposed goal-oriented structuring of replay buffer allows the agent to
automatically highlight sub-goals for solving complex hierarchical tasks in
demonstrations. Our method is universal and can be integrated into various
off-policy methods. It surpasses all known existing state-of-the-art RL methods
using expert demonstrations on various model environments. The solution based
on our algorithm beats all the solutions for the famous MineRL competition and
allows the agent to mine a diamond in the Minecraft environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2006.06869v1' target='_blank'>Feudal Steering: Hierarchical Learning for Steering Angle Prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Faith Johnson, Kristin Dana</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-06-11 23:17:55</h6>
<p class='card-text'>We consider the challenge of automated steering angle prediction for self
driving cars using egocentric road images. In this work, we explore the use of
feudal networks, used in hierarchical reinforcement learning (HRL), to devise a
vehicle agent to predict steering angles from first person, dash-cam images of
the Udacity driving dataset. Our method, Feudal Steering, is inspired by recent
work in HRL consisting of a manager network and a worker network that operate
on different temporal scales and have different goals. The manager works at a
temporal scale that is relatively coarse compared to the worker and has a
higher level, task-oriented goal space. Using feudal learning to divide the
task into manager and worker sub-networks provides more accurate and robust
prediction. Temporal abstraction in driving allows more complex primitives than
the steering angle at a single time instance. Composite actions comprise a
subroutine or skill that can be re-used throughout the driving sequence. The
associated subroutine id is the manager network's goal, so that the manager
seeks to succeed at the high level task (e.g. a sharp right turn, a slight
right turn, moving straight in traffic, or moving straight unencumbered by
traffic). The steering angle at a particular time instance is the worker
network output which is regulated by the manager's high level task. We
demonstrate state-of-the art steering angle prediction results on the Udacity
dataset.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2006.06814v4' target='_blank'>Modelling Hierarchical Structure between Dialogue Policy and Natural
  Language Generator with Option Framework for Task-oriented Dialogue System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, Yunjie Gu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-06-11 20:55:28</h6>
<p class='card-text'>Designing task-oriented dialogue systems is a challenging research topic,
since it needs not only to generate utterances fulfilling user requests but
also to guarantee the comprehensibility. Many previous works trained end-to-end
(E2E) models with supervised learning (SL), however, the bias in annotated
system utterances remains as a bottleneck. Reinforcement learning (RL) deals
with the problem through using non-differentiable evaluation metrics (e.g., the
success rate) as rewards. Nonetheless, existing works with RL showed that the
comprehensibility of generated system utterances could be corrupted when
improving the performance on fulfilling user requests. In our work, we (1)
propose modelling the hierarchical structure between dialogue policy and
natural language generator (NLG) with the option framework, called HDNO, where
the latent dialogue act is applied to avoid designing specific dialogue act
representations; (2) train HDNO via hierarchical reinforcement learning (HRL),
as well as suggest the asynchronous updates between dialogue policy and NLG
during training to theoretically guarantee their convergence to a local
maximizer; and (3) propose using a discriminator modelled with language models
as an additional reward to further improve the comprehensibility. We test HDNO
on MultiWoz 2.0 and MultiWoz 2.1, the datasets on multi-domain dialogues, in
comparison with word-level E2E model trained with RL, LaRL and HDSA, showing
improvements on the performance evaluated by automatic evaluation metrics and
human evaluation. Finally, we demonstrate the semantic meanings of latent
dialogue acts to show the explanability for HDNO.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2006.04037v1' target='_blank'>Reinforcement Learning for Multi-Product Multi-Node Inventory Management
  in Supply Chains</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nazneen N Sultana, Hardik Meisheri, Vinita Baniwal, Somjit Nath, Balaraman Ravindran, Harshad Khadilkar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-06-07 04:02:59</h6>
<p class='card-text'>This paper describes the application of reinforcement learning (RL) to
multi-product inventory management in supply chains. The problem description
and solution are both adapted from a real-world business solution. The novelty
of this problem with respect to supply chain literature is (i) we consider
concurrent inventory management of a large number (50 to 1000) of products with
shared capacity, (ii) we consider a multi-node supply chain consisting of a
warehouse which supplies three stores, (iii) the warehouse, stores, and
transportation from warehouse to stores have finite capacities, (iv) warehouse
and store replenishment happen at different time scales and with realistic time
lags, and (v) demand for products at the stores is stochastic. We describe a
novel formulation in a multi-agent (hierarchical) reinforcement learning
framework that can be used for parallelised decision-making, and use the
advantage actor critic (A2C) algorithm with quantised action spaces to solve
the problem. Experiments show that the proposed approach is able to handle a
multi-objective reward comprised of maximising product sales and minimising
wastage of perishable products.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2005.11729v2' target='_blank'>GoChat: Goal-oriented Chatbots with Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianfeng Liu, Feiyang Pan, Ling Luo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-05-24 12:14:19</h6>
<p class='card-text'>A chatbot that converses like a human should be goal-oriented (i.e., be
purposeful in conversation), which is beyond language generation. However,
existing dialogue systems often heavily rely on cumbersome hand-crafted rules
or costly labelled datasets to reach the goals. In this paper, we propose
Goal-oriented Chatbots (GoChat), a framework for end-to-end training chatbots
to maximize the longterm return from offline multi-turn dialogue datasets. Our
framework utilizes hierarchical reinforcement learning (HRL), where the
high-level policy guides the conversation towards the final goal by determining
some sub-goals, and the low-level policy fulfills the sub-goals by generating
the corresponding utterance for response. In our experiments on a real-world
dialogue dataset for anti-fraud in financial, our approach outperforms previous
methods on both the quality of response generation as well as the success rate
of accomplishing the goal.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2005.03420v3' target='_blank'>Curious Hierarchical Actor-Critic Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Frank Röder, Manfred Eppe, Phuong D. H. Nguyen, Stefan Wermter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-05-07 12:44:26</h6>
<p class='card-text'>Hierarchical abstraction and curiosity-driven exploration are two common
paradigms in current reinforcement learning approaches to break down difficult
problems into a sequence of simpler ones and to overcome reward sparsity.
However, there is a lack of approaches that combine these paradigms, and it is
currently unknown whether curiosity also helps to perform the hierarchical
abstraction. As a novelty and scientific contribution, we tackle this issue and
develop a method that combines hierarchical reinforcement learning with
curiosity. Herein, we extend a contemporary hierarchical actor-critic approach
with a forward model to develop a hierarchical notion of curiosity. We
demonstrate in several continuous-space environments that curiosity can more
than double the learning performance and success rates for most of the
investigated benchmarking problems. We also provide our source code and a
supplementary video.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2005.02835v1' target='_blank'>TAG : Type Auxiliary Guiding for Code Comment Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruichu Cai, Zhihao Liang, Boyan Xu, Zijian Li, Yuexing Hao, Yao Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-05-06 14:04:13</h6>
<p class='card-text'>Existing leading code comment generation approaches with the
structure-to-sequence framework ignores the type information of the
interpretation of the code, e.g., operator, string, etc. However, introducing
the type information into the existing framework is non-trivial due to the
hierarchical dependence among the type information. In order to address the
issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for
the code comment generation task which considers the source code as an N-ary
tree with type information associated with each node. Specifically, our
framework is featured with a Type-associated Encoder and a Type-restricted
Decoder which enables adaptive summarization of the source code. We further
propose a hierarchical reinforcement learning method to resolve the training
difficulties of our proposed framework. Extensive evaluations demonstrate the
state-of-the-art performance of our framework with both the auto-evaluated
metrics and case studies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2004.14254v2' target='_blank'>Hierarchical Reinforcement Learning for Automatic Disease Diagnosis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Cheng Zhong, Kangenbei Liao, Wei Chen, Qianlong Liu, Baolin Peng, Xuanjing Huang, Jiajie Peng, Zhongyu Wei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-04-29 15:02:41</h6>
<p class='card-text'>Motivation: Disease diagnosis oriented dialogue system models the interactive
consultation procedure as Markov Decision Process and reinforcement learning
algorithms are used to solve the problem. Existing approaches usually employ a
flat policy structure that treat all symptoms and diseases equally for action
making. This strategy works well in the simple scenario when the action space
is small, however, its efficiency will be challenged in the real environment.
Inspired by the offline consultation process, we propose to integrate a
hierarchical policy structure of two levels into the dialogue systemfor policy
learning. The high-level policy consists of amastermodel that is responsible
for triggering a low-levelmodel, the lowlevel policy consists of several
symptom checkers and a disease classifier. The proposed policy structure is
capable to deal with diagnosis problem including large number of diseases and
symptoms.
  Results: Experimental results on three real-world datasets and a synthetic
dataset demonstrate that our hierarchical framework achieves higher accuracy
and symptom recall in disease diagnosis compared with existing systems. We
construct a benchmark including datasets and implementation of existing
algorithms to encourage follow-up researches.
  Availability: The code and data is available from
https://github.com/FudanDISC/DISCOpen-MedBox-DialoDiagnosis
  Contact: 21210980124@m.fudan.edu.cn
  Supplementary information: Supplementary data are available at Bioinformatics
online.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2004.11543v4' target='_blank'>Continuous Deep Hierarchical Reinforcement Learning for Ground-Air Swarm
  Shepherding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hung The Nguyen, Tung Duy Nguyen, Vu Phi Tran, Matthew Garratt, Kathryn Kasmarik, Sreenatha Anavatti, Michael Barlow, Hussein A. Abbass</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-04-24 05:56:19</h6>
<p class='card-text'>The control and guidance of multi-robots (swarm) is a non-trivial problem due
to the complexity inherent in the coupled interaction among the group. Whether
the swarm is cooperative or non-cooperative, lessons can be learnt from
sheepdogs herding sheep. Biomimicry of shepherding offers computational methods
for swarm control with the potential to generalize and scale in different
environments. However, learning to shepherd is complex due to the large search
space that a machine learner is faced with. We present a deep hierarchical
reinforcement learning approach for shepherding, whereby an unmanned aerial
vehicle (UAV) learns to act as an aerial sheepdog to control and guide a swarm
of unmanned ground vehicles (UGVs). The approach extends our previous work on
machine education to decompose the search space into a hierarchically organized
curriculum. Each lesson in the curriculum is learnt by a deep reinforcement
learning model. The hierarchy is formed by fusing the outputs of the model. The
approach is demonstrated first in a high-fidelity robotic-operating-system
(ROS)-based simulation environment, then with physical UGVs and a UAV in an
in-door testing facility. We investigate the ability of the method to
generalize as the models move from simulation to the real-world and as the
models move from one scale to another.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2004.06213v2' target='_blank'>Combined Model for Partially-Observable and Non-Observable Task
  Switching: Solving Hierarchical Reinforcement Learning Problems Statically
  and Dynamically with Transfer Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nibraas Khan, Joshua Phillips</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-04-13 21:44:54</h6>
<p class='card-text'>An integral function of fully autonomous robots and humans is the ability to
focus attention on a few relevant percepts to reach a certain goal while
disregarding irrelevant percepts. Humans and animals rely on the interactions
between the Pre-Frontal Cortex (PFC) and the Basal Ganglia (BG) to achieve this
focus called Working Memory (WM). The Working Memory Toolkit (WMtk) was
developed based on a computational neuroscience model of this phenomenon with
Temporal Difference (TD) Learning for autonomous systems. Recent adaptations of
the toolkit either utilize Abstract Task Representations (ATRs) to solve
Non-Observable (NO) tasks or storage of past input features to solve
Partially-Observable (PO) tasks, but not both. We propose a new model,
PONOWMtk, which combines both approaches, ATRs and input storage, with a static
or dynamic number of ATRs. The results of our experiments show that PONOWMtk
performs effectively for tasks that exhibit PO, NO, or both properties.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2003.05647v4' target='_blank'>50 years since the Marr, Ito, and Albus models of the cerebellum</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mitsuo Kawato, Shogo Ohmae, Huu Hoang, Terry Sanger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-03-12 07:38:30</h6>
<p class='card-text'>Fifty years have passed since David Marr, Masao Ito, and James Albus proposed
seminal models of cerebellar functions. These models share the essential
concept that parallel-fiber-Purkinje-cell synapses undergo plastic changes,
guided by climbing-fiber activities during sensorimotor learning. However, they
differ in several important respects, including holistic versus complementary
roles of the cerebellum, pattern recognition versus control as computational
objectives, potentiation versus depression of synaptic plasticity, teaching
signals versus error signals transmitted by climbing-fibers, sparse expansion
coding by granule cells, and cerebellar internal models. In this review, we
evaluate the different features of the three models based on recent
computational and experimental studies. While acknowledging that the three
models have greatly advanced our understanding of cerebellar control mechanisms
in eye movements and classical conditioning, we propose a new direction for
computational frameworks of the cerebellum. That is, hierarchical reinforcement
learning with multiple internal models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2002.05954v4' target='_blank'>Learning Functionally Decomposed Hierarchies for Continuous Control
  Tasks with Path Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sammy Christen, Lukas Jendele, Emre Aksan, Otmar Hilliges</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-02-14 10:19:52</h6>
<p class='card-text'>We present HiDe, a novel hierarchical reinforcement learning architecture
that successfully solves long horizon control tasks and generalizes to unseen
test scenarios. Functional decomposition between planning and low-level control
is achieved by explicitly separating the state-action spaces across the
hierarchy, which allows the integration of task-relevant knowledge per layer.
We propose an RL-based planner to efficiently leverage the information in the
planning layer of the hierarchy, while the control layer learns a
goal-conditioned control policy. The hierarchy is trained jointly but allows
for the modular transfer of policy layers across hierarchies of different
agents. We experimentally show that our method generalizes across unseen test
environments and can scale to 3x horizon length compared to both learning and
non-learning based methods. We evaluate on complex continuous control tasks
with sparse rewards, including navigation and robot manipulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2002.02080v1' target='_blank'>Temporal-adaptive Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wen-Ji Zhou, Yang Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-02-06 02:52:21</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) helps address large-scale and
sparse reward issues in reinforcement learning. In HRL, the policy model has an
inner representation structured in levels. With this structure, the
reinforcement learning task is expected to be decomposed into corresponding
levels with sub-tasks, and thus the learning can be more efficient. In HRL,
although it is intuitive that a high-level policy only needs to make macro
decisions in a low frequency, the exact frequency is hard to be simply
determined. Previous HRL approaches often employed a fixed-time skip strategy
or learn a terminal condition without taking account of the context, which,
however, not only requires manual adjustments but also sacrifices some decision
granularity. In this paper, we propose the \emph{temporal-adaptive hierarchical
policy learning} (TEMPLE) structure, which uses a temporal gate to adaptively
control the high-level policy decision frequency. We train the TEMPLE structure
with PPO and test its performance in a range of environments including 2-D
rooms, Mujoco tasks, and Atari games. The results show that the TEMPLE
structure can lead to improved performance in these environments with a
sequential adaptive high-level control.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2001.10474v3' target='_blank'>Coagent Networks Revisited</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Modjtaba Shokrian Zini, Mohammad Pedramfar, Matthew Riemer, Ahmadreza Moradipari, Miao Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-01-28 17:31:23</h6>
<p class='card-text'>Coagent networks formalize the concept of arbitrary networks of stochastic
agents that collaborate to take actions in a reinforcement learning
environment. Prominent examples of coagent networks in action include
approaches to hierarchical reinforcement learning (HRL), such as those using
options, which attempt to address the exploration exploitation trade-off by
introducing abstract actions at different levels by sequencing multiple
stochastic networks within the HRL agents. We first provide a unifying
perspective on the many diverse examples that fall under coagent networks. We
do so by formalizing the rules of execution in a coagent network, enabled by
the novel and intuitive idea of execution paths in a coagent network. Motivated
by parameter sharing in the hierarchical option-critic architecture, we revisit
the coagent network theory and achieve a much shorter proof of the policy
gradient theorem using our idea of execution paths, without any assumption on
how parameters are shared among coagents. We then generalize our setting and
proof to include the scenario where coagents act asynchronously. This new
perspective and theorem also lead to more mathematically accurate and
performant algorithms than those in the existing literature. Lastly, by running
nonstationary RL experiments, we survey the performance and properties of
different generalizations of option-critic models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2001.09816v1' target='_blank'>Hierarchical Reinforcement Learning for Self-Driving Decision-Making
  without Reliance on Labeled Driving Data</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingliang Duan, Shengbo Eben Li, Yang Guan, Qi Sun, Bo Cheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-01-27 14:26:04</h6>
<p class='card-text'>Decision making for self-driving cars is usually tackled by manually encoding
rules from drivers' behaviors or imitating drivers' manipulation using
supervised learning techniques. Both of them rely on mass driving data to cover
all possible driving scenarios. This paper presents a hierarchical
reinforcement learning method for decision making of self-driving cars, which
does not depend on a large amount of labeled driving data. This method
comprehensively considers both high-level maneuver selection and low-level
motion control in both lateral and longitudinal directions. We firstly
decompose the driving tasks into three maneuvers, including driving in lane,
right lane change and left lane change, and learn the sub-policy for each
maneuver. Then, a master policy is learned to choose the maneuver policy to be
executed in the current state. All policies including master policy and
maneuver policies are represented by fully-connected neural networks and
trained by using asynchronous parallel reinforcement learners (APRL), which
builds a mapping from the sensory outputs to driving decisions. Different state
spaces and reward functions are designed for each maneuver. We apply this
method to a highway driving scenario, which demonstrates that it can realize
smooth and safe decision making for self-driving cars.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2001.05864v2' target='_blank'>Weakly Supervised Video Summarization by Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiyan Chen, Li Tao, Xueting Wang, Toshihiko Yamasaki</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-01-12 07:47:02</h6>
<p class='card-text'>Conventional video summarization approaches based on reinforcement learning
have the problem that the reward can only be received after the whole summary
is generated. Such kind of reward is sparse and it makes reinforcement learning
hard to converge. Another problem is that labelling each frame is tedious and
costly, which usually prohibits the construction of large-scale datasets. To
solve these problems, we propose a weakly supervised hierarchical reinforcement
learning framework, which decomposes the whole task into several subtasks to
enhance the summarization quality. This framework consists of a manager network
and a worker network. For each subtask, the manager is trained to set a subgoal
only by a task-level binary label, which requires much fewer labels than
conventional approaches. With the guide of the subgoal, the worker predicts the
importance scores for video frames in the subtask by policy gradient according
to both global reward and innovative defined sub-rewards to overcome the sparse
problem. Experiments on two benchmark datasets show that our proposal has
achieved the best performance, even better than supervised approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2001.02122v1' target='_blank'>Hierarchical Reinforcement Learning as a Model of Human Task
  Interleaving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Christoph Gebhardt, Antti Oulasvirta, Otmar Hilliges</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2020-01-04 17:53:28</h6>
<p class='card-text'>How do people decide how long to continue in a task, when to switch, and to
which other task? Understanding the mechanisms that underpin task interleaving
is a long-standing goal in the cognitive sciences. Prior work suggests greedy
heuristics and a policy maximizing the marginal rate of return. However, it is
unclear how such a strategy would allow for adaptation to everyday environments
that offer multiple tasks with complex switch costs and delayed rewards. Here
we develop a hierarchical model of supervisory control driven by reinforcement
learning (RL). The supervisory level learns to switch using task-specific
approximate utility estimates, which are computed on the lower level. A
hierarchically optimal value function decomposition can be learned from
experience, even in conditions with multiple tasks and arbitrary and uncertain
reward and cost structures. The model reproduces known empirical effects of
task interleaving. It yields better predictions of individual-level data than a
myopic baseline in a six-task problem (N=211). The results support hierarchical
RL as a plausible model of task interleaving.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1912.07544v2' target='_blank'>Planning with Abstract Learned Models While Learning Transferable
  Subtasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:John Winder, Stephanie Milani, Matthew Landen, Erebus Oh, Shane Parr, Shawn Squire, Marie desJardins, Cynthia Matuszek</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-12-16 17:47:57</h6>
<p class='card-text'>We introduce an algorithm for model-based hierarchical reinforcement learning
to acquire self-contained transition and reward models suitable for
probabilistic planning at multiple levels of abstraction. We call this
framework Planning with Abstract Learned Models (PALM). By representing
subtasks symbolically using a new formal structure, the lifted abstract Markov
decision process (L-AMDP), PALM learns models that are independent and modular.
Through our experiments, we show how PALM integrates planning and execution,
facilitating a rapid and efficient learning of abstract, hierarchical models.
We also demonstrate the increased potential for learned models to be
transferred to new and related tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1912.02368v3' target='_blank'>Inter-Level Cooperation in Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abdul Rahman Kreidieh, Glen Berseth, Brandon Trabucco, Samyak Parajuli, Sergey Levine, Alexandre M. Bayen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-12-05 03:56:44</h6>
<p class='card-text'>Hierarchies of temporally decoupled policies present a promising approach for
enabling structured exploration in complex long-term planning problems. To
fully achieve this approach an end-to-end training paradigm is needed. However,
training these multi-level policies has had limited success due to challenges
arising from interactions between the goal-assigning and goal-achieving levels
within a hierarchy. In this article, we consider the policy optimization
process as a multi-agent process. This allows us to draw on connections between
communication and cooperation in multi-agent RL, and demonstrate the benefits
of increased cooperation between sub-policies on the training performance of
the overall policy. We introduce a simple yet effective technique for inducing
inter-level cooperation by modifying the objective function and subsequent
gradients of higher-level policies. Experimental results on a wide variety of
simulated robotics and traffic control tasks demonstrate that inducing
cooperation results in stronger performing policies and increased sample
efficiency on a set of difficult long time horizon tasks. We also find that
goal-conditioned policies trained using our method display better transfer to
new tasks, highlighting the benefits of our method in learning task-agnostic
lower-level behaviors. Videos and code are available at:
https://sites.google.com/berkeley.edu/cooperative-hrl.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1911.10425v4' target='_blank'>Combined Model for Partially-Observable and Non-Observable Task
  Switching: Solving Hierarchical Reinforcement Learning Problems Statically
  and Dynamically with Transfer Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nibraas Khan, Joshua Phillips</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-11-23 22:01:52</h6>
<p class='card-text'>An integral function of fully autonomous robots and humans is the ability to
focus attention on a few relevant percepts to reach a certain goal while
disregarding irrelevant percepts. Humans and animals rely on the interactions
between the Pre-Frontal Cortex (PFC) and the Basal Ganglia (BG) to achieve this
focus called Working Memory (WM). The Working Memory Toolkit (WMtk) was
developed based on a computational neuroscience model of this phenomenon with
Temporal Difference (TD) Learning for autonomous systems. Recent adaptations of
the toolkit either utilize Abstract Task Representations (ATRs) to solve
Non-Observable (NO) tasks or storage of past input features to solve
Partially-Observable (PO) tasks, but not both. We propose a new model,
PONOWMtk, which combines both approaches, ATRs and input storage, with a static
or dynamic number of ATRs. The results of our experiments show that PONOWMtk
performs effectively for tasks that exhibit PO, NO, or both properties.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1911.10164v1' target='_blank'>Efficient Exploration through Intrinsic Motivation Learning for
  Unsupervised Subgoal Discovery in Model-Free Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jacob Rafati, David C. Noelle</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-11-18 23:30:36</h6>
<p class='card-text'>Efficient exploration for automatic subgoal discovery is a challenging
problem in Hierarchical Reinforcement Learning (HRL). In this paper, we show
that intrinsic motivation learning increases the efficiency of exploration,
leading to successful subgoal discovery. We introduce a model-free subgoal
discovery method based on unsupervised learning over a limited memory of
agent's experiences during intrinsic motivation. Additionally, we offer a
unified approach to learning representations in model-free HRL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1911.04936v1' target='_blank'>Combinatorial Optimization by Graph Pointer Networks and Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qiang Ma, Suwen Ge, Danyang He, Darshan Thaker, Iddo Drori</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-11-12 15:39:21</h6>
<p class='card-text'>In this work, we introduce Graph Pointer Networks (GPNs) trained using
reinforcement learning (RL) for tackling the traveling salesman problem (TSP).
GPNs build upon Pointer Networks by introducing a graph embedding layer on the
input, which captures relationships between nodes. Furthermore, to approximate
solutions to constrained combinatorial optimization problems such as the TSP
with time windows, we train hierarchical GPNs (HGPNs) using RL, which learns a
hierarchical policy to find an optimal city permutation under constraints. Each
layer of the hierarchy is designed with a separate reward function, resulting
in stable training. Our results demonstrate that GPNs trained on small-scale
TSP50/100 problems generalize well to larger-scale TSP500/1000 problems, with
shorter tour lengths and faster computational times. We verify that for
constrained TSP problems such as the TSP with time windows, the feasible
solutions found via hierarchical RL training outperform previous baselines. In
the spirit of reproducible research we make our data, models, and code publicly
available.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1911.03799v1' target='_blank'>Hierarchical Reinforcement Learning Method for Autonomous Vehicle
  Behavior Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiqian Qiao, Zachariah Tyree, Priyantha Mudalige, Jeff Schneider, John M. Dolan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-11-09 23:19:59</h6>
<p class='card-text'>In this work, we propose a hierarchical reinforcement learning (HRL)
structure which is capable of performing autonomous vehicle planning tasks in
simulated environments with multiple sub-goals. In this hierarchical structure,
the network is capable of 1) learning one task with multiple sub-goals
simultaneously; 2) extracting attentions of states according to changing
sub-goals during the learning process; 3) reusing the well-trained network of
sub-goals for other similar tasks with the same sub-goals. The states are
defined as processed observations which are transmitted from the perception
system of the autonomous vehicle. A hybrid reward mechanism is designed for
different hierarchical layers in the proposed HRL structure. Compared to
traditional RL methods, our algorithm is more sample-efficient since its
modular design allows reusing the policies of sub-goals across similar tasks.
The results show that the proposed method converges to an optimal policy faster
than traditional RL methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1910.14472v1' target='_blank'>Learning Fairness in Multi-Agent Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiechuan Jiang, Zongqing Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-10-31 13:59:37</h6>
<p class='card-text'>Fairness is essential for human society, contributing to stability and
productivity. Similarly, fairness is also the key for many multi-agent systems.
Taking fairness into multi-agent learning could help multi-agent systems become
both efficient and stable. However, learning efficiency and fairness
simultaneously is a complex, multi-objective, joint-policy optimization. To
tackle these difficulties, we propose FEN, a novel hierarchical reinforcement
learning model. We first decompose fairness for each agent and propose
fair-efficient reward that each agent learns its own policy to optimize. To
avoid multi-objective conflict, we design a hierarchy consisting of a
controller and several sub-policies, where the controller maximizes the
fair-efficient reward by switching among the sub-policies that provides diverse
behaviors to interact with the environment. FEN can be trained in a fully
decentralized way, making it easy to be deployed in real-world applications.
Empirically, we show that FEN easily learns both fairness and efficiency and
significantly outperforms baselines in a variety of multi-agent scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1910.11432v1' target='_blank'>HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation
  with Mobile Manipulators</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chengshu Li, Fei Xia, Roberto Martin-Martin, Silvio Savarese</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-10-24 21:34:29</h6>
<p class='card-text'>Most common navigation tasks in human environments require auxiliary arm
interactions, e.g. opening doors, pressing buttons and pushing obstacles away.
This type of navigation tasks, which we call Interactive Navigation, requires
the use of mobile manipulators: mobile bases with manipulation capabilities.
Interactive Navigation tasks are usually long-horizon and composed of
heterogeneous phases of pure navigation, pure manipulation, and their
combination. Using the wrong part of the embodiment is inefficient and hinders
progress. We propose HRL4IN, a novel Hierarchical RL architecture for
Interactive Navigation tasks. HRL4IN exploits the exploration benefits of HRL
over flat RL for long-horizon tasks thanks to temporally extended commitments
towards subgoals. Different from other HRL solutions, HRL4IN handles the
heterogeneous nature of the Interactive Navigation task by creating subgoals in
different spaces in different phases of the task. Moreover, HRL4IN selects
different parts of the embodiment to use for each phase, improving energy
efficiency. We evaluate HRL4IN against flat PPO and HAC, a state-of-the-art HRL
algorithm, on Interactive Navigation in two environments - a 2D grid-world
environment and a 3D environment with physics simulation. We show that HRL4IN
significantly outperforms its baselines in terms of task performance and energy
efficiency. More information is available at
https://sites.google.com/view/hrl4in.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1910.09508v1' target='_blank'>Multi-agent Hierarchical Reinforcement Learning with Dynamic Termination</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongge Han, Wendelin Boehmer, Michael Wooldridge, Alex Rogers</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-10-21 16:54:49</h6>
<p class='card-text'>In a multi-agent system, an agent's optimal policy will typically depend on
the policies chosen by others. Therefore, a key issue in multi-agent systems
research is that of predicting the behaviours of others, and responding
promptly to changes in such behaviours. One obvious possibility is for each
agent to broadcast their current intention, for example, the currently executed
option in a hierarchical reinforcement learning framework. However, this
approach results in inflexibility of agents if options have an extended
duration and are dynamic. While adjusting the executed option at each step
improves flexibility from a single-agent perspective, frequent changes in
options can induce inconsistency between an agent's actual behaviour and its
broadcast intention. In order to balance flexibility and predictability, we
propose a dynamic termination Bellman equation that allows the agents to
flexibly terminate their options. We evaluate our model empirically on a set of
multi-agent pursuit and taxi tasks, and show that our agents learn to adapt
flexibly across scenarios that require different termination behaviours.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1910.09260v1' target='_blank'>Human-Like Decision Making: Document-level Aspect Sentiment
  Classification via Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingjing Wang, Changlong Sun, Shoushan Li, Jiancheng Wang, Luo Si, Min Zhang, Xiaozhong Liu, Guodong Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-10-21 10:55:46</h6>
<p class='card-text'>Recently, neural networks have shown promising results on Document-level
Aspect Sentiment Classification (DASC). However, these approaches often offer
little transparency w.r.t. their inner working mechanisms and lack
interpretability. In this paper, to simulating the steps of analyzing aspect
sentiment in a document by human beings, we propose a new Hierarchical
Reinforcement Learning (HRL) approach to DASC. This approach incorporates
clause selection and word selection strategies to tackle the data noise problem
in the task of DASC. First, a high-level policy is proposed to select
aspect-relevant clauses and discard noisy clauses. Then, a low-level policy is
proposed to select sentiment-relevant words and discard noisy words inside the
selected clauses. Finally, a sentiment rating predictor is designed to provide
reward signals to guide both clause and word selection. Experimental results
demonstrate the impressive effectiveness of the proposed approach to DASC over
the state-of-the-art baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1910.04450v1' target='_blank'>Hierarchical Reinforcement Learning with Advantage-Based Auxiliary
  Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Siyuan Li, Rui Wang, Minxue Tang, Chongjie Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-10-10 09:39:15</h6>
<p class='card-text'>Hierarchical Reinforcement Learning (HRL) is a promising approach to solving
long-horizon problems with sparse and delayed rewards. Many existing HRL
algorithms either use pre-trained low-level skills that are unadaptable, or
require domain-specific information to define low-level rewards. In this paper,
we aim to adapt low-level skills to downstream tasks while maintaining the
generality of reward design. We propose an HRL framework which sets auxiliary
rewards for low-level skill training based on the advantage function of the
high-level policy. This auxiliary reward enables efficient, simultaneous
learning of the high-level policy and low-level skills without using
task-specific knowledge. In addition, we also theoretically prove that
optimizing low-level skills with this auxiliary reward will increase the task
return for the joint policy. Experimental results show that our algorithm
dramatically outperforms other state-of-the-art HRL methods in Mujoco domains.
We also find both low-level and high-level policies trained by our algorithm
transferable.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1909.13607v4' target='_blank'>MGHRL: Meta Goal-generation for Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haotian Fu, Hongyao Tang, Jianye Hao, Wulong Liu, Chen Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-09-30 11:55:17</h6>
<p class='card-text'>Most meta reinforcement learning (meta-RL) methods learn to adapt to new
tasks by directly optimizing the parameters of policies over primitive action
space. Such algorithms work well in tasks with relatively slight difference.
However, when the task distribution becomes wider, it would be quite
inefficient to directly learn such a meta-policy. In this paper, we propose a
new meta-RL algorithm called Meta Goal-generation for Hierarchical RL (MGHRL).
Instead of directly generating policies over primitive action space for new
tasks, MGHRL learns to generate high-level meta strategies over subgoals given
past experience and leaves the rest of how to achieve subgoals as independent
RL subtasks. Our empirical results on several challenging simulated robotics
environments show that our method enables more efficient and generalized
meta-learning from past experience.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1909.12465v1' target='_blank'>Playing Atari Ball Games with Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hua Huang, Adrian Barbu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-09-27 02:09:34</h6>
<p class='card-text'>Human beings are particularly good at reasoning and inference from just a few
examples. When facing new tasks, humans will leverage knowledge and skills
learned before, and quickly integrate them with the new task. In addition to
learning by experimentation, human also learn socio-culturally through
instructions and learning by example. In this way humans can learn much faster
compared with most current artificial intelligence algorithms in many tasks. In
this paper, we test the idea of speeding up machine learning through social
learning. We argue that in solving real-world problems, especially when the
task is designed by humans, and/or for humans, there are typically instructions
from user manuals and/or human experts which give guidelines on how to better
accomplish the tasks. We argue that these instructions have tremendous value in
designing a reinforcement learning system which can learn in human fashion, and
we test the idea by playing the Atari games Tennis and Pong. We experimentally
demonstrate that the instructions provide key information about the task, which
can be used to decompose the learning task into sub-systems and construct
options for the temporally extended planning, and dramatically accelerate the
learning process.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1909.12324v1' target='_blank'>Learning Generalizable Locomotion Skills with Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianyu Li, Nathan Lambert, Roberto Calandra, Franziska Meier, Akshara Rai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-09-26 18:21:12</h6>
<p class='card-text'>Learning to locomote to arbitrary goals on hardware remains a challenging
problem for reinforcement learning. In this paper, we present a hierarchical
learning framework that improves sample-efficiency and generalizability of
locomotion skills on real-world robots. Our approach divides the problem of
goal-oriented locomotion into two sub-problems: learning diverse primitives
skills, and using model-based planning to sequence these skills. We parametrize
our primitives as cyclic movements, improving sample-efficiency of learning on
a 18 degrees of freedom robot. Then, we learn coarse dynamics models over
primitive cycles and use them in a model predictive control framework. This
allows us to learn to walk to arbitrary goals up to 12m away, after about two
hours of training from scratch on hardware. Our results on a Daisy hexapod
hardware and simulation demonstrate the efficacy of our approach at reaching
distant targets, in different environments and with sensory noise.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1909.10618v2' target='_blank'>Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ofir Nachum, Haoran Tang, Xingyu Lu, Shixiang Gu, Honglak Lee, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-09-23 21:11:30</h6>
<p class='card-text'>Hierarchical reinforcement learning has demonstrated significant success at
solving difficult reinforcement learning (RL) tasks. Previous works have
motivated the use of hierarchy by appealing to a number of intuitive benefits,
including learning over temporally extended transitions, exploring over
temporally extended periods, and training and exploring in a more semantically
meaningful action space, among others. However, in fully observed, Markovian
settings, it is not immediately clear why hierarchical RL should provide
benefits over standard "shallow" RL architectures. In this work, we isolate and
evaluate the claimed benefits of hierarchical RL on a suite of tasks
encompassing locomotion, navigation, and manipulation. Surprisingly, we find
that most of the observed benefits of hierarchy can be attributed to improved
exploration, as opposed to easier policy learning or imposed hierarchical
structures. Given this insight, we present exploration techniques inspired by
hierarchy that achieve performance competitive with hierarchical RL while at
the same time being much simpler to use and implement.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1909.07547v3' target='_blank'>Hierarchical Reinforcement Learning for Open-Domain Dialog</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abdelrhman Saleh, Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Rosalind Picard</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-09-17 01:57:18</h6>
<p class='card-text'>Open-domain dialog generation is a challenging problem; maximum likelihood
training can lead to repetitive outputs, models have difficulty tracking
long-term conversational goals, and training on standard movie or online
datasets may lead to the generation of inappropriate, biased, or offensive
text. Reinforcement Learning (RL) is a powerful framework that could
potentially address these issues, for example by allowing a dialog model to
optimize for reducing toxicity and repetitiveness. However, previous approaches
which apply RL to open-domain dialog generation do so at the word level, making
it difficult for the model to learn proper credit assignment for long-term
conversational rewards. In this paper, we propose a novel approach to
hierarchical reinforcement learning, VHRL, which uses policy gradients to tune
the utterance-level embedding of a variational sequence model. This
hierarchical approach provides greater flexibility for learning long-term,
conversational rewards. We use self-play and RL to optimize for a set of
human-centered conversation metrics, and show that our approach provides
significant improvements -- in terms of both human evaluation and automatic
metrics -- over state-of-the-art dialog models, including Transformers.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1909.01646v1' target='_blank'>LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based
  Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Leonard Adolphs, Thomas Hofmann</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-09-04 09:30:55</h6>
<p class='card-text'>While Reinforcement Learning (RL) approaches lead to significant achievements
in a variety of areas in recent history, natural language tasks remained mostly
unaffected, due to the compositional and combinatorial nature that makes them
notoriously hard to optimize. With the emerging field of Text-Based Games
(TBGs), researchers try to bridge this gap. Inspired by the success of RL
algorithms on Atari games, the idea is to develop new methods in a restricted
game world and then gradually move to more complex environments. Previous work
in the area of TBGs has mainly focused on solving individual games. We,
however, consider the task of designing an agent that not just succeeds in a
single game, but performs well across a whole family of games, sharing the same
theme. In this work, we present our deep RL agent--LeDeepChef--that shows
generalization capabilities to never-before-seen games of the same family with
different environments and task descriptions. The agent participated in
Microsoft Research's "First TextWorld Problems: A Language and Reinforcement
Learning Challenge" and outperformed all but one competitor on the final test
set. The games from the challenge all share the same theme, namely cooking in a
modern house environment, but differ significantly in the arrangement of the
rooms, the presented objects, and the specific goal (recipe to cook). To build
an agent that achieves high scores across a whole family of games, we use an
actor-critic framework and prune the action-space by using ideas from
hierarchical reinforcement learning and a specialized module trained on a
recipe database.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1908.07423v2' target='_blank'>Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical
  Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu-Wei Chao, Jimei Yang, Weifeng Chen, Jia Deng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-08-20 15:14:54</h6>
<p class='card-text'>Recent progress on physics-based character animation has shown impressive
breakthroughs on human motion synthesis, through imitating motion capture data
via deep reinforcement learning. However, results have mostly been demonstrated
on imitating a single distinct motion pattern, and do not generalize to
interactive tasks that require flexible motion patterns due to varying
human-object spatial configurations. To bridge this gap, we focus on one class
of interactive tasks -- sitting onto a chair. We propose a hierarchical
reinforcement learning framework which relies on a collection of subtask
controllers trained to imitate simple, reusable mocap motions, and a meta
controller trained to execute the subtasks properly to complete the main task.
We experimentally demonstrate the strength of our approach over different
non-hierarchical and hierarchical baselines. We also show that our approach can
be applied to motion prediction given an image input. A supplementary video can
be found at https://youtu.be/3CeN0OGz2cA.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1907.12477v2' target='_blank'>Semantic RL with Action Grammars: Data-Efficient Learning of
  Hierarchical Task Abstractions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Robert Tjarko Lange, Aldo Faisal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-07-29 15:27:50</h6>
<p class='card-text'>Hierarchical Reinforcement Learning algorithms have successfully been applied
to temporal credit assignment problems with sparse reward signals. However,
state-of-the-art algorithms require manual specification of sub-task
structures, a sample inefficient exploration phase or lack semantic
interpretability. Humans, on the other hand, efficiently detect hierarchical
sub-structures induced by their surroundings. It has been argued that this
inference process universally applies to language, logical reasoning as well as
motor control. Therefore, we propose a cognitive-inspired Reinforcement
Learning architecture which uses grammar induction to identify sub-goal
policies. By treating an on-policy trajectory as a sentence sampled from the
policy-conditioned language of the environment, we identify hierarchical
constituents with the help of unsupervised grammatical inference. The resulting
set of temporal abstractions is called action grammar (Pastra & Aloimonos,
2012) and unifies symbolic and connectionist approaches to Reinforcement
Learning. It can be used to facilitate efficient imitation, transfer and online
learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1907.08313v1' target='_blank'>Learning High-Level Planning Symbols from Intrinsically Motivated
  Experience</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Angelo Oddi, Riccardo Rasconi, Emilio Cartoni, Gabriele Sartor, Gianluca Baldassarre, Vieri Giuliano Santucci</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-07-18 22:42:35</h6>
<p class='card-text'>In symbolic planning systems, the knowledge on the domain is commonly
provided by an expert. Recently, an automatic abstraction procedure has been
proposed in the literature to create a Planning Domain Definition Language
(PDDL) representation, which is the most widely used input format for most
off-the-shelf automated planners, starting from `options', a data structure
used to represent actions within the hierarchical reinforcement learning
framework. We propose an architecture that potentially removes the need for
human intervention. In particular, the architecture first acquires options in a
fully autonomous fashion on the basis of open-ended learning, then builds a
PDDL domain based on symbols and operators that can be used to accomplish
user-defined goals through a standard PDDL planner.
  We start from an implementation of the above mentioned procedure tested on a
set of benchmark domains in which a humanoid robot can change the state of some
objects through direct interaction with the environment. We then investigate
some critical aspects of the information abstraction process that have been
observed, and propose an extension that mitigates such criticalities, in
particular by analysing the type of classifiers that allow a suitable grounding
of symbols.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1907.00664v1' target='_blank'>Learning World Graphs to Accelerate Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenling Shang, Alex Trott, Stephan Zheng, Caiming Xiong, Richard Socher</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-07-01 11:22:52</h6>
<p class='card-text'>In many real-world scenarios, an autonomous agent often encounters various
tasks within a single complex environment. We propose to build a graph
abstraction over the environment structure to accelerate the learning of these
tasks. Here, nodes are important points of interest (pivotal states) and edges
represent feasible traversals between them. Our approach has two stages. First,
we jointly train a latent pivotal state model and a curiosity-driven
goal-conditioned policy in a task-agnostic manner. Second, provided with the
information from the world graph, a high-level Manager quickly finds solution
to new tasks and expresses subgoals in reference to pivotal states to a
low-level Worker. The Worker can then also leverage the graph to easily
traverse to the pivotal states of interest, even across long distance, and
explore non-locally. We perform a thorough ablation study to evaluate our
approach on a suite of challenging maze tasks, demonstrating significant
advantages from the proposed framework over baselines that lack world graph
knowledge in terms of performance and efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1906.11228v3' target='_blank'>Compositional Transfer in Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Markus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost Tobias Springenberg, Michael Neunert, Tim Hertweck, Thomas Lampe, Noah Siegel, Nicolas Heess, Martin Riedmiller</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-06-26 17:42:07</h6>
<p class='card-text'>The successful application of general reinforcement learning algorithms to
real-world robotics applications is often limited by their high data
requirements. We introduce Regularized Hierarchical Policy Optimization (RHPO)
to improve data-efficiency for domains with multiple dominant tasks and
ultimately reduce required platform time. To this end, we employ compositional
inductive biases on multiple levels and corresponding mechanisms for sharing
off-policy transition data across low-level controllers and tasks as well as
scheduling of tasks. The presented algorithm enables stable and fast learning
for complex, real-world domains in the parallel multitask and sequential
transfer case. We show that the investigated types of hierarchy enable positive
transfer while partially mitigating negative interference and evaluate the
benefits of additional incentives for efficient, compositional task solutions
in single task domains. Finally, we demonstrate substantial data-efficiency and
final performance gains over competitive baselines in a week-long, physical
robot stacking experiment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1906.10667v1' target='_blank'>Reinforcement Learning with Competitive Ensembles of
  Information-Constrained Primitives</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anirudh Goyal, Shagun Sodhani, Jonathan Binas, Xue Bin Peng, Sergey Levine, Yoshua Bengio</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-06-25 17:04:48</h6>
<p class='card-text'>Reinforcement learning agents that operate in diverse and complex
environments can benefit from the structured decomposition of their behavior.
Often, this is addressed in the context of hierarchical reinforcement learning,
where the aim is to decompose a policy into lower-level primitives or options,
and a higher-level meta-policy that triggers the appropriate behaviors for a
given situation. However, the meta-policy must still produce appropriate
decisions in all states. In this work, we propose a policy design that
decomposes into primitives, similarly to hierarchical reinforcement learning,
but without a high-level meta-policy. Instead, each primitive can decide for
themselves whether they wish to act in the current state. We use an
information-theoretic mechanism for enabling this decentralized decision: each
primitive chooses how much information it needs about the current state to make
a decision and the primitive that requests the most information about the
current state acts in the world. The primitives are regularized to use as
little information as possible, which leads to natural competition and
specialization. We experimentally demonstrate that this policy architecture
improves over both flat and hierarchical policies in terms of generalization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1906.10099v1' target='_blank'>DynoPlan: Combining Motion Planning and Deep Neural Network based
  Controllers for Safe HRL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel Angelov, Yordan Hristov, Subramanian Ramamoorthy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-06-24 17:34:51</h6>
<p class='card-text'>Many realistic robotics tasks are best solved compositionally, through
control architectures that sequentially invoke primitives and achieve error
correction through the use of loops and conditionals taking the system back to
alternative earlier states. Recent end-to-end approaches to task learning
attempt to directly learn a single controller that solves an entire task, but
this has been difficult for complex control tasks that would have otherwise
required a diversity of local primitive moves, and the resulting solutions are
also not easy to inspect for plan monitoring purposes. In this work, we aim to
bridge the gap between hand designed and learned controllers, by representing
each as an option in a hybrid hierarchical Reinforcement Learning framework -
DynoPlan. We extend the options framework by adding a dynamics model and the
use of a nearness-to-goal heuristic, derived from demonstrations. This
translates the optimization of a hierarchical policy controller to a problem of
planning with a model predictive controller. By unrolling the dynamics of each
option and assessing the expected value of each future state, we can create a
simple switching controller for choosing the optimal policy within a
constrained time horizon similarly to hill climbing heuristic search. The
individual dynamics model allows each option to iterate and be activated
independently of the specific underlying instantiation, thus allowing for a mix
of motion planning and deep neural network based primitives. We can assess the
safety regions of the resulting hybrid controller by investigating the
initiation sets of the different options, and also by reasoning about the
completeness and performance guarantees of the underpinning motion planners.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1906.09528v2' target='_blank'>Neural networks with motivation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sergey A. Shuvaev, Ngoc B. Tran, Marcus Stephenson-Jones, Bo Li, Alexei A. Koulakov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-06-23 01:44:32</h6>
<p class='card-text'>How can animals behave effectively in conditions involving different
motivational contexts? Here, we propose how reinforcement learning neural
networks can learn optimal behavior for dynamically changing motivational
salience vectors. First, we show that Q-learning neural networks with
motivation can navigate in environment with dynamic rewards. Second, we show
that such networks can learn complex behaviors simultaneously directed towards
several goals distributed in an environment. Finally, we show that in Pavlovian
conditioning task, the responses of the neurons in our model resemble the
firing patterns of neurons in the ventral pallidum (VP), a basal ganglia
structure involved in motivated behaviors. We show that, similarly to real
neurons, recurrent networks with motivation are composed of two
oppositely-tuned classes of neurons, responding to positive and negative
rewards. Our model generates predictions for the VP connectivity. We conclude
that networks with motivation can rapidly adapt their behavior to varying
conditions without changes in synaptic strength when expected reward is
modulated by motivation. Such networks may also provide a mechanism for how
hierarchical reinforcement learning is implemented in the brain.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1906.09223v1' target='_blank'>Disentangled Skill Embeddings for Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Janith C. Petangoda, Sergio Pascual-Diaz, Vincent Adam, Peter Vrancx, Jordi Grau-Moya</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-06-21 16:12:15</h6>
<p class='card-text'>We propose a novel framework for multi-task reinforcement learning (MTRL).
Using a variational inference formulation, we learn policies that generalize
across both changing dynamics and goals. The resulting policies are
parametrized by shared parameters that allow for transfer between different
dynamics and goal conditions, and by task-specific latent-space embeddings that
allow for specialization to particular tasks. We show how the latent-spaces
enable generalization to unseen dynamics and goals conditions. Additionally,
policies equipped with such embeddings serve as a space of skills (or options)
for hierarchical reinforcement learning. Since we can change task dynamics and
goals independently, we name our framework Disentangled Skill Embeddings (DSE).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1906.05862v4' target='_blank'>Sub-policy Adaptation for Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexander C. Li, Carlos Florensa, Ignasi Clavera, Pieter Abbeel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-06-13 16:59:48</h6>
<p class='card-text'>Hierarchical reinforcement learning is a promising approach to tackle
long-horizon decision-making problems with sparse rewards. Unfortunately, most
methods still decouple the lower-level skill acquisition process and the
training of a higher level that controls the skills in a new task. Leaving the
skills fixed can lead to significant sub-optimality in the transfer setting. In
this work, we propose a novel algorithm to discover a set of skills, and
continuously adapt them along with the higher level even when training on a new
task. Our main contributions are two-fold. First, we derive a new hierarchical
policy gradient with an unbiased latent-dependent baseline, and we introduce
Hierarchical Proximal Policy Optimization (HiPPO), an on-policy method to
efficiently train all levels of the hierarchy jointly. Second, we propose a
method for training time-abstractions that improves the robustness of the
obtained skills to environment changes. Code and results are available at
sites.google.com/view/hippo-rl</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1905.11353v2' target='_blank'>CoRide: Joint Order Dispatching and Fleet Management for Multi-Scale
  Ride-Hailing Platforms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiarui Jin, Ming Zhou, Weinan Zhang, Minne Li, Zilong Guo, Zhiwei Qin, Yan Jiao, Xiaocheng Tang, Chenxi Wang, Jun Wang, Guobin Wu, Jieping Ye</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-05-27 17:31:52</h6>
<p class='card-text'>How to optimally dispatch orders to vehicles and how to tradeoff between
immediate and future returns are fundamental questions for a typical
ride-hailing platform. We model ride-hailing as a large-scale parallel ranking
problem and study the joint decision-making task of order dispatching and fleet
management in online ride-hailing platforms. This task brings unique challenges
in the following four aspects. First, to facilitate a huge number of vehicles
to act and learn efficiently and robustly, we treat each region cell as an
agent and build a multi-agent reinforcement learning framework. Second, to
coordinate the agents from different regions to achieve long-term benefits, we
leverage the geographical hierarchy of the region grids to perform hierarchical
reinforcement learning. Third, to deal with the heterogeneous and variant
action space for joint order dispatching and fleet management, we design the
action as the ranking weight vector to rank and select the specific order or
the fleet management destination in a unified formulation. Fourth, to achieve
the multi-scale ride-hailing platform, we conduct the decision-making process
in a hierarchical way where a multi-head attention mechanism is utilized to
incorporate the impacts of neighbor agents and capture the key agent in each
scale. The whole novel framework is named as CoRide. Extensive experiments
based on multiple cities real-world data as well as analytic synthetic data
demonstrate that CoRide provides superior performance in terms of platform
revenue and user experience in the task of city-wide hybrid order dispatching
and fleet management over strong baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1905.09668v2' target='_blank'>Hierarchical Reinforcement Learning for Concurrent Discovery of Compound
  and Composable Policies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Domingo Esteban, Leonel Rozo, Darwin G. Caldwell</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-05-23 14:08:35</h6>
<p class='card-text'>A common strategy to deal with the expensive reinforcement learning (RL) of
complex tasks is to decompose them into a collection of subtasks that are
usually simpler to learn as well as reusable for new problems. However, when a
robot learns the policies for these subtasks, common approaches treat every
policy learning process separately. Therefore, all these individual
(composable) policies need to be learned before tackling the learning process
of the complex task through policies composition. Moreover, such composition of
individual policies is usually performed sequentially, which is not suitable
for tasks that require to perform the subtasks concurrently. In this paper, we
propose to combine a set of composable Gaussian policies corresponding to these
subtasks using a set of activation vectors, resulting in a complex Gaussian
policy that is a function of the means and covariances matrices of the
composable policies. Moreover, we propose an algorithm for learning both
compound and composable policies within the same learning process by exploiting
the off-policy data generated from the compound policy. The algorithm is built
on a maximum entropy RL approach to favor exploration during the learning
process. The results of the experiments show that the experience collected with
the compound policy permits not only to solve the complex task but also to
obtain useful composable policies that successfully perform in their
corresponding subtasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1905.08926v1' target='_blank'>Hierarchical Reinforcement Learning for Quadruped Locomotion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Deepali Jain, Atil Iscen, Ken Caluwaerts</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-05-22 02:28:39</h6>
<p class='card-text'>Legged locomotion is a challenging task for learning algorithms, especially
when the task requires a diverse set of primitive behaviors. To solve these
problems, we introduce a hierarchical framework to automatically decompose
complex locomotion tasks. A high-level policy issues commands in a latent space
and also selects for how long the low-level policy will execute the latent
command. Concurrently, the low-level policy uses the latent command and only
the robot's on-board sensors to control the robot's actuators. Our approach
allows the high-level policy to run at a lower frequency than the low-level
one. We test our framework on a path-following task for a dynamic quadruped
robot and we show that steering behaviors automatically emerge in the latent
command space as low-level skills are needed for this task. We then show
efficient adaptation of the trained policy to a different task by transfer of
the trained low-level policy. Finally, we validate the policies on a real
quadruped robot. To the best of our knowledge, this is the first application of
end-to-end hierarchical learning to a real robotic locomotion task.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1905.05180v1' target='_blank'>Learning and Exploiting Multiple Subgoals for Fast Exploration in
  Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Libo Xing</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-05-13 03:13:06</h6>
<p class='card-text'>Hierarchical Reinforcement Learning (HRL) exploits temporally extended
actions, or options, to make decisions from a higher-dimensional perspective to
alleviate the sparse reward problem, one of the most challenging problems in
reinforcement learning. The majority of existing HRL algorithms require either
significant manual design with respect to the specific environment or enormous
exploration to automatically learn options from data. To achieve fast
exploration without using manual design, we devise a multi-goal HRL algorithm,
consisting of a high-level policy Manager and a low-level policy Worker. The
Manager provides the Worker multiple subgoals at each time step. Each subgoal
corresponds to an option to control the environment. Although the agent may
show some confusion at the beginning of training since it is guided by three
diverse subgoals, the agent's behavior policy will quickly learn how to respond
to multiple subgoals from the high-level controller on different occasions. By
exploiting multiple subgoals, the exploration efficiency is significantly
improved. We conduct experiments in Atari's Montezuma's Revenge environment, a
well-known sparse reward environment, and in doing so achieve the same
performance as state-of-the-art HRL methods with substantially reduced training
time cost.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1904.06703v2' target='_blank'>Dot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic
  Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Benjamin Beyret, Ali Shafti, A. Aldo Faisal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-04-14 14:54:14</h6>
<p class='card-text'>Robotic systems are ever more capable of automation and fulfilment of complex
tasks, particularly with reliance on recent advances in intelligent systems,
deep learning and artificial intelligence. However, as robots and humans come
closer in their interactions, the matter of interpretability, or explainability
of robot decision-making processes for the human grows in importance. A
successful interaction and collaboration will only take place through mutual
understanding of underlying representations of the environment and the task at
hand. This is currently a challenge in deep learning systems. We present a
hierarchical deep reinforcement learning system, consisting of a low-level
agent handling the large actions/states space of a robotic system efficiently,
by following the directives of a high-level agent which is learning the
high-level dynamics of the environment and task. This high-level agent forms a
representation of the world and task at hand that is interpretable for a human
operator. The method, which we call Dot-to-Dot, is tested on a MuJoCo-based
model of the Fetch Robotics Manipulator, as well as a Shadow Hand, to test its
performance. Results show efficient learning of complex actions/states spaces
by the low-level agent, and an interpretable representation of the task and
decision-making process learned by the high-level agent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1903.09374v1' target='_blank'>Deep Hierarchical Reinforcement Learning Based Recommendations via
  Multi-goals Abstraction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongyang Zhao, Liang Zhang, Bo Zhang, Lizhou Zheng, Yongjun Bao, Weipeng Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-03-22 06:43:49</h6>
<p class='card-text'>The recommender system is an important form of intelligent application, which
assists users to alleviate from information redundancy. Among the metrics used
to evaluate a recommender system, the metric of conversion has become more and
more important. The majority of existing recommender systems perform poorly on
the metric of conversion due to its extremely sparse feedback signal. To tackle
this challenge, we propose a deep hierarchical reinforcement learning based
recommendation framework, which consists of two components, i.e., high-level
agent and low-level agent. The high-level agent catches long-term sparse
conversion signals, and automatically sets abstract goals for low-level agent,
while the low-level agent follows the abstract goals and interacts with
real-time environment. To solve the inherent problem in hierarchical
reinforcement learning, we propose a novel deep hierarchical reinforcement
learning algorithm via multi-goals abstraction (HRL-MG). Our proposed algorithm
contains three characteristics: 1) the high-level agent generates multiple
goals to guide the low-level agent in different stages, which reduces the
difficulty of approaching high-level goals; 2) different goals share the same
state encoder parameters, which increases the update frequency of the
high-level agent and thus accelerates the convergence of our proposed
algorithm; 3) an appreciate benefit assignment function is designed to allocate
rewards in each goal so as to coordinate different goals in a consistent
direction. We evaluate our proposed algorithm based on a real-world e-commerce
dataset and validate its effectiveness.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1903.03064v1' target='_blank'>RLOC: Neurobiologically Inspired Hierarchical Reinforcement Learning
  Algorithm for Continuous Control of Nonlinear Dynamical Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ekaterina Abramova, Luke Dickens, Daniel Kuhn, Aldo Faisal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-03-07 17:37:53</h6>
<p class='card-text'>Nonlinear optimal control problems are often solved with numerical methods
that require knowledge of system's dynamics which may be difficult to infer,
and that carry a large computational cost associated with iterative
calculations. We present a novel neurobiologically inspired hierarchical
learning framework, Reinforcement Learning Optimal Control, which operates on
two levels of abstraction and utilises a reduced number of controllers to solve
nonlinear systems with unknown dynamics in continuous state and action spaces.
Our approach is inspired by research at two levels of abstraction: first, at
the level of limb coordination human behaviour is explained by linear optimal
feedback control theory. Second, in cognitive tasks involving learning symbolic
level action selection, humans learn such problems using model-free and
model-based reinforcement learning algorithms. We propose that combining these
two levels of abstraction leads to a fast global solution of nonlinear control
problems using reduced number of controllers. Our framework learns the local
task dynamics from naive experience and forms locally optimal infinite horizon
Linear Quadratic Regulators which produce continuous low-level control. A
top-level reinforcement learner uses the controllers as actions and learns how
to best combine them in state space while maximising a long-term reward. A
single optimal control objective function drives high-level symbolic learning
by providing training signals on desirability of each selected controller. We
show that a small number of locally optimal linear controllers are able to
solve global nonlinear control problems with unknown dynamics when combined
with a reinforcement learner in this hierarchical framework. Our algorithm
competes in terms of computational cost and solution quality with sophisticated
control algorithms and we illustrate this with solutions to benchmark problems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1903.01567v1' target='_blank'>Model Primitive Hierarchical Lifelong Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bohan Wu, Jayesh K. Gupta, Mykel J. Kochenderfer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-03-04 22:14:23</h6>
<p class='card-text'>Learning interpretable and transferable subpolicies and performing task
decomposition from a single, complex task is difficult. Some traditional
hierarchical reinforcement learning techniques enforce this decomposition in a
top-down manner, while meta-learning techniques require a task distribution at
hand to learn such decompositions. This paper presents a framework for using
diverse suboptimal world models to decompose complex task solutions into
simpler modular subpolicies. This framework performs automatic decomposition of
a single source task in a bottom up manner, concurrently learning the required
modular subpolicies as well as a controller to coordinate them. We perform a
series of experiments on high dimensional continuous action control tasks to
demonstrate the effectiveness of this approach at both complex single task
learning and lifelong learning. Finally, we perform ablation studies to
understand the importance and robustness of different elements in the framework
and limitations to this approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1902.10140v2' target='_blank'>Planning in Hierarchical Reinforcement Learning: Guarantees for Using
  Local Policies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tom Zahavy, Avinatan Hasidim, Haim Kaplan, Yishay Mansour</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-02-26 15:04:18</h6>
<p class='card-text'>We consider a settings of hierarchical reinforcement learning, in which the
reward is a sum of components. For each component we are given a policy that
maximizes it and our goal is to assemble a policy from the individual policies
that maximizes the sum of the components. We provide theoretical guarantees for
assembling such policies in deterministic MDPs with collectible rewards. Our
approach builds on formulating this problem as a traveling salesman problem
with discounted reward. We focus on local solutions, i.e., policies that only
use information from the current state; thus, they are easy to implement and do
not require substantial computational resources. We propose three local
stochastic policies and prove that they guarantee better performance than any
deterministic local policy in the worst case; experimental results suggest that
they also perform better on average.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1902.08882v1' target='_blank'>Aggregating E-commerce Search Results from Heterogeneous Sources via
  Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ryuichi Takanobu, Tao Zhuang, Minlie Huang, Jun Feng, Haihong Tang, Bo Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-02-24 03:18:43</h6>
<p class='card-text'>In this paper, we investigate the task of aggregating search results from
heterogeneous sources in an E-commerce environment. First, unlike traditional
aggregated web search that merely presents multi-sourced results in the first
page, this new task may present aggregated results in all pages and has to
dynamically decide which source should be presented in the current page.
Second, as pointed out by many existing studies, it is not trivial to rank
items from heterogeneous sources because the relevance scores from different
source systems are not directly comparable. To address these two issues, we
decompose the task into two subtasks in a hierarchical structure: a high-level
task for source selection where we model the sequential patterns of user
behaviors onto aggregated results in different pages so as to understand user
intents and select the relevant sources properly; and a low-level task for item
presentation where we formulate a slot filling process to sequentially present
the items instead of giving each item a relevance score when deciding the
presentation order of heterogeneous items. Since both subtasks can be naturally
formulated as sequential decision problems and learn from the future user
feedback on search results, we build our model with hierarchical reinforcement
learning. Extensive experiments demonstrate that our model obtains remarkable
improvements in search performance metrics, and achieves a higher user
satisfaction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1902.05650v4' target='_blank'>Asynchronous Coagent Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:James E. Kostas, Chris Nota, Philip S. Thomas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-02-15 00:16:10</h6>
<p class='card-text'>Coagent policy gradient algorithms (CPGAs) are reinforcement learning
algorithms for training a class of stochastic neural networks called coagent
networks. In this work, we prove that CPGAs converge to locally optimal
policies. Additionally, we extend prior theory to encompass asynchronous and
recurrent coagent networks. These extensions facilitate the straightforward
design and analysis of hierarchical reinforcement learning algorithms like the
option-critic, and eliminate the need for complex derivations of customized
learning rules for these algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1901.08004v6' target='_blank'>Hierarchical Reinforcement Learning for Multi-agent MOBA Game</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhijian Zhang, Haozheng Li, Luo Zhang, Tianyin Zheng, Ting Zhang, Xiong Hao, Xiaoxin Chen, Min Chen, Fangxu Xiao, Wei Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-01-23 17:08:15</h6>
<p class='card-text'>Real Time Strategy (RTS) games require macro strategies as well as micro
strategies to obtain satisfactory performance since it has large state space,
action space, and hidden information. This paper presents a novel hierarchical
reinforcement learning model for mastering Multiplayer Online Battle Arena
(MOBA) games, a sub-genre of RTS games. The novelty of this work are: (1)
proposing a hierarchical framework, where agents execute macro strategies by
imitation learning and carry out micromanipulations through reinforcement
learning, (2) developing a simple self-learning method to get better sample
efficiency for training, and (3) designing a dense reward function for
multi-agent cooperation in the absence of game engine or Application
Programming Interface (API). Finally, various experiments have been performed
to validate the superior performance of the proposed method over other
state-of-the-art reinforcement learning algorithms. Agent successfully learns
to combat and defeat bronze-level built-in AI with 100% win rate, and
experiments show that our method can create a competitive multi-agent for a
kind of mobile MOBA game {\it King of Glory} in 5v5 mode.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1901.01365v2' target='_blank'>Hierarchical Reinforcement Learning via Advantage-Weighted Information
  Maximization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Takayuki Osa, Voot Tangkaratt, Masashi Sugiyama</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2019-01-05 04:43:05</h6>
<p class='card-text'>Real-world tasks are often highly structured. Hierarchical reinforcement
learning (HRL) has attracted research interest as an approach for leveraging
the hierarchical structure of a given task in reinforcement learning (RL).
However, identifying the hierarchical policy structure that enhances the
performance of RL is not a trivial task. In this paper, we propose an HRL
method that learns a latent variable of a hierarchical policy using mutual
information maximization. Our approach can be interpreted as a way to learn a
discrete and latent representation of the state-action space. To learn option
policies that correspond to modes of the advantage function, we introduce
advantage-weighted importance sampling. In our HRL method, the gating policy
learns to select option policies based on an option-value function, and these
option policies are optimized based on the deterministic policy gradient
method. This framework is derived by leveraging the analogy between a
monolithic policy in standard RL and a hierarchical policy in HRL by using a
deterministic option policy. Experimental results indicate that our HRL
approach can learn a diversity of options and that it can enhance the
performance of RL in continuous control tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1812.09521v1' target='_blank'>Escape Room: A Configurable Testbed for Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jacob Menashe, Peter Stone</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-12-22 12:29:20</h6>
<p class='card-text'>Recent successes in Reinforcement Learning have encouraged a fast-growing
network of RL researchers and a number of breakthroughs in RL research. As the
RL community and the body of RL work grows, so does the need for widely
applicable benchmarks that can fairly and effectively evaluate a variety of RL
algorithms.
  This need is particularly apparent in the realm of Hierarchical Reinforcement
Learning (HRL). While many existing test domains may exhibit hierarchical
action or state structures, modern RL algorithms still exhibit great difficulty
in solving domains that necessitate hierarchical modeling and action planning,
even when such domains are seemingly trivial. These difficulties highlight both
the need for more focus on HRL algorithms themselves, and the need for new
testbeds that will encourage and validate HRL research.
  Existing HRL testbeds exhibit a Goldilocks problem; they are often either too
simple (e.g. Taxi) or too complex (e.g. Montezuma's Revenge from the Arcade
Learning Environment). In this paper we present the Escape Room Domain (ERD), a
new flexible, scalable, and fully implemented testing domain for HRL that
bridges the "moderate complexity" gap left behind by existing alternatives. ERD
is open-source and freely available through GitHub, and conforms to widely-used
public testing interfaces for simple integration and testing with a variety of
public RL agent implementations. We show that the ERD presents a suite of
challenges with scalable difficulty to provide a smooth learning gradient from
Taxi to the Arcade Learning Environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1812.01488v1' target='_blank'>Natural Option Critic</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Saket Tiwari, Philip S. Thomas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-12-04 15:33:26</h6>
<p class='card-text'>The recently proposed option-critic architecture Bacon et al. provide a
stochastic policy gradient approach to hierarchical reinforcement learning.
Specifically, they provide a way to estimate the gradient of the expected
discounted return with respect to parameters that define a finite number of
temporally extended actions, called \textit{options}. In this paper we show how
the option-critic architecture can be extended to estimate the natural gradient
of the expected discounted return. To this end, the central questions that we
consider in this paper are: 1) what is the definition of the natural gradient
in this context, 2) what is the Fisher information matrix associated with an
option's parameterized policy, 3) what is the Fisher information matrix
associated with an option's parameterized termination function, and 4) how can
a compatible function approximation approach be leveraged to obtain natural
gradient estimates for both the parameterized policy and parameterized
termination functions of an option with per-time-step time and space complexity
linear in the total number of parameters. Based on answers to these questions
we introduce the natural option critic algorithm. Experimental results showcase
improvement over the vanilla gradient approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1812.01487v2' target='_blank'>Hyperbolic Embeddings for Learning Options in Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Saket Tiwari, M. Prannoy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-12-04 15:30:23</h6>
<p class='card-text'>Hierarchical reinforcement learning deals with the problem of breaking down
large tasks into meaningful sub-tasks. Autonomous discovery of these sub-tasks
has remained a challenging problem. We propose a novel method of learning
sub-tasks by combining paradigms of routing in computer networks and graph
based skill discovery within the options framework to define meaningful
sub-goals. We apply the recent advancements of learning embeddings using
Riemannian optimisation in the hyperbolic space to embed the state set into the
hyperbolic space and create a model of the environment. In doing so we enforce
a global topology on the states and are able to exploit this topology to learn
meaningful sub-tasks. We demonstrate empirically, both in discrete and
continuous domains, how these embeddings can improve the learning of meaningful
sub-tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1811.09083v1' target='_blank'>Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sainbayar Sukhbaatar, Emily Denton, Arthur Szlam, Rob Fergus</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-11-22 10:15:52</h6>
<p class='card-text'>In hierarchical reinforcement learning a major challenge is determining
appropriate low-level policies. We propose an unsupervised learning scheme,
based on asymmetric self-play from Sukhbaatar et al. (2018), that automatically
learns a good representation of sub-goals in the environment and a low-level
policy that can execute them. A high-level policy can then direct the lower one
by generating a sequence of continuous sub-goal vectors. We evaluate our model
using Mazebase and Mujoco environments, including the challenging AntGather
task. Visualizations of the sub-goal embeddings reveal a logical decomposition
of tasks within the environment. Quantitatively, our approach obtains
compelling performance gains over non-hierarchical approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1811.07819v2' target='_blank'>Learning Actionable Representations with Goal-Conditioned Policies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dibya Ghosh, Abhishek Gupta, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-11-19 17:30:36</h6>
<p class='card-text'>Representation learning is a central challenge across a range of machine
learning areas. In reinforcement learning, effective and functional
representations have the potential to tremendously accelerate learning progress
and solve more challenging problems. Most prior work on representation learning
has focused on generative approaches, learning representations that capture all
underlying factors of variation in the observation space in a more disentangled
or well-ordered manner. In this paper, we instead aim to learn functionally
salient representations: representations that are not necessarily complete in
terms of capturing all factors of variation in the observation space, but
rather aim to capture those factors of variation that are important for
decision making -- that are "actionable." These representations are aware of
the dynamics of the environment, and capture only the elements of the
observation that are necessary for decision making rather than all factors of
variation, without explicit reconstruction of the observation. We show how
these representations can be useful to improve exploration for sparse reward
problems, to enable long horizon hierarchical reinforcement learning, and as a
state representation for learning policies for downstream tasks. We evaluate
our method on a number of simulated environments, and compare it to prior
methods for representation learning, exploration, and hierarchical
reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1811.04324v2' target='_blank'>Diversity-Driven Extensible Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhang Song, Jianyi Wang, Thomas Lukasiewicz, Zhenghua Xu, Mai Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-11-10 23:35:34</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) has recently shown promising
advances on speeding up learning, improving the exploration, and discovering
intertask transferable skills. Most recent works focus on HRL with two levels,
i.e., a master policy manipulates subpolicies, which in turn manipulate
primitive actions. However, HRL with multiple levels is usually needed in many
real-world scenarios, whose ultimate goals are highly abstract, while their
actions are very primitive. Therefore, in this paper, we propose a
diversity-driven extensible HRL (DEHRL), where an extensible and scalable
framework is built and learned levelwise to realize HRL with multiple levels.
DEHRL follows a popular assumption: diverse subpolicies are useful, i.e.,
subpolicies are believed to be more useful if they are more diverse. However,
existing implementations of this diversity assumption usually have their own
drawbacks, which makes them inapplicable to HRL with multiple levels.
Consequently, we further propose a novel diversity-driven solution to achieve
this assumption in DEHRL. Experimental studies evaluate DEHRL with five
baselines from four perspectives in two domains; the results show that DEHRL
outperforms the state-of-the-art baselines in all four aspects.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1811.03925v1' target='_blank'>A Hierarchical Framework for Relation Extraction with Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ryuichi Takanobu, Tianyang Zhang, Jiexi Liu, Minlie Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-11-09 14:33:29</h6>
<p class='card-text'>Most existing methods determine relation types only after all the entities
have been recognized, thus the interaction between relation types and entity
mentions is not fully modeled. This paper presents a novel paradigm to deal
with relation extraction by regarding the related entities as the arguments of
a relation. We apply a hierarchical reinforcement learning (HRL) framework in
this paradigm to enhance the interaction between entity mentions and relation
types. The whole extraction process is decomposed into a hierarchy of two-level
RL policies for relation detection and entity extraction respectively, so that
it is more feasible and natural to deal with overlapping relations. Our model
was evaluated on public datasets collected via distant supervision, and results
show that it gains better performance than existing methods and is more
powerful for extracting overlapping relations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1811.01237v1' target='_blank'>Relation Mention Extraction from Noisy Data with Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jun Feng, Minlie Huang, Yijie Zhang, Yang Yang, Xiaoyan Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-11-03 15:50:27</h6>
<p class='card-text'>In this paper we address a task of relation mention extraction from noisy
data: extracting representative phrases for a particular relation from noisy
sentences that are collected via distant supervision. Despite its significance
and value in many downstream applications, this task is less studied on noisy
data. The major challenges exists in 1) the lack of annotation on mention
phrases, and more severely, 2) handling noisy sentences which do not express a
relation at all. To address the two challenges, we formulate the task as a
semi-Markov decision process and propose a novel hierarchical reinforcement
learning model. Our model consists of a top-level sentence selector to remove
noisy sentences, a low-level mention extractor to extract relation mentions,
and a reward estimator to provide signals to guide data denoising and mention
extraction without explicit annotations. Experimental results show that our
model is effective to extract relation mentions from noisy data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1810.10096v3' target='_blank'>Learning Representations in Model-Free Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jacob Rafati, David C. Noelle</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-10-23 21:24:06</h6>
<p class='card-text'>Common approaches to Reinforcement Learning (RL) are seriously challenged by
large-scale applications involving huge state spaces and sparse delayed reward
feedback. Hierarchical Reinforcement Learning (HRL) methods attempt to address
this scalability issue by learning action selection policies at multiple levels
of temporal abstraction. Abstraction can be had by identifying a relatively
small set of states that are likely to be useful as subgoals, in concert with
the learning of corresponding skill policies to achieve those subgoals. Many
approaches to subgoal discovery in HRL depend on the analysis of a model of the
environment, but the need to learn such a model introduces its own problems of
scale. Once subgoals are identified, skills may be learned through intrinsic
motivation, introducing an internal reward signal marking subgoal attainment.
In this paper, we present a novel model-free method for subgoal discovery using
incremental unsupervised learning over a small memory of the most recent
experiences (trajectories) of the agent. When combined with an intrinsic
motivation learning mechanism, this method learns both subgoals and skills,
based on experiences in the environment. Thus, we offer an original approach to
HRL that does not require the acquisition of a model of the environment,
suitable for large-scale applications. We demonstrate the efficiency of our
method on two RL problems with sparse delayed feedback: a variant of the rooms
environment and the first screen of the ATARI 2600 Montezuma's Revenge game.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1810.01257v2' target='_blank'>Near-Optimal Representation Learning for Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ofir Nachum, Shixiang Gu, Honglak Lee, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-10-02 14:00:14</h6>
<p class='card-text'>We study the problem of representation learning in goal-conditioned
hierarchical reinforcement learning. In such hierarchical structures, a
higher-level controller solves tasks by iteratively communicating goals which a
lower-level policy is trained to reach. Accordingly, the choice of
representation -- the mapping of observation space to goal space -- is crucial.
To study this problem, we develop a notion of sub-optimality of a
representation, defined in terms of expected reward of the optimal hierarchical
policy using this representation. We derive expressions which bound the
sub-optimality and show how these expressions can be translated to
representation learning objectives which may be optimized in practice. Results
on a number of difficult continuous-control tasks show that our approach to
representation learning yields qualitatively better representations as well as
quantitatively better hierarchical policies, compared to existing methods (see
videos at https://sites.google.com/view/representation-hrl).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1809.09095v2' target='_blank'>On Reinforcement Learning for Full-length Game of StarCraft</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhen-Jia Pang, Ruo-Ze Liu, Zhou-Yu Meng, Yi Zhang, Yang Yu, Tong Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-09-23 15:48:28</h6>
<p class='card-text'>StarCraft II poses a grand challenge for reinforcement learning. The main
difficulties of it include huge state and action space and a long-time horizon.
In this paper, we investigate a hierarchical reinforcement learning approach
for StarCraft II. The hierarchy involves two levels of abstraction. One is the
macro-action automatically extracted from expert's trajectories, which reduces
the action space in an order of magnitude yet remains effective. The other is a
two-layer hierarchical architecture which is modular and easy to scale,
enabling a curriculum transferring from simpler tasks to more complex tasks.
The reinforcement training algorithm for this architecture is also
investigated. On a 64x64 map and using restrictive units, we achieve a winning
rate of more than 99\% against the difficulty level-1 built-in AI. Through the
curriculum transfer learning algorithm and a mixture of combat model, we can
achieve over 93\% winning rate of Protoss against the most difficult
non-cheating built-in AI (level-7) of Terran, training within two days using a
single machine with only 48 CPU cores and 8 K40 GPUs. It also shows strong
generalization performance, when tested against never seen opponents including
cheating levels built-in AI and all levels of Zerg and Protoss built-in AI. We
hope this study could shed some light on the future research of large-scale
reinforcement learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1809.08590v1' target='_blank'>Neural Arithmetic Expression Calculator</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaiyu Chen, Yihan Dong, Xipeng Qiu, Zitian Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-09-23 13:05:28</h6>
<p class='card-text'>This paper presents a pure neural solver for arithmetic expression
calculation (AEC) problem. Previous work utilizes the powerful capabilities of
deep neural networks and attempts to build an end-to-end model to solve this
problem. However, most of these methods can only deal with the additive
operations. It is still a challenging problem to solve the complex expression
calculation problem, which includes the adding, subtracting, multiplying,
dividing and bracketing operations. In this work, we regard the arithmetic
expression calculation as a hierarchical reinforcement learning problem. An
arithmetic operation is decomposed into a series of sub-tasks, and each
sub-task is dealt with by a skill module. The skill module could be a basic
module performing elementary operations, or interactive module performing
complex operations by invoking other skill models. With curriculum learning,
our model can deal with a complex arithmetic expression calculation with the
deep hierarchical structure of skill models. Experiments show that our model
significantly outperforms the previous models for arithmetic expression
calculation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1809.03406v2' target='_blank'>Combining imagination and heuristics to learn strategies that generalize</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Erik J Peterson, Necati Alp Müyesser, Timothy Verstynen, Kyle Dunovan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-09-10 15:43:57</h6>
<p class='card-text'>Deep reinforcement learning can match or exceed human performance in stable
contexts, but with minor changes to the environment artificial networks, unlike
humans, often cannot adapt. Humans rely on a combination of heuristics to
simplify computational load and imagination to extend experiential learning to
new and more challenging environments. Motivated by theories of the
hierarchical organization of the human prefrontal networks, we have developed a
model of hierarchical reinforcement learning that combines both heuristics and
imagination into a stumbler-strategist network. We test performance of this
network using Wythoff's game, a gridworld environment with a known optimal
strategy. We show that a heuristic labeling of each position as hot or cold,
combined with imagined play, both accelerates learning and promotes transfer to
novel games, while also improving model interpretability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1808.06740v2' target='_blank'>Interactive Semantic Parsing for If-Then Recipes via Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyu Yao, Xiujun Li, Jianfeng Gao, Brian Sadler, Huan Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-08-21 02:39:08</h6>
<p class='card-text'>Given a text description, most existing semantic parsers synthesize a program
in one shot. However, it is quite challenging to produce a correct program
solely based on the description, which in reality is often ambiguous or
incomplete. In this paper, we investigate interactive semantic parsing, where
the agent can ask the user clarification questions to resolve ambiguities via a
multi-turn dialogue, on an important type of programs called "If-Then recipes."
We develop a hierarchical reinforcement learning (HRL) based agent that
significantly improves the parsing performance with minimal questions to the
user. Results under both simulation and human evaluation show that our agent
substantially outperforms non-interactive semantic parsers and rule-based
agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1807.11150v1' target='_blank'>Learning to Interrupt: A Hierarchical Deep Reinforcement Learning
  Framework for Efficient Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tingguang Li, Jin Pan, Delong Zhu, Max Q. -H. Meng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-07-30 02:39:10</h6>
<p class='card-text'>To achieve scenario intelligence, humans must transfer knowledge to robots by
developing goal-oriented algorithms, which are sometimes insensitive to
dynamically changing environments. While deep reinforcement learning achieves
significant success recently, it is still extremely difficult to be deployed in
real robots directly. In this paper, we propose a hybrid structure named
Option-Interruption in which human knowledge is embedded into a hierarchical
reinforcement learning framework. Our architecture has two key components:
options, represented by existing human-designed methods, can significantly
speed up the training process and interruption mechanism, based on learnable
termination functions, enables our system to quickly respond to the external
environment. To implement this architecture, we derive a set of update rules
based on policy gradient methods and present a complete training process. In
the experiment part, our method is evaluated in Four-room navigation and
exploration task, which shows the efficiency and flexibility of our framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1807.08060v2' target='_blank'>Safe Option-Critic: Learning Safety in the Option-Critic Architecture</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arushi Jain, Khimya Khetarpal, Doina Precup</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-07-21 00:39:23</h6>
<p class='card-text'>Designing hierarchical reinforcement learning algorithms that exhibit safe
behaviour is not only vital for practical applications but also, facilitates a
better understanding of an agent's decisions. We tackle this problem in the
options framework, a particular way to specify temporally abstract actions
which allow an agent to use sub-policies with start and end conditions. We
consider a behaviour as safe that avoids regions of state-space with high
uncertainty in the outcomes of actions. We propose an optimization objective
that learns safe options by encouraging the agent to visit states with higher
behavioural consistency. The proposed objective results in a trade-off between
maximizing the standard expected return and minimizing the effect of model
uncertainty in the return. We propose a policy gradient algorithm to optimize
the constrained objective function. We examine the quantitative and qualitative
behaviour of the proposed approach in a tabular grid-world, continuous-state
puddle-world, and three games from the Arcade Learning Environment: Ms.Pacman,
Amidar, and Q*Bert. Our approach achieves a reduction in the variance of
return, boosts performance in environments with intrinsic variability in the
reward structure, and compares favorably both with primitive actions as well as
with risk-neutral options.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1807.07665v4' target='_blank'>Hierarchical Reinforcement Learning for Zero-shot Generalization with
  Subtask Dependencies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sungryull Sohn, Junhyuk Oh, Honglak Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-07-19 23:51:55</h6>
<p class='card-text'>We introduce a new RL problem where the agent is required to generalize to a
previously-unseen environment characterized by a subtask graph which describes
a set of subtasks and their dependencies. Unlike existing hierarchical
multitask RL approaches that explicitly describe what the agent should do at a
high level, our problem only describes properties of subtasks and relationships
among them, which requires the agent to perform complex reasoning to find the
optimal subtask to execute. To solve this problem, we propose a neural subtask
graph solver (NSGS) which encodes the subtask graph using a recursive neural
network embedding. To overcome the difficulty of training, we propose a novel
non-parametric gradient-based policy, graph reward propagation, to pre-train
our NSGS agent and further finetune it through actor-critic method. The
experimental results on two 2D visual domains show that our agent can perform
complex reasoning to find a near-optimal way of executing the subtask graph and
generalize well to the unseen subtask graphs. In addition, we compare our agent
with a Monte-Carlo tree search (MCTS) method showing that our method is much
more efficient than MCTS, and the performance of NSGS can be further improved
by combining it with MCTS.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1807.07134v1' target='_blank'>Representational efficiency outweighs action efficiency in human program
  induction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sophia Sanborn, David D. Bourgin, Michael Chang, Thomas L. Griffiths</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-07-18 20:20:40</h6>
<p class='card-text'>The importance of hierarchically structured representations for tractable
planning has long been acknowledged. However, the questions of how people
discover such abstractions and how to define a set of optimal abstractions
remain open. This problem has been explored in cognitive science in the problem
solving literature and in computer science in hierarchical reinforcement
learning. Here, we emphasize an algorithmic perspective on learning
hierarchical representations in which the objective is to efficiently encode
the structure of the problem, or, equivalently, to learn an algorithm with
minimal length. We introduce a novel problem-solving paradigm that links
problem solving and program induction under the Markov Decision Process (MDP)
framework. Using this task, we target the question of whether humans discover
hierarchical solutions by maximizing efficiency in number of actions they
generate or by minimizing the complexity of the resulting representation and
find evidence for the primacy of representational efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1807.05424v2' target='_blank'>Hierarchical Reinforcement Learning Framework towards Multi-agent
  Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenhao Ding, Shuaijun Li, Huihuan Qian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-07-14 18:07:32</h6>
<p class='card-text'>In this paper, we propose a navigation algorithm oriented to multi-agent
environment. This algorithm is expressed as a hierarchical framework that
contains a Hidden Markov Model (HMM) and a Deep Reinforcement Learning (DRL)
structure. For simplification, we term our method Hierarchical Navigation
Reinforcement Network (HNRN). In high- level architecture, we train an HMM to
evaluate the agent's perception to obtain a score. According to this score,
adaptive control action will be chosen. While in low-level architecture, two
sub-systems are introduced, one is a differential target- driven system, which
aims at heading to the target; the other is a collision avoidance DRL system,
which is used for avoiding dynamic obstacles. The advantage of this
hierarchical structure is decoupling the target-driven and collision avoidance
tasks, leading to a faster and more stable model to be trained. The experiments
indicate that our algorithm has higher learning efficiency and rate of success
than traditional Velocity Obstacle (VO) algorithms or hybrid DRL method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1806.10792v1' target='_blank'>Hierarchical Reinforcement Learning with Abductive Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kazeto Yamamoto, Takashi Onishi, Yoshimasa Tsuruoka</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-06-28 06:56:19</h6>
<p class='card-text'>One of the key challenges in applying reinforcement learning to real-life
problems is that the amount of train-and-error required to learn a good policy
increases drastically as the task becomes complex. One potential solution to
this problem is to combine reinforcement learning with automated symbol
planning and utilize prior knowledge on the domain. However, existing methods
have limitations in their applicability and expressiveness. In this paper we
propose a hierarchical reinforcement learning method based on abductive
symbolic planning. The planner can deal with user-defined evaluation functions
and is not based on the Herbrand theorem. Therefore it can utilize prior
knowledge of the rewards and can work in a domain where the state space is
unknown. We demonstrate empirically that our architecture significantly
improves learning efficiency with respect to the amount of training examples on
the evaluation domain, in which the state space is unknown and there exist
multiple goals.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1806.05292v1' target='_blank'>Automatic formation of the structure of abstract machines in
  hierarchical reinforcement learning with state clustering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aleksandr I. Panov, Aleksey Skrynnik</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-06-13 22:40:49</h6>
<p class='card-text'>We introduce a new approach to hierarchy formation and task decomposition in
hierarchical reinforcement learning. Our method is based on the Hierarchy Of
Abstract Machines (HAM) framework because HAM approach is able to design
efficient controllers that will realize specific behaviors in real robots. The
key to our algorithm is the introduction of the internal or "mental"
environment in which the state represents the structure of the HAM hierarchy.
The internal action in this environment leads to changes the hierarchy of HAMs.
We propose the classical Q-learning procedure in the internal environment which
allows the agent to obtain an optimal hierarchy. We extends the HAM framework
by adding on-model approach to select the appropriate sub-machine to execute
action sequences for certain class of external environment states. Preliminary
experiments demonstrated the prospects of the method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1806.02813v1' target='_blank'>Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement
  Learning with Trajectory Embeddings</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:John D. Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-06-07 17:49:08</h6>
<p class='card-text'>In this work, we take a representation learning perspective on hierarchical
reinforcement learning, where the problem of learning lower layers in a
hierarchy is transformed into the problem of learning trajectory-level
generative models. We show that we can learn continuous latent representations
of trajectories, which are effective in solving temporally extended and
multi-stage problems. Our proposed model, SeCTAR, draws inspiration from
variational autoencoders, and learns latent representations of trajectories. A
key component of this method is to learn both a latent-conditioned policy and a
latent-conditioned model which are consistent with each other. Given the same
latent, the policy generates a trajectory which should match the trajectory
predicted by the model. This model provides a built-in prediction mechanism, by
predicting the outcome of closed loop policy behavior. We propose a novel
algorithm for performing hierarchical RL with this model, combining model-based
planning in the learned latent space with an unsupervised exploration
objective. We show that our model is effective at reasoning over long horizons
with sparse rewards for several simulated tasks, outperforming standard
reinforcement learning methods and prior methods for hierarchical reasoning,
model-based planning, and exploration.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1805.08296v4' target='_blank'>Data-Efficient Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ofir Nachum, Shixiang Gu, Honglak Lee, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-05-21 21:33:44</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) is a promising approach to extend
traditional reinforcement learning (RL) methods to solve more complex tasks.
Yet, the majority of current HRL methods require careful task-specific design
and on-policy training, making them difficult to apply in real-world scenarios.
In this paper, we study how we can develop HRL algorithms that are general, in
that they do not make onerous additional assumptions beyond standard RL
algorithms, and efficient, in the sense that they can be used with modest
numbers of interaction samples, making them suitable for real-world problems
such as robotic control. For generality, we develop a scheme where lower-level
controllers are supervised with goals that are learned and proposed
automatically by the higher-level controllers. To address efficiency, we
propose to use off-policy experience for both higher and lower-level training.
This poses a considerable challenge, since changes to the lower-level behaviors
change the action space for the higher-level policy, and we introduce an
off-policy correction to remedy this challenge. This allows us to take
advantage of recent advances in off-policy model-free RL to learn both higher-
and lower-level policies using substantially fewer environment interactions
than on-policy algorithms. We term the resulting HRL agent HIRO and find that
it is generally applicable and highly sample-efficient. Our experiments show
that HIRO can be used to learn highly complex behaviors for simulated robots,
such as pushing objects and utilizing them to reach target locations, learning
from only a few million samples, equivalent to a few days of real-time
interaction. In comparisons with a number of prior HRL methods, we find that
our approach substantially outperforms previous state-of-the-art techniques.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1805.08180v2' target='_blank'>Hierarchical Reinforcement Learning with Hindsight</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andrew Levy, Robert Platt, Kate Saenko</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-05-21 17:02:53</h6>
<p class='card-text'>Reinforcement Learning (RL) algorithms can suffer from poor sample efficiency
when rewards are delayed and sparse. We introduce a solution that enables
agents to learn temporally extended actions at multiple levels of abstraction
in a sample efficient and automated fashion. Our approach combines universal
value functions and hindsight learning, allowing agents to learn policies
belonging to different time scales in parallel. We show that our method
significantly accelerates learning in a variety of discrete and continuous
tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1805.07008v1' target='_blank'>Hierarchical Reinforcement Learning with Deep Nested Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marc Brittain, Peng Wei</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-05-18 01:06:36</h6>
<p class='card-text'>Deep hierarchical reinforcement learning has gained a lot of attention in
recent years due to its ability to produce state-of-the-art results in
challenging environments where non-hierarchical frameworks fail to learn useful
policies. However, as problem domains become more complex, deep hierarchical
reinforcement learning can become inefficient, leading to longer convergence
times and poor performance. We introduce the Deep Nested Agent framework, which
is a variant of deep hierarchical reinforcement learning where information from
the main agent is propagated to the low level $nested$ agent by incorporating
this information into the nested agent's state. We demonstrate the
effectiveness and performance of the Deep Nested Agent framework by applying it
to three scenarios in Minecraft with comparisons to a deep non-hierarchical
single agent framework, as well as, a deep hierarchical framework.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1805.04419v1' target='_blank'>Deep Hierarchical Reinforcement Learning Algorithm in Partially
  Observable Markov Decision Processes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Le Pham Tuyen, Ngo Anh Vien, Abu Layek, TaeChoong Chung</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-05-11 14:30:21</h6>
<p class='card-text'>In recent years, reinforcement learning has achieved many remarkable
successes due to the growing adoption of deep learning techniques and the rapid
growth in computing power. Nevertheless, it is well-known that flat
reinforcement learning algorithms are often not able to learn well and
data-efficient in tasks having hierarchical structures, e.g. consisting of
multiple subtasks. Hierarchical reinforcement learning is a principled approach
that is able to tackle these challenging tasks. On the other hand, many
real-world tasks usually have only partial observability in which state
measurements are often imperfect and partially observable. The problems of RL
in such settings can be formulated as a partially observable Markov decision
process (POMDP). In this paper, we study hierarchical RL in POMDP in which the
tasks have only partial observability and possess hierarchical properties. We
propose a hierarchical deep reinforcement learning approach for learning in
hierarchical POMDP. The deep hierarchical RL algorithm is proposed to apply to
both MDP and POMDP learning. We evaluate the proposed algorithm on various
challenging hierarchical POMDP.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1805.03257v1' target='_blank'>Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented
  Visual Dialog</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiaping Zhang, Tiancheng Zhao, Zhou Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-05-08 19:54:47</h6>
<p class='card-text'>Creating an intelligent conversational system that understands vision and
language is one of the ultimate goals in Artificial Intelligence
(AI)~\cite{winograd1972understanding}. Extensive research has focused on
vision-to-language generation, however, limited research has touched on
combining these two modalities in a goal-driven dialog context. We propose a
multimodal hierarchical reinforcement learning framework that dynamically
integrates vision and language for task-oriented visual dialog. The framework
jointly learns the multimodal dialog state representation and the hierarchical
dialog policy to improve both dialog task success and efficiency. We also
propose a new technique, state adaptation, to integrate context awareness in
the dialog state representation. We evaluate the proposed framework and the
state adaptation technique in an image guessing game and achieve promising
results.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1804.07855v3' target='_blank'>Subgoal Discovery for Hierarchical Dialogue Policy Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Da Tang, Xiujun Li, Jianfeng Gao, Chong Wang, Lihong Li, Tony Jebara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-04-20 23:06:44</h6>
<p class='card-text'>Developing agents to engage in complex goal-oriented dialogues is challenging
partly because the main learning signals are very sparse in long conversations.
In this paper, we propose a divide-and-conquer approach that discovers and
exploits the hidden structure of the task to enable efficient policy learning.
First, given successful example dialogues, we propose the Subgoal Discovery
Network (SDN) to divide a complex goal-oriented task into a set of simpler
subgoals in an unsupervised fashion. We then use these subgoals to learn a
multi-level policy by hierarchical reinforcement learning. We demonstrate our
method by building a dialogue agent for the composite task of travel planning.
Experiments with simulated and real users show that our approach performs
competitively against a state-of-the-art method that requires human-defined
subgoals. Moreover, we show that the learned subgoals are often human
comprehensible.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1804.07779v3' target='_blank'>PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement
  Learning for Robust Decision-Making</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fangkai Yang, Daoming Lyu, Bo Liu, Steven Gustafson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-04-20 18:16:43</h6>
<p class='card-text'>Reinforcement learning and symbolic planning have both been used to build
intelligent autonomous agents. Reinforcement learning relies on learning from
interactions with real world, which often requires an unfeasibly large amount
of experience. Symbolic planning relies on manually crafted symbolic knowledge,
which may not be robust to domain uncertainties and changes. In this paper we
present a unified framework {\em PEORL} that integrates symbolic planning with
hierarchical reinforcement learning (HRL) to cope with decision-making in a
dynamic environment with uncertainties.
  Symbolic plans are used to guide the agent's task execution and learning, and
the learned experience is fed back to symbolic knowledge to improve planning.
This method leads to rapid policy search and robust symbolic plans in complex
domains. The framework is tested on benchmark domains of HRL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1804.02808v2' target='_blank'>Latent Space Policies for Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-04-09 04:00:30</h6>
<p class='card-text'>We address the problem of learning hierarchical deep neural network policies
for reinforcement learning. In contrast to methods that explicitly restrict or
cripple lower layers of a hierarchy to force them to use higher-level
modulating signals, each layer in our framework is trained to directly solve
the task, but acquires a range of diverse strategies via a maximum entropy
reinforcement learning objective. Each layer is also augmented with latent
random variables, which are sampled from a prior distribution during the
training of that layer. The maximum entropy objective causes these latent
variables to be incorporated into the layer's policy, and the higher level
layer can directly control the behavior of the lower layer through this latent
space. Furthermore, by constraining the mapping from latent variables to
actions to be invertible, higher layers retain full expressivity: neither the
higher layers nor the lower layers are constrained in their behavior. Our
experimental evaluation demonstrates that we can improve on the performance of
single-layer policies on standard benchmark tasks simply by adding additional
layers, and that our method can solve more complex sparse-reward tasks by
learning higher-level policies on top of high-entropy skills optimized for
simple low-level objectives.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1803.04674v1' target='_blank'>Hierarchical Reinforcement Learning: Approximating Optimal Discounted
  TSP Using Local Policies</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tom Zahavy, Avinatan Hasidim, Haim Kaplan, Yishay Mansour</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2018-03-13 08:13:11</h6>
<p class='card-text'>In this work, we provide theoretical guarantees for reward decomposition in
deterministic MDPs. Reward decomposition is a special case of Hierarchical
Reinforcement Learning, that allows one to learn many policies in parallel and
combine them into a composite solution. Our approach builds on mapping this
problem into a Reward Discounted Traveling Salesman Problem, and then deriving
approximate solutions for it. In particular, we focus on approximate solutions
that are local, i.e., solutions that only observe information about the current
state. Local policies are easy to implement and do not require substantial
computational resources as they do not perform planning. While local
deterministic policies, like Nearest Neighbor, are being used in practice for
hierarchical reinforcement learning, we propose three stochastic policies that
guarantee better performance than any deterministic policy.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1801.00048v1' target='_blank'>Characterizing optimal hierarchical policy inference on graphs via
  non-equilibrium thermodynamics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel McNamee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2017-12-29 22:19:16</h6>
<p class='card-text'>Hierarchies are of fundamental interest in both stochastic optimal control
and biological control due to their facilitation of a range of desirable
computational traits in a control algorithm and the possibility that they may
form a core principle of sensorimotor and cognitive control systems. However, a
theoretically justified construction of state-space hierarchies over all
spatial resolutions and their evolution through a policy inference process
remains elusive. Here, a formalism for deriving such normative representations
of discrete Markov decision processes is introduced in the context of graphs.
The resulting hierarchies correspond to a hierarchical policy inference
algorithm approximating a discrete gradient flow between state-space trajectory
densities generated by the prior and optimal policies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1712.08266v1' target='_blank'>Federated Control with Hierarchical Multi-Agent Deep Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Saurabh Kumar, Pararth Shah, Dilek Hakkani-Tur, Larry Heck</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2017-12-22 00:54:48</h6>
<p class='card-text'>We present a framework combining hierarchical and multi-agent deep
reinforcement learning approaches to solve coordination problems among a
multitude of agents using a semi-decentralized model. The framework extends the
multi-agent learning setup by introducing a meta-controller that guides the
communication between agent pairs, enabling agents to focus on communicating
with only one other agent at any step. This hierarchical decomposition of the
task allows for efficient exploration to learn policies that identify globally
optimal solutions even as the number of collaborating agents increases. We show
promising initial experimental results on a simulated distributed scheduling
problem.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1712.04065v1' target='_blank'>The Eigenoption-Critic Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Miao Liu, Marlos C. Machado, Gerald Tesauro, Murray Campbell</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2017-12-11 23:21:42</h6>
<p class='card-text'>Eigenoptions (EOs) have been recently introduced as a promising idea for
generating a diverse set of options through the graph Laplacian, having been
shown to allow efficient exploration. Despite its initial promising results, a
couple of issues in current algorithms limit its application, namely: (1) EO
methods require two separate steps (eigenoption discovery and reward
maximization) to learn a control policy, which can incur a significant amount
of storage and computation; (2) EOs are only defined for problems with discrete
state-spaces and; (3) it is not easy to take the environment's reward function
into consideration when discovering EOs. To addresses these issues, we
introduce an algorithm termed eigenoption-critic (EOC) based on the
Option-critic (OC) framework [Bacon17], a general hierarchical reinforcement
learning (RL) algorithm that allows learning the intra-option policies
simultaneously with the policy over options. We also propose a generalization
of EOC to problems with continuous state-spaces through the Nystr\"om
approximation. EOC can also be seen as extending OC to nonstationary settings,
where the discovered options are not tailored for a single task.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1712.00948v5' target='_blank'>Learning Multi-Level Hierarchies with Hindsight</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andrew Levy, George Konidaris, Robert Platt, Kate Saenko</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2017-12-04 08:18:08</h6>
<p class='card-text'>Hierarchical agents have the potential to solve sequential decision making
tasks with greater sample efficiency than their non-hierarchical counterparts
because hierarchical agents can break down tasks into sets of subtasks that
only require short sequences of decisions. In order to realize this potential
of faster learning, hierarchical agents need to be able to learn their multiple
levels of policies in parallel so these simpler subproblems can be solved
simultaneously. Yet, learning multiple levels of policies in parallel is hard
because it is inherently unstable: changes in a policy at one level of the
hierarchy may cause changes in the transition and reward functions at higher
levels in the hierarchy, making it difficult to jointly learn multiple levels
of policies. In this paper, we introduce a new Hierarchical Reinforcement
Learning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome
the instability issues that arise when agents try to jointly learn multiple
levels of policies. The main idea behind HAC is to train each level of the
hierarchy independently of the lower levels by training each level as if the
lower level policies are already optimal. We demonstrate experimentally in both
grid world and simulated robotics domains that our approach can significantly
accelerate learning relative to other non-hierarchical and hierarchical
methods. Indeed, our framework is the first to successfully learn 3-level
hierarchies in parallel in tasks with continuous state and action spaces.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1711.11135v3' target='_blank'>Video Captioning via Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, William Yang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2017-11-29 22:23:59</h6>
<p class='card-text'>Video captioning is the task of automatically generating a textual
description of the actions in a video. Although previous work (e.g.
sequence-to-sequence model) has shown promising results in abstracting a coarse
description of a short video, it is still very challenging to caption a video
containing multiple fine-grained actions with a detailed description. This
paper aims to address the challenge by proposing a novel hierarchical
reinforcement learning framework for video captioning, where a high-level
Manager module learns to design sub-goals and a low-level Worker module
recognizes the primitive actions to fulfill the sub-goal. With this
compositional framework to reinforce video captioning at different levels, our
approach significantly outperforms all the baseline methods on a newly
introduced large-scale dataset for fine-grained video captioning. Furthermore,
our non-ensemble model has already achieved the state-of-the-art results on the
widely-used MSR-VTT dataset.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1711.10314v3' target='_blank'>Crossmodal Attentive Skill Learner</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shayegan Omidshafiei, Dong-Ki Kim, Jason Pazis, Jonathan P. How</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2017-11-28 14:38:21</h6>
<p class='card-text'>This paper presents the Crossmodal Attentive Skill Learner (CASL), integrated
with the recently-introduced Asynchronous Advantage Option-Critic (A2OC)
architecture [Harb et al., 2017] to enable hierarchical reinforcement learning
across multiple sensory inputs. We provide concrete examples where the approach
not only improves performance in a single task, but accelerates transfer to new
tasks. We demonstrate the attention mechanism anticipates and identifies useful
latent features, while filtering irrelevant sensor modalities during execution.
We modify the Arcade Learning Environment [Bellemare et al., 2013] to support
audio queries, and conduct evaluations of crossmodal learning in the Atari 2600
game Amidar. Finally, building on the recent work of Babaeizadeh et al. [2017],
we open-source a fast hybrid CPU-GPU implementation of CASL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1711.00129v2' target='_blank'>Automata-Guided Hierarchical Reinforcement Learning for Skill
  Composition</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiao Li, Yao Ma, Calin Belta</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2017-10-31 22:21:02</h6>
<p class='card-text'>Skills learned through (deep) reinforcement learning often generalizes poorly
across domains and re-training is necessary when presented with a new task. We
present a framework that combines techniques in \textit{formal methods} with
\textit{reinforcement learning} (RL). The methods we provide allows for
convenient specification of tasks with logical expressions, learns hierarchical
policies (meta-controller and low-level controllers) with well-defined
intrinsic rewards, and construct new skills from existing ones with little to
no additional exploration. We evaluate the proposed methods in a simple grid
world simulation as well as a more complicated kitchen environment in AI2Thor</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1708.00463v1' target='_blank'>Hierarchical Subtask Discovery With Non-Negative Matrix Factorization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Adam C. Earle, Andrew M. Saxe, Benjamin Rosman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2017-08-01 18:19:40</h6>
<p class='card-text'>Hierarchical reinforcement learning methods offer a powerful means of
planning flexible behavior in complicated domains. However, learning an
appropriate hierarchical decomposition of a domain into subtasks remains a
substantial challenge. We present a novel algorithm for subtask discovery,
based on the recently introduced multitask linearly-solvable Markov decision
process (MLMDP) framework. The MLMDP can perform never-before-seen tasks by
representing them as a linear combination of a previously learned basis set of
tasks. In this setting, the subtask discovery problem can naturally be posed as
finding an optimal low-rank approximation of the set of tasks the agent will
face in a domain. We use non-negative matrix factorization to discover this
minimal basis set of tasks, and show that the technique learns intuitive
decompositions in a variety of domains. Our method has several qualitatively
desirable features: it is not limited to learning subtasks with single goal
states, instead learning distributed patterns of preferred states; it learns
qualitatively different hierarchical decompositions in the same domain
depending on the ensemble of tasks the agent will face; and it may be
straightforwardly iterated to obtain deeper hierarchical decompositions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1706.06210v2' target='_blank'>Sub-domain Modelling for Dialogue Management with Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Paweł Budzianowski, Stefan Ultes, Pei-Hao Su, Nikola Mrkšić, Tsung-Hsien Wen, Iñigo Casanueva, Lina Rojas-Barahona, Milica Gašić</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2017-06-19 23:15:22</h6>
<p class='card-text'>Human conversation is inherently complex, often spanning many different
topics/domains. This makes policy learning for dialogue systems very
challenging. Standard flat reinforcement learning methods do not provide an
efficient framework for modelling such dialogues. In this paper, we focus on
the under-explored problem of multi-domain dialogue management. First, we
propose a new method for hierarchical reinforcement learning using the option
framework. Next, we show that the proposed architecture learns faster and
arrives at a better policy than the existing flat ones do. Moreover, we show
how pretrained policies can be adapted to more complex systems with an
additional set of new actions. In doing that, we show that our approach has the
potential to facilitate policy optimisation for more sophisticated multi-domain
dialogue systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1705.06769v2' target='_blank'>Feature Control as Intrinsic Motivation for Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nat Dilokthanakul, Christos Kaplanis, Nick Pawlowski, Murray Shanahan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2017-05-18 19:00:43</h6>
<p class='card-text'>The problem of sparse rewards is one of the hardest challenges in
contemporary reinforcement learning. Hierarchical reinforcement learning (HRL)
tackles this problem by using a set of temporally-extended actions, or options,
each of which has its own subgoal. These subgoals are normally handcrafted for
specific tasks. Here, though, we introduce a generic class of subgoals with
broad applicability in the visual domain. Underlying our approach (in common
with work using "auxiliary tasks") is the hypothesis that the ability to
control aspects of the environment is an inherently useful skill to have. We
incorporate such subgoals in an end-to-end hierarchical reinforcement learning
system and test two variants of our algorithm on a number of games from the
Atari suite. We highlight the advantage of our approach in one of the hardest
games -- Montezuma's revenge -- for which the ability to handle sparse rewards
is key. Our agent learns several times faster than the current state-of-the-art
HRL agent in this game, reaching a similar level of performance. UPDATE
22/11/17: We found that a standard A3C agent with a simple shaped reward, i.e.
extrinsic reward + feature control intrinsic reward, has comparable performance
to our agent in Montezuma Revenge. In light of the new experiments performed,
the advantage of our HRL approach can be attributed more to its ability to
learn useful features from intrinsic rewards rather than its ability to explore
and reuse abstracted skills with hierarchical components. This has led us to a
new conclusion about the result.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1704.03012v1' target='_blank'>Stochastic Neural Networks for Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Carlos Florensa, Yan Duan, Pieter Abbeel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2017-04-10 18:41:28</h6>
<p class='card-text'>Deep reinforcement learning has achieved many impressive results in recent
years. However, tasks with sparse rewards or long horizons continue to pose
significant challenges. To tackle these important problems, we propose a
general framework that first learns useful skills in a pre-training
environment, and then leverages the acquired skills for learning faster in
downstream tasks. Our approach brings together some of the strengths of
intrinsic motivation and hierarchical methods: the learning of useful skill is
guided by a single proxy reward, the design of which requires very minimal
domain knowledge about the downstream tasks. Then a high-level policy is
trained on top of these skills, providing a significant improvement of the
exploration and allowing to tackle sparse rewards in the downstream tasks. To
efficiently pre-train a large span of skills, we use Stochastic Neural Networks
combined with an information-theoretic regularizer. Our experiments show that
this combination is effective in learning a wide span of interpretable skills
in a sample-efficient way, and can significantly boost the learning performance
uniformly across a wide range of downstream tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1703.01161v2' target='_blank'>FeUdal Networks for Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, Koray Kavukcuoglu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2017-03-03 14:05:11</h6>
<p class='card-text'>We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical
reinforcement learning. Our approach is inspired by the feudal reinforcement
learning proposal of Dayan and Hinton, and gains power and efficacy by
decoupling end-to-end learning across multiple levels -- allowing it to utilise
different resolutions of time. Our framework employs a Manager module and a
Worker module. The Manager operates at a lower temporal resolution and sets
abstract goals which are conveyed to and enacted by the Worker. The Worker
generates primitive actions at every tick of the environment. The decoupled
structure of FuN conveys several benefits -- in addition to facilitating very
long timescale credit assignment it also encourages the emergence of
sub-policies associated with different goals set by the Manager. These
properties allow FuN to dramatically outperform a strong baseline agent on
tasks that involve long-term credit assignment or memorisation. We demonstrate
the performance of our proposed system on a range of tasks from the ATARI suite
and also from a 3D DeepMind Lab environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1610.02847v1' target='_blank'>Situational Awareness by Risk-Conscious Skills</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel J. Mankowitz, Aviv Tamar, Shie Mannor</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2016-10-10 11:01:32</h6>
<p class='card-text'>Hierarchical Reinforcement Learning has been previously shown to speed up the
convergence rate of RL planning algorithms as well as mitigate feature-based
model misspecification (Mankowitz et. al. 2016a,b, Bacon 2015). To do so, it
utilizes hierarchical abstractions, also known as skills -- a type of
temporally extended action (Sutton et. al. 1999) to plan at a higher level,
abstracting away from the lower-level details. We incorporate risk sensitivity,
also referred to as Situational Awareness (SA), into hierarchical RL for the
first time by defining and learning risk aware skills in a Probabilistic Goal
Semi-Markov Decision Process (PG-SMDP). This is achieved using our novel
Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which comes
with a theoretical convergence guarantee. We show in a RoboCup soccer domain
that the learned risk aware skills exhibit complex human behaviors such as
`time-wasting' in a soccer game. In addition, the learned risk aware skills are
able to mitigate reward-based model misspecification.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1606.07374v2' target='_blank'>Multi-Stage Temporal Difference Learning for 2048-like Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kun-Hao Yeh, I-Chen Wu, Chu-Hsuan Hsueh, Chia-Chuan Chang, Chao-Chin Liang, Han Chiang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2016-06-23 16:58:33</h6>
<p class='card-text'>Szubert and Jaskowski successfully used temporal difference (TD) learning
together with n-tuple networks for playing the game 2048. However, we observed
a phenomenon that the programs based on TD learning still hardly reach large
tiles. In this paper, we propose multi-stage TD (MS-TD) learning, a kind of
hierarchical reinforcement learning method, to effectively improve the
performance for the rates of reaching large tiles, which are good metrics to
analyze the strength of 2048 programs. Our experiments showed significant
improvements over the one without using MS-TD learning. Namely, using 3-ply
expectimax search, the program with MS-TD learning reached 32768-tiles with a
rate of 18.31%, while the one with TD learning did not reach any. After further
tuned, our 2048 program reached 32768-tiles with a rate of 31.75% in 10,000
games, and one among these games even reached a 65536-tile, which is the first
ever reaching a 65536-tile to our knowledge. In addition, MS-TD learning method
can be easily applied to other 2048-like games, such as Threes. Based on MS-TD
learning, our experiments for Threes also demonstrated similar performance
improvement, where the program with MS-TD learning reached 6144-tiles with a
rate of 7.83%, while the one with TD learning only reached 0.45%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1606.06355v1' target='_blank'>A Hierarchical Reinforcement Learning Method for Persistent
  Time-Sensitive Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiao Li, Calin Belta</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2016-06-20 22:43:29</h6>
<p class='card-text'>Reinforcement learning has been applied to many interesting problems such as
the famous TD-gammon and the inverted helicopter flight. However, little effort
has been put into developing methods to learn policies for complex persistent
tasks and tasks that are time-sensitive. In this paper, we take a step towards
solving this problem by using signal temporal logic (STL) as task
specification, and taking advantage of the temporal abstraction feature that
the options framework provide. We show via simulation that a relatively easy to
implement algorithm that combines STL and options can learn a satisfactory
policy with a small number of training cases</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1605.05359v3' target='_blank'>Option Discovery in Hierarchical Reinforcement Learning using
  Spatio-Temporal Clustering</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aravind Srinivas, Ramnandan Krishnamurthy, Peeyush Kumar, Balaraman Ravindran</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2016-05-17 20:44:19</h6>
<p class='card-text'>This paper introduces an automated skill acquisition framework in
reinforcement learning which involves identifying a hierarchical description of
the given task in terms of abstract states and extended actions between
abstract states. Identifying such structures present in the task provides ways
to simplify and speed up reinforcement learning algorithms. These structures
also help to generalize such algorithms over multiple tasks without relearning
policies from scratch. We use ideas from dynamical systems to find metastable
regions in the state space and associate them with abstract states. The
spectral clustering algorithm PCCA+ is used to identify suitable abstractions
aligned to the underlying structure. Skills are defined in terms of the
sequence of actions that lead to transitions between such abstract states. The
connectivity information from PCCA+ is used to generate these skills or
options. These skills are independent of the learning task and can be
efficiently reused across a variety of tasks defined over the same model. This
approach works well even without the exact model of the environment by using
sample trajectories to construct an approximate estimate. We also present our
approach to scaling the skill acquisition framework to complex tasks with large
state spaces for which we perform state aggregation using the representation
learned from an action conditional video prediction network and use the skill
acquisition framework on the aggregated state space.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1604.08153v3' target='_blank'>Classifying Options for Deep Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan, Anil Anthony Bharath</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2016-04-27 17:48:39</h6>
<p class='card-text'>In this paper we combine one method for hierarchical reinforcement learning -
the options framework - with deep Q-networks (DQNs) through the use of
different "option heads" on the policy network, and a supervisory network for
choosing between the different options. We utilise our setup to investigate the
effects of architectural constraints in subtasks with positive and negative
transfer, across a range of network capacities. We empirically show that our
augmented DQN has lower sample complexity when simultaneously learning subtasks
with negative transfer, without degrading performance when learning subtasks
with positive transfer.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1603.08869v1' target='_blank'>Algorithms for Batch Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tiancheng Zhao, Mohammad Gowayyed</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2016-03-29 18:17:17</h6>
<p class='card-text'>Hierarchical Reinforcement Learning (HRL) exploits temporal abstraction to
solve large Markov Decision Processes (MDP) and provide transferable subtask
policies. In this paper, we introduce an off-policy HRL algorithm: Hierarchical
Q-value Iteration (HQI). We show that it is possible to effectively learn
recursive optimal policies for any valid hierarchical decomposition of the
original MDP, given a fixed dataset collected from a flat stochastic behavioral
policy. We first formally prove the convergence of the algorithm for tabular
MDP. Then our experiments on the Taxi domain show that HQI converges faster
than a flat Q-value Iteration and enjoys easy state abstraction. Also, we
demonstrate that our algorithm is able to learn optimal policies for different
hierarchical structures from the same fixed dataset, which enables model
comparison without recollecting data.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1603.03267v1' target='_blank'>Hierarchical Linearly-Solvable Markov Decision Problems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anders Jonsson, Vicenç Gómez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2016-03-10 13:50:31</h6>
<p class='card-text'>We present a hierarchical reinforcement learning framework that formulates
each task in the hierarchy as a special type of Markov decision process for
which the Bellman equation is linear and has analytical solution. Problems of
this type, called linearly-solvable MDPs (LMDPs) have interesting properties
that can be exploited in a hierarchical setting, such as efficient learning of
the optimal value function or task compositionality. The proposed hierarchical
approach can also be seen as a novel alternative to solving LMDPs with large
state spaces. We derive a hierarchical version of the so-called Z-learning
algorithm that learns different tasks simultaneously and show empirically that
it significantly outperforms the state-of-the-art learning methods in two
classical hierarchical reinforcement learning domains: the taxi domain and an
autonomous guided vehicle task.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1506.02312v1' target='_blank'>A Framework for Constrained and Adaptive Behavior-Based Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Renato de Pontes Pereira, Paulo Martins Engel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2015-06-07 20:52:31</h6>
<p class='card-text'>Behavior Trees are commonly used to model agents for robotics and games,
where constrained behaviors must be designed by human experts in order to
guarantee that these agents will execute a specific chain of actions given a
specific set of perceptions. In such application areas, learning is a desirable
feature to provide agents with the ability to adapt and improve interactions
with humans and environment, but often discarded due to its unreliability. In
this paper, we propose a framework that uses Reinforcement Learning nodes as
part of Behavior Trees to address the problem of adding learning capabilities
in constrained agents. We show how this framework relates to Options in
Hierarchical Reinforcement Learning, ensuring convergence of nested learning
nodes, and we empirically show that the learning nodes do not affect the
execution of other nodes in the tree.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1412.6451v1' target='_blank'>Grounding Hierarchical Reinforcement Learning Models for Knowledge
  Transfer</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mark Wernsdorfer, Ute Schmid</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2014-12-19 17:41:59</h6>
<p class='card-text'>Methods of deep machine learning enable to to reuse low-level representations
efficiently for generating more abstract high-level representations.
Originally, deep learning has been applied passively (e.g., for classification
purposes). Recently, it has been extended to estimate the value of actions for
autonomous agents within the framework of reinforcement learning (RL). Explicit
models of the environment can be learned to augment such a value function.
Although "flat" connectionist methods have already been used for model-based
RL, up to now, only model-free variants of RL have been equipped with methods
from deep learning. We propose a variant of deep model-based RL that enables an
agent to learn arbitrarily abstract hierarchical representations of its
environment. In this paper, we present research on how such hierarchical
representations can be grounded in sensorimotor interaction between an agent
and its environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1303.4638v1' target='_blank'>On Improving Energy Efficiency within Green Femtocell Networks: A
  Hierarchical Reinforcement Learning Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xianfu Chen, Honggang Zhang, Tao Chen, Mika Lasanen, Jacques Palicot</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2013-03-13 10:22:09</h6>
<p class='card-text'>One of the efficient solutions of improving coverage and increasing capacity
in cellular networks is the deployment of femtocells. As the cellular networks
are becoming more complex, energy consumption of whole network infrastructure
is becoming important in terms of both operational costs and environmental
impacts. This paper investigates energy efficiency of two-tier femtocell
networks through combining game theory and stochastic learning. With the
Stackelberg game formulation, a hierarchical reinforcement learning framework
is applied for studying the joint expected utility maximization of macrocells
and femtocells subject to the minimum signal-to-interference-plus-noise-ratio
requirements. In the learning procedure, the macrocells act as leaders and the
femtocells are followers. At each time step, the leaders commit to dynamic
strategies based on the best responses of the followers, while the followers
compete against each other with no further information but the leaders'
transmission parameters. In this paper, we propose two reinforcement learning
based intelligent algorithms to schedule each cell's stochastic power levels.
Numerical experiments are presented to validate the investigations. The results
show that the two learning algorithms substantially improve the energy
efficiency of the femtocell networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1209.2790v1' target='_blank'>Improving Energy Efficiency in Femtocell Networks: A Hierarchical
  Reinforcement Learning Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xianfu Chen, Honggang Zhang, Tao Chen, Mika Lasanen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2012-09-13 06:47:26</h6>
<p class='card-text'>This paper investigates energy efficiency for two-tier femtocell networks
through combining game theory and stochastic learning. With the Stackelberg
game formulation, a hierarchical reinforcement learning framework is applied to
study the joint average utility maximization of macrocells and femtocells
subject to the minimum signal-to-interference-plus-noise-ratio requirements.
The macrocells behave as the leaders and the femtocells are followers during
the learning procedure. At each time step, the leaders commit to dynamic
strategies based on the best responses of the followers, while the followers
compete against each other with no further information but the leaders'
strategy information. In this paper, we propose two learning algorithms to
schedule each cell's stochastic power levels, leading by the macrocells.
Numerical experiments are presented to validate the proposed studies and show
that the two learning algorithms substantially improve the energy efficiency of
the femtocell networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1206.6851v1' target='_blank'>A compact, hierarchical Q-function decomposition</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bhaskara Marthi, Stuart Russell, David Andre</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2012-06-27 16:24:43</h6>
<p class='card-text'>Previous work in hierarchical reinforcement learning has faced a dilemma:
either ignore the values of different possible exit states from a subroutine,
thereby risking suboptimal behavior, or represent those values explicitly
thereby incurring a possibly large representation cost because exit values
refer to nonlocal aspects of the world (i.e., all subsequent rewards). This
paper shows that, in many cases, one can avoid both of these problems. The
solution is based on recursively decomposing the exit value function in terms
of Q-functions at higher levels of the hierarchy. This leads to an intuitively
appealing runtime architecture in which a parent subroutine passes to its child
a value function on the exit states and the child reasons about how its choices
affect the exit value. We also identify structural conditions on the value
function and transition distributions that allow much more concise
representations of exit state distributions, leading to further state
abstraction. In essence, the only variables whose exit values need be
considered are those that the parent cares about and the child affects. We
demonstrate the utility of our algorithms on a series of increasingly complex
environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1104.5059v1' target='_blank'>Reducing Commitment to Tasks with Off-Policy Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mitchell Keith Bloch</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2011-04-27 00:58:52</h6>
<p class='card-text'>In experimenting with off-policy temporal difference (TD) methods in
hierarchical reinforcement learning (HRL) systems, we have observed unwanted
on-policy learning under reproducible conditions. Here we present modifications
to several TD methods that prevent unintentional on-policy learning from
occurring. These modifications create a tension between exploration and
learning. Traditional TD methods require commitment to finishing subtasks
without exploration in order to update Q-values for early actions with high
probability. One-step intra-option learning and temporal second difference
traces (TSDT) do not suffer from this limitation. We demonstrate that our HRL
system is efficient without commitment to completion of subtasks in a
cliff-walking domain, contrary to a widespread claim in the literature that it
is critical for efficiency of learning. Furthermore, decreasing commitment as
exploration progresses is shown to improve both online performance and the
resultant policy in the taxicab domain, opening a new avenue for research into
when it is more beneficial to continue with the current subtask or to replan.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/1012.2599v1' target='_blank'>A Tutorial on Bayesian Optimization of Expensive Cost Functions, with
  Application to Active User Modeling and Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Eric Brochu, Vlad M. Cora, Nando de Freitas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2010-12-12 22:53:04</h6>
<p class='card-text'>We present a tutorial on Bayesian optimization, a method of finding the
maximum of expensive cost functions. Bayesian optimization employs the Bayesian
technique of setting a prior over the objective function and combining it with
evidence to get a posterior function. This permits a utility-based selection of
the next observation to make on the objective function, which must take into
account both exploration (sampling from areas of high uncertainty) and
exploitation (sampling areas likely to offer improvement over the current best
observation). We also present two detailed extensions of Bayesian optimization,
with experiments---active user modelling with preferences, and hierarchical
reinforcement learning---and a discussion of the pros and cons of Bayesian
optimization based on our experiences.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/cs/9905015v1' target='_blank'>State Abstraction in MAXQ Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas G. Dietterich</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:1999-05-21 14:49:39</h6>
<p class='card-text'>Many researchers have explored methods for hierarchical reinforcement
learning (RL) with temporal abstractions, in which abstract actions are defined
that can perform many primitive actions before terminating. However, little is
known about learning with state abstractions, in which aspects of the state
space are ignored. In previous work, we developed the MAXQ method for
hierarchical RL. In this paper, we define five conditions under which state
abstraction can be combined with the MAXQ value function decomposition. We
prove that the MAXQ-Q learning algorithm converges under these conditions and
show experimentally that state abstraction is important for the successful
application of MAXQ-Q learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/cs/9905014v1' target='_blank'>Hierarchical Reinforcement Learning with the MAXQ Value Function
  Decomposition</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas G. Dietterich</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:1999-05-21 14:26:07</h6>
<p class='card-text'>This paper presents the MAXQ approach to hierarchical reinforcement learning
based on decomposing the target Markov decision process (MDP) into a hierarchy
of smaller MDPs and decomposing the value function of the target MDP into an
additive combination of the value functions of the smaller MDPs. The paper
defines the MAXQ hierarchy, proves formal results on its representational
power, and establishes five conditions for the safe use of state abstractions.
The paper presents an online model-free learning algorithm, MAXQ-Q, and proves
that it converges wih probability 1 to a kind of locally-optimal policy known
as a recursively optimal policy, even in the presence of the five kinds of
state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q
through a series of experiments in three domains and shows experimentally that
MAXQ-Q (with state abstractions) converges to a recursively optimal policy much
faster than flat Q learning. The fact that MAXQ learns a representation of the
value function has an important benefit: it makes it possible to compute and
execute an improved, non-hierarchical policy via a procedure similar to the
policy improvement step of policy iteration. The paper demonstrates the
effectiveness of this non-hierarchical execution experimentally. Finally, the
paper concludes with a comparison to related work and a discussion of the
design tradeoffs in hierarchical reinforcement learning.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>