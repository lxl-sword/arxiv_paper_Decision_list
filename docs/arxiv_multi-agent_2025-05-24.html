<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>multi-agent - 2025-05-24</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>multi-agent - 2025-05-24</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.14544v1' target='_blank'>Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic
  Signal Optimization: A Simulation Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Saahil Mahato</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 15:59:44</h6>
<p class='card-text'>Urban traffic congestion, particularly at intersections, significantly
impacts travel time, fuel consumption, and emissions. Traditional fixed-time
signal control systems often lack the adaptability to manage dynamic traffic
patterns effectively. This study explores the application of multi-agent
reinforcement learning (MARL) to optimize traffic signal coordination across
multiple intersections within a simulated environment. Utilizing Pygame, a
simulation was developed to model a network of interconnected intersections
with randomly generated vehicle flows to reflect realistic traffic variability.
A decentralized MARL controller was implemented, in which each traffic signal
operates as an autonomous agent, making decisions based on local observations
and information from neighboring agents. Performance was evaluated against a
baseline fixed-time controller using metrics such as average vehicle wait time
and overall throughput. The MARL approach demonstrated statistically
significant improvements, including reduced average waiting times and improved
throughput. These findings suggest that MARL-based dynamic control strategies
hold substantial promise for improving urban traffic management efficiency.
More research is recommended to address scalability and real-world
implementation challenges.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13834v1' target='_blank'>Toward Real-World Cooperative and Competitive Soccer with Quadrupedal
  Robot Teams</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhi Su, Yuman Gao, Emily Lukas, Yunfei Li, Jiaze Cai, Faris Tulbah, Fei Gao, Chao Yu, Zhongyu Li, Yi Wu, Koushil Sreenath</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 02:20:54</h6>
<p class='card-text'>Achieving coordinated teamwork among legged robots requires both fine-grained
locomotion control and long-horizon strategic decision-making. Robot soccer
offers a compelling testbed for this challenge, combining dynamic, competitive,
and multi-agent interactions. In this work, we present a hierarchical
multi-agent reinforcement learning (MARL) framework that enables fully
autonomous and decentralized quadruped robot soccer. First, a set of highly
dynamic low-level skills is trained for legged locomotion and ball
manipulation, such as walking, dribbling, and kicking. On top of these, a
high-level strategic planning policy is trained with Multi-Agent Proximal
Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning
framework allows agents to adapt to diverse opponent strategies and gives rise
to sophisticated team behaviors, including coordinated passing, interception,
and dynamic role allocation. With an extensive ablation study, the proposed
learning method shows significant advantages in the cooperative and competitive
multi-agent soccer game. We deploy the learned policies to real quadruped
robots relying solely on onboard proprioception and decentralized localization,
with the resulting system supporting autonomous robot-robot and robot-human
soccer matches on indoor and outdoor soccer courts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.12811v1' target='_blank'>Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei-Chen Liao, Ti-Rong Wu, I-Chen Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-19 07:40:42</h6>
<p class='card-text'>Multi-agent reinforcement Learning (MARL) is often challenged by the sight
range dilemma, where agents either receive insufficient or excessive
information from their environment. In this paper, we propose a novel method,
called Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes
an Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight
range during training. Experiment results show several advantages of using DSR.
First, we demonstrate using DSR achieves better performance in three common
MARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse
(RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show
that DSR consistently improves performance across multiple MARL algorithms,
including QMIX and MAPPO. Third, DSR offers suitable sight ranges for different
training steps, thereby accelerating the training process. Finally, DSR
provides additional interpretability by indicating the optimal sight range used
during training. Unlike existing methods that rely on global information or
communication mechanisms, our approach operates solely based on the individual
sight ranges of agents. This approach offers a practical and efficient solution
to the sight range dilemma, making it broadly applicable to real-world complex
environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15841v1' target='_blank'>Optimizing Resource Allocation for QoS and Stability in Dynamic VLC-NOMA
  Networks via MARL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aubida A. Al-Hameed, Safwan Hafeedh Younus, Mohamad A. Ahmed, Abdullah Baz</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-17 18:54:56</h6>
<p class='card-text'>Visible Light Communication (VLC) combined with Non-Orthogonal Multiple
Access (NOMA) offers a promising solution for dense indoor wireless networks.
Yet, managing resources effectively is challenged by VLC network dynamic
conditions involving user mobility and light dimming. In addition to satisfying
Quality of Service (QoS) and network stability requirements. Traditional
resource allocation methods and simpler RL approaches struggle to jointly
optimize QoS and stability under the dynamic conditions of mobile VLC-NOMA
networks. This paper presents MARL frameworks tailored to perform complex joint
optimization of resource allocation (NOMA power, user scheduling) and network
stability (interference, handovers), considering heterogeneous QoS, user
mobility, and dimming in VLC-NOMA systems. Our MARL frameworks capture dynamic
channel conditions and diverse user QoS , enabling effective joint
optimization. In these frameworks, VLC access points (APs) act as intelligent
agents, learning to allocate power and schedule users to satisfy diverse
requirements while maintaining network stability by managing interference and
minimizing disruptive handovers. We conduct a comparative analysis of two key
MARL paradigms: 1) Centralized Training with Decentralized Execution (CTDE) and
2) Centralized Training with Centralized Execution (CTCE). Comprehensive
simulations validate the effectiveness of both tailored MARL frameworks and
demonstrate an ability to handle complex optimization. The results show key
trade-offs, as the CTDE approach achieved approximately 16\% higher for High
priority (HP) user QoS satisfaction, while the CTCE approach yielded nearly 7
dB higher average SINR and 12\% lower ping-pong handover ratio, offering
valuable insights into the performance differences between these paradigms in
complex VLC-NOMA network scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.11461v1' target='_blank'>Signal attenuation enables scalable decentralized multi-agent
  reinforcement learning over networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wesley A Suttle, Vipul K Sharma, Brian M Sadler</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-16 17:14:37</h6>
<p class='card-text'>Classic multi-agent reinforcement learning (MARL) methods require that agents
enjoy global state observability, preventing development of decentralized
algorithms and limiting scalability. Recent work has shown that, under
assumptions on decaying inter-agent influence, global observability can be
replaced by local neighborhood observability at each agent, enabling
decentralization and scalability. Real-world applications enjoying such decay
properties remain underexplored, however, despite the fact that signal power
decay, or signal attenuation, due to path loss is an intrinsic feature of many
problems in wireless communications and radar networks. In this paper, we show
that signal attenuation enables decentralization in MARL by considering the
illustrative special case of performing power allocation for target detection
in a radar network. To achieve this, we propose two new constrained multi-agent
Markov decision process formulations of this power allocation problem, derive
local neighborhood approximations for global value function and gradient
estimates and establish corresponding error bounds, and develop decentralized
saddle point policy gradient algorithms for solving the proposed problems. Our
approach, though oriented towards the specific radar network problem we
consider, provides a useful model for future extensions to additional problems
in wireless communications and radar networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.11311v1' target='_blank'>Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for
  Aerial Combat Tactics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ardian Selmonaj, Alessandro Antonucci, Adrian Schneider, Michael Rüegsegger, Matthias Sommer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-16 14:36:30</h6>
<p class='card-text'>Artificial intelligence (AI) is reshaping strategic planning, with
Multi-Agent Reinforcement Learning (MARL) enabling coordination among
autonomous agents in complex scenarios. However, its practical deployment in
sensitive military contexts is constrained by the lack of explainability, which
is an essential factor for trust, safety, and alignment with human strategies.
This work reviews and assesses current advances in explainability methods for
MARL with a focus on simulated air combat scenarios. We proceed by adapting
various explainability techniques to different aerial combat scenarios to gain
explanatory insights about the model behavior. By linking AI-generated tactics
with human-understandable reasoning, we emphasize the need for transparency to
ensure reliable deployment and meaningful human-machine interaction. By
illuminating the crucial importance of explainability in advancing MARL for
operational defense, our work supports not only strategic planning but also the
training of military personnel with insightful and comprehensible analyses.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.11100v1' target='_blank'>Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent
  Generalizable Behaviors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lang Feng, Jiahao Lin, Dong Xing, Li Zhang, De Ma, Gang Pan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-16 10:31:10</h6>
<p class='card-text'>Population-population generalization is a challenging problem in multi-agent
reinforcement learning (MARL), particularly when agents encounter unseen
co-players. However, existing self-play-based methods are constrained by the
limitation of inside-space generalization. In this study, we propose
Bidirectional Distillation (BiDist), a novel mixed-play framework, to overcome
this limitation in MARL. BiDist leverages knowledge distillation in two
alternating directions: forward distillation, which emulates the historical
policies' space and creates an implicit self-play, and reverse distillation,
which systematically drives agents towards novel distributions outside the
known policy space in a non-self-play manner. In addition, BiDist operates as a
concise and efficient solution without the need for the complex and costly
storage of past policies. We provide both theoretical analysis and empirical
evidence to support BiDist's effectiveness. Our results highlight its
remarkable generalization ability across a variety of cooperative, competitive,
and social dilemma tasks, and reveal that BiDist significantly diversifies the
policy distribution space. We also present comprehensive ablation studies to
reinforce BiDist's effectiveness and key success factors. Source codes are
available in the supplementary material.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.09756v1' target='_blank'>Community-based Multi-Agent Reinforcement Learning with Transfer and
  Active Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhaoyang Shi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-14 19:42:43</h6>
<p class='card-text'>We propose a new framework for multi-agent reinforcement learning (MARL),
where the agents cooperate in a time-evolving network with latent community
structures and mixed memberships. Unlike traditional neighbor-based or fixed
interaction graphs, our community-based framework captures flexible and
abstract coordination patterns by allowing each agent to belong to multiple
overlapping communities. Each community maintains shared policy and value
functions, which are aggregated by individual agents according to personalized
membership weights. We also design actor-critic algorithms that exploit this
structure: agents inherit community-level estimates for policy updates and
value learning, enabling structured information sharing without requiring
access to other agents' policies. Importantly, our approach supports both
transfer learning by adapting to new agents or tasks via membership estimation,
and active learning by prioritizing uncertain communities during exploration.
Theoretically, we establish convergence guarantees under linear function
approximation for both actor and critic updates. To our knowledge, this is the
first MARL framework that integrates community structure, transferability, and
active learning with provable guarantees.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.08630v1' target='_blank'>Credit Assignment and Efficient Exploration based on Influence Scope in
  Multi-agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuai Han, Mehdi Dastani, Shihan Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-13 14:49:26</h6>
<p class='card-text'>Training cooperative agents in sparse-reward scenarios poses significant
challenges for multi-agent reinforcement learning (MARL). Without clear
feedback on actions at each step in sparse-reward setting, previous methods
struggle with precise credit assignment among agents and effective exploration.
In this paper, we introduce a novel method to deal with both credit assignment
and exploration problems in reward-sparse domains. Accordingly, we propose an
algorithm that calculates the Influence Scope of Agents (ISA) on states by
taking specific value of the dimensions/attributes of states that can be
influenced by individual agents. The mutual dependence between agents' actions
and state attributes are then used to calculate the credit assignment and to
delimit the exploration space for each individual agent. We then evaluate ISA
in a variety of sparse-reward multi-agent scenarios. The results show that our
method significantly outperforms the state-of-art baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.08448v1' target='_blank'>Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning
  with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanggang Xu, Weijie Hong, Jirong Zha, Geng Chen, Jianfeng Zheng, Chen-Chun Hsia, Xinlei Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-13 11:23:25</h6>
<p class='card-text'>In disaster scenarios, establishing robust emergency communication networks
is critical, and unmanned aerial vehicles (UAVs) offer a promising solution to
rapidly restore connectivity. However, organizing UAVs to form multi-hop
networks in large-scale dynamic environments presents significant challenges,
including limitations in algorithmic scalability and the vast exploration space
required for coordinated decision-making. To address these issues, we propose
MRLMN, a novel framework that integrates multi-agent reinforcement learning
(MARL) and large language models (LLMs) to jointly optimize UAV agents toward
achieving optimal networking performance. The framework incorporates a grouping
strategy with reward decomposition to enhance algorithmic scalability and
balance decision-making across UAVs. In addition, behavioral constraints are
applied to selected key UAVs to improve the robustness of the network.
Furthermore, the framework integrates LLM agents, leveraging knowledge
distillation to transfer their high-level decision-making capabilities to MARL
agents. This enhances both the efficiency of exploration and the overall
training process. In the distillation module, a Hungarian algorithm-based
matching scheme is applied to align the decision outputs of the LLM and MARL
agents and define the distillation loss. Extensive simulation results validate
the effectiveness of our approach, demonstrating significant improvements in
network performance, including enhanced coverage and communication quality.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.08222v1' target='_blank'>Scaling Multi Agent Reinforcement Learning for Underwater Acoustic
  Tracking via Autonomous Vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Matteo Gallici, Ivan Masmitja, Mario Martín</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-13 04:42:30</h6>
<p class='card-text'>Autonomous vehicles (AV) offer a cost-effective solution for scientific
missions such as underwater tracking. Recently, reinforcement learning (RL) has
emerged as a powerful method for controlling AVs in complex marine
environments. However, scaling these techniques to a fleet--essential for
multi-target tracking or targets with rapid, unpredictable motion--presents
significant computational challenges. Multi-Agent Reinforcement Learning (MARL)
is notoriously sample-inefficient, and while high-fidelity simulators like
Gazebo's LRAUV provide 100x faster-than-real-time single-robot simulations,
they offer no significant speedup for multi-vehicle scenarios, making MARL
training impractical. To address these limitations, we propose an iterative
distillation method that transfers high-fidelity simulations into a simplified,
GPU-accelerated environment while preserving high-level dynamics. This approach
achieves up to a 30,000x speedup over Gazebo through parallelization, enabling
efficient training via end-to-end GPU acceleration. Additionally, we introduce
a novel Transformer-based architecture (TransfMAPPO) that learns multi-agent
policies invariant to the number of agents and targets, significantly improving
sample efficiency. Following large-scale curriculum learning conducted entirely
on GPU, we perform extensive evaluations in Gazebo, demonstrating that our
method maintains tracking errors below 5 meters over extended durations, even
in the presence of multiple fast-moving targets. This work bridges the gap
between large-scale MARL training and high-fidelity deployment, providing a
scalable framework for autonomous fleet control in real-world sea missions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.08825v1' target='_blank'>Multi-source Plume Tracing via Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Pedro Antonio Alarcon Granadeno, Theodore Chambers, Jane Cleland-Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-12 21:33:15</h6>
<p class='card-text'>Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon
gas leak (2015) demonstrate the urgent need for rapid and reliable plume
tracing algorithms to protect public health and the environment. Traditional
methods, such as gradient-based or biologically inspired approaches, often fail
in realistic, turbulent conditions. To address these challenges, we present a
Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing
multiple airborne pollution sources using a swarm of small uncrewed aerial
systems (sUAS). Our method models the problem as a Partially Observable Markov
Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific
Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical
action-observation pairs, effectively approximating latent states. Unlike prior
work, we use a general-purpose simulation environment based on the Gaussian
Plume Model (GPM), incorporating realistic elements such as a three-dimensional
environment, sensor noise, multiple interacting agents, and multiple plume
sources. The incorporation of action histories as part of the inputs further
enhances the adaptability of our model in complex, partially observable
environments. Extensive simulations show that our algorithm significantly
outperforms conventional approaches. Specifically, our model allows agents to
explore only 1.29\% of the environment to successfully locate pollution
sources.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.06771v1' target='_blank'>JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shalin Anand Jain, Jiazhen Liu, Siva Kailas, Harish Ravichandar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-10 22:38:39</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) has emerged as a promising solution
for learning complex and scalable coordination behaviors in multi-robot
systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics
relevance and hardware deployment, leaving multi-robot learning researchers to
develop bespoke environments and hardware testbeds dedicated to the development
and evaluation of their individual contributions. The Multi-Agent RL Benchmark
and Learning Environment for the Robotarium (MARBLER) is an exciting recent
step in providing a standardized robotics-relevant platform for MARL, by
bridging the Robotarium testbed with existing MARL software infrastructure.
However, MARBLER lacks support for parallelization and GPU/TPU execution,
making the platform prohibitively slow compared to modern MARL environments and
hindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end
simulation, learning, deployment, and benchmarking platform for the Robotarium.
JaxRobotarium enables rapid training and deployment of multi-robot
reinforcement learning (MRRL) policies with realistic robot dynamics and safety
constraints, supporting both parallelization and hardware acceleration. Our
generalizable learning interface provides an easy-to-use integration with SOTA
MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight
standardized coordination scenarios, including four novel scenarios that bring
established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a
realistic robotics setting. We demonstrate that JaxRobotarium retains high
simulation fidelity while achieving dramatic speedups over baseline (20x in
training and 150x in simulation), and provides an open-access sim-to-real
evaluation pipeline through the Robotarium testbed, accelerating and
democratizing access to multi-robot learning research and evaluation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.06706v2' target='_blank'>Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxuan Zheng, Yihe Zhou, Feiyang Xu, Mingli Song, Shunyu Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-10 17:04:33</h6>
<p class='card-text'>Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the
curse of dimensionality, as the exponential growth in agent interactions
significantly increases computational complexity and impedes learning
efficiency. To mitigate this, existing efforts that rely on Mean Field (MF)
simplify the interaction landscape by approximating neighboring agents as a
single mean agent, thus reducing overall complexity to pairwise interactions.
However, these MF methods inevitably fail to account for individual
differences, leading to aggregation noise caused by inaccurate iterative
updates during MF learning. In this paper, we propose a Bi-level Mean Field
(BMF) method to capture agent diversity with dynamic grouping in large-scale
MARL, which can alleviate aggregation noise via bi-level interaction.
Specifically, BMF introduces a dynamic group assignment module, which employs a
Variational AutoEncoder (VAE) to learn the representations of agents,
facilitating their dynamic grouping over time. Furthermore, we propose a
bi-level interaction module to model both inter- and intra-group interactions
for effective neighboring aggregation. Experiments across various tasks
demonstrate that the proposed BMF yields results superior to the
state-of-the-art methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.05968v1' target='_blank'>Offline Multi-agent Reinforcement Learning via Score Decomposition</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dan Qiao, Wenhao Li, Shanchao Yang, Hongyuan Zha, Baoxiang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-09 11:42:31</h6>
<p class='card-text'>Offline multi-agent reinforcement learning (MARL) faces critical challenges
due to distributional shifts, further exacerbated by the high dimensionality of
joint action spaces and the diversity in coordination strategies and quality
among agents. Conventional approaches, including independent learning
frameworks and value decomposition methods based on pessimistic principles,
remain susceptible to out-of-distribution (OOD) joint actions and often yield
suboptimal performance. Through systematic analysis of prevalent offline MARL
benchmarks, we identify that this limitation primarily stems from the
inherently multimodal nature of joint collaborative policies induced by offline
data collection. To address these challenges, we propose a novel two-stage
framework: First, we employ a diffusion-based generative model to explicitly
capture the complex behavior policy, enabling accurate modeling of diverse
multi-agent coordination patterns. Second, we introduce a sequential score
function decomposition mechanism to regularize individual policies and enable
decentralized execution. Extensive experiments on continuous control tasks
demonstrate state-of-the-art performance across multiple standard offline MARL
benchmarks, outperforming existing methods by 26.3\% in normalized returns. Our
approach provides new insights into offline coordination and equilibrium
selection in cooperative multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.05967v1' target='_blank'>Learning Power Control Protocol for In-Factory 6G Subnetworks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Uyoata E. Uyoata, Gilberto Berardinelli, Ramoni Adeogun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-09 11:39:18</h6>
<p class='card-text'>In-X Subnetworks are envisioned to meet the stringent demands of short-range
communication in diverse 6G use cases. In the context of In-Factory scenarios,
effective power control is critical to mitigating the impact of interference
resulting from potentially high subnetwork density. Existing approaches to
power control in this domain have predominantly emphasized the data plane,
often overlooking the impact of signaling overhead. Furthermore, prior work has
typically adopted a network-centric perspective, relying on the assumption of
complete and up-to-date channel state information (CSI) being readily available
at the central controller. This paper introduces a novel multi-agent
reinforcement learning (MARL) framework designed to enable access points to
autonomously learn both signaling and power control protocols in an In-Factory
Subnetwork environment. By formulating the problem as a partially observable
Markov decision process (POMDP) and leveraging multi-agent proximal policy
optimization (MAPPO), the proposed approach achieves significant advantages.
The simulation results demonstrate that the learning-based method reduces
signaling overhead by a factor of 8 while maintaining a buffer flush rate that
lags the ideal "Genie" approach by only 5%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.05262v1' target='_blank'>Enhancing Cooperative Multi-Agent Reinforcement Learning with State
  Modelling and Adversarial Exploration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andreas Kontogiannis, Konstantinos Papathanasiou, Yi Shen, Giorgos Stamou, Michael M. Zavlanos, George Vouros</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-08 14:07:20</h6>
<p class='card-text'>Learning to cooperate in distributed partially observable environments with
no communication abilities poses significant challenges for multi-agent deep
reinforcement learning (MARL). This paper addresses key concerns in this
domain, focusing on inferring state representations from individual agent
observations and leveraging these representations to enhance agents'
exploration and collaborative task execution policies. To this end, we propose
a novel state modelling framework for cooperative MARL, where agents infer
meaningful belief representations of the non-observable state, with respect to
optimizing their own policies, while filtering redundant and less informative
joint state information. Building upon this framework, we propose the MARL SMPE
algorithm. In SMPE, agents enhance their own policy's discriminative abilities
under partial observability, explicitly by incorporating their beliefs into the
policy network, and implicitly by adopting an adversarial type of exploration
policies which encourages agents to discover novel, high-value states while
improving the discriminative abilities of others. Experimentally, we show that
SMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative
tasks from the MPE, LBF, and RWARE benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.03949v1' target='_blank'>Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock
  Trading</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:John Christopher Tidwell, John Storm Tidwell</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-06 19:55:57</h6>
<p class='card-text'>This project addresses the challenge of automated stock trading, where
traditional methods and direct reinforcement learning (RL) struggle with market
noise, complexity, and generalization. Our proposed solution is an integrated
deep learning framework combining a Convolutional Neural Network (CNN) to
identify patterns in technical indicators formatted as images, a Long
Short-Term Memory (LSTM) network to capture temporal dependencies across both
price history and technical indicators, and a Deep Q-Network (DQN) agent which
learns the optimal trading policy (buy, sell, hold) based on the features
extracted by the CNN and LSTM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.03586v3' target='_blank'>Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning
  Framework for Mitigating Delayed Observation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Songchen Fu, Siang Chen, Shaojing Zhao, Letian Bai, Ta Li, Yonghong Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-06 14:47:56</h6>
<p class='card-text'>In real-world multi-agent systems (MASs), observation delays are ubiquitous,
preventing agents from making decisions based on the environment's true state.
An individual agent's local observation often consists of multiple components
from other agents or dynamic entities in the environment. These discrete
observation components with varying delay characteristics pose significant
challenges for multi-agent reinforcement learning (MARL). In this paper, we
first formulate the decentralized stochastic individual delay partially
observable Markov decision process (DSID-POMDP) by extending the standard
Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL
training framework for addressing stochastic individual delays, along with
recommended implementations for its constituent modules. We implement the
DSID-POMDP's observation generation pattern using standard MARL benchmarks,
including MPE and SMAC. Experiments demonstrate that baseline MARL methods
suffer severe performance degradation under fixed and unfixed delays. The
RDC-enhanced approach mitigates this issue, remarkably achieving ideal
delay-free performance in certain delay scenarios while maintaining
generalizability. Our work provides a novel perspective on multi-agent delayed
observation problems and offers an effective solution framework. The source
code is available at https://anonymous.4open.science/r/RDC-pymarl-4512/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.03558v1' target='_blank'>Multi-Agent Reinforcement Learning Scheduling to Support Low Latency in
  Teleoperated Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Giacomo Avanzi, Marco Giordani, Michele Zorzi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-06 14:11:21</h6>
<p class='card-text'>The teleoperated driving (TD) scenario comes with stringent Quality of
Service (QoS) communication constraints, especially in terms of end-to-end
(E2E) latency and reliability. In this context, Predictive Quality of Service
(PQoS), possibly combined with Reinforcement Learning (RL) techniques, is a
powerful tool to estimate QoS degradation and react accordingly. For example,
an intelligent agent can be trained to select the optimal compression
configuration for automotive data, and reduce the file size whenever QoS
conditions deteriorate. However, compression may inevitably compromise data
quality, with negative implications for the TD application. An alternative
strategy involves operating at the Radio Access Network (RAN) level to optimize
radio parameters based on current network conditions, while preserving data
quality. In this paper, we propose Multi-Agent Reinforcement Learning (MARL)
scheduling algorithms, based on Proximal Policy Optimization (PPO), to
dynamically and intelligently allocate radio resources to minimize E2E latency
in a TD scenario. We evaluate two training paradigms, i.e., decentralized
learning with local observations (IPPO) vs. centralized aggregation (MAPPO), in
conjunction with two resource allocation strategies, i.e., proportional
allocation (PA) and greedy allocation (GA). We prove via ns-3 simulations that
MAPPO, combined with GA, achieves the best results in terms of latency,
especially as the number of vehicles increases.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.03533v1' target='_blank'>Small-Scale-Fading-Aware Resource Allocation in Wireless Federated
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiacheng Wang, Le Liang, Hao Ye, Chongtao Guo, Shi Jin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-06 13:41:59</h6>
<p class='card-text'>Judicious resource allocation can effectively enhance federated learning (FL)
training performance in wireless networks by addressing both system and
statistical heterogeneity. However, existing strategies typically rely on block
fading assumptions, which overlooks rapid channel fluctuations within each
round of FL gradient uploading, leading to a degradation in FL training
performance. Therefore, this paper proposes a small-scale-fading-aware resource
allocation strategy using a multi-agent reinforcement learning (MARL)
framework. Specifically, we establish a one-step convergence bound of the FL
algorithm and formulate the resource allocation problem as a decentralized
partially observable Markov decision process (Dec-POMDP), which is subsequently
solved using the QMIX algorithm. In our framework, each client serves as an
agent that dynamically determines spectrum and power allocations within each
coherence time slot, based on local observations and a reward derived from the
convergence analysis. The MARL setting reduces the dimensionality of the action
space and facilitates decentralized decision-making, enhancing the scalability
and practicality of the solution. Experimental results demonstrate that our
QMIX-based resource allocation strategy significantly outperforms baseline
methods across various degrees of statistical heterogeneity. Additionally,
ablation studies validate the critical importance of incorporating small-scale
fading dynamics, highlighting its role in optimizing FL performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.02293v1' target='_blank'>Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning
  with Layered Safety</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jason J. Choi, Jasmine Jerry Aloor, Jingqi Li, Maria G. Mendoza, Hamsa Balakrishnan, Claire J. Tomlin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-04 23:42:52</h6>
<p class='card-text'>Preventing collisions in multi-robot navigation is crucial for deployment.
This requirement hinders the use of learning-based approaches, such as
multi-agent reinforcement learning (MARL), on their own due to their lack of
safety guarantees. Traditional control methods, such as reachability and
control barrier functions, can provide rigorous safety guarantees when
interactions are limited only to a small number of robots. However, conflicts
between the constraints faced by different agents pose a challenge to safe
multi-agent coordination.
  To overcome this challenge, we propose a method that integrates multiple
layers of safety by combining MARL with safety filters. First, MARL is used to
learn strategies that minimize multiple agent interactions, where multiple
indicates more than two. Particularly, we focus on interactions likely to
result in conflicting constraints within the engagement distance. Next, for
agents that enter the engagement distance, we prioritize pairs requiring the
most urgent corrective actions. Finally, a dedicated safety filter provides
tactical corrective actions to resolve these conflicts. Crucially, the design
decisions for all layers of this framework are grounded in reachability
analysis and a control barrier-value function-based filtering mechanism.
  We validate our Layered Safe MARL framework in 1) hardware experiments using
Crazyflie drones and 2) high-density advanced aerial mobility (AAM) operation
scenarios, where agents navigate to designated waypoints while avoiding
collisions. The results show that our method significantly reduces conflict
while maintaining safety without sacrificing much efficiency (i.e., shorter
travel time and distance) compared to baselines that do not incorporate layered
safety. The project website is available at
https://dinamo-mit.github.io/Layered-Safe-MARL/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.02215v1' target='_blank'>Interpretable Emergent Language Using Inter-Agent Transformers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mannan Bhardwaj</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-04 18:57:57</h6>
<p class='card-text'>This paper explores the emergence of language in multi-agent reinforcement
learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and
CommNet enable agent communication but lack interpretability. We propose
Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention
to learn symbolic, human-understandable communication protocols. Through
experiments, DIAT demonstrates the ability to encode observations into
interpretable vocabularies and meaningful embeddings, effectively solving
cooperative tasks. These results highlight the potential of DIAT for
interpretable communication in complex multi-agent environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.01885v1' target='_blank'>Securing 5G and Beyond-Enabled UAV Networks: Resilience Through
  Multiagent Learning and Transformers Detection</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joseanne Viana, Hamed Farkhari, Victor P Gil Jimenez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-03 18:27:00</h6>
<p class='card-text'>Achieving resilience remains a significant challenge for Unmanned Aerial
Vehicle (UAV) communications in 5G and 6G networks. Although UAVs benefit from
superior positioning capabilities, rate optimization techniques, and extensive
line-of-sight (LoS) range, these advantages alone cannot guarantee high
reliability across diverse UAV use cases. This limitation becomes particularly
evident in urban environments, where UAVs face vulnerability to jamming attacks
and where LoS connectivity is frequently compromised by buildings and other
physical obstructions. This paper introduces DET-FAIR- WINGS (
Detection-Enhanced Transformer Framework for AI-Resilient Wireless Networks in
Ground UAV Systems), a novel solution designed to enhance reliability in UAV
communications under attacks. Our system leverages multi-agent reinforcement
learning (MARL) and transformer-based detection algorithms to identify attack
patterns within the network and subsequently select the most appropriate
mechanisms to strengthen reliability in authenticated UAV-Base Station links.
The DET-FAIR-WINGS approach integrates both discrete and continuous parameters.
Discrete parameters include retransmission attempts, bandwidth partitioning,
and notching mechanisms, while continuous parameters encompass beam angles and
elevations from both the Base Station (BS) and user devices. The detection part
integrates a transformer in the agents to speed up training. Our findings
demonstrate that replacing fixed retransmission counts with AI-integrated
flexible approaches in 5G networks significantly reduces latency by optimizing
decision-making processes within 5G layers.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.00540v1' target='_blank'>Emergence of Roles in Robotic Teams with Model Sharing and Limited
  Communication</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ian O'Flynn, Harun Šiljak</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-01 14:05:46</h6>
<p class='card-text'>We present a reinforcement learning strategy for use in multi-agent foraging
systems in which the learning is centralised to a single agent and its model is
periodically disseminated among the population of non-learning agents. In a
domain where multi-agent reinforcement learning (MARL) is the common approach,
this approach aims to significantly reduce the computational and energy demands
compared to approaches such as MARL and centralised learning models. By
developing high performing foraging agents, these approaches can be translated
into real-world applications such as logistics, environmental monitoring, and
autonomous exploration. A reward function was incorporated into this approach
that promotes role development among agents, without explicit directives. This
led to the differentiation of behaviours among the agents. The implicit
encouragement of role differentiation allows for dynamic actions in which
agents can alter roles dependent on their interactions with the environment
without the need for explicit communication between agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.01453v1' target='_blank'>Safe and Efficient CAV Lane Changing using Decentralised Safety Shields</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bharathkumar Hegde, Melanie Bouroche</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-30 09:11:09</h6>
<p class='card-text'>Lane changing is a complex decision-making problem for Connected and
Autonomous Vehicles (CAVs) as it requires balancing traffic efficiency with
safety. Although traffic efficiency can be improved by using vehicular
communication for training lane change controllers using Multi-Agent
Reinforcement Learning (MARL), ensuring safety is difficult. To address this
issue, we propose a decentralised Hybrid Safety Shield (HSS) that combines
optimisation and a rule-based approach to guarantee safety. Our method applies
control barrier functions to constrain longitudinal and lateral control inputs
of a CAV to ensure safe manoeuvres. Additionally, we present an architecture to
integrate HSS with MARL, called MARL-HSS, to improve traffic efficiency while
ensuring safety. We evaluate MARL-HSS using a gym-like environment that
simulates an on-ramp merging scenario with two levels of traffic densities,
such as light and moderate densities. The results show that HSS provides a
safety guarantee by strictly enforcing a dynamic safety constraint defined on a
time headway, even in moderate traffic density that offers challenging lane
change scenarios. Moreover, the proposed method learns stable policies compared
to the baseline, a state-of-the-art MARL lane change controller without a
safety shield. Further policy evaluation shows that our method achieves a
balance between safety and traffic efficiency with zero crashes and comparable
average speeds in light and moderate traffic densities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.21278v1' target='_blank'>Robust Multi-agent Communication Based on Decentralization-Oriented
  Adversarial Training</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xuyan Ma, Yawen Wang, Junjie Wang, Xiaofei Xie, Boyu Wu, Shoubin Li, Fanjiang Xu, Qing Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-30 03:14:50</h6>
<p class='card-text'>In typical multi-agent reinforcement learning (MARL) problems, communication
is important for agents to share information and make the right decisions.
However, due to the complexity of training multi-agent communication, existing
methods often fall into the dilemma of local optimization, which leads to the
concentration of communication in a limited number of channels and presents an
unbalanced structure. Such unbalanced communication policy are vulnerable to
abnormal conditions, where the damage of critical communication channels can
trigger the crash of the entire system. Inspired by decentralization theory in
sociology, we propose DMAC, which enhances the robustness of multi-agent
communication policies by retraining them into decentralized patterns.
Specifically, we train an adversary DMAC\_Adv which can dynamically identify
and mask the critical communication channels, and then apply the adversarial
samples generated by DMAC\_Adv to the adversarial learning of the communication
policy to force the policy in exploring other potential communication schemes
and transition to a decentralized structure. As a training method to improve
robustness, DMAC can be fused with any learnable communication policy
algorithm. The experimental results in two communication policies and four
multi-agent tasks demonstrate that DMAC achieves higher improvement on
robustness and performance of communication policy compared with two
state-of-the-art and commonly-used baselines. Also, the results demonstrate
that DMAC can achieve decentralized communication structure with acceptable
communication cost.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.21164v1' target='_blank'>Learning Large-Scale Competitive Team Behaviors with Mean-Field
  Interactions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bhavini Jeloka, Yue Guan, Panagiotis Tsiotras</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-29 20:31:59</h6>
<p class='card-text'>State-of-the-art multi-agent reinforcement learning (MARL) algorithms such as
MADDPG and MAAC fail to scale in situations where the number of agents becomes
large. Mean-field theory has shown encouraging results in modeling macroscopic
agent behavior for teams with a large number of agents through a continuum
approximation of the agent population and its interaction with the environment.
In this work, we extend proximal policy optimization (PPO) to the mean-field
domain by introducing the Mean-Field Multi-Agent Proximal Policy Optimization
(MF-MAPPO), a novel algorithm that utilizes the effectiveness of the
finite-population mean-field approximation in the context of zero-sum
competitive multi-agent games between two teams. The proposed algorithm can be
easily scaled to hundreds and thousands of agents in each team as shown through
numerical experiments. In particular, the algorithm is applied to realistic
applications such as large-scale offense-defense battlefield scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.03771v1' target='_blank'>OneDSE: A Unified Microprocessor Metric Prediction and Design Space
  Exploration Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ritik Raj, Akshat Ramachandran, Jeff Nye, Shashank Nemawarkar, Tushar Krishna</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-29 19:19:52</h6>
<p class='card-text'>With the diminishing returns of Moore Law scaling and as power constraints
become more impactful, processor designs rely on architectural innovation to
achieve differentiating performance. Innovation complexity has increased the
design space of modern high-performance processors. This work offers an
efficient and novel design space exploration (DSE) solution to these challenges
of modern CPU design. We identify three key challenges in past DSE approaches:
(a) Metric prediction is slow and inaccurate for unseen workloads,
microarchitectures, (b) Search is slow and inaccurate in CPU parameter space,
and (c) A Single model is unable to learn the huge design space. We present
OneDSE, a unified metric predictor and CPU parameter explorer to mitigate these
challenges with three key techniques: (a) Transformer-based workload-Aware CPU
DSE (TrACE) predictor that outperforms state-of-the-art ANN-based prediction
methods by 2.75x and 6.12x with and without fine-tuning, respectively, on
several benchmarks; (b) a novel metric space search approach that outperforms
optimized metaheuristics by 1.19x while reducing search time by an order of
magnitude; (c) MARL-based multi-agent framework that achieves a 10.6% reduction
in prediction error compared to its non-MARL counterpart, enabling more
accurate and efficient exploration of the CPU design space.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.20529v1' target='_blank'>Safe Bottom-Up Flexibility Provision from Distributed Energy Resources</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Costas Mylonas, Emmanouel Varvarigos, Georgios Tsaousoglou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-29 08:16:15</h6>
<p class='card-text'>Modern renewables-based power systems need to tap on the flexibility of
Distributed Energy Resources (DERs) connected to distribution networks. It is
important, however, that DER owners/users remain in control of their assets,
decisions, and objectives. At the same time, the dynamic landscape of
DER-penetrated distribution networks calls for agile, data-driven flexibility
management frameworks. In the face of these developments, the Multi-Agent
Reinforcement Learning (MARL) paradigm is gaining significant attention, as a
distributed and data-driven decision-making policy. This paper addresses the
need for bottom-up DER management decisions to account for the distribution
network's safety-related constraints. While the related literature on safe MARL
typically assumes that network characteristics are available and incorporated
into the policy's safety layer, which implies active DSO engagement, this paper
ensures that self-organized DER communities are enabled to provide
distribution-network-safe flexibility services without relying on the
aspirational and problematic requirement of bringing the DSO in the
decision-making loop.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>