<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-11-15</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-11-15</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.10598v1' target='_blank'>Optimizing the flight path for a scouting Uncrewed Aerial Vehicle</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Raghav Adhikari, Sachet Khatiwada, Suman Poudel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 18:36:05</h6>
<p class='card-text'>Post-disaster situations pose unique navigation challenges. One of those challenges is the unstructured nature of the environment, which makes it hard to layout paths for rescue vehicles. We propose the use of Uncrewed Aerial Vehicle (UAV) in such scenario to perform reconnaissance across the environment. To accomplish this, we propose an optimization-based approach to plan a path for the UAV at optimal height where the sensors of the UAV can cover the most area and collect data with minimum uncertainty.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.10586v1' target='_blank'>Safe Planning in Interactive Environments via Iterative Policy Updates and Adversarially Robust Conformal Prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Omid Mirzaeedodangeh, Eliot Shekhtman, Nikolai Matni, Lars Lindemann</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 18:25:54</h6>
<p class='card-text'>Safe planning of an autonomous agent in interactive environments -- such as the control of a self-driving vehicle among pedestrians and human-controlled vehicles -- poses a major challenge as the behavior of the environment is unknown and reactive to the behavior of the autonomous agent. This coupling gives rise to interaction-driven distribution shifts where the autonomous agent's control policy may change the environment's behavior, thereby invalidating safety guarantees in existing work. Indeed, recent works have used conformal prediction (CP) to generate distribution-free safety guarantees using observed data of the environment. However, CP's assumption on data exchangeability is violated in interactive settings due to a circular dependency where a control policy update changes the environment's behavior, and vice versa. To address this gap, we propose an iterative framework that robustly maintains safety guarantees across policy updates by quantifying the potential impact of a planned policy update on the environment's behavior. We realize this via adversarially robust CP where we perform a regular CP step in each episode using observed data under the current policy, but then transfer safety guarantees across policy updates by analytically adjusting the CP result to account for distribution shifts. This adjustment is performed based on a policy-to-trajectory sensitivity analysis, resulting in a safe, episodic open-loop planner. We further conduct a contraction analysis of the system providing conditions under which both the CP results and the policy updates are guaranteed to converge. We empirically demonstrate these safety and convergence guarantees on a two-dimensional car-pedestrian case study. To the best of our knowledge, these are the first results that provide valid safety guarantees in such interactive settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.10403v1' target='_blank'>nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingxing Peng, Ruoyu Yao, Xusen Guo, Jun Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 15:23:30</h6>
<p class='card-text'>Recent advances in closed-loop planning benchmarks have significantly improved the evaluation of autonomous vehicles. However, existing benchmarks still rely on rule-based reactive agents such as the Intelligent Driver Model (IDM), which lack behavioral diversity and fail to capture realistic human interactions, leading to oversimplified traffic dynamics. To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework. Our benchmark replaces the rule-based IDM agents with noise-decoupled diffusion-based reactive agents and introduces an interaction-aware agent selection mechanism to ensure both realism and computational efficiency. Furthermore, we extend the benchmark with two additional metrics to enable a more comprehensive assessment of planning performance. Extensive experiments demonstrate that our reactive agent model produces more realistic, diverse, and human-like traffic behaviors, leading to a benchmark environment that better reflects real-world interactive driving. We further reimplement a collection of rule-based, learning-based, and hybrid planning approaches within our nuPlan-R benchmark, providing a clearer reflection of planner performance in complex interactive scenarios and better highlighting the advantages of learning-based planners in handling complex and dynamic scenarios. These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation. We will open-source the code for the new benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.10300v1' target='_blank'>Generalizable Slum Detection from Satellite Imagery with Mixture-of-Experts</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sumin Lee, Sungwon Park, Jeasurk Yang, Jihee Kim, Meeyoung Cha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 13:35:50</h6>
<p class='card-text'>Satellite-based slum segmentation holds significant promise in generating global estimates of urban poverty. However, the morphological heterogeneity of informal settlements presents a major challenge, hindering the ability of models trained on specific regions to generalize effectively to unseen locations. To address this, we introduce a large-scale high-resolution dataset and propose GRAM (Generalized Region-Aware Mixture-of-Experts), a two-phase test-time adaptation framework that enables robust slum segmentation without requiring labeled data from target regions. We compile a million-scale satellite imagery dataset from 12 cities across four continents for source training. Using this dataset, the model employs a Mixture-of-Experts architecture to capture region-specific slum characteristics while learning universal features through a shared backbone. During adaptation, prediction consistency across experts filters out unreliable pseudo-labels, allowing the model to generalize effectively to previously unseen regions. GRAM outperforms state-of-the-art baselines in low-resource settings such as African cities, offering a scalable and label-efficient solution for global slum mapping and data-driven urban planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.10198v1' target='_blank'>Population-mobility coevolution drives spatial heterogeneity of cities</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Huang, Yuming Lin, Jiazhen Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 11:13:24</h6>
<p class='card-text'>The spatial heterogeneity of cities -- the uneven distribution of population and activities -- is fundamental to urban dynamics and related to critical issues such as infrastructure overload, housing affordability, and social inequality. Despite sharing similar scaling laws of population and mobility, cities exhibit vastly different spatial patterns. This paradox call for a mechanistic explanation for the emergence of spatial heterogeneity, while existing qualitative or descriptive studies fail to capture the underlying mechanisms. Here, we propose a coupled dynamical model that describe the intra-city population-mobility coevolution, explaining spatial heterogeneity as an emergent outcome of mutual feedback between the fast-changing mobility and the slow-adapting population. Our model is validated on over 388 million records from eight diverse global cities, successfully reproduces both the statistical laws and realistic spatial patterns. We find out realistic heterogeneity emerges as a distinct stable state, intermediate between disordered homogeneity and unsustainable super-hub dominance. Moreover, we theoretically and empirically show that populated areas are predominantly shaped by coevolution strength, while the increasing distance decay leads cities through a three-phase transition of homogeneity-heterogeneity-homogeneity. Besides, functional attractiveness between areas consistently enhances the ordered heterogeneous structure. Simulations of real-world planning scenarios -- including crisis-induced lockdown, planned zone expansions, and dispersal from congested centers -- indicate that integrated population-mobility policies are more cost-effective than single interventions. Our model can provides mechanistic, high-resolution insights to rigorously inform policy design.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.10164v1' target='_blank'>Two Constraint Compilation Methods for Lifted Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Periklis Mantenoglou, Luigi Bonassi, Enrico Scala, Pedro Zuidberg Dos Martires</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 10:24:31</h6>
<p class='card-text'>We study planning in a fragment of PDDL with qualitative state-trajectory constraints, capturing safety requirements, task ordering conditions, and intermediate sub-goals commonly found in real-world problems. A prominent approach to tackle such problems is to compile their constraints away, leading to a problem that is supported by state-of-the-art planners. Unfortunately, existing compilers do not scale on problems with a large number of objects and high-arity actions, as they necessitate grounding the problem before compilation. To address this issue, we propose two methods for compiling away constraints without grounding, making them suitable for large-scale planning problems. We prove the correctness of our compilers and outline their worst-case time complexity. Moreover, we present a reproducible empirical evaluation on the domains used in the latest International Planning Competition. Our results demonstrate that our methods are efficient and produce planning specifications that are orders of magnitude more succinct than the ones produced by compilers that ground the domain, while remaining competitive when used for planning with a state-of-the-art planner.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.10102v1' target='_blank'>On the design of a profession-oriented course on Theoretical Mechanics for physics education students</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marianne Korner, Christos N. Likos</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 09:06:04</h6>
<p class='card-text'>We report on a profession-oriented course we offered at the University of Vienna, aimed at physics education teacher students. The course on Theoretical Classical Mechanics has been conceived and designed from its outset with the explicit goal of bridging the gap between the abstract, mathematical notions employed in Theoretical Physics with the concrete future needs of prospective teachers in their profession. We aimed at countering both the negative attitudes of students towards Theoretical Physics and the interrelated skepticism of professors regarding the students mathematical proficiency. Our main findings are that these goals can indeed be achieved through a careful selection of course material and the associated mathematical tools, by closely interwoven lecture topics and exercises, and thorough planning according to principles for high-school teaching known from science education research. Establishing close connections between the material taught in the course and the students future occupation as high-school teachers has proven to be of utmost importance. This is possible without any sacrifice of mathematical rigor or of the quality of Physics presented.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09907v1' target='_blank'>Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongxian Wei, Yilin Zhao, Li Shen, Xinrui Chen, Runxi Cheng, Sinan Du, Hao Yu, Gang Liu, Jiahong Yan, Chun Yuan, Dian Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 03:08:51</h6>
<p class='card-text'>Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09892v1' target='_blank'>Benders Decomposition for Passenger-Oriented Train Timetabling with Hybrid Periodicity</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiyuan Yao, Anita Schöbel, Lei Nie, Sven Jäger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 02:55:21</h6>
<p class='card-text'>Periodic timetables are widely adopted in passenger railway operations due to their regular service patterns and well-coordinated train connections. However, fluctuations in passenger demand require varying train services across different periods, necessitating adjustments to the periodic timetable. This study addresses a hybrid periodic train timetabling problem, which enhances the flexibility and demand responsiveness of a given periodic timetable through schedule adjustments and aperiodic train insertions, taking into account the rolling stock circulation. Since timetable modifications may affect initial passenger routes, passenger routing is incorporated into the problem to guide planning decisions towards a passenger-oriented objective. Using a time-space network representation, the problem is formulated as a dynamic railway service network design model with resource constraints. To handle the complexity of real-world instances, we propose a decomposition-based algorithm integrating Benders decomposition and column generation, enhanced with multiple preprocessing and accelerating techniques. Numerical experiments demonstrate the effectiveness of the algorithm and highlight the advantage of hybrid periodic timetables in reducing passenger travel costs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09882v1' target='_blank'>Truth, Justice, and Secrecy: Cake Cutting Under Privacy Constraints</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaron Salman, Tamir Tassa, Omer Lev, Roie Zivan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 02:28:01</h6>
<p class='card-text'>Cake-cutting algorithms, which aim to fairly allocate a continuous resource based on individual agent preferences, have seen significant progress over the past two decades. Much of the research has concentrated on fairness, with comparatively less attention given to other important aspects. Chen et al. (2010) introduced an algorithm that, in addition to ensuring fairness, was strategyproof -- meaning agents had no incentive to misreport their valuations. However, even in the absence of strategic incentives to misreport, agents may still hesitate to reveal their true preferences due to privacy concerns (e.g., when allocating advertising time between firms, revealing preferences could inadvertently expose planned marketing strategies or product launch timelines). In this work, we extend the strategyproof algorithm of Chen et al. by introducing a privacy-preserving dimension. To the best of our knowledge, we present the first private cake-cutting protocol, and, in addition, this protocol is also envy-free and strategyproof. Our approach replaces the algorithm's centralized computation with a novel adaptation of cryptographic techniques, enabling privacy without compromising fairness or strategyproofness. Thus, our protocol encourages agents to report their true preferences not only because they are not incentivized to lie, but also because they are protected from having their preferences exposed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09862v1' target='_blank'>Mock Observations for the CSST Mission: CPI-C -- Targets for High Contrast Imaging</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yi-Ming Zhu, Gang Zhao, Jiang-Pei Dou, Zhong-Hua Lv, Yi-Li Chen, Bo Ma, Zhao-Jun Yan, Jing Tang, Ran Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 01:42:41</h6>
<p class='card-text'>We introduce CPISM, a simulation program developed for the Cool Planet Imaging Coronagraph (CPI-C) on the China Space Station Telescope (CSST). CPISM supports high-contrast exoplanet imaging by simulating observational conditions and instrumental effects to optimize target selection and observation strategies. The modular design includes target modeling, imaging simulation, observational effects, detector response, and data product generation modules, enabling flexible and realistic synthetic observations. Validation through simulations of a bright star shows strong agreement with theoretical expectations, confirming the program's accuracy. CPISM's modular design allows flexibility, accommodating different stellar and planetary models, and can simulate instrumental noise, cosmic rays, and other observational effects. This tool aids in data processing, signal-to-noise ratio analysis, and high-contrast photometry, contributing to future exoplanet discovery and characterization efforts. The program's outputs will enhance observation planning and scientific return for the CPI-C mission, providing critical insights into exoplanetary systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09853v1' target='_blank'>ConSurv: Multimodal Continual Learning for Survival Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dianzhi Yu, Conghao Xiong, Yankai Chen, Wenqian Cui, Xinni Zhang, Yifei Zhang, Hao Chen, Joseph J. Y. Sung, Irwin King</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 01:25:26</h6>
<p class='card-text'>Survival prediction of cancers is crucial for clinical practice, as it informs mortality risks and influences treatment plans. However, a static model trained on a single dataset fails to adapt to the dynamically evolving clinical environment and continuous data streams, limiting its practical utility. While continual learning (CL) offers a solution to learn dynamically from new datasets, existing CL methods primarily focus on unimodal inputs and suffer from severe catastrophic forgetting in survival prediction. In real-world scenarios, multimodal inputs often provide comprehensive and complementary information, such as whole slide images and genomics; and neglecting inter-modal correlations negatively impacts the performance. To address the two challenges of catastrophic forgetting and complex inter-modal interactions between gigapixel whole slide images and genomics, we propose ConSurv, the first multimodal continual learning (MMCL) method for survival analysis. ConSurv incorporates two key components: Multi-staged Mixture of Experts (MS-MoE) and Feature Constrained Replay (FCR). MS-MoE captures both task-shared and task-specific knowledge at different learning stages of the network, including two modality encoders and the modality fusion component, learning inter-modal relationships. FCR further enhances learned knowledge and mitigates forgetting by restricting feature deviation of previous data at different levels, including encoder-level features of two modalities and the fusion-level representations. Additionally, we introduce a new benchmark integrating four datasets, Multimodal Survival Analysis Incremental Learning (MSAIL), for comprehensive evaluation in the CL setting. Extensive experiments demonstrate that ConSurv outperforms competing methods across multiple metrics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09836v1' target='_blank'>Provably Safe Stein Variational Clarity-Aware Informative Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kaleb Ben Naveed, Utkrisht Sahai, Anouck Girard, Dimitra Panagou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-13 00:35:43</h6>
<p class='card-text'>Autonomous robots are increasingly deployed for information-gathering tasks in environments that vary across space and time. Planning informative and safe trajectories in such settings is challenging because information decays when regions are not revisited. Most existing planners model information as static or uniformly decaying, ignoring environments where the decay rate varies spatially; those that model non-uniform decay often overlook how it evolves along the robot's motion, and almost all treat safety as a soft penalty. In this paper, we address these challenges. We model uncertainty in the environment using clarity, a normalized representation of differential entropy from our earlier work that captures how information improves through new measurements and decays over time when regions are not revisited. Building on this, we present Stein Variational Clarity-Aware Informative Planning, a framework that embeds clarity dynamics within trajectory optimization and enforces safety through a low-level filtering mechanism based on our earlier gatekeeper framework for safety verification. The planner performs Bayesian inference-based learning via Stein variational inference, refining a distribution over informative trajectories while filtering each nominal Stein informative trajectory to ensure safety. Hardware experiments and simulations across environments with varying decay rates and obstacles demonstrate consistent safety and reduced information deficits.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09790v1' target='_blank'>A Robust Task-Level Control Architecture for Learned Dynamical Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Eshika Pathak, Ahmed Aboudonia, Sandeep Banik, Naira Hovakimyan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 22:45:32</h6>
<p class='card-text'>Dynamical system (DS)-based learning from demonstration (LfD) is a powerful tool for generating motion plans in the operation (`task') space of robotic systems. However, the realization of the generated motion plans is often compromised by a ''task-execution mismatch'', where unmodeled dynamics, persistent disturbances, and system latency cause the robot's actual task-space state to diverge from the desired motion trajectory. We propose a novel task-level robust control architecture, L1-augmented Dynamical Systems (L1-DS), that explicitly handles the task-execution mismatch in tracking a nominal motion plan generated by any DS-based LfD scheme. Our framework augments any DS-based LfD model with a nominal stabilizing controller and an L1 adaptive controller. Furthermore, we introduce a windowed Dynamic Time Warping (DTW)-based target selector, which enables the nominal stabilizing controller to handle temporal misalignment for improved phase-consistent tracking. We demonstrate the efficacy of our architecture on the LASA and IROS handwriting datasets.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09760v1' target='_blank'>Coherent Optical Quantum Computing-Aided Resource Optimization for Transportation Digital Twin Construction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huixiang Zhang, Mahzabeen Emu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 21:42:19</h6>
<p class='card-text'>Constructing realistic digital twins for applications such as training autonomous driving models requires the efficient allocation of real-world data, yet data sovereignty regulations present a major challenge. To address this, we tackle the optimization problem faced by metaverse service providers (MSPs) responsible for allocating geographically constrained data resources. We propose a two-stage stochastic integer programming (SIP) model that incorporates reservation and on-demand planning, enabling MSPs to efficiently subscribe and allocate data from specific regions to clients for training their models on local road conditions. The SIP model is transformed into a quadratic unconstrained binary optimization (QUBO) formulation and implemented for the first time at a practical scale on a 550-qubit coherent Ising machine (CIM), representing an exploratory step toward future quantum computing paradigms. Our approach introduces an MSP-centric framework for compliant data collection under sovereignty constraints, a hybrid cost model combining deterministic fees with probabilistic penalties, and a practical implementation on quantum hardware. Experimental results demonstrate that CIM-based optimization finds high-quality solutions with millisecond-scale ($10^3$ second) computation times, significantly outperforming quantum-inspired solvers like PyQUBO. Although classical solvers such as Gurobi can achieve marginally better solution quality, CIM is orders of magnitude faster, establishing a practical paradigm for quantum-enhanced resource management.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09724v1' target='_blank'>PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yunqian Cheng, Benjamin Princen, Roberto Manduchi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 20:30:53</h6>
<p class='card-text'>Indoor localization in GPS-denied environments is crucial for applications like emergency response and assistive navigation. Vision-based methods such as PALMS enable infrastructure-free localization using only a floor plan and a stationary scan, but are limited by the short range of smartphone LiDAR and ambiguity in indoor layouts. We propose PALMS$+$, a modular, image-based system that addresses these challenges by reconstructing scale-aligned 3D point clouds from posed RGB images using a foundation monocular depth estimation model (Depth Pro), followed by geometric layout matching via convolution with the floor plan. PALMS$+$ outputs a posterior over the location and orientation, usable for direct or sequential localization. Evaluated on the Structured3D and a custom campus dataset consisting of 80 observations across four large campus buildings, PALMS$+$ outperforms PALMS and F3Loc in stationary localization accuracy -- without requiring any training. Furthermore, when integrated with a particle filter for sequential localization on 33 real-world trajectories, PALMS$+$ achieved lower localization errors compared to other methods, demonstrating robustness for camera-free tracking and its potential for infrastructure-free applications. Code and data are available at https://github.com/Head-inthe-Cloud/PALMS-Plane-based-Accessible-Indoor-Localization-Using-Mobile-Smartphones</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09695v1' target='_blank'>A Shared-Autonomy Construction Robotic System for Overhead Works</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:David Minkwan Kim, K. M. Brian Lee, Yong Hyeok Seo, Nikola Raicevic, Runfa Blark Li, Kehan Long, Chan Seon Yoon, Dong Min Kang, Byeong Jo Lim, Young Pyoung Kim, Nikolay Atanasov, Truong Nguyen, Se Woong Jun, Young Wook Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 19:51:42</h6>
<p class='card-text'>We present the ongoing development of a robotic system for overhead work such as ceiling drilling. The hardware platform comprises a mobile base with a two-stage lift, on which a bimanual torso is mounted with a custom-designed drilling end effector and RGB-D cameras. To support teleoperation in dynamic environments with limited visibility, we use Gaussian splatting for online 3D reconstruction and introduce motion parameters to model moving objects. For safe operation around dynamic obstacles, we developed a neural configuration-space barrier approach for planning and control. Initial feasibility studies demonstrate the capability of the hardware in drilling, bolting, and anchoring, and the software in safe teleoperation in a dynamic environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09549v1' target='_blank'>Breadth-First Search vs. Restarting Random Walks for Escaping Uninformed Heuristic Regions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel Platnick, Dawson Tomasz, Eamon Earl, Sourena Khanzadeh, Richard Valenzano</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 18:53:37</h6>
<p class='card-text'>Greedy search methods like Greedy Best-First Search (GBFS) and Enforced Hill-Climbing (EHC) often struggle when faced with Uninformed Heuristic Regions (UHRs) like heuristic local minima or plateaus. In this work, we theoretically and empirically compare two popular methods for escaping UHRs in breadth-first search (BrFS) and restarting random walks (RRWs). We first derive the expected runtime of escaping a UHR using BrFS and RRWs, based on properties of the UHR and the random walk procedure, and then use these results to identify when RRWs will be faster in expectation than BrFS. We then evaluate these methods for escaping UHRs by comparing standard EHC, which uses BrFS to escape UHRs, to variants of EHC called EHC-RRW, which use RRWs for that purpose. EHC-RRW is shown to have strong expected runtime guarantees in cases where EHC has previously been shown to be effective. We also run experiments with these approaches on PDDL planning benchmarks to better understand their relative effectiveness for escaping UHRs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09533v1' target='_blank'>Digital Co-Founders: Transforming Imagination into Viable Solo Business via Agentic AI</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Farhad Rezazadeh, Pegah Bonehgazy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 18:30:58</h6>
<p class='card-text'>This paper investigates how individual entrepreneurs can turn creative ideas into successful solo businesses in an era increasingly shaped by Artificial Intelligence (AI) agents. It highlights the key steps that connect personal vision, structured experimentation, and lasting value creation, and shows how AI agents can act as digital co-founders throughout this journey. Building on research in entrepreneurship, creativity, and innovation, we present a framework with three key stages: (1) Imagination shaping, where vague goals become clear value propositions, supported by AI agents that help with market scanning, idea refinement, and rapid concept generation; (2) Reality testing, where these ideas are tested through low-cost experiments, structured feedback loops, and efficient execution, with AI agents automating tasks such as prototyping, content creation, customer interaction, and data analysis; and (3) Reality scaling, where successful ideas are transformed into repeatable processes, scalable market strategies, and long-term business models, increasingly operated and optimized by autonomous or semi-autonomous AI workflows. We focus on the specific context of solopreneurship, characterized by limited human resources, complete accountability for decision-making, and a strong association between the founder's identity and the business. The framework clearly identifies key enabling factors such as mental adaptability, effective planning, and successful human-AI collaboration within digital ecosystems. It also thoughtfully addresses ongoing challenges, like uncertainty and cognitive overload, which are heightened by our constant connectivity.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09477v1' target='_blank'>Latent Planning via Embedding Arithmetic: A Contrastive Approach to Strategic Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andrew Hamara, Greg Hamerly, Pablo Rivas, Andrew C. Freeman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 16:50:07</h6>
<p class='card-text'>Planning in high-dimensional decision spaces is increasingly being studied through the lens of learned representations. Rather than training policies or value heads, we investigate whether planning can be carried out directly in an evaluation-aligned embedding space. We introduce SOLIS, which learns such a space using supervised contrastive learning. In this representation, outcome similarity is captured by proximity, and a single global advantage vector orients the space from losing to winning regions. Candidate actions are then ranked according to their alignment with this direction, reducing planning to vector operations in latent space. We demonstrate this approach in chess, where SOLIS uses only a shallow search guided by the learned embedding to reach competitive strength under constrained conditions. More broadly, our results suggest that evaluation-aligned latent planning offers a lightweight alternative to traditional dynamics models or policy learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09450v1' target='_blank'>How does the Performance of the Data-driven Traffic Flow Forecasting Models deteriorate with Increasing Forecasting Horizon? An Extensive Approach Considering Statistical, Machine Learning and Deep Learning Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amanta Sherfenaz, Nazmul Haque, Protiva Sadhukhan Prova, Md Asif Raihan, Md. Hadiuzzaman</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 16:06:35</h6>
<p class='card-text'>With rapid urbanization in recent decades, traffic congestion has intensified due to increased movement of people and goods. As planning shifts from demand-based to supply-oriented strategies, Intelligent Transportation Systems (ITS) have become essential for managing traffic within existing infrastructure. A core ITS function is traffic forecasting, enabling proactive measures like ramp metering, signal control, and dynamic routing through platforms such as Google Maps. This study assesses the performance of statistical, machine learning (ML), and deep learning (DL) models in forecasting traffic speed and flow using real-world data from California's Harbor Freeway, sourced from the Caltrans Performance Measurement System (PeMS). Each model was evaluated over 20 forecasting windows (up to 1 hour 40 minutes) using RMSE, MAE, and R-Square metrics. Results show ANFIS-GP performs best at early windows with RMSE of 0.038, MAE of 0.0276, and R-Square of 0.9983, while Bi-LSTM is more robust for medium-term prediction due to its capacity to model long-range temporal dependencies, achieving RMSE of 0.1863, MAE of 0.0833, and R-Square of 0.987 at a forecasting of 20. The degradation in model performance was quantified using logarithmic transformation, with slope values used to measure robustness. Among DL models, Bi-LSTM had the flattest slope (0.0454 RMSE, 0.0545 MAE for flow), whereas ANFIS-GP had 0.1058 for RMSE and 0.1037 for flow MAE. The study concludes by identifying hybrid models as a promising future direction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09419v1' target='_blank'>POLAR-2 -- Latest Developments of the Next Generation GRB Polarimeter</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Johannes Hulsman, Philipp Azzarello, Joerg Bayer, Franck Cadoux, Mariachiara Celato, Nicolas De Angelis, Yannick Favre, Aaron Feder, Jochen Greiner, Alejandro Guzman, Coralie Husi, Vishal Kumar, Hancheng Li, Mobin Mobaseri, Gabriel Pelleriti, Agnieszka Pollo, Nicolas Produit, Dominik Rybka, Andrea Santangelo, Jianchao Sun, Chris Tenzer, Xin Wu, Shuang-Nan Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 15:34:54</h6>
<p class='card-text'>Gamma-Ray Bursts (GRBs) are among the most energetic events in the Universe. Despite over 50 years of research and measurements their prompt emission remains poorly understood, with key questions surrounding the structure of relativistic jets, magnetic field configurations, and dominant radiation mechanisms. Polarization measurements are critical in resolving these uncertainties. The POLAR mission, operational in 2016-2017 on Tiangong-2, provided the most statistically significant GRB polarization data. Its results indicated low time-averaged polarization with hints of temporal evolution. However, POLAR's limited sensitivity, small effective area, and restricted energy range prevented more detailed time- and energy-resolved analyses in addition to a larger sample of GRB polarization measurements. POLAR-2 is designed to address these limitations by offering a fourfold increase in effective area (at least) and an extended energy range of 30-800 keV by utilizing Silicon Photomultipliers (SiPMs) and an updated module design, enabling the differentiation of competing GRB emission models. The instrument comprises of 100 polarimeter modules (each with 64 plastic scintillator bars), wherein the polarization angle is extracted through Compton Scattering of the gammas. The polarimeter module design was validated during an ESRF beam test campaign in 2023. The instrument was developed by a joint effort of Switzerland, China, Poland and Germany and is planned for launch in 2027. Currently, POLAR-2 is in its production phase with the first module targets being produced. We will provide an overview of the current status of the development.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09405v1' target='_blank'>Rapid-response characterization of near-Earth asteroid 2024 YR4 during a Torino Scale 3 alert</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maxime Devogèle, Olivier R. Hainaut, Marco Micheli, Petr Pravec, Juan Luis Cano, Francisco Ocaña, Luca Conversi, Nicholas Moskovitz, Julia de León, Zuri Gray, Mikael Granvik, Grigori Fedorets, Jules Bourdelle de Micas, Simone Ieva, Elisabetta Dotto, Tracie Beuden, Carson Fuls, Teddy Kareta, Stefano Bagnulo, Maria Antonella Barucci, Mirel Birlan, Andrea Farina, Kamil Hornoch, Petr Fatka, Peter Kušnirák, Francesca Ferri, Marcello Fulchignoni, Monica Lazzarin, Fiorangela La Forgia, Elena Mazzotta Epifani, Alessandra Mura, Davide Perna, Philippe Bendjoya, Jean-Pierre Rivet, Alberto Cellino</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 15:18:14</h6>
<p class='card-text'>On 27 December 2024, near-Earth object (NEO) 2024 YR$_4$ was discovered by the ATLAS survey and identified as a virtual impactor. A few weeks later, it eventually reached level 3 on the Torino Scale and was the first and only asteroid to be ever classified at that level. Here we report an intensive observational campaign combining time-series photometry in the visible, broadband visible and near-infrared colors, and low-resolution visible reflectance spectroscopy to assess its physical properties. Fourier analysis of the lightcurves yields a synodic rotation period of $P = 19.46341 \pm 0.00008$ min, placing 2024 YR$_4$ among the fast rotators, even if such rotation is common for objects of similar $H$ magnitude. Its visible and near-infrared colors and spectra are most consistent with an Sq or K taxonomic classification, though some ambiguity remains. Finally, its phase curve exhibits a notably shallow slope ($G = 0.51 \pm 0.11$), from which we derive an absolute magnitude of $H_\mathrm{R} = 23.82\pm0.09$ mag. After color correction and taking into account other models for the phase function, we report an absolute magnitude of $H_\mathrm{V} = 24.14\pm0.25$ mag. These characterizations, rotation period, taxonomy, and surface properties, would have been crucial for risk assessment and mitigation planning had the initially high impact probability scenario been confirmed, underscoring the importance for planetary defense of a rapid, coordinated international response.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09335v1' target='_blank'>A Methodology for Developing Foundational Transformer Models in Collider Physics Analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:E. Abasov, L. Dudko, E. Iudin, A. Markina, P. Volkov, M. Perfilov, A. Zaborenko</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 13:48:32</h6>
<p class='card-text'>We present a methodology for training foundational transformer models capable of processing collider data with diverse kinematic signatures. Our universal foundation model is designed for simultaneous analysis of all processes involving from one to four top-quarks production with their corresponding background processes. The approach employs multi-task pre-training on combined datasets of simulated events, enabling the model to capture the full spectrum of interaction physics while extracting universal patterns across different final states prior to task-specific fine-tuning. This unified architecture eliminates the need for separate analysis frameworks for different final signatures and specific tasks. The transformer-based pre-training strategy explicitly preserves unique interaction patterns through adaptive attention mechanisms while establishing cross-process correlations. We plan to demonstrate how this architecture maintains sensitivity to rare high-multiplicity topologies (3t and 4t) without compromising performance on conventional channels ($t\bar{t}$, $tX$, $t\bar{t}H$), effectively bridging the gap between disparate analysis paradigms in collider physics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09321v1' target='_blank'>Distributionally Robust Joint Planning of Coastal Distribution Network and PV-Storage-EV Stations</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenhao Gao, Yongheng Wang, Wei Chen, Xinwei Shen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 13:30:21</h6>
<p class='card-text'>The rapid integration of renewable energy resources, such as tidal and photovoltaic (PV) power, coupled with the growing deployment of electric vehicle (EV) charging infrastructure, necessitates coordinated planning for coastal urban distribution networks (DN). This paper presents a tri-layer distributionally robust optimization framework to jointly optimize the sitting of PV-storage-EV stations (PSES) and the configuration of coastal DNs, addressing uncertainties related to power load, PV generation, and EV charging demands. At the upper layer, optimal PSES siting and network topology decisions are made to minimize total investment and operational costs. The middle-layer formulation tackles worst case uncertainty scenarios via the optimal power flow model, utilizing ambiguity sets to capture correlated uncertainties. To handle non-convexities introduced by binary variables for energy storage systems, we propose and rigorously prove the exactness of a novel relaxation approach. At the lower layer, considering the dynamic pricing driven by tidal energy fluctuations, operational decisions-including electricity procurement and carbon emissions are optimized. An inexact column-and-constraint generation (i-CCG) algorithm is developed for efficient problem-solving. Numerical results from a realistic 47-node coastal DN in China illustrate that the proposed method effectively reduces costs and ensures robust, low-carbon planning under substantial uncertainties.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09275v1' target='_blank'>HyperD: Hybrid Periodicity Decoupling Framework for Traffic Forecasting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Minlan Shao, Zijian Zhang, Yili Wang, Yiwei Dai, Xu Shen, Xin Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 12:42:22</h6>
<p class='card-text'>Accurate traffic forecasting plays a vital role in intelligent transportation systems, enabling applications such as congestion control, route planning, and urban mobility optimization.However, traffic forecasting remains challenging due to two key factors: (1) complex spatial dependencies arising from dynamic interactions between road segments and traffic sensors across the network, and (2) the coexistence of multi-scale periodic patterns (e.g., daily and weekly periodic patterns driven by human routines) with irregular fluctuations caused by unpredictable events (e.g., accidents, weather, or construction). To tackle these challenges, we propose HyperD (Hybrid Periodic Decoupling), a novel framework that decouples traffic data into periodic and residual components. The periodic component is handled by the Hybrid Periodic Representation Module, which extracts fine-grained daily and weekly patterns using learnable periodic embeddings and spatial-temporal attention. The residual component, which captures non-periodic, high-frequency fluctuations, is modeled by the Frequency-Aware Residual Representation Module, leveraging complex-valued MLP in frequency domain. To enforce semantic separation between the two components, we further introduce a Dual-View Alignment Loss, which aligns low-frequency information with the periodic branch and high-frequency information with the residual branch. Extensive experiments on four real-world traffic datasets demonstrate that HyperD achieves state-of-the-art prediction accuracy, while offering superior robustness under disturbances and improved computational efficiency compared to existing methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09219v1' target='_blank'>Planning in Branch-and-Bound: Model-Based Reinforcement Learning for Exact Combinatorial Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Paul Strang, Zacharie Alès, Côme Bissuel, Safia Kedad-Sidhoum, Emmanuel Rachelson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 11:28:08</h6>
<p class='card-text'>Mixed-Integer Linear Programming (MILP) lies at the core of many real-world combinatorial optimization (CO) problems, traditionally solved by branch-and-bound (B&B). A key driver influencing B&B solvers efficiency is the variable selection heuristic that guides branching decisions. Looking to move beyond static, hand-crafted heuristics, recent work has explored adapting traditional reinforcement learning (RL) algorithms to the B&B setting, aiming to learn branching strategies tailored to specific MILP distributions. In parallel, RL agents have achieved remarkable success in board games, a very specific type of combinatorial problems, by leveraging environment simulators to plan via Monte Carlo Tree Search (MCTS). Building on these developments, we introduce Plan-and-Branch-and-Bound (PlanB&B), a model-based reinforcement learning (MBRL) agent that leverages a learned internal model of the B&B dynamics to discover improved branching strategies. Computational experiments empirically validate our approach, with our MBRL branching agent outperforming previous state-of-the-art RL methods across four standard MILP benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09073v1' target='_blank'>Good-for-MDP State Reduction for Stochastic LTL Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Christoph Weinhuber, Giuseppe De Giacomo, Yong Li, Sven Schewe, Qiyi Tang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 07:48:09</h6>
<p class='card-text'>We study stochastic planning problems in Markov Decision Processes (MDPs) with goals specified in Linear Temporal Logic (LTL). The state-of-the-art approach transforms LTL formulas into good-for-MDP (GFM) automata, which feature a restricted form of nondeterminism. These automata are then composed with the MDP, allowing the agent to resolve the nondeterminism during policy synthesis. A major factor affecting the scalability of this approach is the size of the generated automata. In this paper, we propose a novel GFM state-space reduction technique that significantly reduces the number of automata states. Our method employs a sophisticated chain of transformations, leveraging recent advances in good-for-games minimisation developed for adversarial settings. In addition to our theoretical contributions, we present empirical results demonstrating the practical effectiveness of our state-reduction technique. Furthermore, we introduce a direct construction method for formulas of the form $\mathsf{G}\mathsf{F}\varphi$, where $\varphi$ is a co-safety formula. This construction is provably single-exponential in the worst case, in contrast to the general doubly-exponential complexity. Our experiments confirm the scalability advantages of this specialised construction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09052v1' target='_blank'>Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu Wang, Hui Wang, Jiake Ge, Xin Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 07:06:33</h6>
<p class='card-text'>Exact subgraph matching on large-scale graphs remains a challenging problem due to high computational complexity and distributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed environments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, memory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves "minimum edge cut + load balancing + non-interruptible queries" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='https://arxiv.org/abs/2511.09038v1' target='_blank'>Test Plan Generation for Live Testing of Cloud Services</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Oussama Jebbar, Ferhat Khendek, Maria Toeroe</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-11-12 06:41:59</h6>
<p class='card-text'>Live testing is performed in the production environment ideally without causing unacceptable disturbance to the production traffic. Thus, test activities have to be orchestrated properly to avoid interferences with the production traffic. A test plan is the road map that specifies how the test activities need to be orchestrated. Developing a test plan includes tasks such as test configuration selection/generation, test configuration deployment planning, creating the test runs schedule, choosing strategies to mitigate the risk of interferences, etc. The manual design of a test plan is tedious and error prone. This task becomes harder especially when the systems are large and complex. In this paper we propose an approach for automating test plans generation. With this approach we aim at reducing service disruption that may be induced by the testing activities in production. We illustrate our approach with a case study and discuss its different aspects.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>