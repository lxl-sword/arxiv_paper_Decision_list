<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>multi-agent - 2025-03-08</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>multi-agent - 2025-03-08</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04262v1' target='_blank'>Guidelines for Applying RL and MARL in Cybersecurity Applications</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vasilios Mavroudis, Gregory Palmer, Sara Farmer, Kez Smithson Whitehead, David Foster, Adam Price, Ian Miles, Alberto Caron, Stephen Pasteris</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 09:46:16</h6>
<p class='card-text'>Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)
have emerged as promising methodologies for addressing challenges in automated
cyber defence (ACD). These techniques offer adaptive decision-making
capabilities in high-dimensional, adversarial environments. This report
provides a structured set of guidelines for cybersecurity professionals and
researchers to assess the suitability of RL and MARL for specific use cases,
considering factors such as explainability, exploration needs, and the
complexity of multi-agent coordination. It also discusses key algorithmic
approaches, implementation challenges, and real-world constraints, such as data
scarcity and adversarial interference. The report further outlines open
research questions, including policy optimality, agent cooperation levels, and
the integration of MARL systems into operational cybersecurity frameworks. By
bridging theoretical advancements and practical deployment, these guidelines
aim to enhance the effectiveness of AI-driven cyber defence strategies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03889v1' target='_blank'>Pretrained LLMs as Real-Time Controllers for Robot Operated Serial
  Production Line</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhammad Waseem, Kshitij Bhatta, Chen Li, Qing Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 20:43:49</h6>
<p class='card-text'>The manufacturing industry is undergoing a transformative shift, driven by
cutting-edge technologies like 5G, AI, and cloud computing. Despite these
advancements, effective system control, which is crucial for optimizing
production efficiency, remains a complex challenge due to the intricate,
knowledge-dependent nature of manufacturing processes and the reliance on
domain-specific expertise. Conventional control methods often demand heavy
customization, considerable computational resources, and lack transparency in
decision-making. In this work, we investigate the feasibility of using Large
Language Models (LLMs), particularly GPT-4, as a straightforward, adaptable
solution for controlling manufacturing systems, specifically, mobile robot
scheduling. We introduce an LLM-based control framework to assign mobile robots
to different machines in robot assisted serial production lines, evaluating its
performance in terms of system throughput. Our proposed framework outperforms
traditional scheduling approaches such as First-Come-First-Served (FCFS),
Shortest Processing Time (SPT), and Longest Processing Time (LPT). While it
achieves performance that is on par with state-of-the-art methods like
Multi-Agent Reinforcement Learning (MARL), it offers a distinct advantage by
delivering comparable throughput without the need for extensive retraining.
These results suggest that the proposed LLM-based solution is well-suited for
scenarios where technical expertise, computational resources, and financial
investment are limited, while decision transparency and system scalability are
critical concerns.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03796v1' target='_blank'>Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent
  Reinforcement Learning in USV Swarm</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 14:33:18</h6>
<p class='card-text'>Multi-Agent Reinforcement Learning (MARL) has shown promise in solving
complex problems involving cooperation and competition among agents, such as an
Unmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance,
and vessel protection. However, aligning system behavior with user preferences
is challenging due to the difficulty of encoding expert intuition into reward
functions. To address the issue, we propose a Reinforcement Learning with Human
Feedback (RLHF) approach for MARL that resolves credit-assignment challenges
through an Agent-Level Feedback system categorizing feedback into intra-agent,
inter-agent, and intra-team types. To overcome the challenges of direct human
feedback, we employ a Large Language Model (LLM) evaluator to validate our
approach using feedback scenarios such as region constraints, collision
avoidance, and task allocation. Our method effectively refines USV swarm
policies, addressing key challenges in multi-agent systems while maintaining
fairness and performance consistency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02913v1' target='_blank'>Towards Robust Multi-UAV Collaboration: MARL with Noise-Resilient
  Communication and Attention Mechanisms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zilin Zhao, Chishui Chen, Haotian Shi, Jiale Chen, Xuanlin Yue, Zhejian Yang, Yang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-04 08:05:14</h6>
<p class='card-text'>Efficient path planning for unmanned aerial vehicles (UAVs) is crucial in
remote sensing and information collection. As task scales expand, the
cooperative deployment of multiple UAVs significantly improves information
collection efficiency. However, collaborative communication and decision-making
for multiple UAVs remain major challenges in path planning, especially in noisy
environments. To efficiently accomplish complex information collection tasks in
3D space and address robust communication issues, we propose a multi-agent
reinforcement learning (MARL) framework for UAV path planning based on the
Counterfactual Multi-Agent Policy Gradients (COMA) algorithm. The framework
incorporates attention mechanism-based UAV communication protocol and
training-deployment system, significantly improving communication robustness
and individual decision-making capabilities in noisy conditions. Experiments
conducted on both synthetic and real-world datasets demonstrate that our method
outperforms existing algorithms in terms of path planning efficiency and
robustness, especially in noisy environments, achieving a 78\% improvement in
entropy reduction.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.02077v1' target='_blank'>$\text{M}^3\text{HF}$: Multi-agent Reinforcement Learning from
  Multi-phase Human Feedback of Mixed Quality</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyan Wang, Zhicheng Zhang, Fei Fang, Yali Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 21:58:10</h6>
<p class='card-text'>Designing effective reward functions in multi-agent reinforcement learning
(MARL) is a significant challenge, often leading to suboptimal or misaligned
behaviors in complex, coordinated environments. We introduce Multi-agent
Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality
($\text{M}^3\text{HF}$), a novel framework that integrates multi-phase human
feedback of mixed quality into the MARL training process. By involving humans
with diverse expertise levels to provide iterative guidance,
$\text{M}^3\text{HF}$ leverages both expert and non-expert feedback to
continuously refine agents' policies. During training, we strategically pause
agent learning for human evaluation, parse feedback using large language models
to assign it appropriately and update reward functions through predefined
templates and adaptive weight by using weight decay and performance-based
adjustments. Our approach enables the integration of nuanced human insights
across various levels of quality, enhancing the interpretability and robustness
of multi-agent cooperation. Empirical results in challenging environments
demonstrate that $\text{M}^3\text{HF}$ significantly outperforms
state-of-the-art methods, effectively addressing the complexities of reward
design in MARL and enabling broader human participation in the training
process.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01458v1' target='_blank'>SrSv: Integrating Sequential Rollouts with Sequential Value Estimation
  for Multi-agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xu Wan, Chao Yang, Cheng Yang, Jie Song, Mingyang Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 12:17:18</h6>
<p class='card-text'>Although multi-agent reinforcement learning (MARL) has shown its success
across diverse domains, extending its application to large-scale real-world
systems still faces significant challenges. Primarily, the high complexity of
real-world environments exacerbates the credit assignment problem,
substantially reducing training efficiency. Moreover, the variability of agent
populations in large-scale scenarios necessitates scalable decision-making
mechanisms. To address these challenges, we propose a novel framework:
Sequential rollout with Sequential value estimation (SrSv). This framework aims
to capture agent interdependence and provide a scalable solution for
cooperative MARL. Specifically, SrSv leverages the autoregressive property of
the Transformer model to handle varying populations through sequential action
rollout. Furthermore, to capture the interdependence of policy distributions
and value functions among multiple agents, we introduce an innovative
sequential value estimation methodology and integrates the value approximation
into an attention-based sequential model. We evaluate SrSv on three benchmarks:
Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, and DubinsCars.
Experimental results demonstrate that SrSv significantly outperforms baseline
methods in terms of training efficiency without compromising convergence
performance. Moreover, when implemented in a large-scale DubinsCar system with
1,024 agents, our framework surpasses existing benchmarks, highlighting the
excellent scalability of SrSv.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01168v1' target='_blank'>Relativistic BGK model of Marle for polyatomic gases near equilibrium</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Byung-Hoon Hwang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-03 04:32:06</h6>
<p class='card-text'>In this paper, we consider the direct application of the relativistic
extended thermodynamics theory of polyatomic gases developed in [Ann. Phys. 377
(2017) 414--445] to the relativistic BGK model proposed by Marle. We present
the perturbed Marle model around the generalized J\"{u}ttner distribution and
investigate the properties of the linear operator. Then we prove the global
existence and large-time behavior of classical solutions when the initial data
is sufficiently close to a global equilibrium.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.01017v1' target='_blank'>Real-World Deployment and Assessment of a Multi-Agent Reinforcement
  Learning-Based Variable Speed Limit Control System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhang Zhang, Zhiyao Zhang, Junyi Ji, Marcos Quiñones-Grueiro, William Barbour, Derek Gloudemans, Gergely Zachár, Clay Weston, Gautam Biswas, Daniel B. Work</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 21:09:16</h6>
<p class='card-text'>This article presents the first field deployment of a multi-agent
reinforcement learning (MARL) based variable speed limit (VSL) control system
on Interstate 24 (I-24) near Nashville, Tennessee. We design and demonstrate a
full pipeline from training MARL agents in a traffic simulator to a field
deployment on a 17-mile segment of I-24 encompassing 67 VSL controllers. The
system was launched on March 8th, 2024, and has made approximately 35 million
decisions on 28 million trips in six months of operation. We apply an invalid
action masking mechanism and several safety guards to ensure real-world
constraints. The MARL-based implementation operates up to 98% of the time, with
the safety guards overriding the MARL decisions for the remaining time. We
evaluate the performance of the MARL-based algorithm in comparison to a
previously deployed non-RL VSL benchmark algorithm on I-24. Results show that
the MARL-based VSL control system achieves a superior performance. The accuracy
of correctly warning drivers about slowing traffic ahead is improved by 14% and
the response delay to non-recurrent congestion is reduced by 75%. The
preliminary data shows that the VSL control system has reduced the crash rate
by 26% and the secondary crash rate by 50%. We open-sourced the deployed
MARL-based VSL algorithm at https://github.com/Lab-Work/marl-vsl-controller.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00684v1' target='_blank'>Factorized Deep Q-Network for Cooperative Multi-Agent Reinforcement
  Learning in Victim Tagging</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maria Ana Cardei, Afsaneh Doryab</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-02 01:32:09</h6>
<p class='card-text'>Mass casualty incidents (MCIs) are a growing concern, characterized by
complexity and uncertainty that demand adaptive decision-making strategies. The
victim tagging step in the emergency medical response must be completed quickly
and is crucial for providing information to guide subsequent time-constrained
response actions. In this paper, we present a mathematical formulation of
multi-agent victim tagging to minimize the time it takes for responders to tag
all victims. Five distributed heuristics are formulated and evaluated with
simulation experiments. The heuristics considered are on-the go, practical
solutions that represent varying levels of situational uncertainty in the form
of global or local communication capabilities, showcasing practical
constraints. We further investigate the performance of a multi-agent
reinforcement learning (MARL) strategy, factorized deep Q-network (FDQN), to
minimize victim tagging time as compared to baseline heuristics. Extensive
simulations demonstrate that between the heuristics, methods with local
communication are more efficient for adaptive victim tagging, specifically
choosing the nearest victim with the option to replan. Analyzing all
experiments, we find that our FDQN approach outperforms heuristics in
smaller-scale scenarios, while heuristics excel in more complex scenarios. Our
experiments contain diverse complexities that explore the upper limits of MARL
capabilities for real-world applications and reveal key insights.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.00372v1' target='_blank'>Nucleolus Credit Assignment for Effective Coalitions in Multi-agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yugu Li, Zehong Cao, Jianglin Qiao, Siyi Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-01 07:01:58</h6>
<p class='card-text'>In cooperative multi-agent reinforcement learning (MARL), agents typically
form a single grand coalition based on credit assignment to tackle a composite
task, often resulting in suboptimal performance. This paper proposed a
nucleolus-based credit assignment grounded in cooperative game theory, enabling
the autonomous partitioning of agents into multiple small coalitions that can
effectively identify and complete subtasks within a larger composite task.
Specifically, our designed nucleolus Q-learning could assign fair credits to
each agent, and the nucleolus Q-operator provides theoretical guarantees with
interpretability for both learning convergence and the stability of the formed
small coalitions. Through experiments on Predator-Prey and StarCraft scenarios
across varying difficulty levels, our approach demonstrated the emergence of
multiple effective coalitions during MARL training, leading to faster learning
and superior performance in terms of win rate and cumulative rewards especially
in hard and super-hard environments, compared to four baseline methods. Our
nucleolus-based credit assignment showed the promise for complex composite
tasks requiring effective subteams of agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20217v1' target='_blank'>MARVEL: Multi-Agent Reinforcement Learning for constrained field-of-View
  multi-robot Exploration in Large-scale environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jimmy Chiun, Shizhe Zhang, Yizhuo Wang, Yuhong Cao, Guillaume Sartoretti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 15:58:42</h6>
<p class='card-text'>In multi-robot exploration, a team of mobile robot is tasked with efficiently
mapping an unknown environments. While most exploration planners assume
omnidirectional sensors like LiDAR, this is impractical for small robots such
as drones, where lightweight, directional sensors like cameras may be the only
option due to payload constraints. These sensors have a constrained
field-of-view (FoV), which adds complexity to the exploration problem,
requiring not only optimal robot positioning but also sensor orientation during
movement. In this work, we propose MARVEL, a neural framework that leverages
graph attention networks, together with novel frontiers and orientation
features fusion technique, to develop a collaborative, decentralized policy
using multi-agent reinforcement learning (MARL) for robots with constrained
FoV. To handle the large action space of viewpoints planning, we further
introduce a novel information-driven action pruning strategy. MARVEL improves
multi-robot coordination and decision-making in challenging large-scale indoor
environments, while adapting to various team sizes and sensor configurations
(i.e., FoV and sensor range) without additional training. Our extensive
evaluation shows that MARVEL's learned policies exhibit effective coordinated
behaviors, outperforming state-of-the-art exploration planners across multiple
metrics. We experimentally demonstrate MARVEL's generalizability in large-scale
environments, of up to 90m by 90m, and validate its practical applicability
through successful deployment on a team of real drone hardware.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20065v1' target='_blank'>RouteRL: Multi-agent reinforcement learning framework for urban route
  choice with autonomous vehicles</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ahmet Onur Akman, Anastasia Psarou, Łukasz Gorczyca, Zoltán György Varga, Grzegorz Jamróz, Rafał Kucharski</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 13:13:09</h6>
<p class='card-text'>RouteRL is a novel framework that integrates multi-agent reinforcement
learning (MARL) with a microscopic traffic simulation, facilitating the testing
and development of efficient route choice strategies for autonomous vehicles
(AVs). The proposed framework simulates the daily route choices of driver
agents in a city, including two types: human drivers, emulated using behavioral
route choice models, and AVs, modeled as MARL agents optimizing their policies
for a predefined objective. RouteRL aims to advance research in MARL, transport
modeling, and human-AI interaction for transportation applications. This study
presents a technical report on RouteRL, outlines its potential research
contributions, and showcases its impact via illustrative examples.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19717v1' target='_blank'>Exponential Topology-enabled Scalable Communication in Multi-agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinran Li, Xiaolu Wang, Chenjia Bai, Jun Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 03:15:31</h6>
<p class='card-text'>In cooperative multi-agent reinforcement learning (MARL), well-designed
communication protocols can effectively facilitate consensus among agents,
thereby enhancing task performance. Moreover, in large-scale multi-agent
systems commonly found in real-world applications, effective communication
plays an even more critical role due to the escalated challenge of partial
observability compared to smaller-scale setups. In this work, we endeavor to
develop a scalable communication protocol for MARL. Unlike previous methods
that focus on selecting optimal pairwise communication links-a task that
becomes increasingly complex as the number of agents grows-we adopt a global
perspective on communication topology design. Specifically, we propose
utilizing the exponential topology to enable rapid information dissemination
among agents by leveraging its small-diameter and small-size properties. This
approach leads to a scalable communication protocol, named ExpoComm. To fully
unlock the potential of exponential graphs as communication topologies, we
employ memory-based message processors and auxiliary tasks to ground messages,
ensuring that they reflect global information and benefit decision-making.
Extensive experiments on large-scale cooperative benchmarks, including MAgent
and Infrastructure Management Planning, demonstrate the superior performance
and robust zero-shot transferability of ExpoComm compared to existing
communication strategies. The code is publicly available at
https://github.com/LXXXXR/ExpoComm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19675v1' target='_blank'>Joint Power Allocation and Phase Shift Design for Stacked Intelligent
  Metasurfaces-aided Cell-Free Massive MIMO Systems with MARL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiyang Zhu, Jiayi Zhang, Enyu Shi, Ziheng Liu, Chau Yuen, Bo Ai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 01:34:34</h6>
<p class='card-text'>Cell-free (CF) massive multiple-input multiple-output (mMIMO) systems offer
high spectral efficiency (SE) through multiple distributed access points (APs).
However, the large number of antennas increases power consumption. We propose
incorporating stacked intelligent metasurfaces (SIM) into CF mMIMO systems as a
cost-effective, energy-efficient solution. This paper focuses on optimizing the
joint power allocation of APs and the phase shift of SIMs to maximize the sum
SE. To address this complex problem, we introduce a fully distributed
multi-agent reinforcement learning (MARL) algorithm. Our novel algorithm, the
noisy value method with a recurrent policy in multi-agent policy optimization
(NVR-MAPPO), enhances performance by encouraging diverse exploration under
centralized training and decentralized execution. Simulations demonstrate that
NVR-MAPPO significantly improves sum SE and robustness across various
scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.19297v1' target='_blank'>Combining Planning and Reinforcement Learning for Solving Relational
  Multiagent Domains</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nikhilesh Prabhakar, Ranveer Singh, Harsha Kokel, Sriraam Natarajan, Prasad Tadepalli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-26 16:55:23</h6>
<p class='card-text'>Multiagent Reinforcement Learning (MARL) poses significant challenges due to
the exponential growth of state and action spaces and the non-stationary nature
of multiagent environments. This results in notable sample inefficiency and
hinders generalization across diverse tasks. The complexity is further
pronounced in relational settings, where domain knowledge is crucial but often
underutilized by existing MARL algorithms. To overcome these hurdles, we
propose integrating relational planners as centralized controllers with
efficient state abstractions and reinforcement learning. This approach proves
to be sample-efficient and facilitates effective task transfer and
generalization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16608v1' target='_blank'>Toward Dependency Dynamics in Multi-Agent Reinforcement Learning for
  Traffic Signal Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuli Zhang, Shangbo Wang, Dongyao Jia, Pengfei Fan, Ruiyuan Jiang, Hankang Gu, Andy H. F. Chow</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-23 15:29:12</h6>
<p class='card-text'>Reinforcement learning (RL) emerges as a promising data-driven approach for
adaptive traffic signal control (ATSC) in complex urban traffic networks, with
deep neural networks substantially augmenting its learning capabilities.
However, centralized RL becomes impractical for ATSC involving multiple agents
due to the exceedingly high dimensionality of the joint action space.
Multi-agent RL (MARL) mitigates this scalability issue by decentralizing
control to local RL agents. Nevertheless, this decentralized method introduces
new challenges: the environment becomes partially observable from the
perspective of each local agent due to constrained inter-agent communication.
Both centralized RL and MARL exhibit distinct strengths and weaknesses,
particularly under heavy intersectional traffic conditions. In this paper, we
justify that MARL can achieve the optimal global Q-value by separating into
multiple IRL (Independent Reinforcement Learning) processes when no spill-back
congestion occurs (no agent dependency) among agents (intersections). In the
presence of spill-back congestion (with agent dependency), the maximum global
Q-value can be achieved by using centralized RL. Building upon the conclusions,
we propose a novel Dynamic Parameter Update Strategy for Deep Q-Network
(DQN-DPUS), which updates the weights and bias based on the dependency dynamics
among agents, i.e. updating only the diagonal sub-matrices for the scenario
without spill-back congestion. We validate the DQN-DPUS in a simple network
with two intersections under varying traffic, and show that the proposed
strategy can speed up the convergence rate without sacrificing optimal
exploration. The results corroborate our theoretical findings, demonstrating
the efficacy of DQN-DPUS in optimizing traffic signal control.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.16496v1' target='_blank'>PMAT: Optimizing Action Generation Order in Multi-Agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kun Hu, Muning Wen, Xihuai Wang, Shao Zhang, Yiwei Shi, Minne Li, Minglong Li, Ying Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-23 08:30:14</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) faces challenges in coordinating
agents due to complex interdependencies within multi-agent systems. Most MARL
algorithms use the simultaneous decision-making paradigm but ignore the
action-level dependencies among agents, which reduces coordination efficiency.
In contrast, the sequential decision-making paradigm provides finer-grained
supervision for agent decision order, presenting the potential for handling
dependencies via better decision order management. However, determining the
optimal decision order remains a challenge. In this paper, we introduce Action
Generation with Plackett-Luce Sampling (AGPS), a novel mechanism for agent
decision order optimization. We model the order determination task as a
Plackett-Luce sampling process to address issues such as ranking instability
and vanishing gradient during the network training process. AGPS realizes
credit-based decision order determination by establishing a bridge between the
significance of agents' local observations and their decision credits, thus
facilitating order optimization and dependency management. Integrating AGPS
with the Multi-Agent Transformer, we propose the Prioritized Multi-Agent
Transformer (PMAT), a sequential decision-making MARL algorithm with decision
order optimization. Experiments on benchmarks including StarCraft II
Multi-Agent Challenge, Google Research Football, and Multi-Agent MuJoCo show
that PMAT outperforms state-of-the-art algorithms, greatly enhancing
coordination efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.14606v1' target='_blank'>Curiosity Driven Multi-agent Reinforcement Learning for 3D Game Testing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Raihana Ferdous, Fitsum Kifetew, Davide Prandi, Angelo Susi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-20 14:43:46</h6>
<p class='card-text'>Recently testing of games via autonomous agents has shown great promise in
tackling challenges faced by the game industry, which mainly relied on either
manual testing or record/replay. In particular Reinforcement Learning (RL)
solutions have shown potential by learning directly from playing the game
without the need for human intervention. In this paper, we present cMarlTest,
an approach for testing 3D games through curiosity driven Multi-Agent
Reinforcement Learning (MARL). cMarlTest deploys multiple agents that work
collaboratively to achieve the testing objective. The use of multiple agents
helps resolve issues faced by a single agent approach. We carried out
experiments on different levels of a 3D game comparing the performance of
cMarlTest with a single agent RL variant. Results are promising where,
considering three different types of coverage criteria, cMarlTest achieved
higher coverage. cMarlTest was also more efficient in terms of the time taken,
with respect to the single agent based variant.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.13188v1' target='_blank'>Autonomous Vehicles Using Multi-Agent Reinforcement Learning for Routing
  Decisions Can Harm Urban Traffic</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Anastasia Psarou, Ahmet Onur Akman, Łukasz Gorczyca, Michał Hoffmann, Zoltán György Varga, Grzegorz Jamróz, Rafał Kucharski</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-18 13:37:02</h6>
<p class='card-text'>Autonomous vehicles (AVs) using Multi-Agent Reinforcement Learning (MARL) for
simultaneous route optimization may destabilize traffic environments, with
human drivers possibly experiencing longer travel times. We study this
interaction by simulating human drivers and AVs. Our experiments with standard
MARL algorithms reveal that, even in trivial cases, policies often fail to
converge to an optimal solution or require long training periods. The problem
is amplified by the fact that we cannot rely entirely on simulated training, as
there are no accurate models of human routing behavior. At the same time,
real-world training in cities risks destabilizing urban traffic systems,
increasing externalities, such as $CO_2$ emissions, and introducing
non-stationarity as human drivers adapt unpredictably to AV behaviors.
Centralization can improve convergence in some cases, however, it raises
privacy concerns for the travelers' destination data. In this position paper,
we argue that future research must prioritize realistic benchmarks, cautious
deployment strategies, and tools for monitoring and regulating AV routing
behaviors to ensure sustainable and equitable urban mobility systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11882v3' target='_blank'>Leveraging Dual Process Theory in Language Agent Framework for Real-time
  Simultaneous Human-AI Collaboration</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-17 15:09:45</h6>
<p class='card-text'>Agents built on large language models (LLMs) have excelled in turn-by-turn
human-AI collaboration but struggle with simultaneous tasks requiring real-time
interaction. Latency issues and the challenge of inferring variable human
strategies hinder their ability to make autonomous decisions without explicit
instructions. Through experiments with current independent System 1 and System
2 methods, we validate the necessity of using Dual Process Theory (DPT) in
real-time tasks. We propose DPT-Agent, a novel language agent framework that
integrates System 1 and System 2 for efficient real-time simultaneous human-AI
collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and
code-as-policy for fast, intuitive, and controllable decision-making.
DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous
reflection to infer human intentions and perform reasoning-based autonomous
decisions. We demonstrate the effectiveness of DPT-Agent through further
experiments with rule-based agents and human collaborators, showing significant
improvements over mainstream LLM-based frameworks. DPT-Agent can effectively
help LLMs convert correct slow thinking and reasoning into executable actions,
thereby improving performance. To the best of our knowledge, DPT-Agent is the
first language agent framework that achieves successful real-time simultaneous
human-AI collaboration autonomously. Code of DPT-Agent can be found in
https://github.com/sjtu-marl/DPT-Agent.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.11260v1' target='_blank'>Scalable Multi-Agent Offline Reinforcement Learning and the Role of
  Information</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Riccardo Zamboni, Enrico Brunetti, Marcello Restelli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-16 20:28:42</h6>
<p class='card-text'>Offline Reinforcement Learning (RL) focuses on learning policies solely from
a batch of previously collected data. offering the potential to leverage such
datasets effectively without the need for costly or risky active exploration.
While recent advances in Offline Multi-Agent RL (MARL) have shown promise, most
existing methods either rely on large datasets jointly collected by all agents
or agent-specific datasets collected independently. The former approach ensures
strong performance but raises scalability concerns, while the latter emphasizes
scalability at the expense of performance guarantees. In this work, we propose
a novel scalable routine for both dataset collection and offline learning.
Agents first collect diverse datasets coherently with a pre-specified
information-sharing network and subsequently learn coherent localized policies
without requiring either full observability or falling back to complete
decentralization. We theoretically demonstrate that this structured approach
allows a multi-agent extension of the seminal Fitted Q-Iteration (FQI)
algorithm to globally converge, in high probability, to near-optimal policies.
The convergence is subject to error terms that depend on the informativeness of
the shared information. Furthermore, we show how this approach allows to bound
the inherent error of the supervised-learning phase of FQI with the mutual
information between shared and unshared information. Our algorithm, SCAlable
Multi-agent FQI (SCAM-FQI), is then evaluated on a distributed decision-making
problem. The empirical results align with our theoretical findings, supporting
the effectiveness of SCAM-FQI in achieving a balance between scalability and
policy performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.10148v1' target='_blank'>Cooperative Multi-Agent Planning with Adaptive Skill Synthesis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiyuan Li, Wenshuai Zhao, Joni Pajarinen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-14 13:23:18</h6>
<p class='card-text'>Despite much progress in training distributed artificial intelligence (AI),
building cooperative multi-agent systems with multi-agent reinforcement
learning (MARL) faces challenges in sample efficiency, interpretability, and
transferability. Unlike traditional learning-based methods that require
extensive interaction with the environment, large language models (LLMs)
demonstrate remarkable capabilities in zero-shot planning and complex
reasoning. However, existing LLM-based approaches heavily rely on text-based
observations and struggle with the non-Markovian nature of multi-agent
interactions under partial observability. We present COMPASS, a novel
multi-agent architecture that integrates vision-language models (VLMs) with a
dynamic skill library and structured communication for decentralized
closed-loop decision-making. The skill library, bootstrapped from
demonstrations, evolves via planner-guided tasks to enable adaptive strategies.
COMPASS propagates entity information through multi-hop communication under
partial observability. Evaluations on the improved StarCraft Multi-Agent
Challenge (SMACv2) demonstrate COMPASS achieves up to 30\% higher win rates
than state-of-the-art MARL algorithms in symmetric scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.09780v1' target='_blank'>Incentivize without Bonus: Provably Efficient Model-based Online
  Multi-agent RL for Markov Games</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tong Yang, Bo Dai, Lin Xiao, Yuejie Chi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-13 21:28:51</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) lies at the heart of a plethora of
applications involving the interaction of a group of agents in a shared unknown
environment. A prominent framework for studying MARL is Markov games, with the
goal of finding various notions of equilibria in a sample-efficient manner,
such as the Nash equilibrium (NE) and the coarse correlated equilibrium (CCE).
However, existing sample-efficient approaches either require tailored
uncertainty estimation under function approximation, or careful coordination of
the players. In this paper, we propose a novel model-based algorithm, called
VMG, that incentivizes exploration via biasing the empirical estimate of the
model parameters towards those with a higher collective best-response values of
all the players when fixing the other players' policies, thus encouraging the
policy to deviate from its current equilibrium for more exploration. VMG is
oblivious to different forms of function approximation, and permits
simultaneous and uncoupled policy updates of all players. Theoretically, we
also establish that VMG achieves a near-optimal regret for finding both the NEs
of two-player zero-sum Markov games and CCEs of multi-player general-sum Markov
games under linear function approximation in an online environment, which
nearly match their counterparts with sophisticated uncertainty quantification.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.08985v1' target='_blank'>Few is More: Task-Efficient Skill-Discovery for Multi-Task Offline
  Multi-Agent Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xun Wang, Zhuoran Li, Hai Zhong, Longbo Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-13 05:47:57</h6>
<p class='card-text'>As a data-driven approach, offline MARL learns superior policies solely from
offline datasets, ideal for domains rich in historical data but with high
interaction costs and risks. However, most existing methods are task-specific,
requiring retraining for new tasks, leading to redundancy and inefficiency. To
address this issue, in this paper, we propose a task-efficient multi-task
offline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL).
Unlike existing offline skill-discovery methods, SD-CQL discovers skills by
reconstructing the next observation. It then evaluates fixed and variable
actions separately and employs behavior-regularized conservative Q-learning to
execute the optimal action for each skill. This approach eliminates the need
for local-global alignment and enables strong multi-task generalization from
limited small-scale source tasks. Substantial experiments on StarCraftII
demonstrates the superior generalization performance and task-efficiency of
SD-CQL. It achieves the best performance on $\textbf{10}$ out of $14$ task
sets, with up to $\textbf{65%}$ improvement on individual task sets, and is
within $4\%$ of the best baseline on the remaining four.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.07635v1' target='_blank'>Distributed Value Decomposition Networks with Networked Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guilherme S. Varela, Alberto Sardinha, Francisco S. Melo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-11 15:23:05</h6>
<p class='card-text'>We investigate the problem of distributed training under partial
observability, whereby cooperative multi-agent reinforcement learning agents
(MARL) maximize the expected cumulative joint reward. We propose distributed
value decomposition networks (DVDN) that generate a joint Q-function that
factorizes into agent-wise Q-functions. Whereas the original value
decomposition networks rely on centralized training, our approach is suitable
for domains where centralized training is not possible and agents must learn by
interacting with the physical environment in a decentralized manner while
communicating with their peers. DVDN overcomes the need for centralized
training by locally estimating the shared objective. We contribute with two
innovative algorithms, DVDN and DVDN (GT), for the heterogeneous and
homogeneous agents settings respectively. Empirically, both algorithms
approximate the performance of value decomposition networks, in spite of the
information loss during communication, as demonstrated in ten MARL tasks in
three standard environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06976v1' target='_blank'>Who is Helping Whom? Analyzing Inter-dependencies to Evaluate
  Cooperation in Human-AI Teaming</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Upasana Biswas, Siddhant Bhambri, Subbarao Kambhampati</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 19:16:20</h6>
<p class='card-text'>The long-standing research challenges of Human-AI Teaming(HAT) and Zero-shot
Cooperation(ZSC) have been tackled by applying multi-agent reinforcement
learning(MARL) to train an agent by optimizing the environment reward function
and evaluating their performance through task performance metrics such as task
reward. However, such evaluation focuses only on task completion, while being
agnostic to `how' the two agents work with each other. Specifically, we are
interested in understanding the cooperation arising within the team when
trained agents are paired with humans. To formally address this problem, we
propose the concept of interdependence to measure how much agents rely on each
other's actions to achieve the shared goal, as a key metric for evaluating
cooperation in human-agent teams. Towards this, we ground this concept through
a symbolic formalism and define evaluation metrics that allow us to assess the
degree of reliance between the agents' actions. We pair state-of-the-art agents
trained through MARL for HAT, with learned human models for the the popular
Overcooked domain, and evaluate the team performance for these human-agent
teams. Our results demonstrate that trained agents are not able to induce
cooperative behavior, reporting very low levels of interdependence across all
the teams. We also report that teaming performance of a team is not necessarily
correlated with the task reward.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06113v1' target='_blank'>Towards Bio-inspired Heuristically Accelerated Reinforcement Learning
  for Adaptive Underwater Multi-Agents Behaviour</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Antoine Vivien, Thomas Chaffre, Matthew Stephenson, Eva Artusi, Paulo Santos, Benoit Clement, Karl Sammut</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 02:47:33</h6>
<p class='card-text'>This paper describes the problem of coordination of an autonomous Multi-Agent
System which aims to solve the coverage planning problem in a complex
environment. The considered applications are the detection and identification
of objects of interest while covering an area. These tasks, which are highly
relevant for space applications, are also of interest among various domains
including the underwater context, which is the focus of this study. In this
context, coverage planning is traditionally modelled as a Markov Decision
Process where a coordinated MAS, a swarm of heterogeneous autonomous underwater
vehicles, is required to survey an area and search for objects. This MDP is
associated with several challenges: environment uncertainties, communication
constraints, and an ensemble of hazards, including time-varying and
unpredictable changes in the underwater environment. MARL algorithms can solve
highly non-linear problems using deep neural networks and display great
scalability against an increased number of agents. Nevertheless, most of the
current results in the underwater domain are limited to simulation due to the
high learning time of MARL algorithms. For this reason, a novel strategy is
introduced to accelerate this convergence rate by incorporating biologically
inspired heuristics to guide the policy during training. The PSO method, which
is inspired by the behaviour of a group of animals, is selected as a heuristic.
It allows the policy to explore the highest quality regions of the action and
state spaces, from the beginning of the training, optimizing the
exploration/exploitation trade-off. The resulting agent requires fewer
interactions to reach optimal performance. The method is applied to the MSAC
algorithm and evaluated for a 2D covering area mission in a continuous control
environment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05812v1' target='_blank'>Multi-Agent Reinforcement Learning in Wireless Distributed Networks for
  6G</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiayi Zhang, Ziheng Liu, Yiyang Zhu, Enyu Shi, Bokai Xu, Chau Yuen, Dusit Niyato, Mérouane Debbah, Shi Jin, Bo Ai, Xuemin, Shen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-09 08:35:09</h6>
<p class='card-text'>The introduction of intelligent interconnectivity between the physical and
human worlds has attracted great attention for future sixth-generation (6G)
networks, emphasizing massive capacity, ultra-low latency, and unparalleled
reliability. Wireless distributed networks and multi-agent reinforcement
learning (MARL), both of which have evolved from centralized paradigms, are two
promising solutions for the great attention. Given their distinct capabilities,
such as decentralization and collaborative mechanisms, integrating these two
paradigms holds great promise for unleashing the full power of 6G, attracting
significant research and development attention. This paper provides a
comprehensive study on MARL-assisted wireless distributed networks for 6G. In
particular, we introduce the basic mathematical background and evolution of
wireless distributed networks and MARL, as well as demonstrate their
interrelationships. Subsequently, we analyze different structures of wireless
distributed networks from the perspectives of homogeneous and heterogeneous.
Furthermore, we introduce the basic concepts of MARL and discuss two typical
categories, including model-based and model-free. We then present critical
challenges faced by MARL-assisted wireless distributed networks, providing
important guidance and insights for actual implementation. We also explore an
interplay between MARL-assisted wireless distributed networks and emerging
techniques, such as information bottleneck and mirror learning, delivering
in-depth analyses and application scenarios. Finally, we outline several
compelling research directions for future MARL-assisted wireless distributed
networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05573v1' target='_blank'>Low-Rank Agent-Specific Adaptation (LoRASA) for Multi-Agent Policy
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Beining Zhang, Aditya Kapoor, Mingfei Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 13:57:53</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) often relies on \emph{parameter
sharing (PS)} to scale efficiently. However, purely shared policies can stifle
each agent's unique specialization, reducing overall performance in
heterogeneous environments. We propose \textbf{Low-Rank Agent-Specific
Adaptation (LoRASA)}, a novel approach that treats each agent's policy as a
specialized ``task'' fine-tuned from a shared backbone. Drawing inspiration
from parameter-efficient transfer methods, LoRASA appends small, low-rank
adaptation matrices to each layer of the shared policy, naturally inducing
\emph{parameter-space sparsity} that promotes both specialization and
scalability. We evaluate LoRASA on challenging benchmarks including the
StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent MuJoCo (MAMuJoCo),
implementing it atop widely used algorithms such as MAPPO and A2PO. Across
diverse tasks, LoRASA matches or outperforms existing baselines \emph{while
reducing memory and computational overhead}. Ablation studies on adapter rank,
placement, and timing validate the method's flexibility and efficiency. Our
results suggest LoRASA's potential to establish a new norm for MARL policy
parameterization: combining a shared foundation for coordination with low-rank
agent-specific refinements for individual specialization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05453v1' target='_blank'>LLM-Powered Decentralized Generative Agents with Adaptive Hierarchical
  Knowledge Graph for Cooperative Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hanqing Yang, Jingdi Chen, Marie Siew, Tania Lorido-Botran, Carlee Joe-Wong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 05:26:02</h6>
<p class='card-text'>Developing intelligent agents for long-term cooperation in dynamic open-world
scenarios is a major challenge in multi-agent systems. Traditional Multi-agent
Reinforcement Learning (MARL) frameworks like centralized training
decentralized execution (CTDE) struggle with scalability and flexibility. They
require centralized long-term planning, which is difficult without custom
reward functions, and face challenges in processing multi-modal data. CTDE
approaches also assume fixed cooperation strategies, making them impractical in
dynamic environments where agents need to adapt and plan independently. To
address decentralized multi-agent cooperation, we propose Decentralized
Adaptive Knowledge Graph Memory and Structured Communication System (DAMCS) in
a novel Multi-agent Crafter environment. Our generative agents, powered by
Large Language Models (LLMs), are more scalable than traditional MARL agents by
leveraging external knowledge and language for long-term planning and
reasoning. Instead of fully sharing information from all past experiences,
DAMCS introduces a multi-modal memory system organized as a hierarchical
knowledge graph and a structured communication protocol to optimize agent
cooperation. This allows agents to reason from past interactions and share
relevant information efficiently. Experiments on novel multi-agent open-world
tasks show that DAMCS outperforms both MARL and LLM baselines in task
efficiency and collaboration. Compared to single-agent scenarios, the two-agent
scenario achieves the same goal with 63% fewer steps, and the six-agent
scenario with 74% fewer steps, highlighting the importance of adaptive memory
and structured communication in achieving long-term goals. We publicly release
our project at: https://happyeureka.github.io/damcs.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>