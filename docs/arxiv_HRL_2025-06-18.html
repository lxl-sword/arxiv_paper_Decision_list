<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>HRL - 2025-06-18</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>HRL - 2025-06-18</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.14391v1' target='_blank'>HiLight: A Hierarchical Reinforcement Learning Framework with Global
  Adversarial Guidance for Large-Scale Traffic Signal Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yaqiao Zhu, Hongkai Wen, Geyong Min, Man Luo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-17 10:39:42</h6>
<p class='card-text'>Efficient traffic signal control (TSC) is essential for mitigating urban
congestion, yet existing reinforcement learning (RL) methods face challenges in
scaling to large networks while maintaining global coordination. Centralized RL
suffers from scalability issues, while decentralized approaches often lack
unified objectives, resulting in limited network-level efficiency. In this
paper, we propose HiLight, a hierarchical reinforcement learning framework with
global adversarial guidance for large-scale TSC. HiLight consists of a
high-level Meta-Policy, which partitions the traffic network into subregions
and generates sub-goals using a Transformer-LSTM architecture, and a low-level
Sub-Policy, which controls individual intersections with global awareness. To
improve the alignment between global planning and local execution, we introduce
an adversarial training mechanism, where the Meta-Policy generates challenging
yet informative sub-goals, and the Sub-Policy learns to surpass these targets,
leading to more effective coordination. We evaluate HiLight across both
synthetic and real-world benchmarks, and additionally construct a large-scale
Manhattan network with diverse traffic conditions, including peak transitions,
adverse weather, and holiday surges. Experimental results show that HiLight
exhibits significant advantages in large-scale scenarios and remains
competitive across standard benchmarks of varying sizes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.14045v1' target='_blank'>Discovering Temporal Structure: An Overview of Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Martin Klissarov, Akhil Bagaria, Ziyan Luo, George Konidaris, Doina Precup, Marlos C. Machado</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-16 22:36:32</h6>
<p class='card-text'>Developing agents capable of exploring, planning and learning in complex
open-ended environments is a grand challenge in artificial intelligence (AI).
Hierarchical reinforcement learning (HRL) offers a promising solution to this
challenge by discovering and exploiting the temporal structure within a stream
of experience. The strong appeal of the HRL framework has led to a rich and
diverse body of literature attempting to discover a useful structure. However,
it is still not clear how one might define what constitutes good structure in
the first place, or the kind of problems in which identifying it may be
helpful. This work aims to identify the benefits of HRL from the perspective of
the fundamental challenges in decision-making, as well as highlight its impact
on the performance trade-offs of AI agents. Through these benefits, we then
cover the families of methods that discover temporal structure in HRL, ranging
from learning directly from online experience to offline datasets, to
leveraging large language models (LLMs). Finally, we highlight the challenges
of temporal structure discovery and the domains that are particularly
well-suited for such endeavours.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.09366v1' target='_blank'>SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation
  via Skill Blending</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxuan Kuang, Haoran Geng, Amine Elhafsi, Tan-Dzung Do, Pieter Abbeel, Jitendra Malik, Marco Pavone, Yue Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-11 03:24:26</h6>
<p class='card-text'>Humanoid robots hold significant potential in accomplishing daily tasks
across diverse environments thanks to their flexibility and human-like
morphology. Recent works have made significant progress in humanoid whole-body
control and loco-manipulation leveraging optimal control or reinforcement
learning. However, these methods require tedious task-specific tuning for each
task to achieve satisfactory behaviors, limiting their versatility and
scalability to diverse tasks in daily scenarios. To that end, we introduce
SkillBlender, a novel hierarchical reinforcement learning framework for
versatile humanoid loco-manipulation. SkillBlender first pretrains
goal-conditioned task-agnostic primitive skills, and then dynamically blends
these skills to accomplish complex loco-manipulation tasks with minimal
task-specific reward engineering. We also introduce SkillBench, a parallel,
cross-embodiment, and diverse simulated benchmark containing three embodiments,
four primitive skills, and eight challenging loco-manipulation tasks,
accompanied by a set of scientific evaluation metrics balancing accuracy and
feasibility. Extensive simulated experiments show that our method significantly
outperforms all baselines, while naturally regularizing behaviors to avoid
reward hacking, resulting in more accurate and feasible movements for diverse
loco-manipulation tasks in our daily scenarios. Our code and benchmark will be
open-sourced to the community to facilitate future research. Project page:
https://usc-gvl.github.io/SkillBlender-web/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.08292v1' target='_blank'>From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via
  Bayesian Nash Equilibrium</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xie Yi, Zhanke Zhou, Chentao Cao, Qiyu Niu, Tongliang Liu, Bo Han</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 23:49:14</h6>
<p class='card-text'>Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.07744v1' target='_blank'>Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Seungho Baek, Taegeon Park, Jongchan Park, Seungjun Oh, Yusung Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 13:26:23</h6>
<p class='card-text'>Existing offline hierarchical reinforcement learning methods rely on
high-level policy learning to generate subgoal sequences. However, their
efficiency degrades as task horizons increase, and they lack effective
strategies for stitching useful state transitions across different
trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that
formulates subgoal selection as a graph search problem rather than learning an
explicit high-level policy. By embedding states into a Temporal Distance
Representation (TDR) space, GAS clusters semantically similar states from
different trajectories into unified graph nodes, enabling efficient transition
stitching. A shortest-path algorithm is then applied to select subgoal
sequences within the graph, while a low-level policy learns to reach the
subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)
metric, which filters out noisy or inefficient transition states, significantly
enhancing task performance. GAS outperforms prior offline HRL methods across
locomotion, navigation, and manipulation tasks. Notably, in the most
stitching-critical task, it achieves a score of 88.3, dramatically surpassing
the previous state-of-the-art score of 1.0. Our source code is available at:
https://github.com/qortmdgh4141/GAS.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.04549v1' target='_blank'>Discounting and Drug Seeking in Biological Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vardhan Palod, Pranav Mahajan, Veeky Baths, Boris S. Gutkin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 01:54:09</h6>
<p class='card-text'>Despite a strong desire to quit, individuals with long-term substance use
disorder (SUD) often struggle to resist drug use, even when aware of its
harmful consequences. This disconnect between knowledge and compulsive behavior
reflects a fundamental cognitive-behavioral conflict in addiction.
Neurobiologically, differential cue-induced activity within striatal
subregions, along with dopamine-mediated connectivity from the ventral to the
dorsal striatum, contributes to compulsive drug-seeking. However, the
functional mechanism linking these findings to behavioral conflict remains
unclear. Another hallmark of addiction is temporal discounting: individuals
with drug dependence exhibit steeper discount rates than non-users. Assuming
the ventral-dorsal striatal organization reflects a gradient from cognitive to
motor representations, addiction can be modeled within a hierarchical
reinforcement learning (HRL) framework. However, integrating discounting into
biologically grounded HRL remains an open challenge. In this work, we build on
a model showing how action choices reinforced with drug rewards become
insensitive to the negative consequences that follow. We address the
integration of discounting by ensuring natural reward values converge across
all levels in the HRL hierarchy, while drug rewards diverge due to their
dopaminergic effects. Our results show that high discounting amplifies
drug-seeking across the hierarchy, linking faster discounting with increased
addiction severity and impulsivity. We demonstrate alignment with empirical
findings on temporal discounting and propose testable predictions, establishing
addiction as a disorder of hierarchical decision-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.02050v1' target='_blank'>Decoupled Hierarchical Reinforcement Learning with State Abstraction for
  Discrete Grids</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qingyu Xiao, Yuanlin Chang, Youtian Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-01 06:36:19</h6>
<p class='card-text'>Effective agent exploration remains a core challenge in reinforcement
learning (RL) for complex discrete state-space environments, particularly under
partial observability. This paper presents a decoupled hierarchical RL
framework integrating state abstraction (DcHRL-SA) to address this issue. The
proposed method employs a dual-level architecture, consisting of a high level
RL-based actor and a low-level rule-based policy, to promote effective
exploration. Additionally, state abstraction method is incorporated to cluster
discrete states, effectively lowering state dimensionality. Experiments
conducted in two discrete customized grid environments demonstrate that the
proposed approach consistently outperforms PPO in terms of exploration
efficiency, convergence speed, cumulative reward, and policy stability. These
results demonstrate a practical approach for integrating decoupled hierarchical
policies and state abstraction in discrete grids with large-scale exploration
space. Code will be available at https://github.com/XQY169/DcHRL-SA.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.21750v1' target='_blank'>Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional
  Subgoals</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vivienne Huiling Wang, Tinghuai Wang, Joni Pajarinen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-27 20:38:44</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) learns to make decisions on
multiple levels of temporal abstraction. A key challenge in HRL is that the
low-level policy changes over time, making it difficult for the high-level
policy to generate effective subgoals. To address this issue, the high-level
policy must capture a complex subgoal distribution while also accounting for
uncertainty in its estimates. We propose an approach that trains a conditional
diffusion model regularized by a Gaussian Process (GP) prior to generate a
complex variety of subgoals while leveraging principled GP uncertainty
quantification. Building on this framework, we develop a strategy that selects
subgoals from both the diffusion policy and GP's predictive mean. Our approach
outperforms prior HRL methods in both sample efficiency and performance on
challenging continuous control benchmarks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.21410v1' target='_blank'>MRSD: Multi-Resolution Skill Discovery for HRL Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shashank Sharma, Janina Hoffmann, Vinay Namboodiri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-27 16:38:55</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) relies on abstract skills to solve
long-horizon tasks efficiently. While existing skill discovery methods learns
these skills automatically, they are limited to a single skill per task. In
contrast, humans learn and use both fine-grained and coarse motor skills
simultaneously. Inspired by human motor control, we propose Multi-Resolution
Skill Discovery (MRSD), an HRL framework that learns multiple skill encoders at
different temporal resolutions in parallel. A high-level manager dynamically
selects among these skills, enabling adaptive control strategies over time. We
evaluate MRSD on tasks from the DeepMind Control Suite and show that it
outperforms prior state-of-the-art skill discovery and HRL methods, achieving
faster convergence and higher final performance. Our findings highlight the
benefits of integrating multi-resolution skills in HRL, paving the way for more
versatile and efficient agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19761v1' target='_blank'>Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents
  via Offline Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zican Hu, Wei Liu, Xiaoye Qu, Xiangyu Yue, Chunlin Chen, Zhi Wang, Yu Cheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 09:43:40</h6>
<p class='card-text'>While showing sophisticated reasoning abilities, large language models (LLMs)
still struggle with long-horizon decision-making tasks due to deficient
exploration and long-term credit assignment, especially in sparse-reward
scenarios. Inspired by the divide-and-conquer principle, we propose an
innovative framework **GLIDER** (**G**rounding **L**anguage Models as
Eff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical
**R**einforcement Learning) that introduces a parameter-efficient and generally
applicable hierarchy to LLM policies. We develop a scheme where the low-level
controller is supervised with abstract, step-by-step plans that are learned and
instructed by the high-level policy. This design decomposes complicated
problems into a series of coherent chain-of-thought reasoning sub-tasks,
providing flexible temporal abstraction to significantly enhance exploration
and learning for long-horizon tasks. Furthermore, GLIDER facilitates fast
online adaptation to non-stationary environments owing to the strong
transferability of its task-agnostic low-level skills. Experiments on
ScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent
performance gains, along with enhanced generalization capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.06218v1' target='_blank'>Let Humanoids Hike! Integrative Skill Development on Complex Trails</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kwan-Yee Lin, Stella X. Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-09 17:53:02</h6>
<p class='card-text'>Hiking on complex trails demands balance, agility, and adaptive
decision-making over unpredictable terrain. Current humanoid research remains
fragmented and inadequate for hiking: locomotion focuses on motor skills
without long-term goals or situational awareness, while semantic navigation
overlooks real-world embodiment and local terrain variability. We propose
training humanoids to hike on complex trails, driving integrative skill
development across visual perception, decision making, and motor execution. We
develop a learning framework, LEGO-H, that enables a vision-equipped humanoid
robot to hike complex trails autonomously. We introduce two technical
innovations: 1) A temporal vision transformer variant - tailored into
Hierarchical Reinforcement Learning framework - anticipates future local goals
to guide movement, seamlessly integrating locomotion with goal-directed
navigation. 2) Latent representations of joint movement patterns, combined with
hierarchical metric learning - enhance Privileged Learning scheme - enable
smooth policy transfer from privileged training to onboard execution. These
components allow LEGO-H to handle diverse physical and environmental challenges
without relying on predefined motion patterns. Experiments across varied
simulated trails and robot morphologies highlight LEGO-H's versatility and
robustness, positioning hiking as a compelling testbed for embodied autonomy
and LEGO-H as a baseline for future humanoid development.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.04579v1' target='_blank'>Implicitly Aligning Humans and Autonomous Agents through Shared Task
  Abstractions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Stéphane Aroca-Ouellette, Miguel Aroca-Ouellette, Katharina von der Wense, Alessandro Roncone</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-07 17:19:17</h6>
<p class='card-text'>In collaborative tasks, autonomous agents fall short of humans in their
capability to quickly adapt to new and unfamiliar teammates. We posit that a
limiting factor for zero-shot coordination is the lack of shared task
abstractions, a mechanism humans rely on to implicitly align with teammates. To
address this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework
leveraging hierarchical reinforcement learning to mimic the structured approach
humans use in collaboration. We evaluate HA$^2$ in the Overcooked environment,
demonstrating statistically significant improvement over existing baselines
when paired with both unseen agents and humans, providing better resilience to
environmental shifts, and outperforming all state-of-the-art methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.04317v2' target='_blank'>Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ruize Zhang, Sirui Xiang, Zelai Xu, Feng Gao, Shilong Ji, Wenhao Tang, Wenbo Ding, Chao Yu, Yu Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-07 11:04:36</h6>
<p class='card-text'>In this paper, we tackle the problem of learning to play 3v3 multi-drone
volleyball, a new embodied competitive task that requires both high-level
strategic coordination and low-level agile control. The task is turn-based,
multi-agent, and physically grounded, posing significant challenges due to its
long-horizon dependencies, tight inter-agent coupling, and the underactuated
dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play
(HCSP), a hierarchical reinforcement learning framework that separates
centralized high-level strategic decision-making from decentralized low-level
motion control. We design a three-stage population-based training pipeline to
enable both strategy and skill to emerge from scratch without expert
demonstrations: (I) training diverse low-level skills, (II) learning high-level
strategy via self-play with fixed low-level controllers, and (III) joint
fine-tuning through co-self-play. Experiments show that HCSP achieves superior
performance, outperforming non-hierarchical self-play and rule-based
hierarchical baselines with an average 82.9% win rate and a 71.5% win rate
against the two-stage variant. Moreover, co-self-play leads to emergent team
behaviors such as role switching and coordinated formations, demonstrating the
effectiveness of our hierarchical design and training scheme. The project page
is at https://sites.google.com/view/hi-co-self-play.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.02439v1' target='_blank'>ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control
  via Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yang Deng, Yaohui Liu, Rui Liang, Dafang Zhao, Donghua Xie, Ittetsu Taniguchi, Dan Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-05 08:09:36</h6>
<p class='card-text'>The building thermodynamics model, which predicts real-time indoor
temperature changes under potential HVAC (Heating, Ventilation, and Air
Conditioning) control operations, is crucial for optimizing HVAC control in
buildings. While pioneering studies have attempted to develop such models for
various building environments, these models often require extensive data
collection periods and rely heavily on expert knowledge, making the modeling
process inefficient and limiting the reusability of the models. This paper
explores a model ensemble perspective that utilizes existing developed models
as base models to serve a target building environment, thereby providing
accurate predictions while reducing the associated efforts. Given that building
data streams are non-stationary and the number of base models may increase, we
propose a Hierarchical Reinforcement Learning (HRL) approach to dynamically
select and weight the base models. Our approach employs a two-tiered
decision-making process: the high-level focuses on model selection, while the
low-level determines the weights of the selected models. We thoroughly evaluate
the proposed approach through offline experiments and an on-site case study,
and the experimental results demonstrate the effectiveness of our method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.01979v1' target='_blank'>D3HRL: A Distributed Hierarchical Reinforcement Learning Approach Based
  on Causal Discovery and Spurious Correlation Detection</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenran Zhao, Dianxi Shi, Mengzhu Wang, Jianqiang Xia, Huanhuan Yang, Songchang Jin, Shaowu Yang, Chunping Qiu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-04 03:59:01</h6>
<p class='card-text'>Current Hierarchical Reinforcement Learning (HRL) algorithms excel in
long-horizon sequential decision-making tasks but still face two challenges:
delay effects and spurious correlations. To address them, we propose a causal
HRL approach called D3HRL. First, D3HRL models delayed effects as causal
relationships across different time spans and employs distributed causal
discovery to learn these relationships. Second, it employs conditional
independence testing to eliminate spurious correlations. Finally, D3HRL
constructs and trains hierarchical policies based on the identified true causal
relationships. These three steps are iteratively executed, gradually exploring
the complete causal chain of the task. Experiments conducted in 2D-MineCraft
and MiniGrid show that D3HRL demonstrates superior sensitivity to delay effects
and accurately identifies causal relationships, leading to reliable
decision-making in complex environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.18794v2' target='_blank'>Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation
  with Autonomous Mobile Robots</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Brendon Johnson, Alfredo Weitzenfeld</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-26 04:30:10</h6>
<p class='card-text'>Hierarchical reinforcement learning (HRL) is hypothesized to be able to take
advantage of the inherent hierarchy in robot learning tasks with sparse reward
schemes, in contrast to more traditional reinforcement learning algorithms. In
this research, hierarchical reinforcement learning is evaluated and contrasted
with standard reinforcement learning in complex navigation tasks. We evaluate
unique characteristics of HRL, including their ability to create sub-goals and
the termination function. We constructed experiments to test the differences
between PPO and HRL, different ways of creating sub-goals, manual vs automatic
sub-goal creation, and the effects of the frequency of termination on
performance. These experiments highlight the advantages of HRL and how it
achieves these advantages.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.17356v1' target='_blank'>Comprehend, Divide, and Conquer: Feature Subspace Exploration via
  Multi-Agent Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weiliang Zhang, Xiaohan Huang, Yi Du, Ziyue Qiao, Qingqing Long, Zhen Meng, Yuanchun Zhou, Meng Xiao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-24 08:16:36</h6>
<p class='card-text'>Feature selection aims to preprocess the target dataset, find an optimal and
most streamlined feature subset, and enhance the downstream machine learning
task. Among filter, wrapper, and embedded-based approaches, the reinforcement
learning (RL)-based subspace exploration strategy provides a novel objective
optimization-directed perspective and promising performance. Nevertheless, even
with improved performance, current reinforcement learning approaches face
challenges similar to conventional methods when dealing with complex datasets.
These challenges stem from the inefficient paradigm of using one agent per
feature and the inherent complexities present in the datasets. This observation
motivates us to investigate and address the above issue and propose a novel
approach, namely HRLFS. Our methodology initially employs a Large Language
Model (LLM)-based hybrid state extractor to capture each feature's mathematical
and semantic characteristics. Based on this information, features are
clustered, facilitating the construction of hierarchical agents for each
cluster and sub-cluster. Extensive experiments demonstrate the efficiency,
scalability, and robustness of our approach. Compared to contemporary or the
one-feature-one-agent RL-based approaches, HRLFS improves the downstream ML
performance with iterative feature subspace exploration while accelerating
total run time by reducing the number of agents involved.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.15876v2' target='_blank'>Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement
  Learning for Strategic Confrontation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qizhen Wu, Lei Chen, Kexin Liu, Jinhu Lü</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-22 13:22:58</h6>
<p class='card-text'>In swarm robotics, confrontation scenarios, including strategic
confrontations, require efficient decision-making that integrates discrete
commands and continuous actions. Traditional task and motion planning methods
separate decision-making into two layers, but their unidirectional structure
fails to capture the interdependence between these layers, limiting
adaptability in dynamic environments. Here, we propose a novel bidirectional
approach based on hierarchical reinforcement learning, enabling dynamic
interaction between the layers. This method effectively maps commands to task
allocation and actions to path planning, while leveraging cross-training
techniques to enhance learning across the hierarchical framework. Furthermore,
we introduce a trajectory prediction model that bridges abstract task
representations with actionable planning goals. In our experiments, it achieves
over 80% in confrontation win rate and under 0.01 seconds in decision time,
outperforming existing approaches. Demonstrations through large-scale tests and
real-world robot experiments further emphasize the generalization capabilities
and practical applicability of our method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.14989v1' target='_blank'>Dynamic Legged Ball Manipulation on Rugged Terrains with Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongjie Zhu, Zhuo Yang, Tianhang Wu, Luzhou Ge, Xuesong Li, Qi Liu, Xiang Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-21 09:38:38</h6>
<p class='card-text'>Advancing the dynamic loco-manipulation capabilities of quadruped robots in
complex terrains is crucial for performing diverse tasks. Specifically, dynamic
ball manipulation in rugged environments presents two key challenges. The first
is coordinating distinct motion modalities to integrate terrain traversal and
ball control seamlessly. The second is overcoming sparse rewards in end-to-end
deep reinforcement learning, which impedes efficient policy convergence. To
address these challenges, we propose a hierarchical reinforcement learning
framework. A high-level policy, informed by proprioceptive data and ball
position, adaptively switches between pre-trained low-level skills such as ball
dribbling and rough terrain navigation. We further propose Dynamic
Skill-Focused Policy Optimization to suppress gradients from inactive skills
and enhance critical skill learning. Both simulation and real-world experiments
validate that our methods outperform baseline approaches in dynamic ball
manipulation across rugged terrains, highlighting its effectiveness in
challenging environments. Videos are on our website: dribble-hrl.github.io.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.05553v1' target='_blank'>Federated Hierarchical Reinforcement Learning for Adaptive Traffic
  Signal Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongjie Fu, Lingyun Zhong, Zifan Li, Xuan Di</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-07 23:02:59</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) has shown promise for adaptive
traffic signal control (ATSC), enabling multiple intersections to coordinate
signal timings in real time. However, in large-scale settings, MARL faces
constraints due to extensive data sharing and communication requirements.
Federated learning (FL) mitigates these challenges by training shared models
without directly exchanging raw data, yet traditional FL methods such as FedAvg
struggle with highly heterogeneous intersections. Different intersections
exhibit varying traffic patterns, demands, and road structures, so performing
FedAvg across all agents is inefficient. To address this gap, we propose
Hierarchical Federated Reinforcement Learning (HFRL) for ATSC. HFRL employs
clustering-based or optimization-based techniques to dynamically group
intersections and perform FedAvg independently within groups of intersections
with similar characteristics, enabling more effective coordination and
scalability than standard FedAvg. Our experiments on synthetic and real-world
traffic networks demonstrate that HFRL not only outperforms both decentralized
and standard federated RL approaches but also identifies suitable grouping
patterns based on network structure or traffic demand, resulting in a more
robust framework for distributed, heterogeneous systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.04366v1' target='_blank'>Solving Sokoban using Hierarchical Reinforcement Learning with Landmarks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sergey Pastukhov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-06 05:30:21</h6>
<p class='card-text'>We introduce a novel hierarchical reinforcement learning (HRL) framework that
performs top-down recursive planning via learned subgoals, successfully applied
to the complex combinatorial puzzle game Sokoban. Our approach constructs a
six-level policy hierarchy, where each higher-level policy generates subgoals
for the level below. All subgoals and policies are learned end-to-end from
scratch, without any domain knowledge. Our results show that the agent can
generate long action sequences from a single high-level call. While prior work
has explored 2-3 level hierarchies and subgoal-based planning heuristics, we
demonstrate that deep recursive goal decomposition can emerge purely from
learning, and that such hierarchies can scale effectively to hard puzzle
domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.21677v1' target='_blank'>A tale of two goals: leveraging sequentiality in multi-goal scenarios</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Olivier Serris, Stéphane Doncieux, Olivier Sigaud</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-27 16:47:46</h6>
<p class='card-text'>Several hierarchical reinforcement learning methods leverage planning to
create a graph or sequences of intermediate goals, guiding a lower-level
goal-conditioned (GC) policy to reach some final goals. The low-level policy is
typically conditioned on the current goal, with the aim of reaching it as
quickly as possible. However, this approach can fail when an intermediate goal
can be reached in multiple ways, some of which may make it impossible to
continue toward subsequent goals. To address this issue, we introduce two
instances of Markov Decision Process (MDP) where the optimization objective
favors policies that not only reach the current goal but also subsequent ones.
In the first, the agent is conditioned on both the current and final goals,
while in the second, it is conditioned on the next two goals in the sequence.
We conduct a series of experiments on navigation and pole-balancing tasks in
which sequences of intermediate goals are given. By evaluating policies trained
with TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that,
in most cases, conditioning on the next two goals improves stability and sample
efficiency over other approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19007v1' target='_blank'>Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chak Lam Shek, Pratap Tokekar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 15:49:56</h6>
<p class='card-text'>Large Language Models (LLMs) have shown remarkable promise in reasoning and
decision-making, yet their integration with Reinforcement Learning (RL) for
complex robotic tasks remains underexplored. In this paper, we propose an
LLM-guided hierarchical RL framework, termed LDSC, that leverages LLM-driven
subgoal selection and option reuse to enhance sample efficiency,
generalization, and multi-task adaptability. Traditional RL methods often
suffer from inefficient exploration and high computational cost. Hierarchical
RL helps with these challenges, but existing methods often fail to reuse
options effectively when faced with new tasks. To address these limitations, we
introduce a three-stage framework that uses LLMs for subgoal generation given
natural language description of the task, a reusable option learning and
selection method, and an action-level policy, enabling more effective
decision-making across diverse tasks. By incorporating LLMs for subgoal
prediction and policy guidance, our approach improves exploration efficiency
and enhances learning performance. On average, LDSC outperforms the baseline by
55.9\% in average reward, demonstrating its effectiveness in complex RL
settings. More details and experiment videos could be found in
\href{https://raaslab.org/projects/LDSC/}{this
link\footnote{https://raaslab.org/projects/LDSC}}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.14809v1' target='_blank'>Learning with Expert Abstractions for Efficient Multi-Task Continuous
  Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jeff Jewett, Sandhya Saisubramanian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-19 00:44:23</h6>
<p class='card-text'>Decision-making in complex, continuous multi-task environments is often
hindered by the difficulty of obtaining accurate models for planning and the
inefficiency of learning purely from trial and error. While precise environment
dynamics may be hard to specify, human experts can often provide high-fidelity
abstractions that capture the essential high-level structure of a task and user
preferences in the target environment. Existing hierarchical approaches often
target discrete settings and do not generalize across tasks. We propose a
hierarchical reinforcement learning approach that addresses these limitations
by dynamically planning over the expert-specified abstraction to generate
subgoals to learn a goal-conditioned policy. To overcome the challenges of
learning under sparse rewards, we shape the reward based on the optimal state
value in the abstract model. This structured decision-making process enhances
sample efficiency and facilitates zero-shot generalization. Our empirical
evaluation on a suite of procedurally generated continuous control environments
demonstrates that our approach outperforms existing hierarchical reinforcement
learning methods in terms of sample efficiency, task completion rate,
scalability to complex tasks, and generalization to novel scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.12036v1' target='_blank'>Hierarchical Reinforcement Learning for Safe Mapless Navigation with
  Congestion Estimation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianqi Gao, Xizheng Pang, Qi Liu, Yanjie Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-15 08:03:50</h6>
<p class='card-text'>Reinforcement learning-based mapless navigation holds significant potential.
However, it faces challenges in indoor environments with local minima area.
This paper introduces a safe mapless navigation framework utilizing
hierarchical reinforcement learning (HRL) to enhance navigation through such
areas. The high-level policy creates a sub-goal to direct the navigation
process. Notably, we have developed a sub-goal update mechanism that considers
environment congestion, efficiently avoiding the entrapment of the robot in
local minimum areas. The low-level motion planning policy, trained through safe
reinforcement learning, outputs real-time control instructions based on
acquired sub-goal. Specifically, to enhance the robot's environmental
perception, we introduce a new obstacle encoding method that evaluates the
impact of obstacles on the robot's motion planning. To validate the performance
of our HRL-based navigation framework, we conduct simulations in office, home,
and restaurant environments. The findings demonstrate that our HRL-based
navigation framework excels in both static and dynamic scenarios. Finally, we
implement the HRL-based navigation framework on a TurtleBot3 robot for physical
validation experiments, which exhibits its strong generalization capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06309v1' target='_blank'>On the Fly Adaptation of Behavior Tree-Based Policies through
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marco Iannotta, Johannes A. Stork, Erik Schaffernicht, Todor Stoyanov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-08 18:56:22</h6>
<p class='card-text'>With the rising demand for flexible manufacturing, robots are increasingly
expected to operate in dynamic environments where local -- such as slight
offsets or size differences in workpieces -- are common. We propose to address
the problem of adapting robot behaviors to these task variations with a
sample-efficient hierarchical reinforcement learning approach adapting Behavior
Tree (BT)-based policies. We maintain the core BT properties as an
interpretable, modular framework for structuring reactive behaviors, but extend
their use beyond static tasks by inherently accommodating local task
variations. To show the efficiency and effectiveness of our approach, we
conduct experiments both in simulation and on a Franka Emika Panda 7-DoF, with
the manipulator adapting to different obstacle avoidance and pivoting tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.20380v1' target='_blank'>Multi-Turn Code Generation Through Single-Step Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arnav Kumar Jain, Gonzalo Gonzalez-Pumariega, Wayne Chen, Alexander M Rush, Wenting Zhao, Sanjiban Choudhury</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-27 18:55:05</h6>
<p class='card-text'>We address the problem of code generation from multi-turn execution feedback.
Existing methods either generate code without feedback or use complex,
hierarchical reinforcement learning to optimize multi-turn rewards. We propose
a simple yet scalable approach, $\mu$Code, that solves multi-turn code
generation using only single-step rewards. Our key insight is that code
generation is a one-step recoverable MDP, where the correct code can be
recovered from any intermediate code state in a single turn. $\mu$Code
iteratively trains both a generator to provide code solutions conditioned on
multi-turn execution feedback and a verifier to score the newly generated code.
Experimental evaluations show that our approach achieves significant
improvements over the state-of-the-art baselines. We provide analysis of the
design choices of the reward models and policy, and show the efficacy of
$\mu$Code at utilizing the execution feedback. Our code is available at
https://github.com/portal-cornell/muCode.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.15425v4' target='_blank'>TAG: A Decentralized Framework for Multi-Agent Hierarchical
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Giuseppe Paolo, Abdelhakim Benechehab, Hamza Cherkaoui, Albert Thomas, Balázs Kégl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-21 12:52:16</h6>
<p class='card-text'>Hierarchical organization is fundamental to biological systems and human
societies, yet artificial intelligence systems often rely on monolithic
architectures that limit adaptability and scalability. Current hierarchical
reinforcement learning (HRL) approaches typically restrict hierarchies to two
levels or require centralized training, which limits their practical
applicability. We introduce TAME Agent Framework (TAG), a framework for
constructing fully decentralized hierarchical multi-agent systems. TAG enables
hierarchies of arbitrary depth through a novel LevelEnv concept, which
abstracts each hierarchy level as the environment for the agents above it. This
approach standardizes information flow between levels while preserving loose
coupling, allowing for seamless integration of diverse agent types. We
demonstrate the effectiveness of TAG by implementing hierarchical architectures
that combine different RL agents across multiple levels, achieving improved
performance over classical multi-agent RL baselines on standard benchmarks. Our
results show that decentralized hierarchical organization enhances both
learning speed and final performance, positioning TAG as a promising direction
for scalable multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.06772v2' target='_blank'>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-10 18:51:47</h6>
<p class='card-text'>We present that hierarchical LLM reasoning via scaling thought templates can
effectively optimize the reasoning search space and outperform the mathematical
reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.
We train our ReasonFlux-32B model with only 8 GPUs and introduces three
innovations: (i) a structured and generic thought template library, containing
around 500 high-level thought templates capable of generalizing to similar or
relevant reasoning problems; (ii) performing hierarchical reinforcement
learning on a sequence of thought templates instead of long CoTs, optimizing a
base LLM to plan out an optimal template trajectory for gradually handling
complex problems; (iii) a brand new inference scaling system that enables
hierarchical LLM reasoning by adaptively scaling thought templates at inference
time. With a template trajectory containing more explainable reasoning
structures than DeepSeek-R1 and o3-mini, our ReasonFlux-32B significantly
advances math reasoning capabilities to state-of-the-art levels. Notably, on
the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview
by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an
average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and
45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2502.05537v1' target='_blank'>Sequential Stochastic Combinatorial Optimization Using Hierarchal
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinsong Feng, Zihan Yu, Yanhai Xiong, Haipeng Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-02-08 12:00:30</h6>
<p class='card-text'>Reinforcement learning (RL) has emerged as a promising tool for combinatorial
optimization (CO) problems due to its ability to learn fast, effective, and
generalizable solutions. Nonetheless, existing works mostly focus on one-shot
deterministic CO, while sequential stochastic CO (SSCO) has rarely been studied
despite its broad applications such as adaptive influence maximization (IM) and
infectious disease intervention. In this paper, we study the SSCO problem where
we first decide the budget (e.g., number of seed nodes in adaptive IM)
allocation for all time steps, and then select a set of nodes for each time
step. The few existing studies on SSCO simplify the problems by assuming a
uniformly distributed budget allocation over the time horizon, yielding
suboptimal solutions. We propose a generic hierarchical RL (HRL) framework
called wake-sleep option (WS-option), a two-layer option-based framework that
simultaneously decides adaptive budget allocation on the higher layer and node
selection on the lower layer. WS-option starts with a coherent formulation of
the two-layer Markov decision processes (MDPs), capturing the interdependencies
between the two layers of decisions. Building on this, WS-option employs
several innovative designs to balance the model's training stability and
computational efficiency, preventing the vicious cyclic interference issue
between the two layers. Empirical results show that WS-option exhibits
significantly improved effectiveness and generalizability compared to
traditional methods. Moreover, the learned model can be generalized to larger
graphs, which significantly reduces the overhead of computational resources.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>