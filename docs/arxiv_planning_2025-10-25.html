<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-10-25</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-10-25</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20818v1' target='_blank'>VAMOS: A Hierarchical Vision-Language-Action Model for
  Capability-Modulated and Steerable Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mateo Guaman Castro, Sidharth Rajagopal, Daniel Gorbatov, Matt Schmittle, Rohan Baijal, Octi Zhang, Rosario Scalise, Sidharth Talia, Emma Romig, Celso de Melo, Byron Boots, Abhishek Gupta</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 17:59:45</h6>
<p class='card-text'>A fundamental challenge in robot navigation lies in learning policies that
generalize across diverse environments while conforming to the unique physical
constraints and capabilities of a specific embodiment (e.g., quadrupeds can
walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that
decouples semantic planning from embodiment grounding: a generalist planner
learns from diverse, open-world data, while a specialist affordance model
learns the robot's physical constraints and capabilities in safe, low-cost
simulation. We enabled this separation by carefully designing an interface that
lets a high-level planner propose candidate paths directly in image space that
the affordance model then evaluates and re-ranks. Our real-world experiments
show that VAMOS achieves higher success rates in both indoor and complex
outdoor navigation than state-of-the-art model-based and end-to-end learning
methods. We also show that our hierarchical design enables cross-embodied
navigation across legged and wheeled robots and is easily steerable using
natural language. Real-world ablations confirm that the specialist model is key
to embodiment grounding, enabling a single high-level planner to be deployed
across physically distinct wheeled and legged robots. Finally, this model
significantly enhances single-robot reliability, achieving 3X higher success
rates by rejecting physically infeasible plans. Website:
https://vamos-vla.github.io/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20634v1' target='_blank'>Deep Learning in Dental Image Analysis: A Systematic Review of Datasets,
  Methodologies, and Emerging Challenges</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenhuan Zhou, Jingbo Zhu, Yuchen Zhang, Xiaohang Guan, Peng Wang, Tao Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 15:05:06</h6>
<p class='card-text'>Efficient analysis and processing of dental images are crucial for dentists
to achieve accurate diagnosis and optimal treatment planning. However, dental
imaging inherently poses several challenges, such as low contrast, metallic
artifacts, and variations in projection angles. Combined with the subjectivity
arising from differences in clinicians' expertise, manual interpretation often
proves time-consuming and prone to inconsistency. Artificial intelligence
(AI)-based automated dental image analysis (DIA) offers a promising solution to
these issues and has become an integral part of computer-aided dental diagnosis
and treatment. Among various AI technologies, deep learning (DL) stands out as
the most widely applied and influential approach due to its superior feature
extraction and representation capabilities. To comprehensively summarize recent
progress in this field, we focus on the two fundamental aspects of DL
research-datasets and models. In this paper, we systematically review 260
studies on DL applications in DIA, including 49 papers on publicly available
dental datasets and 211 papers on DL-based algorithms. We first introduce the
basic concepts of dental imaging and summarize the characteristics and
acquisition methods of existing datasets. Then, we present the foundational
techniques of DL and categorize relevant models and algorithms according to
different DIA tasks, analyzing their network architectures, optimization
strategies, training methods, and performance. Furthermore, we summarize
commonly used training and evaluation metrics in the DIA domain. Finally, we
discuss the current challenges of existing research and outline potential
future directions. We hope that this work provides a valuable and systematic
reference for researchers in this field. All supplementary materials and
detailed comparison tables will be made publicly available on GitHub.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20597v1' target='_blank'>The Intermodal Railroad Blocking and Railcar Fleet-Management Planning
  Problem</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Julie Kienzle, Serge Bisaillon, Teodor Gabriel Crainic, Emma Frejinger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 14:24:20</h6>
<p class='card-text'>Rail is a cost-effective and relatively low-emission mode for transporting
intermodal containers over long distances. This paper addresses tactical
planning of intermodal railroad operations by introducing a new problem that
simultaneously considers three consolidation processes and the management of a
heterogeneous railcar fleet. We model the problem with a scheduled service
network design with resource management (SSND-RM) formulation, expressed as an
integer linear program. While such formulations are challenging to solve at
scale, we demonstrate that our problem can be tackled with a general-purpose
solver when provided with high-quality warm-start solutions. To this end, we
design a construction heuristic inspired by a relax-and-fix procedure. We
evaluate the methodology on realistic, large-scale instances from our
industrial partner, the Canadian National Railway Company: a North American
Class I railroad. The computational experiments show that the proposed approach
efficiently solves practically relevant instances, and that solutions to the
SSND-RM formulation yield substantially more accurate capacity estimations
compared to those obtained from simpler baseline models. Managerial insights
from our study highlight that ignoring railcar fleet management or container
loading constraints can lead to a severe underestimation of required capacity,
which may result in costly operational inefficiencies. Furthermore, our results
show that the use of multi-platform railcars improves overall capacity
utilization and benefits the network, even if they can locally lead to less
efficient loading as measured by terminal-level slot utilization performance
indicators.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20548v1' target='_blank'>GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering
  via Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinchang Luo, Mingquan Cheng, Fan Wan, Ni Li, Xiaoling Xia, Shuangshuang Tian, Tingcheng Bian, Haiwei Wang, Haohuan Fu, Yan Tao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 13:35:02</h6>
<p class='card-text'>Reinforcement learning has recently shown promise in improving
retrieval-augmented generation (RAG). Despite these advances, its effectiveness
in multi-hop question answering (QA) remains limited by two fundamental
limitations: (i) global planning absence to structure multi-step reasoning, and
(ii) unfaithful execution, which hinders effective query formulation and
consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement
learning framework designed to enhance global reasoning in multi-hop QA.
GlobalRAG decomposes questions into subgoals, coordinates retrieval with
reasoning, and refines evidence iteratively. To guide this process, we
introduce Planning Quality Reward and SubGoal Completion Reward, which
encourage coherent planning and reliable subgoal execution. In addition, a
progressive weight annealing strategy balances process-oriented and
outcome-based objectives. Extensive experiments on both in-domain and
out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms
strong baselines while using only 8k training data (42% of the training data
used by strong baselines), achieving average improvements of 14.2% in both EM
and F1.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20542v1' target='_blank'>A Unified Framework for Zero-Shot Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jacopo Di Ventura, Jan Felix Kleuker, Aske Plaat, Thomas Moerland</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 13:30:26</h6>
<p class='card-text'>Zero-shot reinforcement learning (RL) has emerged as a setting for developing
general agents in an unsupervised manner, capable of solving downstream tasks
without additional training or planning at test-time. Unlike conventional RL,
which optimizes policies for a fixed reward, zero-shot RL requires agents to
encode representations rich enough to support immediate adaptation to any
objective, drawing parallels to vision and language foundation models. Despite
growing interest, the field lacks a common analytical lens.
  We present the first unified framework for zero-shot RL. Our formulation
introduces a consistent notation and taxonomy that organizes existing
approaches and allows direct comparison between them. Central to our framework
is the classification of algorithms into two families: direct representations,
which learn end-to-end mappings from rewards to policies, and compositional
representations, which decompose the representation leveraging the substructure
of the value function. Within this framework, we highlight shared principles
and key differences across methods, and we derive an extended bound for
successor-feature methods, offering a new perspective on their performance in
the zero-shot regime. By consolidating existing work under a common lens, our
framework provides a principled foundation for future research in zero-shot RL
and outlines a clear path toward developing more general agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20496v1' target='_blank'>A Parameter-Linear Formulation of the Optimal Path Following Problem for
  Robotic Manipulator</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tobias Marauli, Hubert Gattringer, Andreas Mueller</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 12:37:32</h6>
<p class='card-text'>In this paper the computational challenges of time-optimal path following are
addressed. The standard approach is to minimize the travel time, which
inevitably leads to singularities at zero path speed, when reformulating the
optimization problem in terms of a path parameter. Thus, smooth trajectory
generation while maintaining a low computational effort is quite challenging,
since the singularities have to be taken into account. To this end, a different
approach is presented in this paper. This approach is based on maximizing the
path speed along a prescribed path. Furthermore, the approach is capable of
planning smooth trajectories numerically efficient. Moreover, the discrete
reformulation of the underlying problem is linear in optimization variables.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20473v1' target='_blank'>Robot Path and Trajectory Planning Considering a Spatially Fixed TCP</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bernhard Rameder, Hubert Gattringer, Andreas Mueller, Ronald Naderer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 12:13:50</h6>
<p class='card-text'>This paper presents a method for planning a trajectory in workspace
coordinates using a spatially fixed tool center point (TCP), while taking into
account the processing path on a part. This approach is beneficial if it is
easier to move the part rather than moving the tool. Whether a mathematical
description that defines the shape to be processed or single points from a
design program are used, the robot path is finally represented using B-splines.
The use of splines enables the path to be continuous with a desired degree,
which finally leads to a smooth robot trajectory. While calculating the robot
trajectory through prescribed orientation, additionally a given velocity at the
TCP has to be considered. The procedure was validated on a real system using an
industrial robot moving an arbitrary defined part.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20440v1' target='_blank'>Multicast-partitioning in Time-triggered Stream Planning for
  Time-Sensitive Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Heiko Geppert, Frank Dürr, Simon Naß, Kurt Rothermel</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 11:20:30</h6>
<p class='card-text'>Multicast allows sending a message to multiple recipients without having to
create and send a separate message for each recipient. This preserves network
bandwidth, which is particularly important in time-sensitive networks. These
networks are commonly used to provide latency-bounded communication for
real-time systems in domains like automotive, avionics, industrial internet of
things, automated shop floors, and smart energy grids. The preserved bandwidth
can be used to admit additional real-time messages with specific quality of
service requirements or to reduce the end-to-end latencies for messages of any
type. However, using multicast communication can complicate traffic planning,
as it requires free queues or available downstream egress ports on all branches
of the multicast tree. In this work, we present a novel multicast partitioning
technique to split multicast trees into smaller multicast or unicast trees.
This allows for a more fine-grained trade-off between bandwidth utilization and
traffic scheduling difficulty. Thus, schedulability in dynamic systems can be
improved, in terms the number of admitted streams and the accumulated network
throughput. We evaluated the multicast partitioning on different network
topologies and with three different scheduling algorithms. With the
partitioning, 5-15\% fewer streams were rejected, while achieving 5-125\% more
network throughput, depending on the scheduling algorithm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20373v1' target='_blank'>FAST-SBF: an automatic procedure for the measurement of Surface
  Brightness Fluctuations for large sky surveys</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Gabriele Riccio, Michele Cantiello, Rebecca Habas, Nandini Hazra, Giuseppe D'Ago, Gabriella Raimondo, John P. Blakeslee, Joseph B. Jensen, Marco Mirabile, Enzo Brocato, Massimo Brescia, Claudia M. Raiteri</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 09:12:42</h6>
<p class='card-text'>The Surface Brightness Fluctuation method is one of the most reliable and
efficient ways of measuring distances to galaxies within 100 Mpc. While recent
implementations have increasingly relied on space-based observations, SBF
remains effective when applied to ground-based data. In particular, deep,
wide-field imaging surveys with sub-arcsecond seeing conditions allows us for
accurate SBF measurements across large samples of galaxies. With the upcoming
next generation wide-area imaging surveys, the thousands of galaxies suitable
for SBF measurements will give us the opportunity to constrain the 3D structure
of the local universe. We present FAST-SBF, a new Python-based pipeline for
measuring SBF, developed to support the analysis of large datasets from
upcoming wide-field imaging surveys such as LSST, Euclid, and Roman. The
procedure, still in the testing and development stage, is designed for
automation and minimal user intervention, offering a fast and flexible approach
to SBF distance estimation. We validate the performance of the procedure on
high-quality imaging data from the Hyper Suprime-Cam Subaru Strategic Program
(HSC-SSP), a precursor to LSST, analyzing a sample of both luminous early-type
galaxies and fainter dwarfs. Our measurements are also compared with recent
results from the Next Generation Virgo Cluster Survey (NGVS) and with the SPoT
stellar population synthesis models. The results show excellent agreement with
published distances, with the capability of measuring the SBF signal also for
faint dwarf galaxies. The pipeline allows the user to completely analyze a
galaxy in relatively short time ($\approx$ minutes) and significantly reduces
the need for user intervention. reduces at minimum the user intervention. The
FAST-SBF tool is planned for public release to support the community in using
SBF as a distance indicator in next-generation surveys.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20335v1' target='_blank'>Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous
  Parking</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zixuan Wu, Hengyuan Zhang, Ting-Hsuan Chen, Yuliang Guo, David Paz, Xinyu Huang, Liu Ren</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 08:35:50</h6>
<p class='card-text'>Parking is a critical pillar of driving safety. While recent end-to-end (E2E)
approaches have achieved promising in-domain results, robustness under domain
shifts (e.g., weather and lighting changes) remains a key challenge. Rather
than relying on additional data, in this paper, we propose Dino-Diffusion
Parking (DDP), a domain-agnostic autonomous parking pipeline that integrates
visual foundation models with diffusion-based planning to enable generalized
perception and robust motion planning under distribution shifts. We train our
pipeline in CARLA at regular setting and transfer it to more adversarial
settings in a zero-shot fashion. Our model consistently achieves a parking
success rate above 90% across all tested out-of-distribution (OOD) scenarios,
with ablation studies confirming that both the network architecture and
algorithmic design significantly enhance cross-domain performance over existing
baselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environment
reconstructed from a real-world parking lot demonstrates promising sim-to-real
transfer.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20310v1' target='_blank'>Multi-Step Reasoning for Embodied Question Answering via Tool
  Augmentation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mingliang Zhai, Hansheng Liang, Xiaomeng Fan, Zhi Gao, Chuanhao Li, Che Sun, Xu Bin, Yuwei Wu, Yunde Jia</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 08:02:08</h6>
<p class='card-text'>Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20308v1' target='_blank'>Hybrid Mixed Integer Linear Programming for Large-Scale Join Order
  Optimisation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Manuel Schönberger, Immanuel Trummer, Wolfgang Mauerer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 07:58:00</h6>
<p class='card-text'>Finding optimal join orders is among the most crucial steps to be performed
by query optimisers. Though extensively studied in data management research,
the problem remains far from solved: While query optimisers rely on exhaustive
search methods to determine ideal solutions for small problems, such methods
reach their limits once queries grow in size. Yet, large queries become
increasingly common in real-world scenarios, and require suitable methods to
generate efficient execution plans. While a variety of heuristics have been
proposed for large-scale query optimisation, they suffer from degrading
solution quality as queries grow in size, or feature highly sub-optimal
worst-case behavior, as we will show.
  We propose a novel method based on the paradigm of mixed integer linear
programming (MILP): By deriving a novel MILP model capable of optimising
arbitrary bushy tree structures, we address the limitations of existing MILP
methods for join ordering, and can rely on highly optimised MILP solvers to
derive efficient tree structures that elude competing methods. To ensure
optimisation efficiency, we embed our MILP method into a hybrid framework,
which applies MILP solvers precisely where they provide the greatest advantage
over competitors, while relying on more efficient methods for less complex
optimisation steps. Thereby, our approach gracefully scales to extremely large
query sizes joining up to 100 relations, and consistently achieves the most
robust plan quality among a large variety of competing join ordering methods.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20275v1' target='_blank'>Classical Feature Embeddings Help in BERT-Based Human Mobility
  Prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yunzhi Liu, Haokai Tan, Rushi Kanjaria, Lihuan Li, Flora D. Salim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 06:59:58</h6>
<p class='card-text'>Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20177v1' target='_blank'>A Contact-Driven Framework for Manipulating in the Blind</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhammad Suhail Saleem, Lai Yuan, Maxim Likhachev</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 03:52:08</h6>
<p class='card-text'>Robots often face manipulation tasks in environments where vision is
inadequate due to clutter, occlusions, or poor lighting--for example, reaching
a shutoff valve at the back of a sink cabinet or locating a light switch above
a crowded shelf. In such settings, robots, much like humans, must rely on
contact feedback to distinguish free from occupied space and navigate around
obstacles. Many of these environments often exhibit strong structural
priors--for instance, pipes often span across sink cabinets--that can be
exploited to anticipate unseen structure and avoid unnecessary collisions. We
present a theoretically complete and empirically efficient framework for
manipulation in the blind that integrates contact feedback with structural
priors to enable robust operation in unknown environments. The framework
comprises three tightly coupled components: (i) a contact detection and
localization module that utilizes joint torque sensing with a contact particle
filter to detect and localize contacts, (ii) an occupancy estimation module
that uses the history of contact observations to build a partial occupancy map
of the workspace and extrapolate it into unexplored regions with learned
predictors, and (iii) a planning module that accounts for the fact that contact
localization estimates and occupancy predictions can be noisy, computing paths
that avoid collisions and complete tasks efficiently without eliminating
feasible solutions. We evaluate the system in simulation and in the real world
on a UR10e manipulator across two domestic tasks--(i) manipulating a valve
under a kitchen sink surrounded by pipes and (ii) retrieving a target object
from a cluttered shelf. Results show that the framework reliably solves these
tasks, achieving up to a 2x reduction in task completion time compared to
baselines, with ablations confirming the contribution of each module.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20161v1' target='_blank'>PathFormer: A Transformer with 3D Grid Constraints for Digital Twin
  Robot-Arm Trajectory Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ahmed Alanazi, Duy Ho, Yugyung Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-23 03:19:59</h6>
<p class='card-text'>Robotic arms require precise, task-aware trajectory planning, yet sequence
models that ignore motion structure often yield invalid or inefficient
executions. We present a Path-based Transformer that encodes robot motion with
a 3-grid (where/what/when) representation and constraint-masked decoding,
enforcing lattice-adjacent moves and workspace bounds while reasoning over task
graphs and action order. Trained on 53,755 trajectories (80% train / 20%
validation), the model aligns closely with ground truth -- 89.44% stepwise
accuracy, 93.32% precision, 89.44% recall, and 90.40% F1 -- with 99.99% of
paths legal by construction. Compiled to motor primitives on an xArm Lite 6
with a depth-camera digital twin, it attains up to 97.5% reach and 92.5% pick
success in controlled tests, and 86.7% end-to-end success across 60
language-specified tasks in cluttered scenes, absorbing slips and occlusions
via local re-grounding without global re-planning. These results show that
path-structured representations enable Transformers to generate accurate,
reliable, and interpretable robot trajectories, bridging graph-based planning
and sequence-based learning and providing a practical foundation for
general-purpose manipulation and sim-to-real transfer.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20019v1' target='_blank'>Machine Learning-Based Localization Accuracy of RFID Sensor Networks via
  RSSI Decision Trees and CAD Modeling for Defense Applications</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Curtis Lee Shull, Merrick Green</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 20:40:50</h6>
<p class='card-text'>Radio Frequency Identification (RFID) tracking may be a viable solution for
defense assets that must be stored in accordance with security guidelines.
However, poor sensor specificity (vulnerabilities include long range detection,
spoofing, and counterfeiting) can lead to erroneous detection and operational
security events. We present a supervised learning simulation with realistic
Received Signal Strength Indicator (RSSI) data and Decision Tree classification
in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some
of the challenges encountered in defense storage. In this work, we focused on
classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw
dataset had approximately 980,000 reads. Class frequencies were imbalanced, and
class weights were calculated to account for class imbalance in this
multi-class setting. The model, trained on stratified subsamples to 5,000
balanced observations, yielded an overall accuracy of 34.2% and F1-scores
greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare
classes (most notably LabZoneC) were often misclassified, even with the use of
class weights. An adjacency-aware confusion matrix was calculated to allow
better interpretation of physically adjacent zones. These results suggest that
RSSI-based decision trees can be applied in realistic simulations to enable
zone-level anomaly detection or misplacement monitoring for defense supply
logistics. Reliable classification performance in low-coverage and low-signal
zones could be improved with better antenna placement or additional sensors and
sensor fusion with other modalities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.20008v1' target='_blank'>Simultaneous learning of state-to-state minimum-time planning and
  control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Swati Dantu, Robert Pěnička, Martin Saska</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 20:20:42</h6>
<p class='card-text'>This paper tackles the challenge of learning a generalizable minimum-time
flight policy for UAVs, capable of navigating between arbitrary start and goal
states while balancing agile flight and stable hovering. Traditional
approaches, particularly in autonomous drone racing, achieve impressive speeds
and agility but are constrained to predefined track layouts, limiting
real-world applicability. To address this, we propose a reinforcement
learning-based framework that simultaneously learns state-to-state minimum-time
planning and control and generalizes to arbitrary state-to-state flights. Our
approach leverages Point Mass Model (PMM) trajectories as proxy rewards to
approximate the true optimal flight objective and employs curriculum learning
to scale the training process efficiently and to achieve generalization. We
validate our method through simulation experiments, comparing it against
Nonlinear Model Predictive Control (NMPC) tracking PMM-generated trajectories
and conducting ablation studies to assess the impact of curriculum learning.
Finally, real-world experiments confirm the robustness of our learned policy in
outdoor environments, demonstrating its ability to generalize and operate on a
small ARM-based single-board computer.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19997v1' target='_blank'>A Framework for the Adoption and Integration of Generative AI in Midsize
  Organizations and Enterprises (FAIGMOE)</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abraham Itzhak Weinberg</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 19:55:31</h6>
<p class='card-text'>Generative Artificial Intelligence (GenAI) presents transformative
opportunities for organizations, yet both midsize organizations and larger
enterprises face distinctive adoption challenges. Midsize organizations
encounter resource constraints and limited AI expertise, while enterprises
struggle with organizational complexity and coordination challenges. Existing
technology adoption frameworks, including TAM (Technology Acceptance Model),
TOE (Technology Organization Environment), and DOI (Diffusion of Innovations)
theory, lack the specificity required for GenAI implementation across these
diverse contexts, creating a critical gap in adoption literature. This paper
introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI
in Midsize Organizations and Enterprises), a conceptual framework addressing
the unique needs of both organizational types. FAIGMOE synthesizes technology
adoption theory, organizational change management, and innovation diffusion
perspectives into four interconnected phases: Strategic Assessment, Planning
and Use Case Development, Implementation and Integration, and
Operationalization and Optimization. Each phase provides scalable guidance on
readiness assessment, strategic alignment, risk governance, technical
architecture, and change management adaptable to organizational scale and
complexity. The framework incorporates GenAI specific considerations including
prompt engineering, model orchestration, and hallucination management that
distinguish it from generic technology adoption frameworks. As a perspective
contribution, FAIGMOE provides the first comprehensive conceptual framework
explicitly addressing GenAI adoption across midsize and enterprise
organizations, offering actionable implementation protocols, assessment
instruments, and governance templates requiring empirical validation through
future research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19949v1' target='_blank'>Surfer 2: The Next Generation of Cross-Platform Computer Use Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mathieu Andreux, Märt Bakler, Yanael Barbier, Hamza Ben Chekroun, Emilien Biré, Antoine Bonnet, Riaz Bordie, Nathan Bout, Matthias Brunel, Aleix Cambray, Pierre-Louis Cedoz, Antoine Chassang, Gautier Cloix, Ethan Connelly, Alexandra Constantinou, Ramzi De Coster, Hubert de la Jonquiere, Aurélien Delfosse, Maxime Delpit, Alexis Deprez, Augustin Derupti, Mathieu Diaz, Shannon D'Souza, Julie Dujardin, Abai Edmund, Michael Eickenberg, Armand Fatalot, Wissem Felissi, Isaac Herring, Xavier Koegler, Erwan Le Jumeau de Kergaradec, Aurélien Lac, Maxime Langevin, Corentin Lauverjat, Antonio Loison, Avshalom Manevich, Axel Moyal, Axel Nguyen Kerbel, Marinela Parovic, Julien Revelle, Guillaume Richard, Mats Richter, Ronan Riochet, María Santos, Romain Savidan, Laurent Sifre, Maxime Theillard, Marc Thibault, Ivan Valentini, Tony Wu, Laura Yie, Kai Yuan, Jevgenij Zubovskij</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 18:21:52</h6>
<p class='card-text'>Building agents that generalize across web, desktop, and mobile environments
remains an open challenge, as prior systems rely on environment-specific
interfaces that limit cross-platform deployment. We introduce Surfer 2, a
unified architecture operating purely from visual observations that achieves
state-of-the-art performance across all three environments. Surfer 2 integrates
hierarchical context management, decoupled planning and execution, and
self-verification with adaptive recovery, enabling reliable operation over long
task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on
WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior
systems without task-specific fine-tuning. With multiple attempts, Surfer 2
exceeds human performance on all benchmarks. These results demonstrate that
systematic orchestration amplifies foundation model capabilities and enables
general-purpose computer control through visual interaction alone, while
calling for a next-generation vision language model to achieve Pareto-optimal
cost-efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19818v1' target='_blank'>Semantic World Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jacob Berg, Chuning Zhu, Yanda Bao, Ishan Durugkar, Abhishek Gupta</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 17:53:45</h6>
<p class='card-text'>Planning with world models offers a powerful paradigm for robotic control.
Conventional approaches train a model to predict future frames conditioned on
current frames and actions, which can then be used for planning. However, the
objective of predicting future pixels is often at odds with the actual planning
objective; strong pixel reconstruction does not always correlate with good
planning decisions. This paper posits that instead of reconstructing future
frames as pixels, world models only need to predict task-relevant semantic
information about the future. For such prediction the paper poses world
modeling as a visual question answering problem about semantic information in
future frames. This perspective allows world modeling to be approached with the
same tools underlying vision language models. Thus vision language models can
be trained as "semantic" world models through a supervised finetuning process
on image-action-text data, enabling planning for decision-making while
inheriting many of the generalization and robustness properties from the
pretrained vision-language models. The paper demonstrates how such a semantic
world model can be used for policy improvement on open-ended robotics tasks,
leading to significant generalization improvements over typical paradigms of
reconstruction-based action-conditional world modeling. Website available at
https://weirdlabuw.github.io/swm.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19808v1' target='_blank'>Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yusu Qian, Eli Bocek-Rivele, Liangchen Song, Jialing Tong, Yinfei Yang, Jiasen Lu, Wenze Hu, Zhe Gan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 17:43:15</h6>
<p class='card-text'>Recent advances in multimodal models have demonstrated remarkable text-guided
image editing capabilities, with systems like GPT-4o and Nano-Banana setting
new benchmarks. However, the research community's progress remains constrained
by the absence of large-scale, high-quality, and openly accessible datasets
built from real images. We introduce Pico-Banana-400K, a comprehensive
400K-image dataset for instruction-based image editing. Our dataset is
constructed by leveraging Nano-Banana to generate diverse edit pairs from real
photographs in the OpenImages collection. What distinguishes Pico-Banana-400K
from previous synthetic datasets is our systematic approach to quality and
diversity. We employ a fine-grained image editing taxonomy to ensure
comprehensive coverage of edit types while maintaining precise content
preservation and instruction faithfulness through MLLM-based quality scoring
and careful curation. Beyond single turn editing, Pico-Banana-400K enables
research into complex editing scenarios. The dataset includes three specialized
subsets: (1) a 72K-example multi-turn collection for studying sequential
editing, reasoning, and planning across consecutive modifications; (2) a
56K-example preference subset for alignment research and reward model training;
and (3) paired long-short editing instructions for developing instruction
rewriting and summarization capabilities. By providing this large-scale,
high-quality, and task-rich resource, Pico-Banana-400K establishes a robust
foundation for training and benchmarking the next generation of text-guided
image editing models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19788v2' target='_blank'>Benchmarking World-Model Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Archana Warrier, Dat Nguyen, Michelangelo Naim, Moksh Jain, Yichao Liang, Karen Schroeder, Cambridge Yang, Joshua B. Tenenbaum, Sebastian Vollmer, Kevin Ellis, Zenna Tavares</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 17:23:18</h6>
<p class='card-text'>Model-learning agents should gather information to learn world models that
support many downstream tasks and inferences, such as predicting unobserved
states, estimating near- and far-term consequences of actions, planning action
sequences, and detecting changes in dynamics. Current methods for learning and
evaluating world models diverge from this goal: training and evaluation are
anchored to next-frame prediction, and success is scored by reward maximization
in the same environment. We propose WorldTest, a protocol to evaluate
model-learning agents that separates reward-free interaction from a scored test
phase in a different but related environment. WorldTest is
open-ended$\unicode{x2014}$models should support many different tasks unknown
ahead of time$\unicode{x2014}$and agnostic to model representation, allowing
comparison across approaches. We instantiated WorldTest with AutumnBench, a
suite of 43 interactive grid-world environments and 129 tasks across three
families: masked-frame prediction, planning, and predicting changes to the
causal dynamics. We compared 517 human participants and three frontier models
on AutumnBench. We found that humans outperform the models, and scaling compute
improves performance only in some environments but not others. WorldTest
provides a novel template$\unicode{x2014}$reward-free exploration, derived
tests, and behavior-based scoring$\unicode{x2014}$to evaluate what agents learn
about environment dynamics, and AutumnBench exposes significant headroom in
world-model learning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19781v1' target='_blank'>Nodal Capacity Expansion Planning with Flexible Large-Scale Load Siting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tomas Valencia Zuluaga, Simon Pang, Jean-Paul Watson</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 17:14:37</h6>
<p class='card-text'>We propose explicitly incorporating large-scale load siting into a stochastic
nodal power system capacity expansion planning model that concurrently
co-optimizes generation, transmission and storage expansion. The potential
operational flexibility of some of these large loads is also taken into account
by considering them as consisting of a set of tranches with different
reliability requirements, which are modeled as a constraint on expected served
energy across operational scenarios. We implement our model as a two-stage
stochastic mixed-integer optimization problem with cross-scenario expectation
constraints. To overcome the challenge of scalability, we build upon existing
work to implement this model on a high performance computing platform and
exploit scenario parallelization using an augmented Progressive Hedging
Algorithm. The algorithm is implemented using the bounding features of mpisppy,
which have shown to provide satisfactory provable optimality gaps despite the
absence of theoretical guarantees of convergence. We test our approach to
assess the value of this proactive planning framework on total system cost and
reliability metrics using realistic testcases geographically assigned to San
Diego and South Carolina, with datacenter and direct air capture facilities as
large loads.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19889v1' target='_blank'>From Optimization to Prediction: Transformer-Based Path-Flow Estimation
  to the Traffic Assignment Problem</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mostafa Ameli, Van Anh Le, Sulthana Shams, Alexander Skabardonis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 16:45:12</h6>
<p class='card-text'>The traffic assignment problem is essential for traffic flow analysis,
traditionally solved using mathematical programs under the Equilibrium
principle. These methods become computationally prohibitive for large-scale
networks due to non-linear growth in complexity with the number of OD pairs.
This study introduces a novel data-driven approach using deep neural networks,
specifically leveraging the Transformer architecture, to predict equilibrium
path flows directly. By focusing on path-level traffic distribution, the
proposed model captures intricate correlations between OD pairs, offering a
more detailed and flexible analysis compared to traditional link-level
approaches. The Transformer-based model drastically reduces computation time,
while adapting to changes in demand and network structure without the need for
recalculation. Numerical experiments are conducted on the Manhattan-like
synthetic network, the Sioux Falls network, and the Eastern-Massachusetts
network. The results demonstrate that the proposed model is orders of magnitude
faster than conventional optimization. It efficiently estimates path-level
traffic flows in multi-class networks, reducing computational costs and
improving prediction accuracy by capturing detailed trip and flow information.
The model also adapts flexibly to varying demand and network conditions,
supporting traffic management and enabling rapid `what-if' analyses for
enhanced transportation planning and policy-making.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19752v1' target='_blank'>Learning Affordances at Inference-Time for Vision-Language-Action Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ameesh Shah, William Chen, Adwait Godbole, Federico Mora, Sanjit A. Seshia, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 16:43:29</h6>
<p class='card-text'>Solving complex real-world control tasks often takes multiple tries: if we
fail at first, we reflect on what went wrong, and change our strategy
accordingly to avoid making the same mistake. In robotics,
Vision-Language-Action models (VLAs) offer a promising path towards solving
complex control tasks, but lack the ability to contextually and dynamically
readjust behavior when they fail to accomplish a task. In this work, we
introduce Learning from Inference-Time Execution (LITEN), which connects a VLA
low-level policy to a high-level VLM that conditions on past experiences by
including them in-context, allowing it to learn the affordances and
capabilities of the low-level VLA. Our approach iterates between a reasoning
phase that generates and executes plans for the low-level VLA, and an
assessment phase that reflects on the resulting execution and draws useful
conclusions to be included in future reasoning contexts. Unlike similar
approaches to self-refinement in non-robotics domains, LITEN must reflect on
unstructured real-world robot trajectories (e.g., raw videos), which requires
structured guiderails during assessment. Our experimental results demonstrate
LITEN is able to effectively learn from past experience to generate plans that
use high-affordance instructions to accomplish long-horizon tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19655v1' target='_blank'>LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision
  Language Navigation in Continuous Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongyu Ding, Ziming Xu, Yudong Fang, You Wu, Zixuan Chen, Jieqi Shi, Jing Huo, Yifan Zhang, Yang Gao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 14:58:16</h6>
<p class='card-text'>Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE)
requires an agent to navigate unseen environments based on natural language
instructions without any prior training. Current methods face a critical
trade-off: either rely on environment-specific waypoint predictors that limit
scene generalization, or underutilize the reasoning capabilities of large
models during navigation. We introduce LaViRA, a simple yet effective zero-shot
framework that addresses this dilemma by decomposing action into a
coarse-to-fine hierarchy: Language Action for high-level planning, Vision
Action for perceptual grounding, and Robot Action for robust navigation. This
modular decomposition allows us to leverage the distinct strengths of different
scales of Multimodal Large Language Models (MLLMs) at each stage, creating a
system that is powerful in its reasoning, grounding and practical control.
LaViRA significantly outperforms existing state-of-the-art methods on the
VLN-CE benchmark, demonstrating superior generalization capabilities in unseen
environments, while maintaining transparency and efficiency for real-world
deployment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19654v1' target='_blank'>From Forecasting to Planning: Policy World Model for Collaborative
  State-Action Prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhida Zhao, Talas Fu, Yifan Wang, Lijun Wang, Huchuan Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 14:57:51</h6>
<p class='card-text'>Despite remarkable progress in driving world models, their potential for
autonomous systems remains largely untapped: the world models are mostly
learned for world simulation and decoupled from trajectory planning. While
recent efforts aim to unify world modeling and planning in a single framework,
the synergistic facilitation mechanism of world modeling for planning still
requires further exploration. In this work, we introduce a new driving paradigm
named Policy World Model (PWM), which not only integrates world modeling and
trajectory planning within a unified architecture, but is also able to benefit
planning using the learned world knowledge through the proposed action-free
future state forecasting scheme. Through collaborative state-action prediction,
PWM can mimic the human-like anticipatory perception, yielding more reliable
planning performance. To facilitate the efficiency of video forecasting, we
further introduce a dynamically enhanced parallel token generation mechanism,
equipped with a context-guided tokenizer and an adaptive dynamic focal loss.
Despite utilizing only front camera input, our method matches or exceeds
state-of-the-art approaches that rely on multi-view and multi-modal inputs.
Code and model weights will be released at
https://github.com/6550Zhao/Policy-World-Model.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19623v1' target='_blank'>Learning and Simulating Building Evacuation Patterns for Enhanced Safety
  Design Using Generative Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jin Han, Zhe Zheng, Yi Gu, Jia-Rui Lin, Xin-Zheng Lu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 14:20:00</h6>
<p class='card-text'>Evacuation simulation is essential for building safety design, ensuring
properly planned evacuation routes. However, traditional evacuation simulation
relies heavily on refined modeling with extensive parameters, making it
challenging to adopt such methods in a rapid iteration process in early design
stages. Thus, this study proposes DiffEvac, a novel method to learn building
evacuation patterns based on Generative Models (GMs), for efficient evacuation
simulation and enhanced safety design. Initially, a dataset of 399 diverse
functional layouts and corresponding evacuation heatmaps of buildings was
established. Then, a decoupled feature representation is proposed to embed
physical features like layouts and occupant density for GMs. Finally, a
diffusion model based on image prompts is proposed to learn evacuation patterns
from simulated evacuation heatmaps. Compared to existing research using
Conditional GANs with RGB representation, DiffEvac achieves up to a 37.6%
improvement in SSIM, 142% in PSNR, and delivers results 16 times faster,
thereby cutting simulation time to 2 minutes. Case studies further demonstrate
that the proposed method not only significantly enhances the rapid design
iteration and adjustment process with efficient evacuation simulation but also
offers new insights and technical pathways for future safety optimization in
intelligent building design. The research implication is that the approach
lowers the modeling burden, enables large-scale what-if exploration, and
facilitates coupling with multi-objective design tools.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19600v1' target='_blank'>Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qianli Ma, Siyu Wang, Yilin Chen, Yinhao Tang, Yixiang Yang, Chang Guo, Bingjie Gao, Zhening Xing, Yanan Sun, Zhipeng Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 13:53:57</h6>
<p class='card-text'>In the quest for scientific progress, communicating research is as vital as
the discovery itself. Yet, researchers are often sidetracked by the manual,
repetitive chore of building project webpages to make their dense papers
accessible. While automation has tackled static slides and posters, the
dynamic, interactive nature of webpages has remained an unaddressed challenge.
To bridge this gap, we reframe the problem, arguing that the solution lies not
in a single command, but in a collaborative, hierarchical process. We introduce
$\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.
AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline
from narrative planning to multimodal content generation and interactive
rendering. To combat AI hallucination, dedicated "Checker" agents verify each
step against the source paper, while optional human checkpoints ensure the
final product aligns perfectly with the author's vision, transforming the
system from a mere tool into a powerful collaborative assistant. To rigorously
validate our approach, we also construct $\textbf{PageBench}$, the first
benchmark for this new task. Experiments show AutoPage not only generates
high-quality, visually appealing pages but does so with remarkable efficiency
in under 15 minutes for less than \$0.1. Code and dataset will be released at
$\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2510.19594v1' target='_blank'>Exploring Synergies between Twinkle and Ariel: a Pilot Study</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Andrea Bocchieri, Luke Booth, Lorenzo V. Mugnai</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-10-22 13:44:38</h6>
<p class='card-text'>Launching in 2027 and 2029, respectively, Twinkle and Ariel will conduct the
first large-scale homogeneous spectroscopic surveys of the atmospheres of
hundreds of diverse exoplanets. This will fundamentally transition the field to
an era of population-level characterisation. In this pilot study, we aim to
explore possible synergies between Twinkle and Ariel to determine for instance
whether prior Twinkle observations can substantially inform the target
selection and observing strategy of Ariel. This study primarily aims to
encourage further investigation by both consortium communities by showing what
a potential scientific synergy would look like on a promising scientific case
that requires further exploration. For this aim, we select a small subset of
"cool" planets that are also particularly well-suited to be observed by Twinkle
and therefore Ariel. By using representative noise estimates for both missions,
we compute the number of visits required for an observation. Then, we simulate
and retrieve transmission spectra of each target, assuming gaseous,
H2/He-dominated atmospheres and various atmospheric models. For all candidates,
we find that atmospheric parameters are generally retrieved well within 1-sigma
to input values, with Ariel typically achieving tighter constraints. We
demonstrate that for a small subset of cool gaseous planets, exploitable
synergies exist between Twinkle and Ariel observations and Twinkle may very
well provide a vantage point to plan Ariel observations. The true extent of the
potential synergies, far beyond our considered sample, will be determined by
the final target lists. Once Twinkle is operational and its performance is
known, it could reliably inform Ariel's target prioritization and Ariel's
capabilities which are already well-established can help define optimal targets
and observational approaches for Twinkle.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>