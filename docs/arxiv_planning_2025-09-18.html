<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-09-18</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-09-18</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.14210v1' target='_blank'>GLIDE: A Coordinated Aerial-Ground Framework for Search and Rescue in
  Unknown Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Seth Farrell, Chenghao Li, Hongzhan Yu, Hesam Mojtahedi, Sicun Gao, Henrik I. Christensen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 17:39:33</h6>
<p class='card-text'>We present a cooperative aerial-ground search-and-rescue (SAR) framework that
pairs two unmanned aerial vehicles (UAVs) with an unmanned ground vehicle (UGV)
to achieve rapid victim localization and obstacle-aware navigation in unknown
environments. We dub this framework Guided Long-horizon Integrated Drone Escort
(GLIDE), highlighting the UGV's reliance on UAV guidance for long-horizon
planning. In our framework, a goal-searching UAV executes real-time onboard
victim detection and georeferencing to nominate goals for the ground platform,
while a terrain-scouting UAV flies ahead of the UGV's planned route to provide
mid-level traversability updates. The UGV fuses aerial cues with local sensing
to perform time-efficient A* planning and continuous replanning as information
arrives. Additionally, we present a hardware demonstration (using a GEM e6 golf
cart as the UGV and two X500 UAVs) to evaluate end-to-end SAR mission
performance and include simulation ablations to assess the planning stack in
isolation from detection. Empirical results demonstrate that explicit role
separation across UAVs, coupled with terrain scouting and guided planning,
improves reach time and navigation safety in time-critical SAR missions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.14201v1' target='_blank'>Active Inference Framework for Closed-Loop Sensing, Communication, and
  Control in UAV Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guangjin Pan, Liping Bai, Zhuojun Tian, Hui Chen, Mehdi Bennis, Henk Wymeersch</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 17:35:07</h6>
<p class='card-text'>Integrated sensing and communication (ISAC) is a core technology for 6G, and
its application to closed-loop sensing, communication, and control (SCC)
enables various services. Existing SCC solutions often treat sensing and
control separately, leading to suboptimal performance and resource usage. In
this work, we introduce the active inference framework (AIF) into SCC-enabled
unmanned aerial vehicle (UAV) systems for joint state estimation, control, and
sensing resource allocation. By formulating a unified generative model, the
problem reduces to minimizing variational free energy for inference and
expected free energy for action planning. Simulation results show that both
control cost and sensing cost are reduced relative to baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.14144v1' target='_blank'>Algorithms for Optimizing Acyclic Queries</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zheng Luo, Wim Van den Broeck, Guy Van den Broeck, Yisu Remy Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 16:25:09</h6>
<p class='card-text'>Most research on query optimization has centered on binary join algorithms
like hash join and sort-merge join. However, recent years have seen growing
interest in theoretically optimal algorithms, notably Yannakakis' algorithm.
These algorithms rely on join trees, which differ from the operator trees for
binary joins and require new optimization techniques. We propose three
approaches to constructing join trees for acyclic queries. First, we give an
algorithm to enumerate all join trees of an alpha-acyclic query by edits with
amortized constant delay, which forms the basis of a cost-based optimizer for
acyclic joins. Second, we show that the Maximum Cardinality Search algorithm by
Tarjan and Yannakakis constructs a unique shallowest join tree, rooted at any
relation, for a Berge-acyclic query; this tree enables parallel execution of
large join queries. Finally, we prove that any connected left-deep linear plan
for a gamma-acyclic query can be converted into a join tree by a simple
algorithm, allowing reuse of optimization infrastructure developed for binary
joins.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.14127v1' target='_blank'>Energy Efficient Multi Robot Package Delivery under Capacity-Constraints
  via Voronoi-Constrained Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alkesh K. Srivastava, Jared Michael Levin, Philip Dames</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 16:08:42</h6>
<p class='card-text'>We consider the problem of delivering multiple packages from a single pickup
depot to distinct goal locations using a homogeneous fleet of robots with
limited carrying capacity. We propose VCST-RCP, a Voronoi-Constrained Steiner
Tree Relay Coordination Planning framework that constructs sparse relay trunks
using Steiner tree optimization and then synthesizes robot-level pickup, relay,
and delivery schedules. This framework reframes relays from incidental
byproducts into central elements of coordination, offering a contrast with
traditional delivery methods that rely on direct source-to-destination
transport. Extensive experiments show consistent improvements of up to 34%
compared to conventional baselines, underscoring the benefits of incorporating
relays into the delivery process. These improvements translate directly to
enhanced energy efficiency in multi-robot delivery under capacity constraints,
providing a scalable framework for real-world logistics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.14082v1' target='_blank'>FlightDiffusion: Revolutionising Autonomous Drone Training with
  Diffusion Models Generating FPV Video</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Valerii Serpiva, Artem Lykov, Faryal Batool, Vladislav Kozlovskiy, Miguel Altamirano Cabrera, Dzmitry Tsetserukou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 15:28:09</h6>
<p class='card-text'>We present FlightDiffusion, a diffusion-model-based framework for training
autonomous drones from first-person view (FPV) video. Our model generates
realistic video sequences from a single frame, enriched with corresponding
action spaces to enable reasoning-driven navigation in dynamic environments.
Beyond direct policy learning, FlightDiffusion leverages its generative
capabilities to synthesize diverse FPV trajectories and state-action pairs,
facilitating the creation of large-scale training datasets without the high
cost of real-world data collection. Our evaluation demonstrates that the
generated trajectories are physically plausible and executable, with a mean
position error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad
(RMSE 0.24 rad). This approach enables improved policy learning and dataset
scalability, leading to superior performance in downstream navigation tasks.
Results in simulated environments highlight enhanced robustness, smoother
trajectory planning, and adaptability to unseen conditions. An ANOVA revealed
no statistically significant difference between performance in simulation and
reality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD =
0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real
transfer. The generated datasets provide a valuable resource for future UAV
research. This work introduces diffusion-based reasoning as a promising
paradigm for unifying navigation, action generation, and data synthesis in
aerial robotics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.14063v1' target='_blank'>Language Conditioning Improves Accuracy of Aircraft Goal Prediction in
  Untowered Airspace</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sundhar Vinodh Sangeetha, Chih-Yuan Chiu, Sarah H. Q. Li, Shreyas Kousik</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 15:09:12</h6>
<p class='card-text'>Autonomous aircraft must safely operate in untowered airspace, where
coordination relies on voice-based communication among human pilots. Safe
operation requires an aircraft to predict the intent, and corresponding goal
location, of other aircraft. This paper introduces a multimodal framework for
aircraft goal prediction that integrates natural language understanding with
spatial reasoning to improve autonomous decision-making in such environments.
We leverage automatic speech recognition and large language models to
transcribe and interpret pilot radio calls, identify aircraft, and extract
discrete intent labels. These intent labels are fused with observed
trajectories to condition a temporal convolutional network and Gaussian mixture
model for probabilistic goal prediction. Our method significantly reduces goal
prediction error compared to baselines that rely solely on motion history,
demonstrating that language-conditioned prediction increases prediction
accuracy. Experiments on a real-world dataset from an untowered airport
validate the approach and highlight its potential to enable socially aware,
language-conditioned robotic motion planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.14025v1' target='_blank'>TransforMARS: Fault-Tolerant Self-Reconfiguration for Arbitrarily Shaped
  Modular Aerial Robot Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rui Huang, Zhiyu Gao, Siyu Tang, Jialin Zhang, Lei He, Ziqian Zhang, Lin Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 14:28:23</h6>
<p class='card-text'>Modular Aerial Robot Systems (MARS) consist of multiple drone modules that
are physically bound together to form a single structure for flight. Exploiting
structural redundancy, MARS can be reconfigured into different formations to
mitigate unit or rotor failures and maintain stable flight. Prior work on MARS
self-reconfiguration has solely focused on maximizing controllability margins
to tolerate a single rotor or unit fault for rectangular-shaped MARS. We
propose TransforMARS, a general fault-tolerant reconfiguration framework that
transforms arbitrarily shaped MARS under multiple rotor and unit faults while
ensuring continuous in-air stability. Specifically, we develop algorithms to
first identify and construct minimum controllable assemblies containing faulty
units. We then plan feasible disassembly-assembly sequences to transport MARS
units or subassemblies to form target configuration. Our approach enables more
flexible and practical feasible reconfiguration. We validate TransforMARS in
challenging arbitrarily shaped MARS configurations, demonstrating substantial
improvements over prior works in both the capacity of handling diverse
configurations and the number of faults tolerated. The videos and source code
of this work are available at the anonymous repository:
https://anonymous.4open.science/r/TransforMARS-1030/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13972v1' target='_blank'>BIM Informed Visual SLAM for Construction Monitoring</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Asier Bikandi, Miguel Fernandez-Cortizas, Muhammad Shaheer, Ali Tourani, Holger Voos, Jose Luis Sanchez-Lopez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 13:45:06</h6>
<p class='card-text'>Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring
construction sites, where aligning the evolving as-built state with the
as-planned design enables early error detection and reduces costly rework.
LiDAR-based SLAM achieves high geometric precision, but its sensors are
typically large and power-demanding, limiting their use on portable platforms.
Visual SLAM offers a practical alternative with lightweight cameras already
embedded in most mobile devices. however, visually mapping construction
environments remains challenging: repetitive layouts, occlusions, and
incomplete or low-texture structures often cause drift in the trajectory map.
To mitigate this, we propose an RGB-D SLAM system that incorporates the
Building Information Model (BIM) as structural prior knowledge. Instead of
relying solely on visual cues, our system continuously establishes
correspondences between detected wall and their BIM counterparts, which are
then introduced as constraints in the back-end optimization. The proposed
method operates in real time and has been validated on real construction sites,
reducing trajectory error by an average of 23.71% and map RMSE by 7.14%
compared to visual SLAM baselines. These results demonstrate that BIM
constraints enable reliable alignment of the digital plan with the as-built
scene, even under partially constructed conditions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13926v1' target='_blank'>MAP: End-to-End Autonomous Driving with Map-Assisted Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Huilin Yin, Yiming Kan, Daniel Watzenig</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 11:40:46</h6>
<p class='card-text'>In recent years, end-to-end autonomous driving has attracted increasing
attention for its ability to jointly model perception, prediction, and planning
within a unified framework. However, most existing approaches underutilize the
online mapping module, leaving its potential to enhance trajectory planning
largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework. MAP explicitly
integrates segmentation-based map features and the current ego status through a
Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and
a Weight Adapter based on current ego status. Experiments conducted on the
DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%
reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a
44.5% improvement in overall score compared to the UniV2X baseline, even
without post-processing. Furthermore, it achieves top ranking in Track 2 of the
End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS
Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of
overall score. These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems. Our code
is available at https://gitee.com/kymkym/map.git</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13903v1' target='_blank'>PhysicalAgent: Towards General Cognitive Robotics with Foundation World
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Artem Lykov, Jeffrin Sam, Hung Khang Nguyen, Vladislav Kozlovskiy, Yara Mahmoud, Valerii Serpiva, Miguel Altamirano Cabrera, Mikhail Konenkov, Dzmitry Tsetserukou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 11:09:03</h6>
<p class='card-text'>We introduce PhysicalAgent, an agentic framework for robotic manipulation
that integrates iterative reasoning, diffusion-based video generation, and
closed-loop execution. Given a textual instruction, our method generates short
video demonstrations of candidate trajectories, executes them on the robot, and
iteratively re-plans in response to failures. This approach enables robust
recovery from execution errors. We evaluate PhysicalAgent across multiple
perceptual modalities (egocentric, third-person, and simulated) and robotic
embodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing
against state-of-the-art task-specific baselines. Experiments demonstrate that
our method consistently outperforms prior approaches, achieving up to 83%
success on human-familiar tasks. Physical trials reveal that first-attempt
success is limited (20-30%), yet iterative correction increases overall success
to 80% across platforms. These results highlight the potential of video-based
generative reasoning for general-purpose robotic manipulation and underscore
the importance of iterative execution for recovering from initial failures. Our
framework paves the way for scalable, adaptable, and robust robot control.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13882v1' target='_blank'>Repulsive Trajectory Modification and Conflict Resolution for Efficient
  Multi-Manipulator Motion Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junhwa Hong, Beomjoon Lee, Woojin Lee, Changjoo Nam</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 10:20:19</h6>
<p class='card-text'>We propose an efficient motion planning method designed to efficiently find
collision-free trajectories for multiple manipulators. While multi-manipulator
systems offer significant advantages, coordinating their motions is
computationally challenging owing to the high dimensionality of their composite
configuration space. Conflict-Based Search (CBS) addresses this by decoupling
motion planning, but suffers from subsequent conflicts incurred by resolving
existing conflicts, leading to an exponentially growing constraint tree of CBS.
Our proposed method is based on repulsive trajectory modification within the
two-level structure of CBS. Unlike conventional CBS variants, the low-level
planner applies a gradient descent approach using an Artificial Potential
Field. This field generates repulsive forces that guide the trajectory of the
conflicting manipulator away from those of other robots. As a result,
subsequent conflicts are less likely to occur. Additionally, we develop a
strategy that, under a specific condition, directly attempts to find a
conflict-free solution in a single step without growing the constraint tree.
Through extensive tests including physical robot experiments, we demonstrate
that our method consistently reduces the number of expanded nodes in the
constraint tree, achieves a higher success rate, and finds a solution faster
compared to Enhanced CBS and other state-of-the-art algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13839v1' target='_blank'>Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and
  Transformer Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Motonari Kambara, Komei Sugiura</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 09:11:49</h6>
<p class='card-text'>In this work, we address the problem of predicting the future success of
open-vocabulary object manipulation tasks. Conventional approaches typically
determine success or failure after the action has been carried out. However,
they make it difficult to prevent potential hazards and rely on failures to
trigger replanning, thereby reducing the efficiency of object manipulation
sequences. To overcome these challenges, we propose a model, which predicts the
alignment between a pre-manipulation egocentric image with the planned
trajectory and a given natural language instruction. We introduce a Multi-Level
Trajectory Fusion module, which employs a state-of-the-art deep state-space
model and a transformer encoder in parallel to capture multi-level time-series
self-correlation within the end effector trajectory. Our experimental results
indicate that the proposed method outperformed existing methods, including
foundation models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13822v1' target='_blank'>Flow Matching-Based Active Learning for Radio Map Construction with
  Low-Altitude UAVs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hao Sun, Shicong Liu, Xianghao Yu, Ying Sun</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 08:44:03</h6>
<p class='card-text'>The employment of unmanned aerial vehicles (UAVs) in the lowaltitude economy
necessitates precise and real-time radio maps for reliable communication and
safe navigation. However, constructing such maps is hindered by the
infeasibility of exhaustive measurements due to UAVs' limited flight endurance.
To address this, we propose a novel active learning framework for low-altitude
radio map construction based on limited measurements. First, a Plug-and-Play
(PnP)-refined flow matching algorithm is introduced, which leverages flow
matching as a powerful generative prior within a PnP scheme to reconstruct
high-fidelity radio maps. Second, the generative nature of flow matching is
exploited to quantify uncertainty by generating an ensemble of radio maps and
computing the location-wise variance. The resulting uncertainty map guides a
multi-objective candidate selection and then a trajectory is planned via
utility-aware path search (UAPS), directing the UAV to the most informative
locations while taking travel costs into account. Simulation results
demonstrate that our method significantly outperforms the baselines, achieving
more than a 70% reduction in normalized mean squared error (NMSE).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13802v1' target='_blank'>Shell-Type Soft Jig for Holding Objects during Disassembly</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Takuya Kiyokawa, Ryunosuke Takebayashi, Kensuke Harada</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 08:17:07</h6>
<p class='card-text'>This study addresses a flexible holding tool for robotic disassembly. We
propose a shell-type soft jig that securely and universally holds objects,
mitigating the risk of component damage and adapting to diverse shapes while
enabling soft fixation that is robust to recognition, planning, and control
errors. The balloon-based holding mechanism ensures proper alignment and stable
holding performance, thereby reducing the need for dedicated jig design, highly
accurate perception, precise grasping, and finely tuned trajectory planning
that are typically required with conventional fixtures. Our experimental
results demonstrate the practical feasibility of the proposed jig through
performance comparisons with a vise and a jamming-gripper-inspired soft jig.
Tests on ten different objects further showed representative successes and
failures, clarifying the jig's limitations and outlook.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13771v1' target='_blank'>CDFlow: Generative Gradient Flows for Configuration Space Distance
  Fields via Neural ODEs</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mengzhu Li, Yunyu Zhou, He Ying, F. Richard Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 07:38:47</h6>
<p class='card-text'>Signed Distance Fields (SDFs) are a fundamental representation in robot
motion planning. Their configuration-space counterpart, the Configuration Space
Distance Field (CDF), directly encodes distances in joint space, offering a
unified representation for optimization and control. However, existing CDF
formulations face two major challenges in high-degree-of-freedom (DoF) robots:
(1) they effectively return only a single nearest collision configuration,
neglecting the multi-modal nature of minimal-distance collision configurations
and leading to gradient ambiguity; and (2) they rely on sparse sampling of the
collision boundary, which often fails to identify the true closest
configurations, producing oversmoothed approximations and geometric distortion
in high-dimensional spaces. We propose CDFlow, a novel framework that addresses
these limitations by learning a continuous flow in configuration space via
Neural Ordinary Differential Equations (Neural ODEs). We redefine the problem
from finding a single nearest point to modeling the distribution of
minimal-distance collision configurations. We also introduce an adaptive
refinement sampling strategy to generate high-fidelity training data for this
distribution. The resulting Neural ODE implicitly models this multi-modal
distribution and produces a smooth, consistent gradient field-derived as the
expected direction towards the distribution-that mitigates gradient ambiguity
and preserves sharp geometric features. Extensive experiments on high-DoF
motion planning tasks demonstrate that CDFlow significantly improves planning
efficiency, trajectory quality, and robustness compared to existing CDF-based
methods, enabling more robust and efficient planning for collision-aware robots
in complex environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13733v1' target='_blank'>FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with
  Hierarchical Multi-modal Scene Graph</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaolin Zhou, Tingyang Xiao, Liu Liu, Yucheng Wang, Maiyue Chen, Xinrui Meng, Xinjie Wang, Wei Feng, Wei Sui, Zhizhong Su</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 06:36:41</h6>
<p class='card-text'>Visual-Language Navigation (VLN) is a fundamental challenge in robotic
systems, with broad applications for the deployment of embodied agents in
real-world environments. Despite recent advances, existing approaches are
limited in long-range spatial reasoning, often exhibiting low success rates and
high inference latency, particularly in long-range navigation tasks. To address
these limitations, we propose FSR-VLN, a vision-language navigation system that
combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow
Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation
supporting progressive retrieval, from coarse room-level localization to
fine-grained goal view and object identification. Building on HMSG, FSR first
performs fast matching to efficiently select candidate rooms, views, and
objects, then applies VLM-driven refinement for final goal selection. We
evaluated FSR-VLN across four comprehensive indoor datasets collected by
humanoid robots, utilizing 87 instructions that encompass a diverse range of
object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all
datasets, measured by the retrieval success rate (RSR), while reducing the
response time by 82% compared to VLM-based methods on tour videos by activating
slow reasoning only when fast intuition fails. Furthermore, we integrate
FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1
humanoid robot, enabling natural language interaction and real-time navigation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13692v1' target='_blank'>HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point
  Cloud Completion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yadan Zeng, Jiadong Zhou, Xiaohan Li, I-Ming Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 04:49:44</h6>
<p class='card-text'>Point cloud completion is essential for robotic perception, object
reconstruction and supporting downstream tasks like grasp planning, obstacle
avoidance, and manipulation. However, incomplete geometry caused by
self-occlusion and sensor limitations can significantly degrade downstream
reasoning and interaction. To address these challenges, we propose HGACNet, a
novel framework that reconstructs complete point clouds of individual objects
by hierarchically encoding 3D geometric features and fusing them with
image-guided priors from a single-view RGB image. At the core of our approach,
the Hierarchical Graph Attention (HGA) encoder adaptively selects critical
local points through graph attention-based downsampling and progressively
refines hierarchical geometric features to better capture structural continuity
and spatial relationships. To strengthen cross-modal interaction, we further
design a Multi-Scale Cross-Modal Fusion (MSCF) module that performs
attention-based feature alignment between hierarchical geometric features and
structured visual representations, enabling fine-grained semantic guidance for
completion. In addition, we proposed the contrastive loss (C-Loss) to
explicitly align the feature distributions across modalities, improving
completion fidelity under modality discrepancy. Finally, extensive experiments
conducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset
confirm the effectiveness of HGACNet, demonstrating state-of-the-art
performance as well as strong applicability in real-world robotic manipulation
tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13674v1' target='_blank'>Scaling green hydrogen and CCUS via cement-methanol co-production in
  China</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuezhang He, Hongxi Luo, Yuancheng Lin, Carl J. Talsma, Anna Li, Zhenqian Wang, Yujuan Fang, Pei Liu, Jesse D. Jenkins, Eric Larson, Zheng Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 04:02:40</h6>
<p class='card-text'>High costs of green hydrogen and of carbon capture, utilization, and
sequestration (CCUS) have hindered policy ambition and slowed real-world
deployment, despite their importance for decarbonizing hard-to-abate sectors,
including cement and methanol. Given the economic challenges of adopting CCUS
in cement and green hydrogen in methanol production separately, we propose a
renewable-powered co-production system that couples electrolytic hydrogen and
CCUS through molecule exchange. We optimize system configurations using an
hourly-resolved, process-based model incorporating operational flexibility, and
explore integrated strategies for plant-level deployment and CO2 source-sink
matching across China. We find that co-production could reduce CO2 abatement
costs to USD 41-53 per tonne by 2035, significantly lower than approximately
USD 75 for standalone cement CCUS and over USD 120 for standalone
renewable-based methanol. Co-production is preferentially deployed at cement
plants in renewable-rich regions, potentially reshaping national CO2
infrastructure planning. This hydrogen-CCUS coupling paradigm could accelerate
industrial decarbonization and scaling for other applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13646v1' target='_blank'>Vistoria: A Multimodal System to Support Fictional Story Writing through
  Instrumental Text-Image Co-Editing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kexue Fu, Jingfei Huang, Long Ling, Sumin Hong, Yihang Zuo, Ray LC, Toby Jia-jun Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 02:51:15</h6>
<p class='card-text'>Humans think visually-we remember in images, dream in pictures, and use
visual metaphors to communicate. Yet, most creative writing tools remain
text-centric, limiting how authors plan and translate ideas. We present
Vistoria, a system for synchronized text-image co-editing in fictional story
writing that treats visuals and text as coequal narrative materials. A
formative Wizard-of-Oz co-design study with 10 story writers revealed how
sketches, images, and annotations serve as essential instruments for ideation
and organization. Drawing on theories of Instrumental Interaction and
Structural Mapping, Vistoria introduces multimodal operations-lasso, collage,
filters, and perspective shifts that enable seamless narrative exploration
across modalities. A controlled study with 12 participants shows that
co-editing enhances expressiveness, immersion, and collaboration, enabling
writers to explore divergent directions, embrace serendipitous randomness, and
trace evolving storylines. While multimodality increased cognitive demand,
participants reported stronger senses of authorship and agency. These findings
demonstrate how multimodal co-editing expands creative potential by balancing
abstraction and concreteness in narrative development.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13637v1' target='_blank'>Delta Matters: An Analytically Tractable Model for $β$-$δ$
  Discounting Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yasunori Akagi, Takeshi Kurashima</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 02:24:12</h6>
<p class='card-text'>Humans exhibit time-inconsistent behavior, in which planned actions diverge
from executed actions. Understanding time inconsistency and designing
appropriate interventions is a key research challenge in computer science and
behavioral economics. Previous work focuses on progress-based tasks and derives
a closed-form description of agent behavior, from which they obtain optimal
intervention strategies. They model time-inconsistency using the
$\beta$-$\delta$ discounting (quasi-hyperbolic discounting), but the analysis
is limited to the case $\delta = 1$. In this paper, we relax that constraint
and show that a closed-form description of agent behavior remains possible for
the general case $0 < \delta \le 1$. Based on this result, we derive the
conditions under which agents abandon tasks and develop efficient methods for
computing optimal interventions. Our analysis reveals that agent behavior and
optimal interventions depend critically on the value of $\delta$, suggesting
that fixing $\delta = 1$ in many prior studies may unduly simplify real-world
decision-making processes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13633v1' target='_blank'>DeepLogit: A sequentially constrained explainable deep learning modeling
  approach for transport policy analysis</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jeremy Oon, Rakhi Manohar Mepparambath, Ling Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-17 02:08:34</h6>
<p class='card-text'>Despite the significant progress of deep learning models in multitude of
applications, their adaption in planning and policy related areas remains
challenging due to the black-box nature of these models. In this work, we
develop a set of DeepLogit models that follow a novel sequentially constrained
approach in estimating deep learning models for transport policy analysis. In
the first step of the proposed approach, we estimate a convolutional neural
network (CNN) model with only linear terms, which is equivalent of a
linear-in-parameter multinomial logit model. We then estimate other deep
learning models by constraining the parameters that need interpretability at
the values obtained in the linear-in-parameter CNN model and including higher
order terms or by introducing advanced deep learning architectures like
Transformers. Our approach can retain the interpretability of the selected
parameters, yet provides significantly improved model accuracy than the
discrete choice model. We demonstrate our approach on a transit route choice
example using real-world transit smart card data from Singapore. This study
shows the potential for a unifying approach, where theory-based discrete choice
model (DCM) and data-driven AI models can leverage each other's strengths in
interpretability and predictive power. With the availability of larger datasets
and more complex constructions, such approach can lead to more accurate models
using discrete choice models while maintaining its applicability in planning
and policy-related areas. Our code is available on
https://github.com/jeremyoon/route-choice/ .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13595v1' target='_blank'>Leg-Arm Coordinated Operation for Curtain Wall Installation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiao Liu, Weijun Wang, Tianlun Huang, Zhiyong Wang, Wei Feng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 23:42:42</h6>
<p class='card-text'>With the acceleration of urbanization, the number of high-rise buildings and
large public facilities is increasing, making curtain walls an essential
component of modern architecture with widespread applications. Traditional
curtain wall installation methods face challenges such as variable on-site
terrain, high labor intensity, low construction efficiency, and significant
safety risks. Large panels often require multiple workers to complete
installation. To address these issues, based on a hexapod curtain wall
installation robot, we design a hierarchical optimization-based whole-body
control framework for coordinated arm-leg planning tailored to three key tasks:
wall installation, ceiling installation, and floor laying. This framework
integrates the motion of the hexapod legs with the operation of the folding arm
and the serial-parallel manipulator. We conduct experiments on the hexapod
curtain wall installation robot to validate the proposed control method,
demonstrating its capability in performing curtain wall installation tasks. Our
results confirm the effectiveness of the hierarchical optimization-based
arm-leg coordination framework for the hexapod robot, laying the foundation for
its further application in complex construction site environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13579v1' target='_blank'>TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Momchil S. Tomov, Sang Uk Lee, Hansford Hendrago, Jinwook Huh, Teawon Han, Forbes Howington, Rafael da Silva, Gianmarco Bernasconi, Marc Heim, Samuel Findler, Xiaonan Ji, Alexander Boule, Michael Napoli, Kuo Chen, Jesse Miller, Boaz Floor, Yunqing Hu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 22:37:37</h6>
<p class='card-text'>We present TreeIRL, a novel planner for autonomous driving that combines
Monte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to
achieve state-of-the-art performance in simulation and in real-world driving.
The core idea is to use MCTS to find a promising set of safe candidate
trajectories and a deep IRL scoring function to select the most human-like
among them. We evaluate TreeIRL against both classical and state-of-the-art
planners in large-scale simulations and on 500+ miles of real-world autonomous
driving in the Las Vegas metropolitan area. Test scenarios include dense urban
traffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves
the best overall performance, striking a balance between safety, progress,
comfort, and human-likeness. To our knowledge, our work is the first
demonstration of MCTS-based planning on public roads and underscores the
importance of evaluating planners across a diverse set of metrics and in
real-world environments. TreeIRL is highly extensible and could be further
improved with reinforcement learning and imitation learning, providing a
framework for exploring different combinations of classical and learning-based
approaches to solve the planning bottleneck in autonomous driving.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13572v1' target='_blank'>Using Visual Language Models to Control Bionic Hands: Assessment of
  Object Perception and Grasp Inference</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ozan Karaali, Hossam Farag, Strahinja Dosen, Cedomir Stefanovic</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 22:27:26</h6>
<p class='card-text'>This study examines the potential of utilizing Vision Language Models (VLMs)
to improve the perceptual capabilities of semi-autonomous prosthetic hands. We
introduce a unified benchmark for end-to-end perception and grasp inference,
evaluating a single VLM to perform tasks that traditionally require complex
pipelines with separate modules for object detection, pose estimation, and
grasp planning. To establish the feasibility and current limitations of this
approach, we benchmark eight contemporary VLMs on their ability to perform a
unified task essential for bionic grasping. From a single static image, they
should (1) identify common objects and their key properties (name, shape,
orientation, and dimensions), and (2) infer appropriate grasp parameters (grasp
type, wrist rotation, hand aperture, and number of fingers). A corresponding
prompt requesting a structured JSON output was employed with a dataset of 34
snapshots of common objects. Key performance metrics, including accuracy for
categorical attributes (e.g., object name, shape) and errors in numerical
estimates (e.g., dimensions, hand aperture), along with latency and cost, were
analyzed. The results demonstrated that most models exhibited high performance
in object identification and shape recognition, while accuracy in estimating
dimensions and inferring optimal grasp parameters, particularly hand rotation
and aperture, varied more significantly. This work highlights the current
capabilities and limitations of VLMs as advanced perceptual modules for
semi-autonomous control of bionic limbs, demonstrating their potential for
effective prosthetic applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13561v1' target='_blank'>GuardianPWA: Enhancing Security Throughout the Progressive Web App
  Installation Lifecycle</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mengxiao Wang, Guofei Gu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 22:01:18</h6>
<p class='card-text'>Progressive Web App (PWA) installation is critical for integrating web and
mobile app functionalities, offering a seamless user experience. However,
ensuring the security of the PWA installation lifecycle is essential for
maintaining user trust and privacy. This paper introduces the GUARDIANPWA
framework, a comprehensive approach to analyzing the PWA installation mechanism
based on the CIA security principles (Confidentiality, Integrity, and
Availability) and identifying areas where browser vendors fail to comply with
these principles. Our study revealed 203 instances of non-compliance with
security principles, highlighting how these irregularities in the PWA
installation lifecycle can lead to potential violations of user privacy. For
instance, in Firefox, PWAs installed in private mode incorrectly appear in
normal mode, risking user confidentiality. Additionally, 29,465 PWAs are at
risk because Samsung Internet does not display origins when PWAs navigate to
third-party websites, undermining integrity. These findings were reported to
browser vendors, leading to Firefox acknowledging four issues, resolving one,
and planning to resolve two others. GUARDIANPWA supports developers by
analyzing PWA manifest files for syntactic and semantic correctness, offering
actionable recommendations, and helping to create PWAs that align with security
best practices. By using GUARDIANPWA, developers and users can address critical
security gaps and enhance compliance with CIA principles throughout the PWA
installation lifecycle.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13501v1' target='_blank'>Trajectory Tracking with Reachability-Guided Quadratic Programming and
  Freeze-Resume</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hossein Gholampour, Logan E. Beaver</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 19:56:16</h6>
<p class='card-text'>Many robotic systems must follow planned paths yet pause safely and resume
when people or objects intervene. We present an output-space method for systems
whose tracked output can be feedback-linearized to a double integrator (e.g.,
manipulators). The approach has two parts. Offline, we perform a pre-run
reachability check to verify that the motion plan respects speed and
acceleration magnitude limits. Online, we apply a quadratic program to track
the motion plan under the same limits. We use a one-step reachability test to
bound the maximum disturbance the system is capable of rejecting. When the
state coincides with the reference path we recover perfect tracking in the
deterministic case, and we correct errors using a KKT-inspired weight. We
demonstrate that safety stops and unplanned deviations are handled efficiently,
and the system returns to the motion plan without replanning. We demonstrate
our system's improved performance over pure pursuit in simulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13484v1' target='_blank'>MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Liu Liu, Alexandra Kudaeva, Marco Cipriano, Fatimeh Al Ghannam, Freya Tan, Gerard de Melo, Andres Sevtsuk</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 19:31:40</h6>
<p class='card-text'>Understanding group-level social interactions in public spaces is crucial for
urban planning, informing the design of socially vibrant and inclusive
environments. Detecting such interactions from images involves interpreting
subtle visual cues such as relations, proximity, and co-movement - semantically
complex signals that go beyond traditional object detection. To address this
challenge, we introduce a social group region detection task, which requires
inferring and spatially grounding visual regions defined by abstract
interpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level
Engagement), a modular three-stage pipeline that integrates: (1) off-the-shelf
human detection and depth estimation, (2) VLM-based reasoning to classify
pairwise social affiliation, and (3) a lightweight spatial aggregation
algorithm to localize socially connected groups. To support this task and
encourage future research, we present a new dataset of 100K urban street-view
images annotated with bounding boxes and labels for both individuals and
socially interacting groups. The annotations combine human-created labels and
outputs from the MINGLE pipeline, ensuring semantic richness and broad coverage
of real-world scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13465v1' target='_blank'>Magnetic measurements of Fermilab rapid-cycling Booster gradient magnets</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:J. DiMarco, D. Assell, T. Cummings, D. Johnson, V. Kashikhin, M. Kifarkis, J. Kuharik, J. Larson, M. Mubarak, S. Poopathi, K. Triplett</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 19:02:54</h6>
<p class='card-text'>Fermilab is upgrading its Booster synchrotron to increase ramp rate and
intensity. This is part of the Proton Improvement Plan (PIP-II) that will allow
the Main Injector to achieve proton beam power of 1.2 MW within the next few
years. This upgrade includes running the 55-year-old Booster magnets at 20 Hz
instead of the usual 15 Hz, and construction of some shorter and wider aperture
versions of these combined-function gradient magnets. Magnetic measurements
were performed to characterize the present 15 Hz AC performance, and then again
with 20 Hz ramp cycle to ensure performance and compatibility in this new
operating regime. A 3 m-long curved flat-coil was developed for these
measurements using Printed Circuit Board (PCB) technology. The probe also has a
separate 0.5 m-long body-field probe, allowing integral, body, and end fields
to be measured across 100 mm of the magnet aperture. The sampling rate for
these measurements during the AC cycle was 200 kHz, and field resolution was
better than 0.01%. Details of the probe, measurements, and results are
presented.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13437v1' target='_blank'>A new perspective on the CMSSM: Yukawa Unification, DM and the SUSY
  scale</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Stefan Antusch, Shaikh Saad, Vasja Susič</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 18:14:12</h6>
<p class='card-text'>What does third family ($t$-$b$-$\tau$) Yukawa unification, a typical
prediction from embedding the Standard Model (SM) fermions in 16-plets of a
$\mathrm{SO}(10)$ GUT, imply for the scale of the supersymmetric (SUSY)
partners? Which neutralino dark matter (DM) candidate can be realized, and how
large is the DM relic density? In this work, we address these questions in a
simplified SUSY-breaking framework: the Constrained Minimal Supersymmetric
Standard Model (CMSSM). To this end, we recast the parameter space of the CMSSM
in a way that for all parameter points the SM-like Higgs mass is correctly
reproduced. Considering fixed $\tan\beta$ and $\mathrm{sgn}(\mu)$, for every
point in the $(x:=\frac{M_{1/2}}{m_0},y:=\frac{A_0}{m_0})$ parameter plane
ranges for all observables are predicted. This provides a new perspective on
where in parameter space different types of DM are realized, and which value of
the SUSY scale is required in order to explain the observed mass of the SM
Higgs boson. In our analysis we consider and compare two strategies: grid scans
over the $(x,y,\tan\beta)$ parameter region and MCMC sampling. We find both
techniques yield similar results. For $t$-$b$-$\tau$ unification within $5\,\%$
or $10\,\%$, we find $\mu<0$, the SUSY spectrum showing a characteristic
pattern, and the SUSY scale around $\mathcal{O}(10)\,\mathrm{TeV}$. The extra
MSSM Higgses are the lowest lying new states at $\sim 2\div 3\,\mathrm{TeV}$
(with discovery potential at the HL-LHC), the $\mathcal{O}(10)\,\mathrm{TeV}$
stops and gluino are in reach of a possible FCC-hh, while bino DM has a mass
above $2.5\,\mathrm{TeV}$, is overabundant, and effectively unobservable in
planned direct and indirect detection experiments. The DM relic density
requires a dilution factor of $10<\mathcal{D}<1000$, implying non-standard
cosmology that could leave its imprints in the stochastic gravitational wave
background.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2509.13425v1' target='_blank'>Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework
  for Modeling Complex Predator-Prey Dynamics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Julian Evan Chrisnanto, Yulison Herry Chrisnanto, Ferry Faizal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-09-16 18:02:23</h6>
<p class='card-text'>Ecological systems exhibit complex multi-scale dynamics that challenge
traditional modeling. New methods must capture temporal oscillations and
emergent spatiotemporal patterns while adhering to conservation principles. We
present the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,
a deep learning architecture integrating physics-informed neural networks
(PINNs) and conservation laws to model predator-prey dynamics across
dimensional scales. The framework provides a unified solution for both ordinary
(ODE) and partial (PDE) differential equation systems, describing temporal
cycles and reaction-diffusion patterns within a single neural network
architecture. Our methodology uses automatic differentiation to enforce physics
constraints and adaptive loss weighting to balance data fidelity with physical
consistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%
correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures
complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).
Validation confirms conservation law adherence within 0.5% and shows a 10-50x
computational speedup for inference compared to numerical solvers. USPIL also
enables mechanistic understanding through interpretable physics constraints,
facilitating parameter discovery and sensitivity analysis not possible with
purely data-driven methods. Its ability to transition between dimensional
formulations opens new avenues for multi-scale ecological modeling. These
capabilities make USPIL a transformative tool for ecological forecasting,
conservation planning, and understanding ecosystem resilience, establishing
physics-informed deep learning as a powerful and scientifically rigorous
paradigm.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>