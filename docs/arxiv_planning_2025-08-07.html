<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>planning - 2025-08-07</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>planning - 2025-08-07</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04642v1' target='_blank'>RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Baihui Xiao, Chengjian Feng, Zhijian Huang, Feng yan, Yujie Zhong, Lin Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 17:07:25</h6>
<p class='card-text'>Collecting real-world data for rare high-risk scenarios, long-tailed driving
events, and complex interactions remains challenging, leading to poor
performance of existing autonomous driving systems in these critical
situations. In this paper, we propose RoboTron-Sim that improves real-world
driving in critical situations by utilizing simulated hard cases. First, we
develop a simulated dataset called Hard-case Augmented Synthetic Scenarios
(HASS), which covers 13 high-risk edge-case categories, as well as balanced
environmental conditions such as day/night and sunny/rainy. Second, we
introduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder
(I2E Encoder) to enable multimodal large language models to effectively learn
real-world challenging driving skills from HASS, via adapting to environmental
deviations and hardware differences between real-world and simulated scenarios.
Extensive experiments on nuScenes show that RoboTron-Sim improves driving
performance in challenging scenarios by around 50%, achieving state-of-the-art
results in real-world open-loop planning. Qualitative results further
demonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk
driving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04592v1' target='_blank'>Face-voice Association in Multilingual Environments (FAME) 2026
  Challenge Evaluation Plan</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marta Moscati, Ahmed Abdullah, Muhammad Saad Saeed, Shah Nawaz, Rohan Kumar Das, Muhammad Zaigham Zaheer, Junaid Mir, Muhammad Haroon Yousaf, Khalid Malik, Markus Schedl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 16:09:47</h6>
<p class='card-text'>The advancements of technology have led to the use of multimodal systems in
various real-world applications. Among them, audio-visual systems are among the
most widely used multimodal systems. In the recent years, associating face and
voice of a person has gained attention due to the presence of unique
correlation between them. The Face-voice Association in Multilingual
Environments (FAME) 2026 Challenge focuses on exploring face-voice association
under the unique condition of a multilingual scenario. This condition is
inspired from the fact that half of the world's population is bilingual and
most often people communicate under multilingual scenarios. The challenge uses
a dataset named Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice
association in multilingual environments. This report provides the details of
the challenge, dataset, baseline models, and task details for the FAME
Challenge.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04552v1' target='_blank'>Augmentation-based Domain Generalization and Joint Training from
  Multiple Source Domains for Whole Heart Segmentation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Franz Thaler, Darko Stern, Gernot Plank, Martin Urschler</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 15:37:22</h6>
<p class='card-text'>As the leading cause of death worldwide, cardiovascular diseases motivate the
development of more sophisticated methods to analyze the heart and its
substructures from medical images like Computed Tomography (CT) and Magnetic
Resonance (MR). Semantic segmentations of important cardiac structures that
represent the whole heart are useful to assess patient-specific cardiac
morphology and pathology. Furthermore, accurate semantic segmentations can be
used to generate cardiac digital twin models which allows e.g.
electrophysiological simulation and personalized therapy planning. Even though
deep learning-based methods for medical image segmentation achieved great
advancements over the last decade, retaining good performance under domain
shift -- i.e. when training and test data are sampled from different data
distributions -- remains challenging. In order to perform well on domains known
at training-time, we employ a (1) balanced joint training approach that
utilizes CT and MR data in equal amounts from different source domains.
Further, aiming to alleviate domain shift towards domains only encountered at
test-time, we rely on (2) strong intensity and spatial augmentation techniques
to greatly diversify the available training data. Our proposed whole heart
segmentation method, a 5-fold ensemble with our contributions, achieves the
best performance for MR data overall and a performance similar to the best
performance for CT data when compared to a model trained solely on CT. With
93.33% DSC and 0.8388 mm ASSD for CT and 89.30% DSC and 1.2411 mm ASSD for MR
data, our method demonstrates great potential to efficiently obtain accurate
semantic segmentations from which patient-specific cardiac twin models can be
generated.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04537v1' target='_blank'>Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone,
  Communication-Denied Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alkesh K. Srivastava, Aamodh Suresh, Carlos Nieto-Granda</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 15:23:22</h6>
<p class='card-text'>We address the challenge of multi-robot autonomous hazard mapping in
high-risk, failure-prone, communication-denied environments such as
post-disaster zones, underground mines, caves, and planetary surfaces. In these
missions, robots must explore and map hazards while minimizing the risk of
failure due to environmental threats or hardware limitations. We introduce a
behavior-adaptive, information-theoretic planning framework for multi-robot
teams grounded in the concept of Behavioral Entropy (BE), that generalizes
Shannon entropy (SE) to capture diverse human-like uncertainty evaluations.
Building on this formulation, we propose the Behavior-Adaptive Path Planning
(BAPP) framework, which modulates information gathering strategies via a
tunable risk-sensitivity parameter, and present two planning algorithms:
BAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for
safe deployment under high risk. We provide theoretical insights on the
informativeness of the proposed BAPP framework and validate its effectiveness
through both single-robot and multi-robot simulations. Our results show that
the BAPP stack consistently outperforms Shannon-based and random strategies:
BAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot
survivability with minimal loss in information gain. In multi-agent
deployments, BAPP scales effectively through spatial partitioning, mobile base
relocation, and role-aware heterogeneity. These findings underscore the value
of behavior-adaptive planning for robust, risk-sensitive exploration in
complex, failure-prone environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04529v1' target='_blank'>ESDD 2026: Environmental Sound Deepfake Detection Challenge Evaluation
  Plan</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Han Yin, Yang Xiao, Rohan Kumar Das, Jisheng Bai, Ting Dang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 15:09:44</h6>
<p class='card-text'>Recent advances in audio generation systems have enabled the creation of
highly realistic and immersive soundscapes, which are increasingly used in film
and virtual reality. However, these audio generators also raise concerns about
potential misuse, such as generating deceptive audio content for fake videos
and spreading misleading information. Existing datasets for environmental sound
deepfake detection (ESDD) are limited in scale and audio types. To address this
gap, we have proposed EnvSDD, the first large-scale curated dataset designed
for ESDD, consisting of 45.25 hours of real and 316.7 hours of fake sound.
Based on EnvSDD, we are launching the Environmental Sound Deepfake Detection
Challenge. Specifically, we present two different tracks: ESDD in Unseen
Generators and Black-Box Low-Resource ESDD, covering various challenges
encountered in real-life scenarios. The challenge will be held in conjunction
with the 2026 IEEE International Conference on Acoustics, Speech, and Signal
Processing (ICASSP 2026).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04486v1' target='_blank'>Quantum circuit complexity and unsupervised machine learning of
  topological order</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanming Che, Clemens Gneiting, Xiaoguang Wang, Franco Nori</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 14:36:10</h6>
<p class='card-text'>Inspired by the close relationship between Kolmogorov complexity and
unsupervised machine learning, we explore quantum circuit complexity, an
important concept in quantum computation and quantum information science, as a
pivot to understand and to build interpretable and efficient unsupervised
machine learning for topological order in quantum many-body systems. To span a
bridge from conceptual power to practical applicability, we present two
theorems that connect Nielsen's quantum circuit complexity for the quantum path
planning between two arbitrary quantum many-body states with fidelity change
and entanglement generation, respectively. Leveraging these connections,
fidelity-based and entanglement-based similarity measures or kernels, which are
more practical for implementation, are formulated. Using the two proposed
kernels, numerical experiments targeting the unsupervised clustering of quantum
phases of the bond-alternating XXZ spin chain, the ground state of Kitaev's
toric code and random product states, are conducted, demonstrating their
superior performance. Relations with classical shadow tomography and shadow
kernel learning are also discussed, where the latter can be naturally derived
and understood from our approach. Our results establish connections between key
concepts and tools of quantum circuit computation, quantum complexity, and
machine learning of topological quantum order.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04442v1' target='_blank'>Automated Generation of Curriculum-Aligned Multiple-Choice Questions for
  Malaysian Secondary Mathematics Using Generative AI</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rohaizah Abdul Wahid, Muhamad Said Nizamuddin Nadim, Suliana Sulaiman, Syahmi Akmal Shaharudin, Muhammad Danial Jupikil, Iqqwan Jasman Su Azlan Su</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 13:30:51</h6>
<p class='card-text'>This paper addresses the critical need for scalable and high-quality
educational assessment tools within the Malaysian education system. It
highlights the potential of Generative AI (GenAI) while acknowledging the
significant challenges of ensuring factual accuracy and curriculum alignment,
especially for low-resource languages like Bahasa Melayu. This research
introduces and compares four incremental pipelines for generating Form 1
Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's
GPT-4o. The methods range from non-grounded prompting (structured and basic) to
Retrieval-Augmented Generation (RAG) approaches (one using the LangChain
framework, one implemented manually). The system is grounded in official
curriculum documents, including teacher-prepared notes and the yearly teaching
plan (RPT). A dual-pronged automated evaluation framework is employed to assess
the generated questions. Curriculum alignment is measured using Semantic
Textual Similarity (STS) against the RPT, while contextual validity is verified
through a novel RAG-based Question-Answering (RAG-QA) method. The results
demonstrate that RAG-based pipelines significantly outperform non-grounded
prompting methods, producing questions with higher curriculum alignment and
factual validity. The study further analyzes the trade-offs between the ease of
implementation of framework-based RAG and the fine-grained control offered by a
manual pipeline. This work presents a validated methodology for generating
curriculum-specific educational content in a low-resource language, introduces
a symbiotic RAG-QA evaluation technique, and provides actionable insights for
the development and deployment of practical EdTech solutions in Malaysia and
similar regions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04436v1' target='_blank'>Reliable and Real-Time Highway Trajectory Planning via Hybrid
  Learning-Optimization Frameworks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yujia Lu, Chong Wei, Lu Ma</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 13:23:07</h6>
<p class='card-text'>Autonomous highway driving presents a high collision risk due to
fast-changing environments and limited reaction time, necessitating reliable
and efficient trajectory planning. This paper proposes a hybrid trajectory
planning framework that integrates the adaptability of learning-based methods
with the formal safety guarantees of optimization-based approaches. The
framework features a two-layer architecture: an upper layer employing a graph
neural network (GNN) trained on real-world highway data to predict human-like
longitudinal velocity profiles, and a lower layer utilizing path optimization
formulated as a mixed-integer quadratic programming (MIQP) problem. The primary
contribution is the lower-layer path optimization model, which introduces a
linear approximation of discretized vehicle geometry to substantially reduce
computational complexity, while enforcing strict spatiotemporal non-overlapping
constraints to formally guarantee collision avoidance throughout the planning
horizon. Experimental results demonstrate that the planner generates highly
smooth, collision-free trajectories in complex real-world emergency scenarios,
achieving success rates exceeding 97% with average planning times of 54 ms,
thereby confirming real-time capability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04406v1' target='_blank'>Deep Learning-based Scalable Image-to-3D Facade Parser for Generating
  Thermal 3D Building Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yinan Yu, Alex Gonzalez-Caceres, Samuel Scheidegger, Sanjay Somanath, Alexander Hollberg</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 12:48:53</h6>
<p class='card-text'>Renovating existing buildings is essential for climate impact. Early-phase
renovation planning requires simulations based on thermal 3D models at Level of
Detail (LoD) 3, which include features like windows. However, scalable and
accurate identification of such features remains a challenge. This paper
presents the Scalable Image-to-3D Facade Parser (SI3FP), a pipeline that
generates LoD3 thermal models by extracting geometries from images using both
computer vision and deep learning. Unlike existing methods relying on
segmentation and projection, SI3FP directly models geometric primitives in the
orthographic image plane, providing a unified interface while reducing
perspective distortions. SI3FP supports both sparse (e.g., Google Street View)
and dense (e.g., hand-held camera) data sources. Tested on typical Swedish
residential buildings, SI3FP achieved approximately 5% error in window-to-wall
ratio estimates, demonstrating sufficient accuracy for early-stage renovation
analysis. The pipeline facilitates large-scale energy renovation planning and
has broader applications in urban development and planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04384v1' target='_blank'>Incorporating Stochastic Models of Controller Behavior into Kinodynamic
  Efficiently Adaptive State Lattices for Mobile Robot Motion Planning in
  Off-Road Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Eric R. Damm, Eli S. Lancaster, Felix A. Sanchez, Kiana Bronder, Jason M. Gregory, Thomas M. Howard</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 12:28:03</h6>
<p class='card-text'>Mobile robot motion planners rely on theoretical models to predict how the
robot will move through the world. However, when deployed on a physical robot,
these models are subject to errors due to real-world physics and uncertainty in
how the lower-level controller follows the planned trajectory. In this work, we
address this problem by presenting three methods of incorporating stochastic
controller behavior into the recombinant search space of the Kinodynamic
Efficiently Adaptive State Lattice (KEASL) planner. To demonstrate this work,
we analyze the results of experiments performed on a Clearpath Robotics Warthog
Unmanned Ground Vehicle (UGV) in an off-road, unstructured environment using
two different perception algorithms, and performed an ablation study using a
full spectrum of simulated environment map complexities. Analysis of the data
found that incorporating stochastic controller sampling into KEASL leads to
more conservative trajectories that decrease predicted collision likelihood
when compared to KEASL without sampling. When compared to baseline planning
with expanded obstacle footprints, the predicted likelihood of collisions
becomes more comparable, but reduces the planning success rate for baseline
search.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04361v1' target='_blank'>OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fuqing Bie, Shiyu Huang, Xijia Tao, Zhiqin Fang, Leyi Pan, Junzhe Chen, Min Ren, Liuyu Xiang, Zhaofeng He</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 11:58:58</h6>
<p class='card-text'>While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04332v1' target='_blank'>DRAMA: A Dynamic and Robust Allocation-based Multi-Agent System for
  Changing Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Naibo Wang, Yifan Zhang, Sai Liu, Xinkui Zhao, Guanjie Cheng, Yueshen Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 11:23:11</h6>
<p class='card-text'>Multi-agent systems (MAS) have demonstrated significant effectiveness in
addressing complex problems through coordinated collaboration among
heterogeneous agents. However, real-world environments and task specifications
are inherently dynamic, characterized by frequent changes, uncertainty, and
variability. Despite this, most existing MAS frameworks rely on static
architectures with fixed agent capabilities and rigid task allocation
strategies, which greatly limits their adaptability to evolving conditions.
This inflexibility poses substantial challenges for sustaining robust and
efficient multi-agent cooperation in dynamic and unpredictable scenarios. To
address these limitations, we propose DRAMA: a Dynamic and Robust
Allocation-based Multi-Agent System designed to facilitate resilient
collaboration in rapidly changing environments. DRAMA features a modular
architecture with a clear separation between the control plane and the worker
plane. Both agents and tasks are abstracted as resource objects with
well-defined lifecycles, while task allocation is achieved via an
affinity-based, loosely coupled mechanism. The control plane enables real-time
monitoring and centralized planning, allowing flexible and efficient task
reassignment as agents join, depart, or become unavailable, thereby ensuring
continuous and robust task execution. The worker plane comprises a cluster of
autonomous agents, each with local reasoning, task execution, the ability to
collaborate, and the capability to take over unfinished tasks from other agents
when needed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04305v1' target='_blank'>Edge2Prompt: Modality-Agnostic Model for Out-of-Distribution Liver
  Segmentation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nathan Hollet, Oumeymah Cherkaoui, Philippe C. Cattin, Sidaty El hadramy</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 10:44:03</h6>
<p class='card-text'>Liver segmentation is essential for preoperative planning in interventions
like tumor resection or transplantation, but implementation in clinical
workflows faces challenges due to modality-specific tools and data scarcity. We
propose Edge2Prompt, a novel pipeline for modality-agnostic liver segmentation
that generalizes to out-of-distribution (OOD) data. Our method integrates
classical edge detection with foundation models. Modality-agnostic edge maps
are first extracted from input images, then processed by a U-Net to generate
logit-based prompts. These prompts condition the Segment Anything Model 2
(SAM-2) to generate 2D liver segmentations, which can then be reconstructed
into 3D volumes. Evaluated on the multi-modal CHAOS dataset, Edge2Prompt
achieves competitive results compared to classical segmentation methods when
trained and tested in-distribution (ID), and outperforms them in data-scarce
scenarios due to the SAM-2 module. Furthermore, it achieves a mean Dice Score
of 86.4% on OOD tasks, outperforming U-Net baselines by 27.4% and other
self-prompting methods by 9.1%, demonstrating its effectiveness. This work
bridges classical and foundation models for clinically adaptable,
data-efficient segmentation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04284v1' target='_blank'>Optimizing Microgrid Composition for Sustainable Data Centers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Julius Irion, Philipp Wiesner, Jonathan Bader, Odej Kao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 10:15:52</h6>
<p class='card-text'>As computing energy demand continues to grow and electrical grid
infrastructure struggles to keep pace, an increasing number of data centers are
being planned with colocated microgrids that integrate on-site renewable
generation and energy storage. However, while existing research has examined
the tradeoffs between operational and embodied carbon emissions in the context
of renewable energy certificates, there is a lack of tools to assess how the
sizing and composition of microgrid components affects long-term sustainability
and power reliability.
  In this paper, we present a novel optimization framework that extends the
computing and energy system co-simulator Vessim with detailed renewable energy
generation models from the National Renewable Energy Laboratory's (NREL) System
Advisor Model (SAM). Our framework simulates the interaction between computing
workloads, on-site renewable production, and energy storage, capturing both
operational and embodied emissions. We use a multi-horizon black-box
optimization to explore efficient microgrid compositions and enable operators
to make more informed decisions when planning energy systems for data centers.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04239v1' target='_blank'>DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time
  Series Forecasting</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chanjuan Liu, Shengzhi Wang, Enqiang Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 09:25:05</h6>
<p class='card-text'>Time series forecasting is crucial in strategic planning and decision-making
across various industries. Traditional forecasting models mainly concentrate on
numerical time series data, often overlooking important textual information
such as events and news, which can significantly affect forecasting accuracy.
While large language models offer a promise for integrating multimodal data,
existing single-prompt frameworks struggle to effectively capture the semantics
of timestamped text, introducing redundant information that can hinder model
performance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt
GPT2-base for Multimodal Time Series), a novel dual-prompt large language model
framework that combines two complementary prompts: an explicit prompt for clear
task instructions and a textual prompt for context-aware embeddings from
time-stamped data. The tokenizer generates the explicit prompt while the
embeddings from the textual prompt are refined through self-attention and
feed-forward networks. Comprehensive experiments conducted on diverse
textural-numerical time series datasets demonstrate that this approach
outperforms state-of-the-art algorithms in time series forecasting. This
highlights the significance of incorporating textual context via a dual-prompt
mechanism to achieve more accurate time series predictions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04229v1' target='_blank'>Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory
  Prediction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yu Liu, Zhijie Liu, Xiao Ren, You-Fu Li, He Kong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 09:04:54</h6>
<p class='card-text'>Predicting pedestrian motion trajectories is critical for path planning and
motion control of autonomous vehicles. However, accurately forecasting crowd
trajectories remains a challenging task due to the inherently multimodal and
uncertain nature of human motion. Recent diffusion-based models have shown
promising results in capturing the stochasticity of pedestrian behavior for
trajectory prediction. However, few diffusion-based approaches explicitly
incorporate the underlying motion intentions of pedestrians, which can limit
the interpretability and precision of prediction models. In this work, we
propose a diffusion-based multimodal trajectory prediction model that
incorporates pedestrians' motion intentions into the prediction framework. The
motion intentions are decomposed into lateral and longitudinal components, and
a pedestrian intention recognition module is introduced to enable the model to
effectively capture these intentions. Furthermore, we adopt an efficient
guidance mechanism that facilitates the generation of interpretable
trajectories. The proposed framework is evaluated on two widely used human
trajectory prediction benchmarks, ETH and UCY, on which it is compared against
state-of-the-art methods. The experimental results demonstrate that our method
achieves competitive performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04170v1' target='_blank'>Agentic-AI based Mathematical Framework for Commercialization of Energy
  Resilience in Electrical Distribution System Planning and Operation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Aniket Johri, Divyanshi Dwivedi, Mayukha Pal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 07:49:37</h6>
<p class='card-text'>The increasing vulnerability of electrical distribution systems to extreme
weather events and cyber threats necessitates the development of economically
viable frameworks for resilience enhancement. While existing approaches focus
primarily on technical resilience metrics and enhancement strategies, there
remains a significant gap in establishing market-driven mechanisms that can
effectively commercialize resilience features while optimizing their deployment
through intelligent decision-making. Moreover, traditional optimization
approaches for distribution network reconfiguration often fail to dynamically
adapt to both normal and emergency conditions. This paper introduces a novel
framework integrating dual-agent Proximal Policy Optimization (PPO) with
market-based mechanisms, achieving an average resilience score of 0.85 0.08
over 10 test episodes. The proposed architecture leverages a dual-agent PPO
scheme, where a strategic agent selects optimal DER-driven switching
configurations, while a tactical agent fine-tunes individual switch states and
grid preferences under budget and weather constraints. These agents interact
within a custom-built dynamic simulation environment that models stochastic
calamity events, budget limits, and resilience-cost trade-offs. A comprehensive
reward function is designed that balances resilience enhancement objectives
with market profitability (with up to 200x reward incentives, resulting in 85%
of actions during calamity steps selecting configurations with 4 DERs),
incorporating factors such as load recovery speed, system robustness, and
customer satisfaction. Over 10 test episodes, the framework achieved a
benefit-cost ratio of 0.12 0.01, demonstrating sustainable market incentives
for resilience investment. This framework creates sustainable market incentives</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04146v1' target='_blank'>Industrial Robot Motion Planning with GPUs: Integration of cuRobo for
  Extended DOF Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Luai Abuelsamen, Harsh Rana, Ho-Wei Lu, Wenhan Tang, Swati Priyadarshini, Gabriel Gomes</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 07:18:33</h6>
<p class='card-text'>Efficient motion planning remains a key challenge in industrial robotics,
especially for multi-axis systems operating in complex environments. This paper
addresses that challenge by integrating GPU-accelerated motion planning through
NVIDIA's cuRobo library into Vention's modular automation platform. By
leveraging accurate CAD-based digital twins and real-time parallel
optimization, our system enables rapid trajectory generation and dynamic
collision avoidance for pick-and-place tasks. We demonstrate this capability on
robots equipped with additional degrees of freedom, including a 7th-axis
gantry, and benchmark performance across various scenarios. The results show
significant improvements in planning speed and robustness, highlighting the
potential of GPU-based planning pipelines for scalable, adaptable deployment in
modern industrial workflows.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04101v1' target='_blank'>NEARL-CLIP: Interacted Query Adaptation with Orthogonal Regularization
  for Medical Vision-Language Understanding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zelin Peng, Yichen Zhao, Yu Huang, Piao Yang, Feilong Tang, Zhengqin Xu, Xiaokang Yang, Wei Shen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 05:44:01</h6>
<p class='card-text'>Computer-aided medical image analysis is crucial for disease diagnosis and
treatment planning, yet limited annotated datasets restrict medical-specific
model development. While vision-language models (VLMs) like CLIP offer strong
generalization capabilities, their direct application to medical imaging
analysis is impeded by a significant domain gap. Existing approaches to bridge
this gap, including prompt learning and one-way modality interaction
techniques, typically focus on introducing domain knowledge to a single
modality. Although this may offer performance gains, it often causes modality
misalignment, thereby failing to unlock the full potential of VLMs. In this
paper, we propose \textbf{NEARL-CLIP} (i\underline{N}teracted qu\underline{E}ry
\underline{A}daptation with o\underline{R}thogona\underline{L} Regularization),
a novel cross-modality interaction VLM-based framework that contains two
contributions: (1) Unified Synergy Embedding Transformer (USEformer), which
dynamically generates cross-modality queries to promote interaction between
modalities, thus fostering the mutual enrichment and enhancement of multi-modal
medical domain knowledge; (2) Orthogonal Cross-Attention Adapter (OCA). OCA
introduces an orthogonality technique to decouple the new knowledge from
USEformer into two distinct components: the truly novel information and the
incremental knowledge. By isolating the learning process from the interference
of incremental knowledge, OCA enables a more focused acquisition of new
information, thereby further facilitating modality interaction and unleashing
the capability of VLMs. Notably, NEARL-CLIP achieves these two contributions in
a parameter-efficient style, which only introduces \textbf{1.46M} learnable
parameters.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04066v1' target='_blank'>DRIVE: Dynamic Rule Inference and Verified Evaluation for
  Constraint-Aware Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Longling Geng, Huangxing Li, Viktor Lado Naess, Mert Pilanci</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 03:56:06</h6>
<p class='card-text'>Understanding and adhering to soft constraints is essential for safe and
socially compliant autonomous driving. However, such constraints are often
implicit, context-dependent, and difficult to specify explicitly. In this work,
we present DRIVE, a novel framework for Dynamic Rule Inference and Verified
Evaluation that models and evaluates human-like driving constraints from expert
demonstrations. DRIVE leverages exponential-family likelihood modeling to
estimate the feasibility of state transitions, constructing a probabilistic
representation of soft behavioral rules that vary across driving contexts.
These learned rule distributions are then embedded into a convex
optimization-based planning module, enabling the generation of trajectories
that are not only dynamically feasible but also compliant with inferred human
preferences. Unlike prior approaches that rely on fixed constraint forms or
purely reward-based modeling, DRIVE offers a unified framework that tightly
couples rule inference with trajectory-level decision-making. It supports both
data-driven constraint generalization and principled feasibility verification.
We validate DRIVE on large-scale naturalistic driving datasets, including inD,
highD, and RoundD, and benchmark it against representative inverse constraint
learning and planning baselines. Experimental results show that DRIVE achieves
0.0% soft constraint violation rates, smoother trajectories, and stronger
generalization across diverse driving scenarios. Verified evaluations further
demonstrate the efficiency, explanability, and robustness of the framework for
real-world deployment.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04043v1' target='_blank'>VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuheng Ji, Yipu Wang, Yuyang Liu, Xiaoshuai Hao, Yue Liu, Yuting Zhao, Huaihai Lyu, Xiaolong Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 03:07:05</h6>
<p class='card-text'>Visual transformation reasoning (VTR) is a vital cognitive capability that
empowers intelligent agents to understand dynamic scenes, model causal
relationships, and predict future states, and thereby guiding actions and
laying the foundation for advanced intelligent systems. However, existing
benchmarks suffer from a sim-to-real gap, limited task complexity, and
incomplete reasoning coverage, limiting their practical use in real-world
scenarios. To address these limitations, we introduce VisualTrans, the first
comprehensive benchmark specifically designed for VTR in real-world
human-object interaction scenarios. VisualTrans encompasses 12 semantically
diverse manipulation tasks and systematically evaluates three essential
reasoning dimensions - spatial, procedural, and quantitative - through 6
well-defined subtask types. The benchmark features 472 high-quality
question-answer pairs in various formats, including multiple-choice, open-ended
counting, and target enumeration. We introduce a scalable data construction
pipeline built upon first-person manipulation videos, which integrates task
selection, image pair extraction, automated metadata annotation with large
multimodal models, and structured question generation. Human verification
ensures the final benchmark is both high-quality and interpretable. Evaluations
of various state-of-the-art vision-language models show strong performance in
static spatial tasks. However, they reveal notable shortcomings in dynamic,
multi-step reasoning scenarios, particularly in areas like intermediate state
recognition and transformation sequence planning. These findings highlight
fundamental weaknesses in temporal modeling and causal reasoning, providing
clear directions for future research aimed at developing more capable and
generalizable VTR systems. The dataset and code are available at
https://github.com/WangYipu2002/VisualTrans.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04037v1' target='_blank'>SEA: Self-Evolution Agent with Step-wise Reward for Computer Use</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Liang Tang, Shuxian Li, Yuhao Cheng, Yukang Huo, Zhepeng Wang, Yiqiang Yan, Kaer Huang, Yanzhe Jing, Tiaonan Duan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 02:57:22</h6>
<p class='card-text'>Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04026v1' target='_blank'>VeriGUI: Verifiable Long-Chain GUI Dataset</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shunyu Liu, Minghao Liu, Huichi Zhou, Zhenyu Cui, Yang Zhou, Yuhao Zhou, Wendong Fan, Ge Zhang, Jiajun Shi, Weihao Xuan, Jiaxing Huang, Shuang Luo, Fang Wu, Heli Qi, Qingcheng Zeng, Ziqi Ren, Jialiang Gao, Jindi Lv, Junjie Wang, Aosong Feng, Heng Zhou, Wangchunshu Zhou, Zhenfei Yin, Wenlong Zhang, Guohao Li, Wenhao Yu, Irene Li, Lei Ma, Lei Bai, Qunshu Lin, Mingli Song, Dacheng Tao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 02:38:18</h6>
<p class='card-text'>Recent studies have delved into constructing autonomous agents capable of
performing complex Graphical User Interface (GUI)-based computer tasks, with
the potential to revolutionize human-computer interaction. Despite encouraging
results, existing efforts mainly focus on short-term interactions and rely on
outcome-only verification, thereby limiting their scalability in real-world GUI
applications that demand long-horizon task decomposition and execution. In this
work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed
to facilitate the development and evaluation of generalist GUI agents operating
in realistic computer environments. Our dataset emphasizes two critical
dimensions: (1) long-chain complexity, with tasks decomposed into a sequence of
interdependent subtasks spanning hundreds of steps, explicitly designed to
allow any subtask to serve as a valid starting point; and (2) subtask-level
verifiability, which enables diverse exploration strategies within each
subtask, while ensuring that each subtask-level goal remains verifiable and
consistent. The dataset consists of GUI task trajectories across both desktop
and web, annotated by human experts. Extensive experiments on VeriGUI using
various agents with different foundation models reveal significant performance
gaps in handling long-horizon tasks, highlighting the need for more robust
planning and decision-making capabilities in GUI agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.04011v1' target='_blank'>StepWrite: Adaptive Planning for Speech-Driven Text Generation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hamza El Alaoui, Atieh Taheri, Yi-Hao Peng, Jeffrey P. Bigham</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-06 01:50:17</h6>
<p class='card-text'>People frequently use speech-to-text systems to compose short texts with
voice. However, current voice-based interfaces struggle to support composing
more detailed, contextually complex texts, especially in scenarios where users
are on the move and cannot visually track progress. Longer-form communication,
such as composing structured emails or thoughtful responses, requires
persistent context tracking, structured guidance, and adaptability to evolving
user intentions--capabilities that conventional dictation tools and voice
assistants do not support. We introduce StepWrite, a large language
model-driven voice-based interaction system that augments human writing ability
by enabling structured, hands-free and eyes-free composition of longer-form
texts while on the move. StepWrite decomposes the writing process into
manageable subtasks and sequentially guides users with contextually-aware
non-visual audio prompts. StepWrite reduces cognitive load by offloading the
context-tracking and adaptive planning tasks to the models. Unlike baseline
methods like standard dictation features (e.g., Microsoft Word) and
conversational voice assistants (e.g., ChatGPT Advanced Voice Mode), StepWrite
dynamically adapts its prompts based on the evolving context and user intent,
and provides coherent guidance without compromising user autonomy. An empirical
evaluation with 25 participants engaging in mobile or stationary hands-occupied
activities demonstrated that StepWrite significantly reduces cognitive load,
improves usability and user satisfaction compared to baseline methods.
Technical evaluations further confirmed StepWrite's capability in dynamic
contextual prompt generation, accurate tone alignment, and effective fact
checking. This work highlights the potential of structured, context-aware voice
interactions in enhancing hands-free and eye-free communication in everyday
multitasking scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.03944v1' target='_blank'>Constraint-Preserving Data Generation for Visuomotor Policy Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kevin Lin, Varun Ragunath, Andrew McAlinden, Aaditya Prasad, Jimmy Wu, Yuke Zhu, Jeannette Bohg</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-05 22:20:02</h6>
<p class='card-text'>Large-scale demonstration data has powered key breakthroughs in robot
manipulation, but collecting that data remains costly and time-consuming. We
present Constraint-Preserving Data Generation (CP-Gen), a method that uses a
single expert trajectory to generate robot demonstrations containing novel
object geometries and poses. These generated demonstrations are used to train
closed-loop visuomotor policies that transfer zero-shot to the real world and
generalize across variations in object geometries and poses. Similar to prior
work using pose variations for data generation, CP-Gen first decomposes expert
demonstrations into free-space motions and robot skills. But unlike those
works, we achieve geometry-aware data generation by formulating robot skills as
keypoint-trajectory constraints: keypoints on the robot or grasped object must
track a reference trajectory defined relative to a task-relevant object. To
generate a new demonstration, CP-Gen samples pose and geometry transforms for
each task-relevant object, then applies these transforms to the object and its
associated keypoints or keypoint trajectories. We optimize robot joint
configurations so that the keypoints on the robot or grasped object track the
transformed keypoint trajectory, and then motion plan a collision-free path to
the first optimized joint configuration. Experiments on 16 simulation tasks and
four real-world tasks, featuring multi-stage, non-prehensile and
tight-tolerance manipulation, show that policies trained using CP-Gen achieve
an average success rate of 77%, outperforming the best baseline that achieves
an average of 50%.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.03890v1' target='_blank'>Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation
  via Neural Processes</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sanghun Jung, Daehoon Gwak, Byron Boots, James Hays</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-05 20:19:02</h6>
<p class='card-text'>Terrain elevation modeling for off-road navigation aims to accurately
estimate changes in terrain geometry in real-time and quantify the
corresponding uncertainties. Having precise estimations and uncertainties plays
a crucial role in planning and control algorithms to explore safe and reliable
maneuver strategies. However, existing approaches, such as Gaussian Processes
(GPs) and neural network-based methods, often fail to meet these needs. They
are either unable to perform in real-time due to high computational demands,
underestimating sharp geometry changes, or harming elevation accuracy when
learned with uncertainties. Recently, Neural Processes (NPs) have emerged as a
promising approach that integrates the Bayesian uncertainty estimation of GPs
with the efficiency and flexibility of neural networks. Inspired by NPs, we
propose an effective NP-based method that precisely estimates sharp elevation
changes and quantifies the corresponding predictive uncertainty without losing
elevation accuracy. Our method leverages semantic features from LiDAR and
camera sensors to improve interpolation and extrapolation accuracy in
unobserved regions. Also, we introduce a local ball-query attention mechanism
to effectively reduce the computational complexity of global attention by 17\%
while preserving crucial local and spatial information. We evaluate our method
on off-road datasets having interesting geometric features, collected from
trails, deserts, and hills. Our results demonstrate superior performance over
baselines and showcase the potential of neural processes for effective and
expressive terrain modeling in complex off-road environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.03863v1' target='_blank'>Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with
  Transfer Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amin Farajzadeh, Hongzhao Zheng, Sarah Dumoulin, Trevor Ha, Halim Yanikomeroglu, Amir Ghasemi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-05 19:24:55</h6>
<p class='card-text'>Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.03858v1' target='_blank'>MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI
  Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Charles L. Wang, Trisha Singhal, Ameya Kelkar, Jason Tuo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-05 19:15:09</h6>
<p class='card-text'>Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.03644v1' target='_blank'>Are We on the Right Way for Assessing Document Retrieval-Augmented
  Generation?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenxuan Shen, Mingjia Wang, Yaochen Wang, Dongping Chen, Junjie Yang, Yao Wan, Weiwei Lin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-05 16:55:02</h6>
<p class='card-text'>Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language
Models (MLLMs) show great promise for complex document understanding, yet their
development is critically hampered by inadequate evaluation. Current benchmarks
often focus on specific part of document RAG system and use synthetic data with
incomplete ground truth and evidence labels, therefore failing to reflect
real-world bottlenecks and challenges. To overcome these limitations, we
introduce Double-Bench: a new large-scale, multilingual, and multimodal
evaluation system that is able to produce fine-grained assessment to each
component within document RAG systems. It comprises 3,276 documents (72,880
pages) and 5,168 single- and multi-hop queries across 6 languages and 4
document types with streamlined dynamic update support for potential data
contamination issues. Queries are grounded in exhaustively scanned evidence
pages and verified by human experts to ensure maximum quality and completeness.
Our comprehensive experiments across 9 state-of-the-art embedding models, 4
MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text
and visual embedding models is narrowing, highlighting the need in building
stronger document retrieval models. Our findings also reveal the
over-confidence dilemma within current document RAG frameworks that tend to
provide answer even without evidence support. We hope our fully open-source
Double-Bench provide a rigorous foundation for future research in advanced
document RAG systems. We plan to retrieve timely corpus and release new
benchmarks on an annual basis.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2508.03605v1' target='_blank'>$β$-Ga$_2$O$_3$--Based Radiation Detector for Proton Therapy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hunter D. Ellis, Imteaz Rahaman, Apostoli Hillas, Botong Li, Vikren Sarkar, Kai Fu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-08-05 16:20:16</h6>
<p class='card-text'>Intensity modulated proton therapy (IMPT) is an advanced cancer treatment
modality that offers significant advantages over conventional X-ray therapies,
particularly in its ability to minimize radiation dose beyond the tumor target.
This reduction in unnecessary irradiation exposure significantly lowers the
risk to surrounding healthy tissue and reduces side effects compared to
conventional X-ray treatments. However, due to the high complexity of IMPT
plans, each plan must be independently validated to ensure the safety and
efficacy of the radiation exposure to the patient. While ion chambers are
currently used for this purpose, their limitations-particularly in angled-beam
measurements and multi-depth assessments-hinder their effectiveness.
Silicon-based detectors, commonly used in X-ray therapy, are unsuitable for
IMPT due to their rapid degradation under proton irradiation. In this study, a
$\beta$-Ga$_2$O$_3$-based metal-semiconductor-metal (MSM) detector was
evaluated and compared with a commercial ion chamber using a MEVION S250i
proton accelerator. The $\beta$-Ga$_2$O$_3$ detector demonstrated reliable
detection of single-pulse proton doses as low as 0.26 MU and exhibited a linear
charge-to-dose relationship across a wide range of irradiation conditions.
Furthermore, its measurement variability was comparable to that of the ion
chamber, with improved sensitivity observed at higher bias voltages. These
results highlight the strong potential of $\beta$-Ga$_2$O$_3$ as a
radiation-hard detector material for accurate dose verification in IMPT.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>