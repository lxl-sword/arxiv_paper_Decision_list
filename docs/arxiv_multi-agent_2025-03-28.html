<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>multi-agent - 2025-03-28</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>multi-agent - 2025-03-28</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.21200v1' target='_blank'>Learning Generalizable Skills from Offline Multi-Task Data for
  Multi-Agent Cooperation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sicong Liu, Yang Shu, Chenjuan Guo, Bin Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-27 06:35:59</h6>
<p class='card-text'>Learning cooperative multi-agent policy from offline multi-task data that can
generalize to unseen tasks with varying numbers of agents and targets is an
attractive problem in many scenarios. Although aggregating general behavior
patterns among multiple tasks as skills to improve policy transfer is a
promising approach, two primary challenges hinder the further advancement of
skill learning in offline multi-task MARL. Firstly, extracting general
cooperative behaviors from various action sequences as common skills lacks
bringing cooperative temporal knowledge into them. Secondly, existing works
only involve common skills and can not adaptively choose independent knowledge
as task-specific skills in each task for fine-grained action execution. To
tackle these challenges, we propose Hierarchical and Separate Skill Discovery
(HiSSD), a novel approach for generalizable offline multi-task MARL through
skill learning. HiSSD leverages a hierarchical framework that jointly learns
common and task-specific skills. The common skills learn cooperative temporal
knowledge and enable in-sample exploitation for offline multi-task MARL. The
task-specific skills represent the priors of each task and achieve a
task-guided fine-grained action execution. To verify the advancement of our
method, we conduct experiments on multi-agent MuJoCo and SMAC benchmarks. After
training the policy using HiSSD on offline multi-task data, the empirical
results show that HiSSD assigns effective cooperative behaviors and obtains
superior performance in unseen tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.20078v1' target='_blank'>Abstracting Geo-specific Terrains to Scale Up Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Volkan Ustun, Soham Hans, Rajay Kumar, Yunzhe Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 21:29:49</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) is increasingly ubiquitous in
training dynamic and adaptive synthetic characters for interactive simulations
on geo-specific terrains. Frameworks such as Unity's ML-Agents help to make
such reinforcement learning experiments more accessible to the simulation
community. Military training simulations also benefit from advances in MARL,
but they have immense computational requirements due to their complex,
continuous, stochastic, partially observable, non-stationary, and
doctrine-based nature. Furthermore, these simulations require geo-specific
terrains, further exacerbating the computational resources problem. In our
research, we leverage Unity's waypoints to automatically generate multi-layered
representation abstractions of the geo-specific terrains to scale up
reinforcement learning while still allowing the transfer of learned policies
between different representations. Our early exploratory results on a novel
MARL scenario, where each side has differing objectives, indicate that
waypoint-based navigation enables faster and more efficient learning while
producing trajectories similar to those taken by expert human players in CSGO
gaming environments. This research points out the potential of waypoint-based
navigation for reducing the computational costs of developing and training MARL
models for military training simulations, where geo-specific terrains and
differing objectives are crucial.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19699v1' target='_blank'>Optimal Path Planning and Cost Minimization for a Drone Delivery System
  Via Model Predictive Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhammad Al-Zafar Khan, Jamal Al-Karaki</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 14:27:29</h6>
<p class='card-text'>In this study, we formulate the drone delivery problem as a control problem
and solve it using Model Predictive Control. Two experiments are performed: The
first is on a less challenging grid world environment with lower
dimensionality, and the second is with a higher dimensionality and added
complexity. The MPC method was benchmarked against three popular Multi-Agent
Reinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action
Learners (JAL), and Value-Decomposition Networks (VDN). It was shown that the
MPC method solved the problem quicker and required fewer optimal numbers of
drones to achieve a minimized cost and navigate the optimal path.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18816v1' target='_blank'>Learning Multi-Robot Coordination through Locality-Based Factorized
  Multi-Agent Actor-Critic Algorithm</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chak Lam Shek, Amrit Singh Bedi, Anjon Basak, Ellen Novoseller, Nick Waytowich, Priya Narayanan, Dinesh Manocha, Pratap Tokekar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 16:00:16</h6>
<p class='card-text'>In this work, we present a novel cooperative multi-agent reinforcement
learning method called \textbf{Loc}ality based \textbf{Fac}torized
\textbf{M}ulti-Agent \textbf{A}ctor-\textbf{C}ritic (Loc-FACMAC). Existing
state-of-the-art algorithms, such as FACMAC, rely on global reward information,
which may not accurately reflect the quality of individual robots' actions in
decentralized systems. We integrate the concept of locality into critic
learning, where strongly related robots form partitions during training. Robots
within the same partition have a greater impact on each other, leading to more
precise policy evaluation. Additionally, we construct a dependency graph to
capture the relationships between robots, facilitating the partitioning
process. This approach mitigates the curse of dimensionality and prevents
robots from using irrelevant information. Our method improves existing
algorithms by focusing on local rewards and leveraging partition-based learning
to enhance training efficiency and performance. We evaluate the performance of
Loc-FACMAC in three environments: Hallway, Multi-cartpole, and
Bounded-Cooperative-Navigation. We explore the impact of partition sizes on the
performance and compare the result with baseline MARL algorithms such as LOMAQ,
FACMAC, and QMIX. The experiments reveal that, if the locality structure is
defined properly, Loc-FACMAC outperforms these baseline algorithms up to 108\%,
indicating that exploiting the locality structure in the actor-critic framework
improves the MARL performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18221v1' target='_blank'>Decentralized Navigation of a Cable-Towed Load using Quadrupedal Robot
  Team via MARL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wen-Tse Chen, Minh Nguyen, Zhongyu Li, Guo Ning Sue, Koushil Sreenath</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-23 21:48:26</h6>
<p class='card-text'>This work addresses the challenge of enabling a team of quadrupedal robots to
collaboratively tow a cable-connected load through cluttered and unstructured
environments while avoiding obstacles. Leveraging cables allows the multi-robot
system to navigate narrow spaces by maintaining slack when necessary. However,
this introduces hybrid physical interactions due to alternating taut and slack
states, with computational complexity that scales exponentially as the number
of agents increases. To tackle these challenges, we developed a scalable and
decentralized system capable of dynamically coordinating a variable number of
quadrupedal robots while managing the hybrid physical interactions inherent in
the load-towing task. At the core of this system is a novel multi-agent
reinforcement learning (MARL)-based planner, designed for decentralized
coordination. The MARL-based planner is trained using a centralized training
with decentralized execution (CTDE) framework, enabling each robot to make
decisions autonomously using only local (ego) observations. To accelerate
learning and ensure effective collaboration across varying team sizes, we
introduce a tailored training curriculum for MARL. Experimental results
highlight the flexibility and scalability of the framework, demonstrating
successful deployment with one to four robots in real-world scenarios and up to
twelve robots in simulation. The decentralized planner maintains consistent
inference times, regardless of the team size. Additionally, the proposed system
demonstrates robustness to environment perturbations and adaptability to
varying load weights. This work represents a step forward in achieving flexible
and efficient multi-legged robotic collaboration in complex and real-world
environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18201v1' target='_blank'>Iterative Multi-Agent Reinforcement Learning: A Novel Approach Toward
  Real-World Multi-Echelon Inventory Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Georg Ziegner, Michael Choi, Hung Mac Chan Le, Sahil Sakhuja, Arash Sarmadi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-23 20:52:21</h6>
<p class='card-text'>Multi-echelon inventory optimization (MEIO) is critical for effective supply
chain management, but its inherent complexity can pose significant challenges.
Heuristics are commonly used to address this complexity, yet they often face
limitations in scope and scalability. Recent research has found deep
reinforcement learning (DRL) to be a promising alternative to traditional
heuristics, offering greater versatility by utilizing dynamic decision-making
capabilities. However, since DRL is known to struggle with the curse of
dimensionality, its relevance to complex real-life supply chain scenarios is
still to be determined. This thesis investigates DRL's applicability to MEIO
problems of increasing complexity. A state-of-the-art DRL model was replicated,
enhanced, and tested across 13 supply chain scenarios, combining diverse
network structures and parameters. To address DRL's challenges with
dimensionality, additional models leveraging graph neural networks (GNNs) and
multi-agent reinforcement learning (MARL) were developed, culminating in the
novel iterative multi-agent reinforcement learning (IMARL) approach. IMARL
demonstrated superior scalability, effectiveness, and reliability in optimizing
inventory policies, consistently outperforming benchmarks. These findings
confirm the potential of DRL, particularly IMARL, to address real-world supply
chain challenges and call for additional research to further expand its
applicability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.17803v1' target='_blank'>A Roadmap Towards Improving Multi-Agent Reinforcement Learning With
  Causal Discovery And Inference</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Giovanni Briglia, Stefano Mariani, Franco Zambonelli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-22 15:49:13</h6>
<p class='card-text'>Causal reasoning is increasingly used in Reinforcement Learning (RL) to
improve the learning process in several dimensions: efficacy of learned
policies, efficiency of convergence, generalisation capabilities, safety and
interpretability of behaviour. However, applications of causal reasoning to
Multi-Agent RL (MARL) are still mostly unexplored. In this paper, we take the
first step in investigating the opportunities and challenges of applying causal
reasoning in MARL. We measure the impact of a simple form of causal
augmentation in state-of-the-art MARL scenarios increasingly requiring
cooperation, and with state-of-the-art MARL algorithms exploiting various
degrees of collaboration between agents. Then, we discuss the positive as well
as negative results achieved, giving us the chance to outline the areas where
further research may help to successfully transfer causal RL to the multi-agent
setting.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.15947v1' target='_blank'>Unreal-MAP: Unreal-Engine-Based General Platform for Multi-Agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tianyi Hu, Qingxu Fu, Zhiqiang Pu, Yuan Wang, Tenghai Qiu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-20 08:40:41</h6>
<p class='card-text'>In this paper, we propose Unreal Multi-Agent Playground (Unreal-MAP), an MARL
general platform based on the Unreal-Engine (UE). Unreal-MAP allows users to
freely create multi-agent tasks using the vast visual and physical resources
available in the UE community, and deploy state-of-the-art (SOTA) MARL
algorithms within them. Unreal-MAP is user-friendly in terms of deployment,
modification, and visualization, and all its components are open-source. We
also develop an experimental framework compatible with algorithms ranging from
rule-based to learning-based provided by third-party frameworks. Lastly, we
deploy several SOTA algorithms in example tasks developed via Unreal-MAP, and
conduct corresponding experimental analyses. We believe Unreal-MAP can play an
important role in the MARL field by closely integrating existing algorithms
with user-customized tasks, thus advancing the field of MARL.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.15703v1' target='_blank'>Predicting Multi-Agent Specialization via Task Parallelizability</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Elizabeth Mieczkowski, Ruaridh Mon-Williams, Neil Bramley, Christopher G. Lucas, Natalia Velez, Thomas L. Griffiths</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-19 21:33:48</h6>
<p class='card-text'>Multi-agent systems often rely on specialized agents with distinct roles
rather than general-purpose agents that perform the entire task independently.
However, the conditions that govern the optimal degree of specialization remain
poorly understood. In this work, we propose that specialist teams outperform
generalist ones when environmental constraints limit task parallelizability --
the potential to execute task components concurrently. Drawing inspiration from
distributed systems, we introduce a heuristic to predict the relative
efficiency of generalist versus specialist teams by estimating the speed-up
achieved when two agents perform a task in parallel rather than focus on
complementary subtasks. We validate this heuristic through three multi-agent
reinforcement learning (MARL) experiments in Overcooked-AI, demonstrating that
key factors limiting task parallelizability influence specialization. We also
observe that as the state space expands, agents tend to converge on specialist
strategies, even when generalist ones are theoretically more efficient,
highlighting potential biases in MARL training algorithms. Our findings provide
a principled framework for interpreting specialization given the task and
environment, and introduce a novel benchmark for evaluating whether MARL finds
optimal strategies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.15615v1' target='_blank'>PEnGUiN: Partially Equivariant Graph NeUral Networks for Sample
  Efficient MARL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joshua McClellan, Greyson Brothers, Furong Huang, Pratap Tokekar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-19 18:01:14</h6>
<p class='card-text'>Equivariant Graph Neural Networks (EGNNs) have emerged as a promising
approach in Multi-Agent Reinforcement Learning (MARL), leveraging symmetry
guarantees to greatly improve sample efficiency and generalization. However,
real-world environments often exhibit inherent asymmetries arising from factors
such as external forces, measurement inaccuracies, or intrinsic system biases.
This paper introduces \textit{Partially Equivariant Graph NeUral Networks
(PEnGUiN)}, a novel architecture specifically designed to address these
challenges. We formally identify and categorize various types of partial
equivariance relevant to MARL, including subgroup equivariance, feature-wise
equivariance, regional equivariance, and approximate equivariance. We
theoretically demonstrate that PEnGUiN is capable of learning both fully
equivariant (EGNN) and non-equivariant (GNN) representations within a unified
framework. Through extensive experiments on a range of MARL problems
incorporating various asymmetries, we empirically validate the efficacy of
PEnGUiN. Our results consistently demonstrate that PEnGUiN outperforms both
EGNNs and standard GNNs in asymmetric environments, highlighting their
potential to improve the robustness and applicability of graph-based MARL
algorithms in real-world scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.15172v1' target='_blank'>Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic
  Spectrum Access Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:George Stamatelis, Angelos-Nikolaos Kanatas, George C. Alexandropoulos</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-19 12:56:23</h6>
<p class='card-text'>Multi-Agent Deep Reinforcement Learning (MADRL) has emerged as a powerful
tool for optimizing decentralized decision-making systems in complex settings,
such as Dynamic Spectrum Access (DSA). However, deploying deep learning models
on resource-constrained edge devices remains challenging due to their high
computational cost. To address this challenge, in this paper, we present a
novel sparse recurrent MARL framework integrating gradual neural network
pruning into the independent actor global critic paradigm. Additionally, we
introduce a harmonic annealing sparsity scheduler, which achieves comparable,
and in certain cases superior, performance to standard linear and polynomial
pruning schedulers at large sparsities. Our experimental investigation
demonstrates that the proposed DSA framework can discover superior policies,
under diverse training conditions, outperforming conventional DSA, MADRL
baselines, and state-of-the-art pruning techniques.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.14576v1' target='_blank'>SocialJax: An Evaluation Suite for Multi-agent Reinforcement Learning in
  Sequential Social Dilemmas</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zihao Guo, Richard Willis, Shuqing Shi, Tristan Tomilin, Joel Z. Leibo, Yali Du</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-18 16:03:59</h6>
<p class='card-text'>Social dilemmas pose a significant challenge in the field of multi-agent
reinforcement learning (MARL). Melting Pot is an extensive framework designed
to evaluate social dilemma environments, providing an evaluation protocol that
measures generalization to new social partners across various test scenarios.
However, running reinforcement learning algorithms in the official Melting Pot
environments demands substantial computational resources. In this paper, we
introduce SocialJax, a suite of sequential social dilemma environments
implemented in JAX. JAX is a high-performance numerical computing library for
Python that enables significant improvements in the operational efficiency of
SocialJax on GPUs and TPUs. Our experiments demonstrate that the training
pipeline of SocialJax achieves a 50\texttimes{} speedup in real-time
performance compared to Melting Pot's RLlib baselines. Additionally, we
validate the effectiveness of baseline algorithms within the SocialJax
environments. Finally, we use Schelling diagrams to verify the social dilemma
properties of these environments, ensuring they accurately capture the dynamics
of social dilemmas.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.14555v1' target='_blank'>A Generalist Hanabi Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Arjun V Sudhakar, Hadi Nekoei, Mathieu Reymond, Miao Liu, Janarthanan Rajendran, Sarath Chandar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-17 22:25:15</h6>
<p class='card-text'>Traditional multi-agent reinforcement learning (MARL) systems can develop
cooperative strategies through repeated interactions. However, these systems
are unable to perform well on any other setting than the one they have been
trained on, and struggle to successfully cooperate with unfamiliar
collaborators. This is particularly visible in the Hanabi benchmark, a popular
2-to-5 player cooperative card-game which requires complex reasoning and
precise assistance to other agents. Current MARL agents for Hanabi can only
learn one specific game-setting (e.g., 2-player games), and play with the same
algorithmic agents. This is in stark contrast to humans, who can quickly adjust
their strategies to work with unfamiliar partners or situations. In this paper,
we introduce Recurrent Replay Relevance Distributed DQN (R3D2), a generalist
agent for Hanabi, designed to overcome these limitations. We reformulate the
task using text, as language has been shown to improve transfer. We then
propose a distributed MARL algorithm that copes with the resulting dynamic
observation- and action-space. In doing so, our agent is the first that can
play all game settings concurrently, and extend strategies learned from one
setting to other ones. As a consequence, our agent also demonstrates the
ability to collaborate with different algorithmic agents -- agents that are
themselves unable to do so. The implementation code is available at:
$\href{https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent}{R3D2-A-Generalist-Hanabi-Agent}$</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.13415v1' target='_blank'>A Comprehensive Survey on Multi-Agent Cooperative Decision-Making:
  Scenarios, Approaches, Challenges and Perspectives</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weiqiang Jin, Hongyang Du, Biao Zhao, Xingwu Tian, Bohang Shi, Guang Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-17 17:45:46</h6>
<p class='card-text'>With the rapid development of artificial intelligence, intelligent
decision-making techniques have gradually surpassed human levels in various
human-machine competitions, especially in complex multi-agent cooperative task
scenarios. Multi-agent cooperative decision-making involves multiple agents
working together to complete established tasks and achieve specific objectives.
These techniques are widely applicable in real-world scenarios such as
autonomous driving, drone navigation, disaster rescue, and simulated military
confrontations. This paper begins with a comprehensive survey of the leading
simulation environments and platforms used for multi-agent cooperative
decision-making. Specifically, we provide an in-depth analysis for these
simulation environments from various perspectives, including task formats,
reward allocation, and the underlying technologies employed. Subsequently, we
provide a comprehensive overview of the mainstream intelligent decision-making
approaches, algorithms and models for multi-agent systems (MAS).
Theseapproaches can be broadly categorized into five types: rule-based
(primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep
multi-agent reinforcement learning (MARL)-based, and large language
models(LLMs)reasoning-based. Given the significant advantages of MARL
andLLMs-baseddecision-making methods over the traditional rule, game theory,
and evolutionary algorithms, this paper focuses on these multi-agent methods
utilizing MARL and LLMs-based techniques. We provide an in-depth discussion of
these approaches, highlighting their methodology taxonomies, advantages, and
drawbacks. Further, several prominent research directions in the future and
potential challenges of multi-agent cooperative decision-making are also
detailed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.13553v1' target='_blank'>LLM-Mediated Guidance of MARL Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Philipp D. Siedler, Ian Gemp</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-16 20:16:13</h6>
<p class='card-text'>In complex multi-agent environments, achieving efficient learning and
desirable behaviours is a significant challenge for Multi-Agent Reinforcement
Learning (MARL) systems. This work explores the potential of combining MARL
with Large Language Model (LLM)-mediated interventions to guide agents toward
more desirable behaviours. Specifically, we investigate how LLMs can be used to
interpret and facilitate interventions that shape the learning trajectories of
multiple agents. We experimented with two types of interventions, referred to
as controllers: a Natural Language (NL) Controller and a Rule-Based (RB)
Controller. The NL Controller, which uses an LLM to simulate human-like
interventions, showed a stronger impact than the RB Controller. Our findings
indicate that agents particularly benefit from early interventions, leading to
more efficient training and higher performance. Both intervention types
outperform the baseline without interventions, highlighting the potential of
LLM-mediated guidance to accelerate training and enhance MARL performance in
challenging environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.13547v1' target='_blank'>Adaptive AUV Hunting Policy with Covert Communication via Diffusion
  Model</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xu Guo, Xiangwang Hou, Minrui Xu, Jianrui Chen, Jingjing Wang, Jun Du, Yong Ren</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-16 13:35:53</h6>
<p class='card-text'>Collaborative underwater target hunting, facilitated by multiple autonomous
underwater vehicles (AUVs), plays a significant role in various domains,
especially military missions. Existing research predominantly focuses on
designing efficient and high-success-rate hunting policy, particularly
addressing the target's evasion capabilities. However, in real-world scenarios,
the target can not only adjust its evasion policy based on its observations and
predictions but also possess eavesdropping capabilities. If communication among
hunter AUVs, such as hunting policy exchanges, is intercepted by the target, it
can adapt its escape policy accordingly, significantly reducing the success
rate of the hunting mission. To address this challenge, we propose a covert
communication-guaranteed collaborative target hunting framework, which ensures
efficient hunting in complex underwater environments while defending against
the target's eavesdropping. To the best of our knowledge, this is the first
study to incorporate the confidentiality of inter-agent communication into the
design of target hunting policy. Furthermore, given the complexity of
coordinating multiple AUVs in dynamic and unpredictable environments, we
propose an adaptive multi-agent diffusion policy (AMADP), which incorporates
the strong generative ability of diffusion models into the multi-agent
reinforcement learning (MARL) algorithm. Experimental results demonstrate that
AMADP achieves faster convergence and higher hunting success rates while
maintaining covertness constraints.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.12122v1' target='_blank'>ICCO: Learning an Instruction-conditioned Coordinator for
  Language-guided Task-aligned Multi-robot Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yoshiki Yano, Kazuki Shibata, Maarten Kokshoorn, Takamitsu Matsubara</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-15 13:03:20</h6>
<p class='card-text'>Recent advances in Large Language Models (LLMs) have permitted the
development of language-guided multi-robot systems, which allow robots to
execute tasks based on natural language instructions. However, achieving
effective coordination in distributed multi-agent environments remains
challenging due to (1) misalignment between instructions and task requirements
and (2) inconsistency in robot behaviors when they independently interpret
ambiguous instructions. To address these challenges, we propose
Instruction-Conditioned Coordinator (ICCO), a Multi-Agent Reinforcement
Learning (MARL) framework designed to enhance coordination in language-guided
multi-robot systems. ICCO consists of a Coordinator agent and multiple Local
Agents, where the Coordinator generates Task-Aligned and Consistent
Instructions (TACI) by integrating language instructions with environmental
states, ensuring task alignment and behavioral consistency. The Coordinator and
Local Agents are jointly trained to optimize a reward function that balances
task efficiency and instruction following. A Consistency Enhancement Term is
added to the learning objective to maximize mutual information between
instructions and robot behaviors, further improving coordination. Simulation
and real-world experiments validate the effectiveness of ICCO in achieving
language-guided task-aligned multi-robot control. The demonstration can be
found at https://yanoyoshiki.github.io/ICCO/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.11488v1' target='_blank'>Unicorn: A Universal and Collaborative Reinforcement Learning Approach
  Towards Generalizable Network-Wide Traffic Signal Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yifeng Zhang, Yilin Liu, Ping Gong, Peizhuo Li, Mingfeng Fan, Guillaume Sartoretti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-14 15:13:42</h6>
<p class='card-text'>Adaptive traffic signal control (ATSC) is crucial in reducing congestion,
maximizing throughput, and improving mobility in rapidly growing urban areas.
Recent advancements in parameter-sharing multi-agent reinforcement learning
(MARL) have greatly enhanced the scalable and adaptive optimization of complex,
dynamic flows in large-scale homogeneous networks. However, the inherent
heterogeneity of real-world traffic networks, with their varied intersection
topologies and interaction dynamics, poses substantial challenges to achieving
scalable and effective ATSC across different traffic scenarios. To address
these challenges, we present Unicorn, a universal and collaborative MARL
framework designed for efficient and adaptable network-wide ATSC. Specifically,
we first propose a unified approach to map the states and actions of
intersections with varying topologies into a common structure based on traffic
movements. Next, we design a Universal Traffic Representation (UTR) module with
a decoder-only network for general feature extraction, enhancing the model's
adaptability to diverse traffic scenarios. Additionally, we incorporate an
Intersection Specifics Representation (ISR) module, designed to identify key
latent vectors that represent the unique intersection's topology and traffic
dynamics through variational inference techniques. To further refine these
latent representations, we employ a contrastive learning approach in a
self-supervised manner, which enables better differentiation of
intersection-specific features. Moreover, we integrate the state-action
dependencies of neighboring agents into policy optimization, which effectively
captures dynamic agent interactions and facilitates efficient regional
collaboration. Our results show that Unicorn outperforms other methods across
various evaluation metrics, highlighting its potential in complex, dynamic
traffic networks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.11726v1' target='_blank'>SPECTra: Scalable Multi-Agent Reinforcement Learning with
  Permutation-Free Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hyunwoo Park, Baekryun Seong, Sang-Ki Ko</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-14 04:26:51</h6>
<p class='card-text'>In cooperative multi-agent reinforcement learning (MARL), the permutation
problem where the state space grows exponentially with the number of agents
reduces sample efficiency. Additionally, many existing architectures struggle
with scalability, relying on a fixed structure tied to a specific number of
agents, limiting their applicability to environments with a variable number of
entities. While approaches such as graph neural networks (GNNs) and
self-attention mechanisms have progressed in addressing these challenges, they
have significant limitations as dense GNNs and self-attention mechanisms incur
high computational costs. To overcome these limitations, we propose a novel
agent network and a non-linear mixing network that ensure
permutation-equivariance and scalability, allowing them to generalize to
environments with various numbers of agents. Our agent network significantly
reduces computational complexity, and our scalable hypernetwork enables
efficient weight generation for non-linear mixing. Additionally, we introduce
curriculum learning to improve training efficiency. Experiments on SMACv2 and
Google Research Football (GRF) demonstrate that our approach achieves superior
learning performance compared to existing methods. By addressing both
permutation-invariance and scalability in MARL, our work provides a more
efficient and adaptable framework for cooperative MARL. Our code is available
at https://github.com/funny-rl/SPECTra.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10907v1' target='_blank'>H2-MARL: Multi-Agent Reinforcement Learning for Pareto Optimality in
  Hospital Capacity Strain and Human Mobility during Epidemic</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xueting Luo, Hao Deng, Jihong Yang, Yao Shen, Huanhuan Guo, Zhiyuan Sun, Mingqing Liu, Jiming Wei, Shengjie Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 21:40:07</h6>
<p class='card-text'>The necessity of achieving an effective balance between minimizing the losses
associated with restricting human mobility and ensuring hospital capacity has
gained significant attention in the aftermath of COVID-19. Reinforcement
learning (RL)-based strategies for human mobility management have recently
advanced in addressing the dynamic evolution of cities and epidemics; however,
they still face challenges in achieving coordinated control at the township
level and adapting to cities of varying scales. To address the above issues, we
propose a multi-agent RL approach that achieves Pareto optimality in managing
hospital capacity and human mobility (H2-MARL), applicable across cities of
different scales. We first develop a township-level infection model with
online-updatable parameters to simulate disease transmission and construct a
city-wide dynamic spatiotemporal epidemic simulator. On this basis, H2-MARL is
designed to treat each division as an agent, with a trade-off dual-objective
reward function formulated and an experience replay buffer enriched with expert
knowledge built. To evaluate the effectiveness of the model, we construct a
township-level human mobility dataset containing over one billion records from
four representative cities of varying scales. Extensive experiments demonstrate
that H2-MARL has the optimal dual-objective trade-off capability, which can
minimize hospital capacity strain while minimizing human mobility restriction
loss. Meanwhile, the applicability of the proposed model to epidemic control in
cities of varying scales is verified, which showcases its feasibility and
versatility in practical applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10049v1' target='_blank'>Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based
  Planner and Graph-based Policy</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziqi Jia, Junjie Li, Xiaoyang Qu, Jianzong Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 05:02:49</h6>
<p class='card-text'>Multi-agent systems (MAS) have shown great potential in executing complex
tasks, but coordination and safety remain significant challenges. Multi-Agent
Reinforcement Learning (MARL) offers a promising framework for agent
collaboration, but it faces difficulties in handling complex tasks and
designing reward functions. The introduction of Large Language Models (LLMs)
has brought stronger reasoning and cognitive abilities to MAS, but existing
LLM-based systems struggle to respond quickly and accurately in dynamic
environments. To address these challenges, we propose LLM-based Graph
Collaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and
MARL. This framework decomposes complex tasks into executable subtasks and
achieves efficient collaboration among multiple agents through graph-based
coordination. Specifically, LGC-MARL consists of two main components: an LLM
planner and a graph-based collaboration meta policy. The LLM planner transforms
complex task instructions into a series of executable subtasks, evaluates the
rationality of these subtasks using a critic model, and generates an action
dependency graph. The graph-based collaboration meta policy facilitates
communication and collaboration among agents based on the action dependency
graph, and adapts to new task environments through meta-learning. Experimental
results on the AI2-THOR simulation platform demonstrate the superior
performance and scalability of LGC-MARL in completing various complex tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09501v2' target='_blank'>ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 16:05:31</h6>
<p class='card-text'>Recent research on Reasoning of Large Language Models (LLMs) has sought to
further enhance their performance by integrating meta-thinking -- enabling
models to monitor, evaluate, and control their reasoning processes for more
adaptive and effective problem-solving. However, current single-agent work
lacks a specialized design for acquiring meta-thinking, resulting in low
efficacy. To address this challenge, we introduce Reinforced Meta-thinking
Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement
Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think
about thinking. ReMA decouples the reasoning process into two hierarchical
agents: a high-level meta-thinking agent responsible for generating strategic
oversight and plans, and a low-level reasoning agent for detailed executions.
Through iterative reinforcement learning with aligned objectives, these agents
explore and learn collaboration, leading to improved generalization and
robustness. Experimental results demonstrate that ReMA outperforms single-agent
RL baselines on complex reasoning tasks, including competitive-level
mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation
studies further illustrate the evolving dynamics of each distinct agent,
providing valuable insights into how the meta-thinking reasoning process
enhances the reasoning capabilities of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08740v1' target='_blank'>Cooperative Bearing-Only Target Pursuit via Multiagent Reinforcement
  Learning: Design and Experiment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianan Li, Zhikun Wang, Susheng Ding, Shiliang Guo, Shiyu Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 08:21:35</h6>
<p class='card-text'>This paper addresses the multi-robot pursuit problem for an unknown target,
encompassing both target state estimation and pursuit control. First, in state
estimation, we focus on using only bearing information, as it is readily
available from vision sensors and effective for small, distant targets.
Challenges such as instability due to the nonlinearity of bearing measurements
and singularities in the two-angle representation are addressed through a
proposed uniform bearing-only information filter. This filter integrates
multiple 3D bearing measurements, provides a concise formulation, and enhances
stability and resilience to target loss caused by limited field of view (FoV).
Second, in target pursuit control within complex environments, where challenges
such as heterogeneity and limited FoV arise, conventional methods like
differential games or Voronoi partitioning often prove inadequate. To address
these limitations, we propose a novel multiagent reinforcement learning (MARL)
framework, enabling multiple heterogeneous vehicles to search, localize, and
follow a target while effectively handling those challenges. Third, to bridge
the sim-to-real gap, we propose two key techniques: incorporating adjustable
low-level control gains in training to replicate the dynamics of real-world
autonomous ground vehicles (AGVs), and proposing spectral-normalized RL
algorithms to enhance policy smoothness and robustness. Finally, we demonstrate
the successful zero-shot transfer of the MARL controllers to AGVs, validating
the effectiveness and practical feasibility of our approach. The accompanying
video is available at https://youtu.be/HO7FJyZiJ3E.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08728v1' target='_blank'>Enhancing Traffic Signal Control through Model-based Reinforcement
  Learning and Policy Reuse</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yihong Li, Chengwei Zhang, Furui Zhan, Wanting Liu, Kailing Zhou, Longji Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 01:21:13</h6>
<p class='card-text'>Multi-agent reinforcement learning (MARL) has shown significant potential in
traffic signal control (TSC). However, current MARL-based methods often suffer
from insufficient generalization due to the fixed traffic patterns and road
network conditions used during training. This limitation results in poor
adaptability to new traffic scenarios, leading to high retraining costs and
complex deployment. To address this challenge, we propose two algorithms:
PLight and PRLight. PLight employs a model-based reinforcement learning
approach, pretraining control policies and environment models using predefined
source-domain traffic scenarios. The environment model predicts the state
transitions, which facilitates the comparison of environmental features.
PRLight further enhances adaptability by adaptively selecting pre-trained
PLight agents based on the similarity between the source and target domains to
accelerate the learning process in the target domain. We evaluated the
algorithms through two transfer settings: (1) adaptability to different traffic
scenarios within the same road network, and (2) generalization across different
road networks. The results show that PRLight significantly reduces the
adaptation time compared to learning from scratch in new TSC scenarios,
achieving optimal performance using similarities between available and target
scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07678v1' target='_blank'>Using a single actor to output personalized policy for different
  intersections</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kailing Zhou, Chengwei Zhang, Furui Zhan, Wanting Liu, Yihong Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 07:55:33</h6>
<p class='card-text'>Recently, with the development of Multi-agent reinforcement learning (MARL),
adaptive traffic signal control (ATSC) has achieved satisfactory results. In
traffic scenarios with multiple intersections, MARL treats each intersection as
an agent and optimizes traffic signal control strategies through learning and
real-time decision-making. Considering that observation distributions of
intersections might be different in real-world scenarios, shared parameter
methods might lack diversity and thus lead to high generalization requirements
in the shared-policy network. A typical solution is to increase the size of
network parameters. However, simply increasing the scale of the network does
not necessarily improve policy generalization, which is validated in our
experiments. Accordingly, an approach that considers both the personalization
of intersections and the efficiency of parameter sharing is required. To this
end, we propose Hyper-Action Multi-Head Proximal Policy Optimization
(HAMH-PPO), a Centralized Training with Decentralized Execution (CTDE) MARL
method that utilizes a shared PPO policy network to deliver personalized
policies for intersections with non-iid observation distributions. The
centralized critic in HAMH-PPO uses graph attention units to calculate the
graph representations of all intersections and outputs a set of value estimates
with multiple output heads for each intersection. The decentralized execution
actor takes the local observation history as input and output distributions of
action as well as a so-called hyper-action to balance the multiple values
estimated from the centralized critic to further guide the updating of TSC
policies. The combination of hyper-action and multi-head values enables
multiple agents to share a single actor-critic while achieving personalized
policies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05383v3' target='_blank'>AVA: Attentive VLM Agent for Mastering StarCraft II</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weiyu Ma, Yuqian Fu, Zecheng Zhang, Bernard Ghanem, Guohao Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 12:54:25</h6>
<p class='card-text'>We introduce Attentive VLM Agent (AVA), a multimodal StarCraft II agent that
aligns artificial agent perception with the human gameplay experience.
Traditional frameworks such as SMAC rely on abstract state representations that
diverge significantly from human perception, limiting the ecological validity
of agent behavior. Our agent addresses this limitation by incorporating RGB
visual inputs and natural language observations that more closely simulate
human cognitive processes during gameplay. The AVA architecture consists of
three integrated components: (1) a vision-language model enhanced with
specialized self-attention mechanisms for strategic unit targeting and
battlefield assessment, (2) a retrieval-augmented generation system that
leverages domain-specific StarCraft II knowledge to inform tactical decisions,
and (3) a dynamic role-based task distribution system that enables coordinated
multi-agent behavior. The experimental evaluation in our proposed AVACraft
environment, which contains 21 multimodal StarCraft II scenarios, demonstrates
that AVA powered by foundation models (specifically Qwen-VL and GPT-4o) can
execute complex tactical maneuvers without explicit training, achieving
comparable performance to traditional MARL methods that require substantial
training iterations. This work establishes a foundation for developing
human-aligned StarCraft II agents and advances the broader research agenda of
multimodal game AI. Our implementation is available at
https://github.com/camel-ai/VLM-Play-StarCraft2.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05092v1' target='_blank'>Multi-Robot Collaboration through Reinforcement Learning and Abstract
  Simulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Adam Labiosa, Josiah P. Hanna</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 02:23:24</h6>
<p class='card-text'>Teams of people coordinate to perform complex tasks by forming abstract
mental models of world and agent dynamics. The use of abstract models contrasts
with much recent work in robot learning that uses a high-fidelity simulator and
reinforcement learning (RL) to obtain policies for physical robots. Motivated
by this difference, we investigate the extent to which so-called abstract
simulators can be used for multi-agent reinforcement learning (MARL) and the
resulting policies successfully deployed on teams of physical robots. An
abstract simulator models the robot's target task at a high-level of
abstraction and discards many details of the world that could impact optimal
decision-making. Policies are trained in an abstract simulator then transferred
to the physical robot by making use of separately-obtained low-level perception
and motion control modules. We identify three key categories of modifications
to the abstract simulator that enable policy transfer to physical robots:
simulation fidelity enhancements, training optimizations and simulation
stochasticity. We then run an empirical study with extensive ablations to
determine the value of each modification category for enabling policy transfer
in cooperative robot soccer tasks. We also compare the performance of policies
produced by our method with a well-tuned non-learning-based behavior
architecture from the annual RoboCup competition and find that our approach
leads to a similar level of performance. Broadly we show that MARL can be use
to train cooperative physical robot behaviors using highly abstract models of
the world.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04262v1' target='_blank'>Guidelines for Applying RL and MARL in Cybersecurity Applications</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Vasilios Mavroudis, Gregory Palmer, Sara Farmer, Kez Smithson Whitehead, David Foster, Adam Price, Ian Miles, Alberto Caron, Stephen Pasteris</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 09:46:16</h6>
<p class='card-text'>Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)
have emerged as promising methodologies for addressing challenges in automated
cyber defence (ACD). These techniques offer adaptive decision-making
capabilities in high-dimensional, adversarial environments. This report
provides a structured set of guidelines for cybersecurity professionals and
researchers to assess the suitability of RL and MARL for specific use cases,
considering factors such as explainability, exploration needs, and the
complexity of multi-agent coordination. It also discusses key algorithmic
approaches, implementation challenges, and real-world constraints, such as data
scarcity and adversarial interference. The report further outlines open
research questions, including policy optimality, agent cooperation levels, and
the integration of MARL systems into operational cybersecurity frameworks. By
bridging theoretical advancements and practical deployment, these guidelines
aim to enhance the effectiveness of AI-driven cyber defence strategies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03889v1' target='_blank'>Pretrained LLMs as Real-Time Controllers for Robot Operated Serial
  Production Line</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhammad Waseem, Kshitij Bhatta, Chen Li, Qing Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 20:43:49</h6>
<p class='card-text'>The manufacturing industry is undergoing a transformative shift, driven by
cutting-edge technologies like 5G, AI, and cloud computing. Despite these
advancements, effective system control, which is crucial for optimizing
production efficiency, remains a complex challenge due to the intricate,
knowledge-dependent nature of manufacturing processes and the reliance on
domain-specific expertise. Conventional control methods often demand heavy
customization, considerable computational resources, and lack transparency in
decision-making. In this work, we investigate the feasibility of using Large
Language Models (LLMs), particularly GPT-4, as a straightforward, adaptable
solution for controlling manufacturing systems, specifically, mobile robot
scheduling. We introduce an LLM-based control framework to assign mobile robots
to different machines in robot assisted serial production lines, evaluating its
performance in terms of system throughput. Our proposed framework outperforms
traditional scheduling approaches such as First-Come-First-Served (FCFS),
Shortest Processing Time (SPT), and Longest Processing Time (LPT). While it
achieves performance that is on par with state-of-the-art methods like
Multi-Agent Reinforcement Learning (MARL), it offers a distinct advantage by
delivering comparable throughput without the need for extensive retraining.
These results suggest that the proposed LLM-based solution is well-suited for
scenarios where technical expertise, computational resources, and financial
investment are limited, while decision transparency and system scalability are
critical concerns.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.03796v2' target='_blank'>Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent
  Reinforcement Learning in USV Swarm</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-05 14:33:18</h6>
<p class='card-text'>Multi-Agent Reinforcement Learning (MARL) has shown promise in solving
complex problems involving cooperation and competition among agents, such as an
Unmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance,
and vessel protection. However, aligning system behavior with user preferences
is challenging due to the difficulty of encoding expert intuition into reward
functions. To address the issue, we propose a Reinforcement Learning with Human
Feedback (RLHF) approach for MARL that resolves credit-assignment challenges
through an Agent-Level Feedback system categorizing feedback into intra-agent,
inter-agent, and intra-team types. To overcome the challenges of direct human
feedback, we employ a Large Language Model (LLM) evaluator to validate our
approach using feedback scenarios such as region constraints, collision
avoidance, and task allocation. Our method effectively refines USV swarm
policies, addressing key challenges in multi-agent systems while maintaining
fairness and performance consistency.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>